{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d772db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ppscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad26797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:36:09.169481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-04 01:36:09.169522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-04 01:36:09.170685: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-04 01:36:09.177675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 01:36:09.919912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-10-04 01:36:21.985915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46689 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'dir': {0: 0.8427345187001839, 1: 1.229427549194991}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 308\u001b[0m\n\u001b[1;32m    301\u001b[0m (X_on_train, X_ex_train, y_price_train, y_dir_train,\n\u001b[1;32m    302\u001b[0m  X_on_test, X_ex_test, y_price_test, y_dir_test, \n\u001b[1;32m    303\u001b[0m  scaler_price, class_weight_dict, df_original, test_indices) \u001b[38;5;241m=\u001b[39m make_sequences(\n\u001b[1;32m    304\u001b[0m     df_features, selected_onchain, selected_external)\n\u001b[1;32m    306\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model_paper(\u001b[38;5;28mlen\u001b[39m(selected_onchain), \u001b[38;5;28mlen\u001b[39m(selected_external))\n\u001b[0;32m--> 308\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_on_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ex_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_price_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dir_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_on_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ex_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_price_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dir_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight_dict\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m rmse, mape, acc, pr_auc, pred_probs \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m    314\u001b[0m     model, X_on_test, X_ex_test, y_price_test, y_dir_test, scaler_price\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    317\u001b[0m total_return, trades, win_rate \u001b[38;5;241m=\u001b[39m naive_backtest(pred_probs, df_original, test_indices)\n",
      "Cell \u001b[0;32mIn[1], line 231\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_on_train, X_ex_train, y_price_train, y_dir_train, X_on_test, X_ex_test, y_price_test, y_dir_test, class_weight_dict)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model, X_on_train, X_ex_train, y_price_train, y_dir_train, \n\u001b[1;32m    228\u001b[0m                 X_on_test, X_ex_test, y_price_test, y_dir_test, class_weight_dict):\n\u001b[1;32m    229\u001b[0m     early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 231\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_on_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ex_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_price_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dir_train\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_on_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ex_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_price_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dir_test\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1712\u001b[0m, in \u001b[0;36m_make_class_weight_map_fn\u001b[0;34m(class_weight)\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_ids \u001b[38;5;241m!=\u001b[39m expected_class_ids:\n\u001b[1;32m   1708\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `class_weight` to be a dict with keys from 0 to one less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1710\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan the number of classes, found \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1711\u001b[0m     )\u001b[38;5;241m.\u001b[39mformat(class_weight)\n\u001b[0;32m-> 1712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1714\u001b[0m class_weight_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m   1715\u001b[0m     [class_weight[\u001b[38;5;28mint\u001b[39m(c)] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m class_ids]\n\u001b[1;32m   1716\u001b[0m )\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_class_weights_map_fn\u001b[39m(\u001b[38;5;241m*\u001b[39mdata):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'dir': {0: 0.8427345187001839, 1: 1.229427549194991}}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "from pytrends.request import TrendReq\n",
    "import ppscore as pps\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, auc, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_onchain_data():\n",
    "    df = pd.read_csv('eth_onchain.csv', index_col=0, parse_dates=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "def load_external_data(start_date, end_date):\n",
    "    eth = yf.download('ETH-USD', start=start_date, end=end_date, progress=False)\n",
    "    if isinstance(eth.columns, pd.MultiIndex):\n",
    "        eth.columns = eth.columns.droplevel(1)\n",
    "    eth_data = pd.DataFrame({\n",
    "        'eth_close': eth['Close'],\n",
    "        'eth_vol': eth['Volume']\n",
    "    })\n",
    "    \n",
    "    spx = yf.download('^GSPC', start=start_date, end=end_date, progress=False)\n",
    "    if isinstance(spx.columns, pd.MultiIndex):\n",
    "        spx.columns = spx.columns.droplevel(1)\n",
    "    spx_data = pd.DataFrame({'spx_close': spx['Close']})\n",
    "    \n",
    "    gold = yf.download('GC=F', start=start_date, end=end_date, progress=False)\n",
    "    if isinstance(gold.columns, pd.MultiIndex):\n",
    "        gold.columns = gold.columns.droplevel(1)\n",
    "    gold_data = pd.DataFrame({'gold_close': gold['Close']})\n",
    "    \n",
    "    brent = yf.download('BZ=F', start=start_date, end=end_date, progress=False)\n",
    "    if isinstance(brent.columns, pd.MultiIndex):\n",
    "        brent.columns = brent.columns.droplevel(1)\n",
    "    brent_data = pd.DataFrame({'brent_close': brent['Close']})\n",
    "    \n",
    "    external = pd.concat([eth_data, spx_data, gold_data, brent_data], axis=1)\n",
    "    \n",
    "    pytrends = TrendReq(hl='en-US', tz=0)\n",
    "    keywords = ['ethereum', 'cryptocurrency', 'bitcoin']\n",
    "    \n",
    "    try:\n",
    "        timeframe_str = f'{start_date} {end_date}'\n",
    "        pytrends.build_payload(keywords, timeframe=timeframe_str)\n",
    "        trends = pytrends.interest_over_time()\n",
    "        if not trends.empty:\n",
    "            trends = trends.drop('isPartial', axis=1, errors='ignore')\n",
    "            trends.columns = ['g_ethereum', 'g_crypto', 'g_bitcoin']\n",
    "            external = external.join(trends)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return external\n",
    "\n",
    "def merge_data(onchain_df, external_df):\n",
    "    merged = onchain_df.join(external_df, how='inner')\n",
    "    merged = merged.fillna(method='ffill').fillna(method='bfill')\n",
    "    merged = merged.dropna()\n",
    "    return merged\n",
    "\n",
    "def feature_engineering(df, split_idx):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['logret'] = np.log(df['eth_close']) - np.log(df['eth_close'].shift(1))\n",
    "    df['ma7'] = df['eth_close'].rolling(window=7, min_periods=1).mean()\n",
    "    df['ma21'] = df['eth_close'].rolling(window=21, min_periods=1).mean()\n",
    "    df['vol7'] = df['logret'].rolling(window=7, min_periods=1).std()\n",
    "    df['ma_cross'] = df['ma7'] - df['ma21']\n",
    "    df['rsi'] = calculate_rsi(df['eth_close'], 14)\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    \n",
    "    base_features = [col for col in df.columns if col not in ['eth_close', 'dayofweek', 'month']]\n",
    "    \n",
    "    for feature in base_features:\n",
    "        for lag in range(1, 8):\n",
    "            df[f'{feature}_lag{lag}'] = df[feature].shift(lag)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def adaptive_feature_selection(df, price_col='eth_close', pps_thresh=0.10, pearson_thresh=0.005):\n",
    "    feature_cols = [col for col in df.columns if col not in [price_col]]\n",
    "    \n",
    "    pps_scores = {}\n",
    "    for col in feature_cols:\n",
    "        try:\n",
    "            score = pps.score(df, col, price_col)['ppscore']\n",
    "            pps_scores[col] = score\n",
    "        except:\n",
    "            pps_scores[col] = 0\n",
    "    \n",
    "    selected_phase1 = [col for col, score in pps_scores.items() if score >= pps_thresh]\n",
    "    \n",
    "    if len(selected_phase1) == 0:\n",
    "        pps_thresh = 0.05\n",
    "        selected_phase1 = [col for col, score in pps_scores.items() if score >= pps_thresh]\n",
    "    \n",
    "    if len(selected_phase1) > 0:\n",
    "        corr = df[selected_phase1].corrwith(df[price_col])\n",
    "        selected_phase2 = [col for col in selected_phase1 if abs(corr[col]) >= pearson_thresh]\n",
    "    else:\n",
    "        selected_phase2 = []\n",
    "    \n",
    "    onchain_keywords = ['tx_count', 'active_addresses', 'new_addresses', 'large_eth_transfers', \n",
    "                        'token_transfers', 'contract_events', 'avg_gas_price', 'total_gas_used', \n",
    "                        'avg_block_size', 'avg_block_difficulty']\n",
    "    \n",
    "    selected_onchain = [col for col in selected_phase2 if any(kw in col for kw in onchain_keywords)]\n",
    "    selected_external = [col for col in selected_phase2 if col not in selected_onchain]\n",
    "    \n",
    "    if len(selected_onchain) == 0:\n",
    "        selected_onchain = [col for col in feature_cols if any(kw in col for kw in onchain_keywords)][:10]\n",
    "    if len(selected_external) == 0:\n",
    "        selected_external = [col for col in feature_cols if col not in selected_onchain][:10]\n",
    "    \n",
    "    return selected_onchain, selected_external\n",
    "\n",
    "def make_sequences(df, selected_onchain, selected_external, price_col='eth_close', timesteps=7):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['target_price'] = df[price_col].shift(-1)\n",
    "    df['target_dir'] = (df[price_col].shift(-1) > df[price_col]).astype(int)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df = df.iloc[split_idx:]\n",
    "    \n",
    "    scaler_on = MinMaxScaler()\n",
    "    scaler_ex = MinMaxScaler()\n",
    "    scaler_price = MinMaxScaler()\n",
    "    \n",
    "    scaler_on.fit(train_df[selected_onchain])\n",
    "    scaler_ex.fit(train_df[selected_external])\n",
    "    scaler_price.fit(train_df[['target_price']])\n",
    "    \n",
    "    df_on_scaled = scaler_on.transform(df[selected_onchain])\n",
    "    df_ex_scaled = scaler_ex.transform(df[selected_external])\n",
    "    target_price_scaled = scaler_price.transform(df[['target_price']])\n",
    "    \n",
    "    X_on, X_ex, y_price, y_dir, indices = [], [], [], [], []\n",
    "    \n",
    "    for i in range(timesteps, len(df)):\n",
    "        X_on.append(df_on_scaled[i-timesteps:i])\n",
    "        X_ex.append(df_ex_scaled[i-timesteps:i])\n",
    "        y_price.append(target_price_scaled[i, 0])\n",
    "        y_dir.append(df['target_dir'].iloc[i])\n",
    "        indices.append(i)\n",
    "    \n",
    "    X_on = np.array(X_on)\n",
    "    X_ex = np.array(X_ex)\n",
    "    y_price = np.array(y_price)\n",
    "    y_dir = np.array(y_dir)\n",
    "    \n",
    "    train_size = split_idx - timesteps\n",
    "    \n",
    "    X_on_train, X_on_test = X_on[:train_size], X_on[train_size:]\n",
    "    X_ex_train, X_ex_test = X_ex[:train_size], X_ex[train_size:]\n",
    "    y_price_train, y_price_test = y_price[:train_size], y_price[train_size:]\n",
    "    y_dir_train, y_dir_test = y_dir[:train_size], y_dir[train_size:]\n",
    "    test_indices = indices[train_size:]\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_dir_train), y=y_dir_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    return (X_on_train, X_ex_train, y_price_train, y_dir_train,\n",
    "            X_on_test, X_ex_test, y_price_test, y_dir_test, \n",
    "            scaler_price, class_weight_dict, df, test_indices)\n",
    "\n",
    "def build_model_paper(n_on, n_ex, timesteps=7):\n",
    "    input_on = Input(shape=(timesteps, n_on), name='onchain_input')\n",
    "    x_on = GRU(256, return_sequences=True)(input_on)\n",
    "    x_on = GRU(256, return_sequences=True)(x_on)\n",
    "    x_on = GRU(256)(x_on)\n",
    "    x_on = Dense(256, activation='relu')(x_on)\n",
    "    x_on = Dense(64, activation='relu')(x_on)\n",
    "    \n",
    "    input_ex = Input(shape=(timesteps, n_ex), name='external_input')\n",
    "    x_ex = LSTM(256, return_sequences=True)(input_ex)\n",
    "    x_ex = LSTM(256, return_sequences=True)(x_ex)\n",
    "    x_ex = LSTM(256)(x_ex)\n",
    "    x_ex = Dense(256, activation='relu')(x_ex)\n",
    "    x_ex = Dense(64, activation='relu')(x_ex)\n",
    "    \n",
    "    concat = Concatenate()([x_on, x_ex])\n",
    "    x = Dense(128, activation='relu')(concat)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    output_price = Dense(1, activation='linear', name='price')(x)\n",
    "    output_dir = Dense(1, activation='sigmoid', name='dir')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_on, input_ex], outputs=[output_price, output_dir])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss={'price': 'mse', 'dir': 'binary_crossentropy'},\n",
    "        loss_weights={'price': 1.0, 'dir': 2.0},\n",
    "        metrics={'price': 'mae', 'dir': 'accuracy'}\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_on_train, X_ex_train, y_price_train, y_dir_train, \n",
    "                X_on_test, X_ex_test, y_price_test, y_dir_test, class_weight_dict):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_on_train, X_ex_train],\n",
    "        {'price': y_price_train, 'dir': y_dir_train},\n",
    "        validation_data=([X_on_test, X_ex_test], {'price': y_price_test, 'dir': y_dir_test}),\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stop],\n",
    "        class_weight={'dir': class_weight_dict},\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, X_on_test, X_ex_test, y_price_test, y_dir_test, scaler_price):\n",
    "    pred_price_scaled, pred_dir = model.predict([X_on_test, X_ex_test], verbose=0)\n",
    "    \n",
    "    pred_price = scaler_price.inverse_transform(pred_price_scaled.reshape(-1, 1)).flatten()\n",
    "    true_price = scaler_price.inverse_transform(y_price_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((pred_price - true_price)**2))\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    mape = np.mean(np.abs((true_price - pred_price) / (true_price + epsilon))) * 100\n",
    "    \n",
    "    pred_dir_binary = (pred_dir.flatten() > 0.5).astype(int)\n",
    "    acc = accuracy_score(y_dir_test, pred_dir_binary)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_dir_test, pred_dir.flatten())\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    return rmse, mape, acc, pr_auc, pred_dir.flatten()\n",
    "\n",
    "def naive_backtest(pred_probs, df_original, test_indices, fee=0.001):\n",
    "    capital = 1.0\n",
    "    trades = 0\n",
    "    wins = 0\n",
    "    \n",
    "    for idx, prob in enumerate(pred_probs[:-1]):\n",
    "        current_idx = test_indices[idx]\n",
    "        next_idx = test_indices[idx + 1] if idx + 1 < len(test_indices) else current_idx + 1\n",
    "        \n",
    "        if next_idx >= len(df_original):\n",
    "            break\n",
    "            \n",
    "        if prob > 0.5:\n",
    "            current_price = df_original['eth_close'].iloc[current_idx]\n",
    "            next_price = df_original['eth_close'].iloc[next_idx]\n",
    "            ret = (next_price - current_price) / current_price - fee\n",
    "            capital *= (1 + ret)\n",
    "            trades += 1\n",
    "            if ret > 0:\n",
    "                wins += 1\n",
    "    \n",
    "    total_return = (capital - 1.0) * 100\n",
    "    win_rate = (wins / trades * 100) if trades > 0 else 0\n",
    "    \n",
    "    return total_return, trades, win_rate\n",
    "\n",
    "onchain_df = load_onchain_data()\n",
    "start_date = onchain_df.index.min().strftime('%Y-%m-%d')\n",
    "end_date = onchain_df.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "external_df = load_external_data(start_date, end_date)\n",
    "merged_df = merge_data(onchain_df, external_df)\n",
    "\n",
    "split_idx = int(len(merged_df) * 0.8)\n",
    "df_features = feature_engineering(merged_df, split_idx)\n",
    "\n",
    "selected_onchain, selected_external = adaptive_feature_selection(df_features)\n",
    "\n",
    "(X_on_train, X_ex_train, y_price_train, y_dir_train,\n",
    " X_on_test, X_ex_test, y_price_test, y_dir_test, \n",
    " scaler_price, class_weight_dict, df_original, test_indices) = make_sequences(\n",
    "    df_features, selected_onchain, selected_external)\n",
    "\n",
    "model = build_model_paper(len(selected_onchain), len(selected_external))\n",
    "\n",
    "model, history = train_model(\n",
    "    model, X_on_train, X_ex_train, y_price_train, y_dir_train,\n",
    "    X_on_test, X_ex_test, y_price_test, y_dir_test, class_weight_dict\n",
    ")\n",
    "\n",
    "rmse, mape, acc, pr_auc, pred_probs = evaluate_model(\n",
    "    model, X_on_test, X_ex_test, y_price_test, y_dir_test, scaler_price\n",
    ")\n",
    "\n",
    "total_return, trades, win_rate = naive_backtest(pred_probs, df_original, test_indices)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{'MODEL EVALUATION RESULTS':^60}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nData Period: {start_date} to {end_date}\")\n",
    "print(f\"Total samples: {len(df_features)}, Train: {len(X_on_train)}, Test: {len(X_on_test)}\")\n",
    "print(f\"\\nFeature Selection:\")\n",
    "print(f\"  On-chain features: {len(selected_onchain)}\")\n",
    "print(f\"  External features: {len(selected_external)}\")\n",
    "print(f\"\\nRegression Metrics:\")\n",
    "print(f\"  RMSE:  {rmse:>10.2f}\")\n",
    "print(f\"  MAPE:  {mape:>10.2f}%\")\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"  Accuracy:  {acc:>10.4f}\")\n",
    "print(f\"  PR-AUC:    {pr_auc:>10.4f}\")\n",
    "print(f\"\\nBacktest Results:\")\n",
    "print(f\"  Total Return:  {total_return:>10.2f}%\")\n",
    "print(f\"  Trades:        {trades:>10}\")\n",
    "print(f\"  Win Rate:      {win_rate:>10.2f}%\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a25a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdf0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9c56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b85250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pytrends.request import TrendReq\n",
    "import ppscore as pps\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_onchain_data():\n",
    "    \"\"\"eth_onchain.csv 파일에서 온체인 데이터를 로드합니다.\"\"\"\n",
    "    print(\"온체인 데이터 로드 중...\")\n",
    "    df = pd.read_csv('eth_onchain.csv', index_col=0, parse_dates=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    print(\"온체인 데이터 로드 완료.\")\n",
    "    return df\n",
    "\n",
    "def fetch_external_data(start_date, end_date):\n",
    "    \"\"\"논문에 언급된 외부 데이터를 API를 통해 직접 가져옵니다.\"\"\"\n",
    "    print(\"외부 데이터 수집 시작...\")\n",
    "    \n",
    "    # 1. 금융 데이터 (YFinance)\n",
    "    tickers = {\n",
    "        'ETH-USD': 'eth', \n",
    "        '^GSPC': 'spx',    # S&P 500\n",
    "        'GC=F': 'gold',    # 금 선물\n",
    "        'BZ=F': 'brent'    # 브렌트유 선물\n",
    "    }\n",
    "    external_data = []\n",
    "    for ticker, name in tickers.items():\n",
    "        print(f\"  - {ticker} 데이터 다운로드 중...\")\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        if not data.empty:\n",
    "            # high, low 컬럼 추가\n",
    "            temp_df = data[['Close', 'Volume', 'High', 'Low']].copy()\n",
    "            temp_df.columns = [f'{name}_close', f'{name}_vol', f'{name}_high', f'{name}_low']\n",
    "            external_data.append(temp_df)\n",
    "        time.sleep(1) # API 과호출 방지\n",
    "\n",
    "    df_financial = pd.concat(external_data, axis=1)\n",
    "\n",
    "    # 2. 구글 트렌드 데이터 (Pytrends)\n",
    "    try:\n",
    "        print(\"  - Google Trends 데이터 다운로드 중...\")\n",
    "        pytrends = TrendReq(hl='en-US', tz=0)\n",
    "        keywords = ['ethereum', 'cryptocurrency']\n",
    "        timeframe = f'{start_date} {end_date}'\n",
    "        pytrends.build_payload(keywords, cat=0, timeframe=timeframe, geo='', gprop='')\n",
    "        df_trends = pytrends.interest_over_time()\n",
    "        if not df_trends.empty:\n",
    "            df_trends = df_trends.drop(columns=['isPartial'], errors='ignore')\n",
    "            df_trends.columns = [f'gtrend_{kw}' for kw in keywords]\n",
    "            df_financial = df_financial.join(df_trends, how='outer')\n",
    "        time.sleep(5) # API 과호출 방지 시간 증가\n",
    "    except Exception as e:\n",
    "        print(f\"Google Trends 데이터 수집 실패: {e}\")\n",
    "\n",
    "    # 데이터 병합 후 전처리\n",
    "    df_financial = df_financial.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"외부 데이터 수집 완료.\")\n",
    "    return df_financial\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"기술적 지표 및 시차(lag) 피처를 생성합니다. (강화된 버전)\"\"\"\n",
    "    print(\"피처 엔지니어링 시작...\")\n",
    "    df_copy = df.copy()\n",
    "    close = df_copy['eth_close']\n",
    "    high = df_copy.get('eth_high', close)\n",
    "    low = df_copy.get('eth_low', close)\n",
    "    \n",
    "    # 1. 기본 지표\n",
    "    df_copy['ma7'] = close.rolling(window=7).mean()\n",
    "    df_copy['ma21'] = close.rolling(window=21).mean()\n",
    "    df_copy['logret'] = np.log(close).diff()\n",
    "    df_copy['vol7'] = df_copy['logret'].rolling(window=7).std()\n",
    "\n",
    "    # 2. MACD\n",
    "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "    df_copy['macd'] = ema12 - ema26\n",
    "    df_copy['macd_signal'] = df_copy['macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # 3. Bollinger Bands\n",
    "    ma20 = close.rolling(window=20).mean()\n",
    "    std20 = close.rolling(window=20).std()\n",
    "    df_copy['bollinger_upper'] = ma20 + (std20 * 2)\n",
    "    df_copy['bollinger_lower'] = ma20 - (std20 * 2)\n",
    "\n",
    "    # 4. RSI\n",
    "    delta = close.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df_copy['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # 5. 시차 피처\n",
    "    for col in list(df.columns) + ['macd', 'rsi']:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[f'{col}_lag1'] = df_copy[col].shift(1)\n",
    "            \n",
    "    print(\"피처 엔지니어링 완료.\")\n",
    "    return df_copy\n",
    "\n",
    "def adaptive_feature_selection(df, target_col='eth_close', pps_threshold=0.05, corr_threshold=0.05):\n",
    "    \"\"\"논문의 2단계 적응형 피처 선택을 구현합니다.\"\"\"\n",
    "    print(\"적응형 피처 선택 시작...\")\n",
    "    feature_candidates = [col for col in df.columns if col not in ['target_price', 'target_dir', target_col]]\n",
    "    \n",
    "    print(f\"  - 1단계: PPS 계산 중 (대상 피처 {len(feature_candidates)}개)...\")\n",
    "    pps_scores = pps.matrix(df, output='df')[['x', 'y', 'ppscore']]\n",
    "    pps_target_scores = pps_scores[pps_scores['y'] == target_col]\n",
    "    \n",
    "    selected_by_pps = pps_target_scores[pps_target_scores['ppscore'] > pps_threshold]['x'].tolist()\n",
    "    print(f\"  - 1단계: {len(selected_by_pps)}개 피처 선택됨 (PPS > {pps_threshold}).\")\n",
    "\n",
    "    if not selected_by_pps:\n",
    "        print(\"경고: PPS를 통과한 피처가 없습니다. 상관계수만으로 진행합니다.\")\n",
    "        selected_by_pps = feature_candidates\n",
    "\n",
    "    print(\"  - 2단계: 상관계수 계산 중...\")\n",
    "    correlations = df[selected_by_pps].corrwith(df[target_col]).abs()\n",
    "    selected_features = correlations[correlations > corr_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"적응형 피처 선택 완료. 최종 {len(selected_features)}개 피처 선택됨.\")\n",
    "    return selected_features\n",
    "\n",
    "def preprocess_and_create_sequences(df, feature_cols, target_price_col, target_dir_col, timesteps=7):\n",
    "    \"\"\"데이터 스케일링 및 시퀀스 생성을 수행합니다.\"\"\"\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df, test_df = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[feature_cols])\n",
    "    test_scaled = scaler.transform(test_df[feature_cols])\n",
    "    \n",
    "    scaler_price = MinMaxScaler()\n",
    "    scaler_price.fit(train_df[[target_price_col]])\n",
    "\n",
    "    def create_sequences_from_data(data, targets_price, targets_dir):\n",
    "        X, y_price, y_dir = [], [], []\n",
    "        for i in range(timesteps, len(data)):\n",
    "            X.append(data[i-timesteps:i])\n",
    "            y_price.append(targets_price[i])\n",
    "            y_dir.append(targets_dir[i])\n",
    "        return np.array(X), np.array(y_price), np.array(y_dir)\n",
    "\n",
    "    X_train, y_price_train, y_dir_train = create_sequences_from_data(train_scaled, train_df[target_price_col].values, train_df[target_dir_col].values)\n",
    "    X_test, y_price_test, y_dir_test = create_sequences_from_data(test_scaled, test_df[target_price_col].values, test_df[target_dir_col].values)\n",
    "    \n",
    "    return X_train, X_test, y_price_train, y_price_test, y_dir_train, y_dir_test, scaler, scaler_price\n",
    "\n",
    "def build_adaptive_model(n_features, timesteps, learning_rate, loss_weights):\n",
    "    \"\"\"하이퍼파라미터를 입력받아 모델을 구축하고 컴파일합니다.\"\"\"\n",
    "    n_features_gru = n_features // 2\n",
    "    n_features_lstm = n_features - n_features_gru\n",
    "    \n",
    "    input_gru = Input(shape=(timesteps, n_features_gru), name='gru_input')\n",
    "    input_lstm = Input(shape=(timesteps, n_features_lstm), name='lstm_input')\n",
    "    \n",
    "    gru_stack = GRU(256, return_sequences=True)(input_gru)\n",
    "    gru_stack = GRU(256)(gru_stack)\n",
    "    \n",
    "    lstm_stack = LSTM(256, return_sequences=True)(input_lstm)\n",
    "    lstm_stack = LSTM(256)(lstm_stack)\n",
    "\n",
    "    price_mlp = Dense(128, activation='relu')(gru_stack)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(price_mlp)\n",
    "\n",
    "    concatenated = Concatenate()([gru_stack, lstm_stack])\n",
    "    \n",
    "    dir_mlp = Dense(128, activation='relu')(concatenated)\n",
    "    dir_mlp = Dropout(0.1)(dir_mlp)\n",
    "    dir_output = Dense(1, activation='sigmoid', name='dir_output')(dir_mlp)\n",
    "\n",
    "    model = Model(inputs=[input_gru, input_lstm], outputs=[price_output, dir_output])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss={'price_output': 'mse', 'dir_output': 'binary_crossentropy'},\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics={'dir_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# --- 1. 데이터 준비 (한 번만 실행) ---\n",
    "onchain_df = load_onchain_data()\n",
    "start_date, end_date = onchain_df.index.min(), onchain_df.index.max()\n",
    "external_df = fetch_external_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "df_merged = onchain_df.join(external_df, how='inner')\n",
    "\n",
    "df_merged['target_price'] = df_merged['eth_close'].shift(-1)\n",
    "df_merged['target_dir'] = (df_merged['target_price'] > df_merged['eth_close']).astype(int)\n",
    "\n",
    "df_features = feature_engineering(df_merged)\n",
    "df_final = df_features.dropna()\n",
    "\n",
    "selected_features = adaptive_feature_selection(df_final)\n",
    "\n",
    "TIMESTEPS = 7\n",
    "X_train, X_test, y_price_train, y_price_test, y_dir_train, y_dir_test, scaler, scaler_price = \\\n",
    "    preprocess_and_create_sequences(df_final, selected_features, 'target_price', 'target_dir', TIMESTEPS)\n",
    "\n",
    "n_f = len(selected_features)\n",
    "n_f_gru = n_f // 2\n",
    "X_train_gru, X_train_lstm = X_train[:, :, :n_f_gru], X_train[:, :, n_f_gru:]\n",
    "X_test_gru, X_test_lstm = X_test[:, :, :n_f_gru], X_test[:, :, n_f_gru:]\n",
    "\n",
    "\n",
    "# --- 2. 하이퍼파라미터 탐색 ---\n",
    "hyperparameter_space = [\n",
    "    {'lr': 1e-4, 'weights': {'price_output': 1.0, 'dir_output': 1.0}},\n",
    "    {'lr': 1e-4, 'weights': {'price_output': 1.0, 'dir_output': 100000.0}},\n",
    "    {'lr': 1e-5, 'weights': {'price_output': 1.0, 'dir_output': 1000000.0}},\n",
    "    {'lr': 1e-5, 'weights': {'price_output': 1.0, 'dir_output': 500000.0}},\n",
    "]\n",
    "\n",
    "results = []\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for i, params in enumerate(hyperparameter_space):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" 하이퍼파라미터 탐색 {i+1}/{len(hyperparameter_space)} \".center(60, \"=\"))\n",
    "    print(f\"Learning Rate: {params['lr']}, Loss Weights: {params['weights']}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model = build_adaptive_model(\n",
    "        n_features=len(selected_features),\n",
    "        timesteps=TIMESTEPS,\n",
    "        learning_rate=params['lr'],\n",
    "        loss_weights=params['weights']\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_dir_output_accuracy', mode='max', patience=15, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        [X_train_gru, X_train_lstm],\n",
    "        {'price_output': y_price_train, 'dir_output': y_dir_train},\n",
    "        validation_data=([X_test_gru, X_test_lstm], {'price_output': y_price_test, 'dir_output': y_dir_test}),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    pred_price, pred_dir_prob = model.predict([X_test_gru, X_test_lstm])\n",
    "    pred_dir = (pred_dir_prob > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_dir_test, pred_dir)\n",
    "    mape = mean_absolute_percentage_error(y_price_test.reshape(-1, 1), scaler_price.transform(pred_price))\n",
    "    \n",
    "    print(f\"\\n결과: Accuracy = {accuracy*100:.2f}%, MAPE = {mape:.4f}%\")\n",
    "    \n",
    "    results.append({'params': params, 'accuracy': accuracy, 'mape': mape})\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "# --- 3. 최종 결과 요약 ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 하이퍼파라미터 탐색 최종 결과 요약 \".center(60, \"=\"))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 결과를 데이터프레임으로 보기 좋게 출력\n",
    "results_df = pd.DataFrame({\n",
    "    'Learning Rate': [r['params']['lr'] for r in results],\n",
    "    'Direction Loss Weight': [r['params']['weights']['dir_output'] for r in results],\n",
    "    'Accuracy (%)': [r['accuracy'] * 100 for r in results],\n",
    "    'MAPE': [r['mape'] for r in results]\n",
    "})\n",
    "print(results_df.round(4))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"가장 높은 정확도: {best_accuracy * 100:.2f}%\")\n",
    "print(f\"최적 하이퍼파라미터: {best_params}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
