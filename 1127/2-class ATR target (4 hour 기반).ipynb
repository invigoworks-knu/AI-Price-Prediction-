{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203e373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 20:42:41.491725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-27 20:42:41.491770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-27 20:42:41.493224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-27 20:42:41.500590: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-27 20:42:42.343808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from scipy.signal import argrelextrema\n",
    "from numba import jit\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import joblib\n",
    "import json\n",
    "import gc\n",
    "import traceback\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import recall_score,accuracy_score,precision_score\n",
    "\n",
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    if indicator is None: return\n",
    "    if isinstance(indicator, pd.DataFrame):\n",
    "        for col in indicator.columns: df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series):\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def add_kimchi_premium_proxy(df):\n",
    "    df_kp = df.copy()\n",
    "    target_coins = ['ETH', 'BTC', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    \n",
    "    for coin in target_coins:\n",
    "        krw_col = f'{coin}_Close'\n",
    "        bin_col = f'{coin}_Bin_Close'\n",
    "        \n",
    "        if krw_col in df_kp.columns and bin_col in df_kp.columns:\n",
    "            ratio_col = f'{coin}_KP_Ratio'\n",
    "            df_kp[ratio_col] = df_kp[krw_col] / (df_kp[bin_col] + 1e-9)\n",
    "            \n",
    "            ma = df_kp[ratio_col].rolling(180).mean()\n",
    "            std = df_kp[ratio_col].rolling(180).std()\n",
    "            df_kp[f'{coin}_KP_Zscore'] = (df_kp[ratio_col] - ma) / (std + 1e-9)\n",
    "            \n",
    "            df_kp[f'{coin}_KP_Change'] = df_kp[ratio_col].pct_change()\n",
    "\n",
    "    return df_kp\n",
    "\n",
    "def add_funding_and_onchain_features(df):\n",
    "    df_on = df.copy()\n",
    "    \n",
    "    if 'fundingRate' in df_on.columns:\n",
    "        df_on['FR_Abs_Signal'] = df_on['fundingRate'].abs()\n",
    "        df_on['FR_Trend'] = df_on['fundingRate'].rolling(6).mean()\n",
    "        df_on['FR_Change'] = df_on['fundingRate'].diff()\n",
    "\n",
    "    tvl_cols = [col for col in df_on.columns if 'tvl' in col.lower()]\n",
    "    for col in tvl_cols:\n",
    "        df_on[f'{col}_1d_chg'] = df_on[col].pct_change(6)\n",
    "        \n",
    "    return df_on\n",
    "\n",
    "def add_cross_crypto_correlations(df):\n",
    "    df_corr = df.copy()\n",
    "    other_coins = ['BTC', 'XRP', 'SOL', 'ADA', 'AVAX', 'DOT']\n",
    "    eth_ret = df['ETH_Close'].pct_change()\n",
    "    \n",
    "    for coin in other_coins:\n",
    "        col_name = f'{coin}_Close'\n",
    "        if col_name in df.columns:\n",
    "            coin_ret = df[col_name].pct_change()\n",
    "            df_corr[f'Corr_ETH_{coin}_24h'] = eth_ret.rolling(6).corr(coin_ret)\n",
    "            df_corr[f'RelStr_ETH_{coin}'] = eth_ret - coin_ret\n",
    "            \n",
    "    return df_corr\n",
    "\n",
    "def add_price_lag_features_first_4h(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    for lag_days in [1, 2, 3, 5, 10]:\n",
    "        lag_periods = lag_days * 6\n",
    "        df_new[f'return_lag_{lag_periods}p'] = close.pct_change(periods=lag_periods).shift(1)\n",
    "    return df_new\n",
    "\n",
    "def calculate_technical_indicators_4h(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    df_ta['ATR_84'] = ta.atr(high, low, close, length=84)\n",
    "    bb = ta.bbands(close, length=120, std=2)\n",
    "    if bb is not None:\n",
    "        df_ta['BB_WIDTH'] = bb.iloc[:, 2]\n",
    "\n",
    "    df_ta['SMA_120'] = ta.sma(close, length=120)\n",
    "    df_ta['SMA_300'] = ta.sma(close, length=300)\n",
    "    df_ta['EMA_72'] = ta.ema(close, length=72)\n",
    "    \n",
    "    df_ta['TREND_SCORE'] = (close > df_ta['SMA_120']).astype(int) + (df_ta['SMA_120'] > df_ta['SMA_300']).astype(int)\n",
    "\n",
    "    df_ta['RSI_84'] = ta.rsi(close, length=84)\n",
    "    safe_add(df_ta, ta.macd, close, fast=72, slow=156, signal=54)\n",
    "\n",
    "    df_ta['OBV'] = ta.obv(close, volume)\n",
    "    df_ta['MFI_84'] = ta.mfi(high, low, close, volume, length=84)\n",
    "    df_ta['VOLUME_RATIO'] = volume / (volume.rolling(120).mean() + 1e-8)\n",
    "\n",
    "    df_ta['UPPER_SHADOW'] = (high - np.maximum(close, open_)) / (high - low + 1e-9)\n",
    "    df_ta['LOWER_SHADOW'] = (np.minimum(close, open_) - low) / (high - low + 1e-9)\n",
    "\n",
    "    for window in [30, 120, 360]:\n",
    "        swing_high = high.rolling(window).max().shift(1)\n",
    "        swing_low = low.rolling(window).min().shift(1)\n",
    "        \n",
    "        df_ta[f'PRICE_VS_HIGH_{window}p'] = close / (swing_high + 1e-9)\n",
    "        df_ta[f'PRICE_VS_LOW_{window}p'] = close / (swing_low + 1e-9)\n",
    "        df_ta[f'BREAKOUT_STR_{window}p'] = (close - swing_high) / (df_ta['ATR_84'] + 1e-9)\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "def add_volatility_regime_features_4h(df):\n",
    "    df_regime = df.copy()\n",
    "    if 'ATR_84' in df.columns:\n",
    "        atr_ma = df['ATR_84'].rolling(120).mean()\n",
    "        df_regime['high_volatility_regime'] = (df['ATR_84'] > atr_ma).astype(int)\n",
    "    return df_regime\n",
    "\n",
    "def calculate_cumulative_volume_delta_4h(df):\n",
    "    df_cvd = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    \n",
    "    df_cvd['volume_delta'] = np.where(close > open_, volume, -volume)\n",
    "    df_cvd['CVD'] = df_cvd['volume_delta'].cumsum()\n",
    "    \n",
    "    df_cvd['CVD_24h'] = df_cvd['volume_delta'].rolling(6).sum()\n",
    "    df_cvd['CVD_7d'] = df_cvd['volume_delta'].rolling(42).sum()\n",
    "    \n",
    "    df_cvd['CVD_Rank_180'] = df_cvd['CVD_24h'].rolling(180).rank(pct=True)\n",
    "    \n",
    "    cvd_slope = df_cvd['CVD'].diff(6)\n",
    "    price_volatility = close.rolling(24).std() + 1e-9\n",
    "    df_cvd['CVD_Slope_Norm'] = cvd_slope / price_volatility \n",
    "    \n",
    "    return df_cvd\n",
    "\n",
    "def add_vwma_features_4h(df):\n",
    "    df_vwma = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    \n",
    "    for period in [20, 50]:\n",
    "        period_4h = period * 6\n",
    "        vwma = (close * volume).rolling(period_4h).sum() / (volume.rolling(period_4h).sum() + 1e-9)\n",
    "        df_vwma[f'Price_div_VWMA_{period}d'] = (close / (vwma + 1e-9)) - 1\n",
    "        \n",
    "    return df_vwma\n",
    "\n",
    "def add_obv_divergence_features_4h(df):\n",
    "    df_obv = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    \n",
    "    if 'OBV' not in df.columns:\n",
    "        df_obv['OBV'] = ta.obv(close, volume)\n",
    "    obv = df_obv['OBV']\n",
    "    \n",
    "    obv_ma_30 = obv.rolling(30).mean()\n",
    "    obv_ma_60 = obv.rolling(60).mean()\n",
    "    df_obv['OBV_Trend'] = np.where(obv_ma_30 > obv_ma_60, 1, -1)\n",
    "    \n",
    "    def is_pivot_high(series, window=2):\n",
    "        center = series.shift(window)\n",
    "        is_max = pd.Series(True, index=series.index)\n",
    "        for i in range(1, window + 1):\n",
    "            is_max &= (center > series.shift(window - i)) & (center > series.shift(window + i))\n",
    "        return is_max\n",
    "\n",
    "    def is_pivot_low(series, window=2):\n",
    "        center = series.shift(window)\n",
    "        is_min = pd.Series(True, index=series.index)\n",
    "        for i in range(1, window + 1):\n",
    "            is_min &= (center < series.shift(window - i)) & (center < series.shift(window + i))\n",
    "        return is_min\n",
    "\n",
    "    price_high = is_pivot_high(close, 2)\n",
    "    obv_high = is_pivot_high(obv, 2)\n",
    "    \n",
    "    price_slope = close.diff(12)\n",
    "    obv_slope = obv.diff(12)\n",
    "    \n",
    "    df_obv['Div_Bullish'] = ((price_slope < 0) & (obv_slope > 0)).astype(int)\n",
    "    df_obv['Div_Bearish'] = ((price_slope > 0) & (obv_slope < 0)).astype(int)\n",
    "    \n",
    "    return df_obv\n",
    "\n",
    "def add_vwap_anchored_features_4h(df):\n",
    "    df_vwap = df.copy()\n",
    "    \n",
    "\n",
    "    if 'date' not in df_vwap.columns and 'timestamp' in df_vwap.columns:\n",
    "        df_vwap['date'] = pd.to_datetime(df_vwap['timestamp'])\n",
    "    else:\n",
    "        df_vwap['date'] = pd.to_datetime(df_vwap['date'])\n",
    "    \n",
    "\n",
    "    close = df_vwap['ETH_Close']\n",
    "    high = df_vwap.get('ETH_High', close)\n",
    "    low = df_vwap.get('ETH_Low', close)\n",
    "    \n",
    "\n",
    "    if 'ETH_Volume' in df_vwap.columns:\n",
    "        volume = df_vwap['ETH_Volume']\n",
    "    elif 'volume' in df_vwap.columns: \n",
    "        volume = df_vwap['volume']\n",
    "    else:\n",
    "        volume = pd.Series(1, index=df_vwap.index)\n",
    "\n",
    "    typical_price = (high + low + close) / 3\n",
    "    \n",
    "    # 3. VWAP 계산 로직\n",
    "    df_vwap['day'] = df_vwap['date'].dt.date\n",
    "    \n",
    "\n",
    "    df_vwap['pv'] = typical_price * volume\n",
    "    df_vwap['cum_pv'] = df_vwap.groupby('day')['pv'].cumsum()\n",
    "    df_vwap['cum_vol'] = volume.groupby(df_vwap['day']).cumsum() \n",
    "    \n",
    "    df_vwap['VWAP_Day'] = df_vwap['cum_pv'] / (df_vwap['cum_vol'] + 1e-9)\n",
    "    df_vwap['Dist_from_VWAP'] = (close / df_vwap['VWAP_Day']) - 1\n",
    "    \n",
    "    # 임시 컬럼 제거\n",
    "    df_vwap = df_vwap.drop(['day', 'pv', 'cum_pv', 'cum_vol'], axis=1, errors='ignore')\n",
    "    \n",
    "    return df_vwap\n",
    "\n",
    "def add_institutional_footprint_features_4h(df):\n",
    "    df_inst = df.copy()\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df_inst.index, data=1))\n",
    "    close = df_inst['ETH_Close']\n",
    "    open_ = df_inst.get('ETH_Open', close)\n",
    "    high = df_inst.get('ETH_High', close)\n",
    "    low = df_inst.get('ETH_Low', close)\n",
    "    \n",
    "    vol_ma = volume.rolling(30).mean()\n",
    "    df_inst['Vol_Spike'] = (volume > (vol_ma * 2.0)).astype(int)\n",
    "    \n",
    "    body_size = (close - open_).abs()\n",
    "    candle_range = high - low\n",
    "    df_inst['Small_Body'] = (body_size < (candle_range * 0.3))\n",
    "    \n",
    "    df_inst['Absorption'] = (df_inst['Vol_Spike'] & df_inst['Small_Body']).astype(int)\n",
    "    \n",
    "    lower_wick = (open_.combine(close, min) - low)\n",
    "    upper_wick = (high - open_.combine(close, max))\n",
    "    \n",
    "    df_inst['Buying_Climax'] = (df_inst['Vol_Spike'] & (upper_wick > body_size * 2)).astype(int)\n",
    "    df_inst['Selling_Climax'] = (df_inst['Vol_Spike'] & (lower_wick > body_size * 2)).astype(int)\n",
    "    \n",
    "    return df_inst\n",
    "\n",
    "\n",
    "def preprocess_non_stationary_features_4h(df):\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    # 1. 접두사 매칭 (기존)\n",
    "    prefixes_to_transform = [\n",
    "        'eth_', 'aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_',\n",
    "        'l2_', 'usdt_', 'total'\n",
    "    ]\n",
    "    \n",
    "    # 2. [핵심 수정] 거시경제 지표 \"정확한 이름\" 추가 (언더바 여부 상관없이)\n",
    "    exact_cols_to_transform = ['SP500', 'GOLD', 'DXY', 'VIX', 'sp500', 'gold', 'dxy', 'vix']\n",
    "\n",
    "    cols_to_transform = []\n",
    "    for col in df_proc.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # (A) 정확한 이름 매칭 (거시경제 지표용)\n",
    "        if col in exact_cols_to_transform:\n",
    "            cols_to_transform.append(col)\n",
    "            continue # 이미 찾았으니 다음 루프로\n",
    "\n",
    "        # (B) 접두사 매칭 (기존 로직)\n",
    "        if col.startswith(tuple(prefixes_to_transform)):\n",
    "            exclude_prefixes = ['fg_', 'funding_', 'open', 'high', 'low', 'close', 'volume']\n",
    "            exclude_keywords = ['_pct_', '_ratio', 'target', 'next_', 'date']\n",
    "            \n",
    "            if not col.startswith(tuple(exclude_prefixes)):\n",
    "                if not any(k in col_lower for k in exclude_keywords):\n",
    "                    cols_to_transform.append(col)\n",
    "    \n",
    "    # 중복 제거\n",
    "    cols_to_transform = list(set(cols_to_transform))\n",
    "    \n",
    "    # 3. 변환 로직 (기존과 동일: 24시간 변화율 & MA 괴리율)\n",
    "    cols_to_drop = []\n",
    "    for col in cols_to_transform:\n",
    "        series = df_proc[col].fillna(method='ffill').replace(0, 1e-9)\n",
    "        \n",
    "        # 24시간 전 대비 변화율 (Frozen 해결사)\n",
    "        df_proc[f'{col}_pct_chg_24h'] = series.pct_change(6)\n",
    "        \n",
    "        # 장기 이동평균 괴리율\n",
    "        ma_180 = series.rolling(window=180, min_periods=60).mean()\n",
    "        df_proc[f'{col}_ma180_ratio'] = (series - ma_180) / (ma_180 + 1e-9)\n",
    "        \n",
    "        cols_to_drop.append(col) # 원본(Frozen된 놈)은 삭제\n",
    "\n",
    "    df_proc = df_proc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    df_proc = df_proc.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df_proc\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df, target_type, method):\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    \n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    keep_keywords = [\n",
    "        '_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv',\n",
    "        'PRICE_VS', 'BREAKOUT', 'UPPER_SHADOW', 'LOWER_SHADOW', 'BB_WIDTH', 'KP', 'FR', 'RelStr',\n",
    "        'CVD', 'VWMA', 'VWAP', 'Div', 'Spike', 'Climax', 'Absorption'\n",
    "    ]\n",
    "    \n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in [k.lower() for k in keep_keywords])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "    return df_transformed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801ac771",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def compute_targets_with_hourly_numba_4h(\n",
    "    base_dates_ts, base_atr, hourly_dates_ts, hourly_open,\n",
    "    hourly_high, hourly_low, lookahead_periods, profit_mult, stop_mult\n",
    "):\n",
    "    n = len(base_dates_ts)\n",
    "    # 초기화: 기본값은 -1 (유효하지 않음) 또는 NaN\n",
    "    targets = np.full(n, -1, dtype=np.int32)\n",
    "    entry_prices = np.full(n, np.nan, dtype=np.float64) # 명시적 float64\n",
    "    tp_prices = np.full(n, np.nan, dtype=np.float64)\n",
    "    sl_prices = np.full(n, np.nan, dtype=np.float64)\n",
    "    \n",
    "    four_hour_ms = 14400000\n",
    "    lookahead_ms = lookahead_periods * four_hour_ms\n",
    "    MIN_PROFIT_THRESHOLD = 0.0025 \n",
    "    \n",
    "    h_start = 0\n",
    "    \n",
    "    # 마지막 데이터는 lookahead 계산 불가하므로 n-1까지만 루프\n",
    "    for i in range(n - 1):\n",
    "        atr = base_atr[i]\n",
    "        # ATR이 없거나 0이면 계산 불가 -> continue (초기값 -1/NaN 유지)\n",
    "        if np.isnan(atr) or atr <= 0: \n",
    "            continue\n",
    "        \n",
    "        entry_start_ts = base_dates_ts[i] + four_hour_ms\n",
    "        entry_end_ts = entry_start_ts + lookahead_ms\n",
    "        \n",
    "        first_entry_idx = -1\n",
    "        \n",
    "        # 1시간봉 매칭 (h_start부터 검색하여 속도 최적화)\n",
    "        for h in range(h_start, len(hourly_dates_ts)):\n",
    "            if hourly_dates_ts[h] >= entry_start_ts:\n",
    "                first_entry_idx = h\n",
    "                h_start = h # 다음 루프를 위해 시작점 업데이트\n",
    "                break\n",
    "        \n",
    "        # 매칭되는 1시간봉이 없으면(데이터 끝부분 등) 계산 불가 -> continue\n",
    "        if first_entry_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        # 진입 가격 확정\n",
    "        entry_price = hourly_open[first_entry_idx]\n",
    "        tp = entry_price + (atr * profit_mult)\n",
    "        sl = entry_price - (atr * stop_mult)\n",
    "        \n",
    "        # [중요] 배열에 값 할당 (i 인덱스 위치에 정확히!)\n",
    "        entry_prices[i] = entry_price\n",
    "        tp_prices[i] = tp\n",
    "        sl_prices[i] = sl\n",
    "        \n",
    "        result = 0 # 기본적으로 실패(0)로 시작\n",
    "        final_idx = -1 \n",
    "        \n",
    "        for h in range(first_entry_idx, len(hourly_dates_ts)):\n",
    "            # 1. 시간 초과 (Lookahead 기간 종료)\n",
    "            if hourly_dates_ts[h] >= entry_end_ts:\n",
    "                final_idx = h\n",
    "                break\n",
    "            \n",
    "            # 2. 손절 (Stop Loss) - 최우선 확인\n",
    "            if hourly_low[h] <= sl:\n",
    "                result = 0\n",
    "                final_idx = -1 # 손절 당했으므로 Timeout 수익 체크 불필요\n",
    "                break\n",
    "                \n",
    "            # 3. 익절 (Take Profit)\n",
    "            if hourly_high[h] >= tp:\n",
    "                result = 1\n",
    "                final_idx = -1 # 익절 했으므로 종료\n",
    "                break\n",
    "        \n",
    "        # 4. Timeout 처리 (아직 포지션 보유 중일 때 수익률 체크)\n",
    "        if final_idx != -1:\n",
    "            exit_price = hourly_open[final_idx]\n",
    "            return_rate = (exit_price - entry_price) / entry_price\n",
    "            \n",
    "            if return_rate >= MIN_PROFIT_THRESHOLD:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "\n",
    "        # 결과 할당\n",
    "        targets[i] = result\n",
    "    \n",
    "    return targets, entry_prices, tp_prices, sl_prices\n",
    "\n",
    "\n",
    "def create_targets_4h(df_4h, df_1h, lookahead=30, profit_mult=1.5, stop_mult=1.0, **kwargs):\n",
    "    df_target = df_4h.copy()\n",
    "    hourly_df = df_1h.copy()\n",
    "    \n",
    "    df_target['date'] = pd.to_datetime(df_target['date'])\n",
    "    hourly_df['datetime'] = pd.to_datetime(hourly_df['datetime'])\n",
    "    \n",
    "    hourly_df.columns = hourly_df.columns.str.lower()\n",
    "    \n",
    "    if 'ATR_84' not in df_target.columns:\n",
    "        df_target['ATR_84'] = ta.atr(\n",
    "            df_target['ETH_High'], df_target['ETH_Low'], df_target['ETH_Close'], length=84\n",
    "        )\n",
    "    \n",
    "    df_target = df_target.sort_values('date').reset_index(drop=True)\n",
    "    hourly_df = hourly_df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    base_dates_ts = df_target['date'].astype(np.int64).values // 10**6\n",
    "    base_atr = df_target['ATR_84'].fillna(method='ffill').fillna(0).to_numpy()\n",
    "    \n",
    "    hourly_dates_ts = hourly_df['datetime'].astype(np.int64).values // 10**6\n",
    "    hourly_open = hourly_df['open'].astype(np.float64).to_numpy()\n",
    "    hourly_high = hourly_df['high'].astype(np.float64).to_numpy()\n",
    "    hourly_low = hourly_df['low'].astype(np.float64).to_numpy()\n",
    "    \n",
    "    targets, entry_prices, tp_prices, sl_prices = compute_targets_with_hourly_numba_4h(\n",
    "        base_dates_ts, base_atr, hourly_dates_ts, hourly_open, hourly_high, hourly_low,\n",
    "        lookahead, profit_mult, stop_mult\n",
    "    )\n",
    "    \n",
    "    df_target['next_direction'] = targets\n",
    "    df_target['real_entry_price'] = entry_prices\n",
    "    df_target['take_profit_price'] = tp_prices\n",
    "    df_target['stop_loss_price'] = sl_prices\n",
    "    \n",
    "    df_target['next_close'] = df_target['ETH_Close'].shift(-1)\n",
    "    df_target['next_open'] = df_target['ETH_Open'].shift(-1)\n",
    "    df_target['next_log_return'] = np.log(df_target['next_close'] / (df_target['next_open'] + 1e-9))\n",
    "    \n",
    "    if lookahead > 0:\n",
    "        df_target = df_target.iloc[:-lookahead]\n",
    "    \n",
    "    valid_before = len(df_target)\n",
    "    df_target = df_target[df_target['next_direction'] != -1].reset_index(drop=True)\n",
    "    valid_after = len(df_target)\n",
    "    \n",
    "    print(f\"Valid Samples: {valid_after}/{valid_before} (Removed: {valid_before - valid_after})\")\n",
    "    \n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d34a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_verified(X_train, y_train, task='class', top_n=20, verbose=True):\n",
    "    if len(X_train) > 10000:\n",
    "        idx = np.random.choice(len(X_train), 10000, replace=False)\n",
    "        X_sub = X_train.iloc[idx]\n",
    "        y_sub = y_train.iloc[idx]\n",
    "    else:\n",
    "        X_sub, y_sub = X_train, y_train\n",
    "\n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_sub, y_sub, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_sub, y_sub, random_state=42, n_neighbors=3)\n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    estimator = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1) if task == 'class' else LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=top_n, step=0.1, verbose=0)\n",
    "    rfe.fit(X_sub, y_sub)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1) if task == 'class' else RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_sub, y_sub)\n",
    "    rf_idx = np.argsort(rf_model.feature_importances_)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "    \n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0: break\n",
    "    \n",
    "    return selected_features, {}\n",
    "\n",
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=20):\n",
    "    atr_col_name = 'ATR_84'\n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(X_train, y_train['next_direction'], task='class', top_n=top_n)\n",
    "        \n",
    "        if atr_col_name not in selected and atr_col_name in X_train.columns:\n",
    "            if len(selected) > 0: selected.pop()\n",
    "            selected.insert(0, atr_col_name)\n",
    "            \n",
    "    print(f\"\\n[Feature Selection] Top {len(selected)} Features Selected:\")\n",
    "    print(f\" -> {', '.join(selected)}\")\n",
    "    return selected, stats\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=20, fold_idx=None, atr_col_name='ATR_84'): \n",
    "    train_df = split_data['train'] \n",
    "    val_df = split_data['val'] \n",
    "    test_df = split_data['test'] \n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" Processing Fold {fold_idx} ({fold_type})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\" Train Period: {train_df['date'].min()} ~ {train_df['date'].max()} (N={len(train_df)})\")\n",
    "    print(f\" Val   Period: {val_df['date'].min()} ~ {val_df['date'].max()} (N={len(val_df)})\")\n",
    "    print(f\" Test  Period: {test_df['date'].min()} ~ {test_df['date'].max()} (N={len(test_df)})\")\n",
    "\n",
    "    target_cols = [\n",
    "        'next_direction', 'next_log_return', 'next_close', 'next_open', \n",
    "        'take_profit_price', 'stop_loss_price', \n",
    "        'ATR_84', 'real_entry_price' \n",
    "    ]\n",
    "\n",
    "    train_processed = train_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    exclude_cols = [col for col in target_cols if col != atr_col_name] + ['date']\n",
    "    feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    balance = y_train['next_direction'].value_counts(normalize=True).to_dict()\n",
    "    print(f\"[Class Balance] Train Set: {balance}\")\n",
    "\n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, y_train, target_type=target_type, top_n=top_n\n",
    "    )\n",
    "\n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_sel), columns=selected_features)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val_sel), columns=selected_features)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_sel), columns=selected_features)\n",
    "\n",
    "    return {\n",
    "        'train': {'X': X_train_scaled, 'y': y_train.reset_index(drop=True), 'dates': train_processed['date'].reset_index(drop=True)},\n",
    "        'val': {'X': X_val_scaled, 'y': y_val.reset_index(drop=True), 'dates': val_processed['date'].reset_index(drop=True)},\n",
    "        'test': {'X': X_test_scaled, 'y': y_test.reset_index(drop=True), 'dates': test_processed['date'].reset_index(drop=True)},\n",
    "        'scaler': scaler, \n",
    "        'selected_features': selected_features,\n",
    "        'stats': {\n",
    "            'fold_idx': fold_idx if fold_idx is not None else split_data.get('fold_idx', 0),\n",
    "            'fold_type': split_data.get('fold_type', 'unknown')\n",
    "        }\n",
    "    }\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, final_test_start='2025-01-01', \n",
    "                              initial_train_size=800, val_size=150, test_size=150, \n",
    "                              step=150, gap_size=30):\n",
    "    \n",
    "    df_period = df[df['date'] >= pd.to_datetime(train_start_date)].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    total_len = len(df_period)\n",
    "    folds = []\n",
    "    current_test_end = total_len\n",
    "    \n",
    "    while True:\n",
    "        test_end_idx = current_test_end\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        train_end_idx = val_start_idx - gap_size\n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0: break\n",
    "        \n",
    "        train_fold = df_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_type': 'walk_forward_rolling_reverse'\n",
    "        })\n",
    "        current_test_end = test_start_idx - gap_size\n",
    "    \n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "        \n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    if len(final_test_df) > 0:\n",
    "        pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "        if len(pre_final_df) >= (initial_train_size + val_size + gap_size):\n",
    "            final_val_end_idx = len(pre_final_df)\n",
    "            final_val_start_idx = final_val_end_idx - val_size\n",
    "            final_train_end_idx = final_val_start_idx - gap_size\n",
    "            final_train_start_idx = final_train_end_idx - initial_train_size\n",
    "            \n",
    "            final_train_data = pre_final_df.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "            final_val_data = pre_final_df.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "            \n",
    "            folds.append({\n",
    "                'train': final_train_data,\n",
    "                'val': final_val_data,\n",
    "                'test': final_test_df,\n",
    "                'fold_idx': len(folds) + 1,\n",
    "                'fold_type': 'final_holdout'\n",
    "            })\n",
    "            \n",
    "    return folds\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, df_hour, train_start_date, **kwargs): \n",
    "    print(f\"\\n Pipeline Started... (Train Start: {train_start_date})\")\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    df = add_kimchi_premium_proxy(df)\n",
    "    df = add_funding_and_onchain_features(df)\n",
    "    df = calculate_technical_indicators_4h(df)\n",
    "    df = calculate_cumulative_volume_delta_4h(df)\n",
    "    df = add_vwma_features_4h(df)\n",
    "    df = add_obv_divergence_features_4h(df)\n",
    "    df = add_vwap_anchored_features_4h(df)\n",
    "    df = add_institutional_footprint_features_4h(df)\n",
    "    \n",
    "    df = add_price_lag_features_first_4h(df)\n",
    "    df = add_cross_crypto_correlations(df)\n",
    "    df = add_volatility_regime_features_4h(df)\n",
    "    df = preprocess_non_stationary_features_4h(df)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    lookahead = kwargs.get('lookahead_periods', 30) \n",
    "    profit_mult = kwargs.get('profit_mult', 2.0)\n",
    "    stop_mult = kwargs.get('stop_mult', 1.0)\n",
    "\n",
    "    df = create_targets_4h(df, df_hour,\n",
    "        lookahead=lookahead, \n",
    "        profit_mult=profit_mult, \n",
    "        stop_mult=stop_mult\n",
    "    )\n",
    "\n",
    "    df = remove_raw_prices_and_transform(df, 'direction', 'tvt')\n",
    "    print(f\"Final Data Shape: {df.shape}\")\n",
    "    \n",
    "    initial_train_size = kwargs.get('initial_train_days', 800) * 6\n",
    "    val_size = 150 * 6\n",
    "    test_size = 150 * 6\n",
    "    gap_size = lookahead\n",
    "\n",
    "    splits = split_walk_forward_method(\n",
    "        df, \n",
    "        train_start_date=train_start_date,\n",
    "        final_test_start=kwargs.get('final_test_start', '2025-01-01'),\n",
    "        initial_train_size=initial_train_size,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size,\n",
    "        step=150 * 6,\n",
    "        gap_size=gap_size\n",
    "    )\n",
    "    print(f\"Data Split Completed. Total {len(splits)} folds generated.\")\n",
    "\n",
    "    result = []\n",
    "    for fold in splits:\n",
    "        res = process_single_split(\n",
    "            fold, \n",
    "            top_n=kwargs.get('top_n', 20), \n",
    "            fold_idx=fold['fold_idx'],\n",
    "            atr_col_name='ATR_84'\n",
    "        )\n",
    "        result.append(res)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3896bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_class_weights(y_train):\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "        return dict(zip(classes, weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_expectancy(y_true, y_prob, th_start=0.5, th_end=0.9, th_step=0.05, rr_ratio=1.5):\n",
    "        \"\"\"\n",
    "        최적의 Expectancy를 찾아 반환하는 내부 헬퍼 함수\n",
    "        \"\"\"\n",
    "        best_exp = -999.0\n",
    "        best_th = 0.5\n",
    "        best_metrics = {'win_rate': 0.0, 'trades': 0}\n",
    "        \n",
    "        for th in np.arange(th_start, th_end, th_step):\n",
    "            preds = (y_prob >= th).astype(int)\n",
    "            n_trades = preds.sum()\n",
    "            \n",
    "            if n_trades < 50: continue # 최소 거래 횟수 필터\n",
    "            \n",
    "            wins = ((preds == 1) & (y_true == 1)).sum()\n",
    "            losses = n_trades - wins\n",
    "            \n",
    "            # Expectancy = (승률 * 수익비) - (패율 * 손실비)\n",
    "            # 여기서는 손실비=1.0 가정\n",
    "            exp = (wins * rr_ratio - losses * 1.0) / n_trades\n",
    "            \n",
    "            if exp > best_exp:\n",
    "                best_exp = exp\n",
    "                best_th = th\n",
    "                best_metrics = {'win_rate': wins/n_trades, 'trades': n_trades}\n",
    "                \n",
    "        return best_exp, best_th, best_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 20, 150),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 60),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 1e-4, 1e-2, log=True),\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # [수정] Expectancy 최적화\n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            exp, _, _ = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "            return exp\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # [수정] 최종 평가 및 출력\n",
    "        train_prob = best_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        train_exp, _, _ = DirectionModels._calculate_expectancy(y_train, train_prob)\n",
    "        val_exp, val_th, val_meta = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "        \n",
    "        print(f\"  [RandomForest] Train Exp: {train_exp:.4f}R | Val Exp: {val_exp:.4f}R (WinRate: {val_meta['win_rate']*100:.1f}%, Trades: {val_meta['trades']}) | Gap: {train_exp - val_exp:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 30, 150),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'class_weight': 'balanced',\n",
    "                'verbosity': -1,\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=callbacks)\n",
    "            \n",
    "            # [수정] Expectancy 최적화\n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            exp, _, _ = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "            return exp\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=25)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'n_estimators': 1000, \n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss', \n",
    "            'class_weight': 'balanced', \n",
    "            'verbosity': -1, \n",
    "            'n_jobs': -1, \n",
    "            'random_state': 42\n",
    "        })\n",
    "        \n",
    "        final_model = lgb.LGBMClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "        \n",
    "        # [수정] 최종 평가 및 출력\n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        train_exp, _, _ = DirectionModels._calculate_expectancy(y_train, train_prob)\n",
    "        val_exp, val_th, val_meta = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "        \n",
    "        print(f\"  [LightGBM]     Train Exp: {train_exp:.4f}R | Val Exp: {val_exp:.4f}R (WinRate: {val_meta['win_rate']*100:.1f}%, Trades: {val_meta['trades']}) | Gap: {train_exp - val_exp:.4f}\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "                'gamma': trial.suggest_float('gamma', 0.5, 10.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'logloss',\n",
    "                'tree_method': 'hist',\n",
    "                'early_stopping_rounds': 30,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            \n",
    "            # [수정] Expectancy 최적화\n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            exp, _, _ = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "            return exp\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=25)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'n_estimators': 1000, \n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss', \n",
    "            'tree_method': 'hist', \n",
    "            'early_stopping_rounds': 50,\n",
    "            'random_state': 42, \n",
    "            'n_jobs': -1\n",
    "        })\n",
    "        \n",
    "        final_model = xgb.XGBClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        # [수정] 최종 평가 및 출력\n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        train_exp, _, _ = DirectionModels._calculate_expectancy(y_train, train_prob)\n",
    "        val_exp, val_th, val_meta = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "        \n",
    "        print(f\"  [XGBoost]      Train Exp: {train_exp:.4f}R | Val Exp: {val_exp:.4f}R (WinRate: {val_meta['win_rate']*100:.1f}%, Trades: {val_meta['trades']}) | Gap: {train_exp - val_exp:.4f}\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'iterations': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 9),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 15),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'Logloss',\n",
    "                'auto_class_weights': 'Balanced',\n",
    "                'logging_level': 'Silent',\n",
    "                'random_seed': 42,\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 30,\n",
    "                'allow_writing_files': False\n",
    "            }\n",
    "            \n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "            \n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            exp, _, _ = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "            return exp\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'iterations': 1000, \n",
    "            'loss_function': 'Logloss', \n",
    "            'eval_metric': 'Logloss',\n",
    "            'auto_class_weights': 'Balanced', \n",
    "            'logging_level': 'Silent',\n",
    "            'random_seed': 42, \n",
    "            'od_type': 'Iter', \n",
    "            'od_wait': 50, \n",
    "            'allow_writing_files': False\n",
    "        })\n",
    "        \n",
    "        final_model = cb.CatBoostClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        \n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        train_exp, _, _ = DirectionModels._calculate_expectancy(y_train, train_prob)\n",
    "        val_exp, val_th, val_meta = DirectionModels._calculate_expectancy(y_val, val_prob)\n",
    "        \n",
    "        print(f\"  [CatBoost]     Train Exp: {train_exp:.4f}R | Val Exp: {val_exp:.4f}R (WinRate: {val_meta['win_rate']*100:.1f}%, Trades: {val_meta['trades']}) | Gap: {train_exp - val_exp:.4f}\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b420447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.best_thresholds = {}\n",
    "        self.save_models = save_models\n",
    "        self.models = {} if save_models else None\n",
    "        self.prediction_logs = {} \n",
    "\n",
    "    def optimize_threshold(self, y_true, buy_prob, min_trades=50, reward_risk_ratio=1.5):\n",
    "        \n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            th = trial.suggest_float('threshold', 0.5, 0.9)\n",
    "            preds = (buy_prob >= th).astype(int)\n",
    "            n_trades = np.sum(preds == 1)\n",
    "            \n",
    "            if n_trades < min_trades: return -999.0\n",
    "            \n",
    "            wins = np.sum((preds == 1) & (y_true == 1))\n",
    "            losses = n_trades - wins\n",
    "            \n",
    "            expectancy = ((wins * reward_risk_ratio) - (losses * 1.0)) / n_trades\n",
    "            if (wins / n_trades) < 0.4: expectancy -= 0.5\n",
    "            \n",
    "            return expectancy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        return study.best_params['threshold']\n",
    "\n",
    "    def evaluate_model(self, model, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                       model_name, profit_mult=1.5, stop_mult=1.0,\n",
    "                       test_dates=None, test_prices=None):\n",
    "        \n",
    "        rr_ratio = profit_mult / stop_mult if stop_mult > 0 else 1.5\n",
    "        \n",
    "        val_prob = model.predict_proba(X_val)[:, 1]\n",
    "        test_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "\n",
    "        best_th = self.optimize_threshold(y_val.astype(int), val_prob, min_trades=50, reward_risk_ratio=rr_ratio)\n",
    "        self.best_thresholds[model_name] = best_th\n",
    "        \n",
    "        # Test 셋 평가\n",
    "        test_preds = (test_prob >= best_th).astype(int)\n",
    "        n_trades = np.sum(test_preds == 1)\n",
    "        y_test_arr = y_test.astype(int)\n",
    "        \n",
    "        if n_trades > 0:\n",
    "            wins = np.sum((test_preds == 1) & (y_test_arr == 1))\n",
    "            losses = n_trades - wins\n",
    "            win_rate = wins / n_trades\n",
    "            expectancy = ((wins * rr_ratio) - (losses * 1.0)) / n_trades\n",
    "        else:\n",
    "            win_rate, expectancy = 0.0, 0.0\n",
    "            \n",
    "        # [추가] 평가 결과 콘솔 출력 (한눈에 확인용)\n",
    "        print(f\"    -> [TEST EVAL] {model_name:<12} | Exp: {expectancy:.4f}R | Win: {win_rate*100:.1f}% | Trades: {n_trades} | Th: {best_th:.2f}\")\n",
    "\n",
    "        if test_dates is not None:\n",
    "            pred_df = pd.DataFrame({\n",
    "                'timestamp': test_dates,\n",
    "                'close': test_prices if test_prices is not None else 0,\n",
    "                'prob': test_prob,\n",
    "                'threshold': best_th,\n",
    "                'signal': test_preds,\n",
    "                'actual_target': y_test_arr\n",
    "            })\n",
    "            if not isinstance(pred_df.index, pd.DatetimeIndex):\n",
    "                pred_df.set_index('timestamp', inplace=True)\n",
    "            self.prediction_logs[model_name] = pred_df\n",
    "\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Threshold': best_th,\n",
    "            'Test_Trades': n_trades,\n",
    "            'Test_WinRate': win_rate,\n",
    "            'Test_Expectancy': expectancy\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if self.save_models: self.models[model_name] = model\n",
    "        return result\n",
    "\n",
    "    def get_summary_dataframe(self): return pd.DataFrame(self.results)\n",
    "    def get_models_dict(self): return self.models or {}\n",
    "    def get_prediction_logs(self): return self.prediction_logs\n",
    "    def get_best_thresholds(self): return self.best_thresholds\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    def _prepare_target(self, y_data):\n",
    "        if isinstance(y_data, pd.DataFrame):\n",
    "            y_data = y_data.iloc[:, 0].values\n",
    "        elif isinstance(y_data, pd.Series):\n",
    "            y_data = y_data.values\n",
    "        y_data = np.array(y_data).flatten()\n",
    "        y_data = np.nan_to_num(y_data, nan=0.0)\n",
    "        return np.round(y_data).astype(int)\n",
    "\n",
    "    def train_all_models(self, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                         profit_mult, stop_mult, ml_models,\n",
    "                         test_dates=None, test_prices=None):\n",
    "        \n",
    "        y_train_arr = self._prepare_target(y_train)\n",
    "        y_val_arr = self._prepare_target(y_val)\n",
    "        y_test_arr = self._prepare_target(y_test)\n",
    "        \n",
    "        for config in ml_models:\n",
    "            try:\n",
    "                # 모델 학습\n",
    "                if config.get('needs_val', False):\n",
    "                    model = config['func'](X_train, y_train_arr, X_val, y_val_arr)\n",
    "                else:\n",
    "                    model = config['func'](X_train, y_train_arr)\n",
    "                \n",
    "                # 평가 (메타 데이터 전달)\n",
    "                self.evaluator.evaluate_model(\n",
    "                    model, X_train, y_train_arr, X_val, y_val_arr, X_test, y_test_arr,\n",
    "                    config['name'], profit_mult, stop_mult,\n",
    "                    test_dates=test_dates, test_prices=test_prices \n",
    "                )\n",
    "                del model\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed {config['name']}: {e}\")\n",
    "                traceback.print_exc()    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def save_fold_results(fold_idx, fold_type, evaluator, trial_name, fold_data, result_dir):\n",
    "    base_dir = f\"{result_dir}/{trial_name}/fold_{fold_idx}_{fold_type}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. 결과 요약 저장\n",
    "    summary = evaluator.get_summary_dataframe()\n",
    "    summary.to_csv(f\"{base_dir}/fold_summary.csv\", index=False)\n",
    "    \n",
    "    # 2. 상세 예측 로그 저장\n",
    "    pred_logs = evaluator.get_prediction_logs()\n",
    "    for model_name, df_log in pred_logs.items():\n",
    "        df_log.to_csv(f\"{base_dir}/predictions_{model_name}.csv\")\n",
    "\n",
    "    # 3. 모델 객체 저장\n",
    "    for name, model in evaluator.get_models_dict().items():\n",
    "        joblib.dump(model, f\"{base_dir}/model_{name}.pkl\")\n",
    "            \n",
    "    # 4. 스케일러 저장\n",
    "    if 'scaler' in fold_data:\n",
    "        joblib.dump(fold_data['scaler'], f\"{base_dir}/scaler.pkl\")\n",
    "        \n",
    "    # 5. [중요] 메타데이터 저장 \n",
    "    meta_data = {\n",
    "        'fold_idx': fold_idx,\n",
    "        'fold_type': fold_type,\n",
    "        'selected_features': fold_data.get('selected_features', []), \n",
    "        'model_thresholds': evaluator.get_best_thresholds(),\n",
    "        'trial_params': {\n",
    "            'profit_mult': fold_data.get('profit_mult'), \n",
    "            'stop_mult': fold_data.get('stop_mult'),     \n",
    "            'lookahead': fold_data.get('lookahead')     \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{base_dir}/metadata.json\", 'w') as f:\n",
    "        json.dump(meta_data, f, indent=4)\n",
    "            \n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b83e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_optimization(df_merged, df_hour, ml_models, n_trials=30):\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    RESULT_DIR = f\"model_results/{TIMESTAMP}_Sniper\"\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "    \n",
    "    LOG_PATH = f\"{RESULT_DIR}/optuna_log.csv\"\n",
    "\n",
    "    price_col = 'ETH_Close' if 'ETH_Close' in df_merged.columns else 'close'\n",
    "    \n",
    "    # 날짜를 인덱스로 설정하여 검색 속도 향상\n",
    "    lookup_df = df_merged[['date', price_col]].copy()\n",
    "    lookup_df['date'] = pd.to_datetime(lookup_df['date'])\n",
    "    lookup_df = lookup_df.set_index('date').sort_index()\n",
    "    \n",
    "    # [Resume] 기존 로그 로드\n",
    "    existing_history = pd.DataFrame()\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        try:\n",
    "            existing_history = pd.read_csv(LOG_PATH)\n",
    "            print(f\"\\n[Resume] Loaded {len(existing_history)} existing trials.\")\n",
    "        except: pass\n",
    "\n",
    "    if not os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"w\") as f:\n",
    "            f.write(\"trial,lookahead,profit_mult,stop_mult,top_n,train_days,score\\n\")\n",
    "\n",
    "    def objective(trial):\n",
    "        nonlocal existing_history \n",
    "        \n",
    "        # 파라미터 제안\n",
    "        lookahead = trial.suggest_int('lookahead', 3, 12, step=3)\n",
    "    \n",
    "        # 2. 익절\n",
    "        p_mult = trial.suggest_float('profit_mult', 1.0,1.8, step=0.2)\n",
    "\n",
    "        # 3. 손절\n",
    "        s_mult = trial.suggest_float('stop_mult', 0.5, 1.0, step=0.1)\n",
    "\n",
    "        # 4. Feature 수\n",
    "        top_n = trial.suggest_int('top_n', 10, 30, step=5)\n",
    "\n",
    "        # 5. 학습 기간\n",
    "        train_days = trial.suggest_int('train_days', 180, 730, step=180)\n",
    "\n",
    "        # 중복 실행 방지 \n",
    "        if not existing_history.empty:\n",
    "            mask = (\n",
    "                (existing_history['lookahead'] == lookahead) &\n",
    "                (np.isclose(existing_history['profit_mult'], p_mult, atol=1e-5)) &\n",
    "                (np.isclose(existing_history['stop_mult'], s_mult, atol=1e-5)) &\n",
    "                (existing_history['top_n'] == top_n) &\n",
    "                (existing_history['train_days'] == train_days)\n",
    "            )\n",
    "            if mask.any():\n",
    "                prev_score = existing_history.loc[mask, 'score'].values[0]\n",
    "                print(f\"[Skip] Already done. Score: {prev_score}\")\n",
    "                return prev_score\n",
    "\n",
    "        trial_name = f\"T{trial.number}_L{lookahead}_P{p_mult:.1f}_S{s_mult:.1f}_N{top_n}\"\n",
    "        print(f\"\\n{'='*60}\\n Starting {trial_name}\\n{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Build Pipeline (데이터 준비)\n",
    "            pipeline_result = build_complete_pipeline_corrected(\n",
    "                df_raw=df_merged,\n",
    "                df_hour=df_hour,\n",
    "                train_start_date='2020-01-01',\n",
    "                final_test_start='2025-01-01',\n",
    "                lookahead_periods=lookahead,\n",
    "                profit_mult=p_mult,\n",
    "                stop_mult=s_mult,\n",
    "                top_n=top_n,\n",
    "                initial_train_days=train_days\n",
    "            )\n",
    "            \n",
    "            fold_scores = []\n",
    "            \n",
    "            # 2. Walk-Forward Loop\n",
    "            for fold_data in pipeline_result:\n",
    "                stats = fold_data.get('stats', {}) \n",
    "                fold_idx = stats.get('fold_idx', 0)\n",
    "                fold_type = stats.get('fold_type', 'unknown')\n",
    "                \n",
    "                print(f\"   >> Running Fold {fold_idx} ({fold_type})\")\n",
    "                \n",
    "                fold_data['profit_mult'] = p_mult\n",
    "                fold_data['stop_mult'] = s_mult\n",
    "                \n",
    "                # =============================================================\n",
    "                # [핵심] 전략팀을 위한 원본 가격 매핑 (Lookup)\n",
    "                # =============================================================\n",
    "                # process_single_split이 저장해둔 날짜를 가져옵니다.\n",
    "                test_dates = pd.to_datetime(fold_data['test']['dates'])\n",
    "                \n",
    "\n",
    "                test_prices = lookup_df.reindex(test_dates)[price_col].values\n",
    "                \n",
    "\n",
    "                test_prices = np.nan_to_num(test_prices, nan=0.0)\n",
    "                # =============================================================\n",
    "\n",
    "                evaluator = ModelEvaluator(save_models=True) \n",
    "                trainer = ModelTrainer(evaluator)\n",
    "                \n",
    "                # 모델 학습 (test_dates와 test_prices를 함께 전달)\n",
    "                trainer.train_all_models(\n",
    "                    fold_data['train']['X'], fold_data['train']['y'],\n",
    "                    fold_data['val']['X'], fold_data['val']['y'],\n",
    "                    fold_data['test']['X'], fold_data['test']['y'],\n",
    "                    p_mult, s_mult, ml_models,\n",
    "                    test_dates=test_dates, \n",
    "                    test_prices=test_prices \n",
    "                )\n",
    "\n",
    "                # 결과 및 모델 저장\n",
    "                summary = save_fold_results(fold_idx, fold_type, evaluator, trial_name, fold_data, RESULT_DIR)\n",
    "                \n",
    "                # 점수 집계 (Expectancy)\n",
    "                if 'Test_Expectancy' in summary.columns:\n",
    "                    best_fold_score = summary['Test_Expectancy'].max()\n",
    "                    fold_scores.append(best_fold_score)\n",
    "                \n",
    "                del evaluator, trainer\n",
    "                gc.collect()\n",
    "            \n",
    "            final_score = np.mean(fold_scores) if fold_scores else -99.0\n",
    "            print(f\"\\n === Trial Score: {final_score:.4f}R ===\")\n",
    "            \n",
    "            # 로그 기록\n",
    "            with open(LOG_PATH, \"a\") as f:\n",
    "                f.write(f\"{trial.number},{lookahead},{p_mult},{s_mult},{top_n},{train_days},{final_score}\\n\")\n",
    "            \n",
    "            # 메모리 DB 업데이트\n",
    "            new_row = pd.DataFrame([[trial.number, lookahead, p_mult, s_mult, top_n, train_days, final_score]], \n",
    "                                   columns=['trial','lookahead','profit_mult','stop_mult','top_n','train_days','score'])\n",
    "            if existing_history.empty: existing_history = new_row\n",
    "            else: existing_history = pd.concat([existing_history, new_row], ignore_index=True)\n",
    "                \n",
    "            return final_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" [Error] Trial Failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return -99.0\n",
    "\n",
    "    # Optuna 실행\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"\\n[Optuna] Best Params: {study.best_params}\")\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05555c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU Detected!\n",
      "Loading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-27 20:42:44,981] A new study created in memory with name: no-name-ff47de3e-06b1-4b9b-9842-9e31514a1fc9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Resume] Loaded 1 existing trials.\n",
      "\n",
      "============================================================\n",
      " Starting T0_L12_P1.0_S0.5_N30\n",
      "============================================================\n",
      "\n",
      " Pipeline Started... (Train Start: 2020-01-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/distributions.py:702: UserWarning: The distribution is specified by [180, 730] and step=180, but the range is not divisible by `step`. It will be replaced by [180, 720].\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_704681/324838015.py:116: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['EMA_72'] = ta.ema(close, length=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Samples: 17806/17889 (Removed: 83)\n",
      "Final Data Shape: (17806, 154)\n",
      "Data Split Completed. Total 9 folds generated.\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-03-08 13:00:00 ~ 2022-02-26 09:00:00 (N=4320)\n",
      " Val   Period: 2022-02-28 13:00:00 ~ 2022-07-28 09:00:00 (N=900)\n",
      " Test  Period: 2022-07-30 13:00:00 ~ 2022-12-27 09:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6518518518518519, 1: 0.34814814814814815}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, return_lag_6p, VIX_ma180_ratio, eth_chain_tvl_pct_chg_24h, VIX_pct_chg_24h, GOLD_ma180_ratio, SP500_ma180_ratio, MACDH_72_156_54, MFI_84, aave_eth_tvl_ma180_ratio, curve-dex_eth_tvl_1d_chg, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Dist_from_VWAP, return_lag_12p, Corr_ETH_BTC_24h, eth_chain_tvl_1d_chg_pct_chg_24h, eth_chain_tvl_1d_chg_ma180_ratio, uniswap_eth_tvl_1d_chg_pct_chg_24h, uniswap_eth_tvl_1d_chg_ma180_ratio, makerdao_eth_tvl_pct_chg_24h, aave_eth_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, lido_eth_tvl_pct_chg_24h, SMA_300, GOLD_pct_chg_24h, DXY_pct_chg_24h, SP500_pct_chg_24h, SMA_120, EMA_72\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 2 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-08-07 13:00:00 ~ 2022-07-28 09:00:00 (N=4320)\n",
      " Val   Period: 2022-07-30 13:00:00 ~ 2022-12-27 09:00:00 (N=900)\n",
      " Test  Period: 2022-12-29 13:00:00 ~ 2023-05-28 09:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6604166666666667, 1: 0.33958333333333335}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, SP500_pct_chg_24h, optimism_tvl_1d_chg, eth_chain_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg, uniswap_eth_tvl_pct_chg_24h, CVD_7d, VIX_pct_chg_24h, VWAP_Day, SP500_ma180_ratio, aave_eth_tvl_pct_chg_24h, PRICE_VS_LOW_30p, Price_div_VWMA_20d, Dist_from_VWAP, return_lag_6p, return_lag_12p, uniswap_eth_tvl_1d_chg_pct_chg_24h, uniswap_eth_tvl_1d_chg_ma180_ratio, makerdao_eth_tvl_pct_chg_24h, lido_eth_tvl_pct_chg_24h, eth_log_return, DXY_pct_chg_24h, GOLD_pct_chg_24h, SMA_300, SMA_120, BB_WIDTH, EMA_72, totalBridgedToUSD_ma180_ratio, FR_Abs_Signal, MACD_72_156_54\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 3 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-01-06 13:00:00 ~ 2022-12-27 09:00:00 (N=4320)\n",
      " Val   Period: 2022-12-29 13:00:00 ~ 2023-05-28 09:00:00 (N=900)\n",
      " Test  Period: 2023-05-30 13:00:00 ~ 2023-10-27 09:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6729166666666667, 1: 0.32708333333333334}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, SP500_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, optimism_tvl_1d_chg, eth_chain_tvl_pct_chg_24h, aave_eth_tvl_pct_chg_24h, SP500_ma180_ratio, VWAP_Day, curve-dex_eth_tvl_1d_chg, PRICE_VS_LOW_360p, CVD_Rank_180, makerdao_eth_tvl_pct_chg_24h, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Price_div_VWMA_20d, Dist_from_VWAP, return_lag_6p, return_lag_12p, Corr_ETH_DOT_24h, eth_chain_tvl_1d_chg_pct_chg_24h, eth_chain_tvl_1d_chg_ma180_ratio, uniswap_eth_tvl_1d_chg_pct_chg_24h, lido_eth_tvl_pct_chg_24h, GOLD_ma180_ratio, GOLD_pct_chg_24h, DXY_pct_chg_24h, VIX_pct_chg_24h, SMA_300, BB_WIDTH, FR_Abs_Signal\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 4 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-06-07 13:00:00 ~ 2023-05-28 09:00:00 (N=4320)\n",
      " Val   Period: 2023-05-30 13:00:00 ~ 2023-10-27 09:00:00 (N=900)\n",
      " Test  Period: 2023-10-29 13:00:00 ~ 2024-03-27 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6831018518518519, 1: 0.31689814814814815}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, SP500_pct_chg_24h, lido_eth_tvl_pct_chg_24h, eth_chain_tvl_pct_chg_24h, totalBridgedToUSD_ma180_ratio, uniswap_eth_tvl_pct_chg_24h, optimism_tvl_1d_chg, SP500_ma180_ratio, VWAP_Day, aave_eth_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, PRICE_VS_HIGH_30p, curve-dex_eth_tvl_1d_chg, totalUnreleased_ma180_ratio, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Dist_from_VWAP, return_lag_6p, return_lag_12p, totalUnreleased_pct_chg_24h, totalCirculating_pct_chg_24h, DXY_pct_chg_24h, VIX_pct_chg_24h, GOLD_pct_chg_24h, SMA_300, BB_WIDTH, SMA_120, fundingRate, DXY_ma180_ratio, MACD_72_156_54\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 5 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-11-06 13:00:00 ~ 2023-10-27 09:00:00 (N=4320)\n",
      " Val   Period: 2023-10-29 13:00:00 ~ 2024-03-27 13:00:00 (N=900)\n",
      " Test  Period: 2024-03-29 17:00:00 ~ 2024-08-26 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6893518518518519, 1: 0.3106481481481482}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, SP500_pct_chg_24h, lido_eth_tvl_pct_chg_24h, totalBridgedToUSD_ma180_ratio, makerdao_eth_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, return_lag_6p, GOLD_ma180_ratio, eth_chain_tvl_pct_chg_24h, PRICE_VS_HIGH_30p, aave_eth_tvl_pct_chg_24h, arbitrum_tvl_1d_chg, optimism_tvl_1d_chg, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Dist_from_VWAP, return_lag_12p, Corr_ETH_XRP_24h, Corr_ETH_ADA_24h, eth_chain_tvl_1d_chg_ma180_ratio, makerdao_eth_tvl_ma180_ratio, GOLD_pct_chg_24h, DXY_pct_chg_24h, VIX_pct_chg_24h, SMA_300, SMA_120, VWAP_Day, fundingRate, BB_WIDTH, FR_Abs_Signal\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 6 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2022-04-07 13:00:00 ~ 2024-03-27 13:00:00 (N=4320)\n",
      " Val   Period: 2024-03-29 17:00:00 ~ 2024-08-26 13:00:00 (N=900)\n",
      " Test  Period: 2024-08-28 17:00:00 ~ 2025-01-25 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6754629629629629, 1: 0.324537037037037}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, SP500_pct_chg_24h, lido_eth_tvl_pct_chg_24h, eth_chain_tvl_pct_chg_24h, optimism_tvl_1d_chg, arbitrum_tvl_1d_chg, DXY_pct_chg_24h, VWAP_Day, aave_eth_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, totalBridgedToUSD_ma180_ratio, PRICE_VS_LOW_120p, curve-dex_eth_tvl_1d_chg, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Dist_from_VWAP, return_lag_6p, Corr_ETH_XRP_24h, RelStr_ETH_XRP, totalUnreleased_ma180_ratio, uniswap_eth_tvl_1d_chg_ma180_ratio, totalBridgedToUSD_pct_chg_24h, GOLD_pct_chg_24h, VIX_pct_chg_24h, SMA_300, EMA_72, fundingRate, SMA_120, BB_WIDTH\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 7 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2022-09-06 13:00:00 ~ 2024-08-26 13:00:00 (N=4320)\n",
      " Val   Period: 2024-08-28 17:00:00 ~ 2025-01-25 13:00:00 (N=900)\n",
      " Test  Period: 2025-01-27 17:00:00 ~ 2025-06-26 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6768518518518518, 1: 0.32314814814814813}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, lido_eth_tvl_pct_chg_24h, eth_chain_tvl_pct_chg_24h, arbitrum_tvl_1d_chg, VIX_pct_chg_24h, PRICE_VS_LOW_120p, makerdao_eth_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, GOLD_ma180_ratio, RSI_84, curve-dex_eth_tvl, aave_eth_tvl_pct_chg_24h, optimism_tvl_1d_chg, curve-dex_eth_tvl_1d_chg, VOLUME_RATIO, PRICE_VS_HIGH_30p, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, BREAKOUT_STR_120p, CVD_24h, Dist_from_VWAP, return_lag_6p, return_lag_12p, totalUnreleased_ma180_ratio, eth_chain_tvl_1d_chg_pct_chg_24h, makerdao_eth_tvl_ma180_ratio, SP500_ma180_ratio, DXY_pct_chg_24h, GOLD_pct_chg_24h, SMA_300\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 8 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2023-02-05 13:00:00 ~ 2025-01-25 13:00:00 (N=4320)\n",
      " Val   Period: 2025-01-27 17:00:00 ~ 2025-06-26 13:00:00 (N=900)\n",
      " Test  Period: 2025-06-28 17:00:00 ~ 2025-11-25 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6719907407407407, 1: 0.3280092592592593}\n",
      "\n",
      "[Feature Selection] Top 30 Features Selected:\n",
      " -> ATR_84, lido_eth_tvl_pct_chg_24h, arbitrum_tvl_1d_chg, eth_chain_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, VIX_pct_chg_24h, optimism_tvl_1d_chg, aave_eth_tvl_pct_chg_24h, GOLD_ma180_ratio, uniswap_eth_tvl_pct_chg_24h, PRICE_VS_HIGH_120p, RSI_84, zksync era_tvl_1d_chg, VOLUME_RATIO, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, BREAKOUT_STR_120p, CVD_24h, Price_div_VWMA_20d, Dist_from_VWAP, return_lag_6p, Corr_ETH_ADA_24h, lido_eth_tvl_1d_chg_ma180_ratio, totalUnreleased_ma180_ratio, SP500_ma180_ratio, GOLD_pct_chg_24h, SP500_pct_chg_24h, DXY_pct_chg_24h, SMA_300, SMA_120\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 9 (final_holdout)\n",
      "================================================================================\n",
      " Train Period: 2022-08-12 21:00:00 ~ 2024-08-01 21:00:00 (N=4320)\n",
      " Val   Period: 2024-08-04 01:00:00 ~ 2024-12-31 21:00:00 (N=900)\n",
      " Test  Period: 2025-01-01 01:00:00 ~ 2025-11-25 13:00:00 (N=1972)\n",
      "[Class Balance] Train Set: {0: 0.6759259259259259, 1: 0.32407407407407407}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Execution Block\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1. GPU Check (Optional)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\" GPU Detected!\")\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "df_merged = pd.read_csv(\"eth_4hour.csv\")\n",
    "df_hour = pd.read_csv(\"eth_hour.csv\")\n",
    "if 'timestamp' in df_merged.columns:\n",
    "    df_merged = df_merged.rename(columns={'timestamp': 'date'})\n",
    "\n",
    "df_merged['date'] = pd.to_datetime(df_merged['date'])\n",
    "df_merged = df_merged.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# [실제 실행]\n",
    "study = run_optuna_optimization(df_merged, df_hour, ML_MODELS_CLASSIFICATION, n_trials=40)\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\" Best Expectancy: {study.best_value:.4f}\")\n",
    "print(\"==================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4bfd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # lookahead = trial.suggest_int('lookahead', 12, 42, step=6)\n",
    "# # p_mult = trial.suggest_float('profit_mult', 1.5, 3.0, step=0.1)\n",
    "# # s_mult = trial.suggest_float('stop_mult', 0.8, 1.2, step=0.1)\n",
    "# # top_n = trial.suggest_int('top_n', 15, 35, step=5)\n",
    "# # train_days = trial.suggest_int('train_days', 365, 1095, step=365)\n",
    "        \n",
    "    \n",
    "    \n",
    "# df_merged = pd.read_csv(\"eth_4hour.csv\")\n",
    "# df_hour = pd.read_csv(\"eth_hour.csv\")\n",
    "# if 'timestamp' in df_merged.columns:\n",
    "#     df_merged = df_merged.rename(columns={'timestamp': 'date'})\n",
    "\n",
    "# df_merged['date'] = pd.to_datetime(df_merged['date'])\n",
    "# df_merged = df_merged.sort_values('date').reset_index(drop=True)\n",
    "# lookahead=12\n",
    "# p_mult=1.5\n",
    "# s_mult=0.8\n",
    "# top_n=20\n",
    "# train_days=365\n",
    "# pipeline_result = build_complete_pipeline_corrected(\n",
    "#                 df_raw=df_merged,\n",
    "#                 df_hour=df_hour,\n",
    "#                 train_start_date='2020-01-01',\n",
    "#                 final_test_start='2025-01-01',\n",
    "#                 lookahead_periods=lookahead,\n",
    "#                 profit_mult=p_mult,\n",
    "#                 stop_mult=s_mult,\n",
    "#                 top_n=top_n,\n",
    "#                 initial_train_days=train_days\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
