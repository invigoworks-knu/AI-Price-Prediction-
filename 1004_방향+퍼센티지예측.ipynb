{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed200439",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT 버전 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a970319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72653f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165db55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 퍼플렉시티 버전 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ddd0f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11 뉴스 로드 시작\n",
      "1/11 뉴스 로드 완료: 25947건\n",
      "2/11 뉴스 감성 집계 시작\n",
      "2/11 뉴스 감성 집계 완료\n",
      "3/11 macro 파일 로드 시작\n",
      "3/11 macro 파일 로드 완료\n",
      "4/11 온체인 로드 시작\n",
      "4/11 온체인 로드 완료\n",
      "5/11 날짜 정렬 및 리인덱스 시작\n",
      "5/11 날짜 정렬 완료\n",
      "6/11 ETH 타깃 및 기술지표 준비 시작\n",
      "6/11 ETH 타깃 및 기술지표 준비 완료\n",
      "7/11 top-n macro 입력 생성 시작\n",
      "7/11 top-n macro 입력 생성 완료\n",
      "8/11 sentiment feature 준비 및 병합\n",
      "8/11 sentiment 준비 완료\n",
      "9/11 PPS 기반 특성 선택 시작\n",
      "9/11 PPS 선택 완료: 4개 특성\n",
      "10/11 정규화 및 교차 특성 생성\n",
      "10/11 정규화 완료\n",
      "11/11 상관계수 기반 2차 필터링\n",
      "11/11 최종 선택 특성 수: 7\n",
      "데이터 준비 완료 - 학습: (1383, 7, 7), 테스트: (346, 7, 7)\n",
      "모델 구축 시작\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 7, 7)]               0         []                            \n",
      "                                                                                                  \n",
      " gru_27 (GRU)                (None, 7, 256)               203520    ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_27 (LSTM)              (None, 7, 256)               270336    ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " gru_28 (GRU)                (None, 7, 256)               394752    ['gru_27[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_28 (LSTM)              (None, 7, 256)               525312    ['lstm_27[0][0]']             \n",
      "                                                                                                  \n",
      " gru_29 (GRU)                (None, 256)                  394752    ['gru_28[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_29 (LSTM)              (None, 256)                  525312    ['lstm_28[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 512)                  0         ['lstm_29[0][0]',             \n",
      " )                                                                   'gru_29[0][0]']              \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 128)                  65664     ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 128)                  32896     ['gru_29[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 128)                  0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " price (Dense)               (None, 1)                    129       ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " direction (Dense)           (None, 1)                    129       ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2412802 (9.20 MB)\n",
      "Trainable params: 2412802 (9.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "모델 구축 완료\n",
      "학습 시작 (Epochs: 12, Batch Size: 64)\n",
      "Epoch 1/12\n",
      "18/18 [==============================] - 8s 88ms/step - loss: 0.7150 - price_loss: 0.0200 - direction_loss: 0.6950 - price_mae: 0.0955 - direction_accuracy: 0.4828 - val_loss: 0.6950 - val_price_loss: 0.0025 - val_direction_loss: 0.6926 - val_price_mae: 0.0380 - val_direction_accuracy: 0.5235\n",
      "Epoch 2/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6980 - price_loss: 0.0031 - direction_loss: 0.6949 - price_mae: 0.0398 - direction_accuracy: 0.4973 - val_loss: 0.6980 - val_price_loss: 0.0021 - val_direction_loss: 0.6959 - val_price_mae: 0.0334 - val_direction_accuracy: 0.4765\n",
      "Epoch 3/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6960 - price_loss: 0.0021 - direction_loss: 0.6939 - price_mae: 0.0320 - direction_accuracy: 0.4928 - val_loss: 0.6940 - val_price_loss: 0.0020 - val_direction_loss: 0.6919 - val_price_mae: 0.0338 - val_direction_accuracy: 0.5235\n",
      "Epoch 4/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6960 - price_loss: 0.0020 - direction_loss: 0.6940 - price_mae: 0.0317 - direction_accuracy: 0.4991 - val_loss: 0.6956 - val_price_loss: 0.0030 - val_direction_loss: 0.6926 - val_price_mae: 0.0442 - val_direction_accuracy: 0.5054\n",
      "Epoch 5/12\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6949 - price_loss: 0.0019 - direction_loss: 0.6930 - price_mae: 0.0306 - direction_accuracy: 0.4982 - val_loss: 0.6952 - val_price_loss: 0.0019 - val_direction_loss: 0.6933 - val_price_mae: 0.0314 - val_direction_accuracy: 0.4765\n",
      "Epoch 6/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6949 - price_loss: 0.0019 - direction_loss: 0.6930 - price_mae: 0.0301 - direction_accuracy: 0.4991 - val_loss: 0.6940 - val_price_loss: 0.0018 - val_direction_loss: 0.6922 - val_price_mae: 0.0307 - val_direction_accuracy: 0.5090\n",
      "Epoch 7/12\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6956 - price_loss: 0.0023 - direction_loss: 0.6933 - price_mae: 0.0348 - direction_accuracy: 0.5081 - val_loss: 0.6944 - val_price_loss: 0.0019 - val_direction_loss: 0.6926 - val_price_mae: 0.0325 - val_direction_accuracy: 0.4946\n",
      "Epoch 8/12\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6955 - price_loss: 0.0017 - direction_loss: 0.6938 - price_mae: 0.0288 - direction_accuracy: 0.4919 - val_loss: 0.6948 - val_price_loss: 0.0023 - val_direction_loss: 0.6925 - val_price_mae: 0.0381 - val_direction_accuracy: 0.5054\n",
      "Epoch 9/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6944 - price_loss: 0.0016 - direction_loss: 0.6928 - price_mae: 0.0280 - direction_accuracy: 0.5027 - val_loss: 0.6954 - val_price_loss: 0.0029 - val_direction_loss: 0.6925 - val_price_mae: 0.0447 - val_direction_accuracy: 0.5090\n",
      "Epoch 10/12\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.6956 - price_loss: 0.0016 - direction_loss: 0.6939 - price_mae: 0.0286 - direction_accuracy: 0.5072 - val_loss: 0.6948 - val_price_loss: 0.0026 - val_direction_loss: 0.6921 - val_price_mae: 0.0420 - val_direction_accuracy: 0.5560\n",
      "Epoch 11/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6949 - price_loss: 0.0018 - direction_loss: 0.6932 - price_mae: 0.0297 - direction_accuracy: 0.5063 - val_loss: 0.6946 - val_price_loss: 0.0017 - val_direction_loss: 0.6929 - val_price_mae: 0.0318 - val_direction_accuracy: 0.5379\n",
      "Epoch 12/12\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6937 - price_loss: 0.0016 - direction_loss: 0.6921 - price_mae: 0.0279 - direction_accuracy: 0.5145 - val_loss: 0.6938 - val_price_loss: 0.0014 - val_direction_loss: 0.6923 - val_price_mae: 0.0274 - val_direction_accuracy: 0.5090\n",
      "예측 및 평가\n",
      "\n",
      "=== 최종 결과 ===\n",
      "가격 예측 RMSE: $224.53\n",
      "가격 예측 MAPE: 5.25%\n",
      "방향 예측 Accuracy: 49.71%\n",
      "방향 예측 ROC-AUC: 0.498\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'\n",
    "ONCHAIN_FILE = 'eth_onchain.csv'\n",
    "NEWS_DIR = \"./news_data\"\n",
    "DEVICE = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n",
    "\n",
    "START_TIME = '2021-01-01'\n",
    "END_TIME = '2025-10-02'\n",
    "\n",
    "L = 7\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "LR = 5e-4\n",
    "TOP_N = 5\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    patterns = [r'(\\d{4})-(\\d{2})-(\\d{2})', r'(\\d{4})(\\d{2})(\\d{2})', r'(\\d{2})-(\\d{2})-(\\d{4})', r'(\\d{2})(\\d{2})(\\d{4})']\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    all_data = []\n",
    "    if not os.path.exists(root_dir):\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        for enc in ['utf-8','cp949','latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        if 'label' not in df.columns:\n",
    "            raise ValueError(f\"{filepath}에 'label' 컬럼이 필요합니다.\")\n",
    "        if 'news' in df.columns:\n",
    "            df = df[['date','news','label']]\n",
    "        else:\n",
    "            df = df[['date','label']]\n",
    "        all_data.append(df)\n",
    "    if len(all_data) == 0:\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.normalize()\n",
    "    return combined_df\n",
    "\n",
    "print(\"1/11 뉴스 로드 시작\")\n",
    "news_df = load_all_news_data(NEWS_DIR)\n",
    "print(f\"1/11 뉴스 로드 완료: {len(news_df)}건\")\n",
    "\n",
    "print(\"2/11 뉴스 감성 집계 시작\")\n",
    "news_df = news_df.sort_values('date')\n",
    "grouped = news_df.groupby('date')['label']\n",
    "daily = grouped.agg(sent_mean='mean', sent_count='count').reset_index().set_index('date')\n",
    "pos = grouped.apply(lambda x: (x==1).sum())\n",
    "neu = grouped.apply(lambda x: (x==0).sum())\n",
    "neg = grouped.apply(lambda x: (x==-1).sum())\n",
    "props = pd.DataFrame({'pos_cnt': pos, 'neu_cnt': neu, 'neg_cnt': neg})\n",
    "daily = daily.join(props)\n",
    "def day_entropy(row):\n",
    "    counts = np.array([row['pos_cnt'], row['neu_cnt'], row['neg_cnt']], dtype=float)\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / s\n",
    "    p_nonzero = p[p>0]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero))\n",
    "daily['sent_entropy'] = daily.apply(day_entropy, axis=1)\n",
    "daily['sent_majority'] = news_df.groupby('date')['label'].apply(lambda sub: int(np.sign(np.round(sub.mean()))) if len(sub)>0 else 0)\n",
    "daily = daily.sort_index()\n",
    "alpha = 0.4\n",
    "daily['sent_mean_ewma'] = daily['sent_mean'].ewm(alpha=alpha, adjust=False).mean()\n",
    "all_dates_news = pd.date_range(daily.index.min(), daily.index.max(), freq='D')\n",
    "daily = daily.reindex(all_dates_news).fillna({'sent_mean':0.0,'sent_count':0,'pos_cnt':0,'neu_cnt':0,'neg_cnt':0,'sent_entropy':0.0,'sent_majority':0,'sent_mean_ewma':0.0}).fillna(0)\n",
    "print(\"2/11 뉴스 감성 집계 완료\")\n",
    "\n",
    "print(\"3/11 macro 파일 로드 시작\")\n",
    "if not os.path.exists(TARGET_MACRO_FILE):\n",
    "    raise FileNotFoundError(f\"{TARGET_MACRO_FILE} 파일이 필요합니다.\")\n",
    "macro_raw = pd.read_csv(TARGET_MACRO_FILE, parse_dates=['Date'])\n",
    "macro_raw['Date'] = pd.to_datetime(macro_raw['Date']).dt.tz_localize(None).dt.normalize()\n",
    "macro_raw = macro_raw.set_index('Date').sort_index()\n",
    "print(\"3/11 macro 파일 로드 완료\")\n",
    "\n",
    "print(\"4/11 온체인 로드 시작\")\n",
    "if not os.path.exists(ONCHAIN_FILE):\n",
    "    raise FileNotFoundError(f\"{ONCHAIN_FILE} 파일이 필요합니다.\")\n",
    "onchain = pd.read_csv(ONCHAIN_FILE, parse_dates=['date']).set_index('date').sort_index()\n",
    "onchain.index = pd.to_datetime(onchain.index)\n",
    "print(\"4/11 온체인 로드 완료\")\n",
    "\n",
    "start = max(macro_raw.index.min(), onchain.index.min(), daily.index.min(), pd.to_datetime(START_TIME))\n",
    "end = min(macro_raw.index.max(), onchain.index.max(), daily.index.max(), pd.to_datetime(END_TIME))\n",
    "date_index_full = pd.date_range(start, end, freq='D')\n",
    "\n",
    "print(\"5/11 날짜 정렬 및 리인덱스 시작\")\n",
    "macro_raw = macro_raw.reindex(date_index_full).ffill().bfill()\n",
    "onchain = onchain.reindex(date_index_full).fillna(0)\n",
    "daily = daily.reindex(date_index_full).fillna(0)\n",
    "print(\"5/11 날짜 정렬 완료\")\n",
    "\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 시작\")\n",
    "eth_cols = ['ETH_Open','ETH_High','ETH_Low','ETH_Close','ETH_Volume']\n",
    "for c in eth_cols:\n",
    "    if c not in macro_raw.columns:\n",
    "        raise ValueError(f\"{c} 컬럼이 macro 파일에 필요합니다.\")\n",
    "eth_price = macro_raw[eth_cols].rename(columns={'ETH_Open':'open','ETH_High':'high','ETH_Low':'low','ETH_Close':'close','ETH_Volume':'volume'})\n",
    "\n",
    "def compute_technical_indicators(df):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    pt = df['close']\n",
    "    N = 14\n",
    "    lowN = df['low'].rolling(N).min()\n",
    "    highN = df['high'].rolling(N).max()\n",
    "    out['stoch_k'] = (pt - lowN) / (highN - lowN + 1e-9) * 100\n",
    "    out['stoch_d'] = out['stoch_k'].rolling(3).mean()\n",
    "    out['williams_r'] = (highN - pt) / (highN - lowN + 1e-9) * 100\n",
    "    out['ad_osc'] = (pt - pt.shift(1)) / (df['high'] - df['low'] + 1e-9)\n",
    "    out['momentum'] = pt - pt.shift(10)\n",
    "    out['disparity7'] = pt / pt.rolling(7).mean() * 100\n",
    "    out['roc'] = pt / pt.shift(12) * 100\n",
    "    return out.fillna(0)\n",
    "\n",
    "tech = compute_technical_indicators(eth_price)\n",
    "target_feats = pd.concat([eth_price, tech, onchain], axis=1).fillna(0)\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 완료\")\n",
    "\n",
    "print(\"7/11 top-n macro 입력 생성 시작\")\n",
    "cols = [c for c in macro_raw.columns if '_' in c]\n",
    "coins = []\n",
    "for c in cols:\n",
    "    coin = c.split('_')[0]\n",
    "    if coin not in coins:\n",
    "        coins.append(coin)\n",
    "coins = [c for c in coins if c.upper() != 'ETH']\n",
    "if len(coins) < TOP_N:\n",
    "    TOP_N = len(coins)\n",
    "selected_coins = coins[:TOP_N]\n",
    "macro_list = []\n",
    "for coin in selected_coins:\n",
    "    needed = [f\"{coin}_Open\", f\"{coin}_Close\", f\"{coin}_High\", f\"{coin}_Low\", f\"{coin}_Volume\"]\n",
    "    for n in needed:\n",
    "        if n not in macro_raw.columns:\n",
    "            raise ValueError(f\"{n} 컬럼이 macro 파일에 필요합니다.\")\n",
    "    arr = macro_raw[needed].values\n",
    "    macro_list.append(arr)\n",
    "macro_array = np.concatenate(macro_list, axis=1)\n",
    "feat_suffix = ['Open','Close','High','Low','Volume']\n",
    "feature_names = []\n",
    "for coin in selected_coins:\n",
    "    for sfx in feat_suffix:\n",
    "        feature_names.append(f\"{coin}_{sfx}\")\n",
    "if macro_array.shape[1] != len(feature_names):\n",
    "    feature_names = [f\"m{i}\" for i in range(macro_array.shape[1])]\n",
    "macro_df = pd.DataFrame(macro_array, index=date_index_full, columns=feature_names)\n",
    "print(\"7/11 top-n macro 입력 생성 완료\")\n",
    "\n",
    "print(\"8/11 sentiment feature 준비 및 병합\")\n",
    "sent_cols = ['sent_mean','sent_count','pos_cnt','neu_cnt','neg_cnt','sent_entropy','sent_mean_ewma']\n",
    "sent_df = daily[sent_cols].fillna(0)\n",
    "print(\"8/11 sentiment 준비 완료\")\n",
    "\n",
    "print(\"9/11 PPS 기반 특성 선택 시작\")\n",
    "def compute_pps(df, target_col, threshold=0.3, sample_size=500):\n",
    "    scores = {}\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.iloc[-sample_size:]\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        X_feature = df_sample[col].values.reshape(-1, 1)\n",
    "        dt = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "        try:\n",
    "            cv_scores = cross_val_score(dt, X_feature, df_sample[target_col], cv=3, scoring='r2')\n",
    "            score = max(0, cv_scores.mean())\n",
    "            scores[col] = score\n",
    "        except:\n",
    "            scores[col] = 0.0\n",
    "    \n",
    "    selected = [k for k, v in scores.items() if v >= threshold]\n",
    "    return selected, scores\n",
    "\n",
    "macro_with_eth = pd.concat([macro_df, target_feats], axis=1)\n",
    "sent_macro_all = pd.concat([sent_df, macro_with_eth], axis=1)\n",
    "\n",
    "pps_selected, pps_scores = compute_pps(sent_macro_all, 'close', threshold=0.01, sample_size=500)\n",
    "print(f\"9/11 PPS 선택 완료: {len(pps_selected)}개 특성\")\n",
    "\n",
    "print(\"10/11 정규화 및 교차 특성 생성\")\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "target_cols = ['close']\n",
    "feature_data = sent_macro_all[pps_selected].values\n",
    "target_data = sent_macro_all[target_cols].values\n",
    "\n",
    "feature_normalized = scaler_features.fit_transform(feature_data)\n",
    "target_normalized = scaler_target.fit_transform(target_data)\n",
    "\n",
    "cross_features = []\n",
    "if feature_normalized.shape[1] >= 2:\n",
    "    for i in range(min(3, feature_normalized.shape[1]-1)):\n",
    "        cross = feature_normalized[:, i] * feature_normalized[:, i+1]\n",
    "        cross_features.append(cross.reshape(-1, 1))\n",
    "\n",
    "if cross_features:\n",
    "    cross_features = np.concatenate(cross_features, axis=1)\n",
    "    feature_normalized = np.concatenate([feature_normalized, cross_features], axis=1)\n",
    "\n",
    "print(\"10/11 정규화 완료\")\n",
    "\n",
    "print(\"11/11 상관계수 기반 2차 필터링\")\n",
    "corr_threshold = 0.05\n",
    "correlations = []\n",
    "for i in range(feature_normalized.shape[1]):\n",
    "    corr = np.corrcoef(feature_normalized[:, i], target_normalized[:, 0])[0, 1]\n",
    "    correlations.append(abs(corr) if not np.isnan(corr) else 0.0)\n",
    "\n",
    "selected_indices = [i for i, c in enumerate(correlations) if c >= corr_threshold]\n",
    "if len(selected_indices) == 0:\n",
    "    selected_indices = list(range(min(10, feature_normalized.shape[1])))\n",
    "\n",
    "feature_normalized = feature_normalized[:, selected_indices]\n",
    "print(f\"11/11 최종 선택 특성 수: {feature_normalized.shape[1]}\")\n",
    "\n",
    "price_direction = np.sign(np.diff(target_data[:, 0], prepend=target_data[0, 0]))\n",
    "price_direction = np.where(price_direction > 0, 1, 0)\n",
    "\n",
    "X_seq, y_price, y_direction = [], [], []\n",
    "for i in range(L, len(feature_normalized)):\n",
    "    X_seq.append(feature_normalized[i-L:i])\n",
    "    y_price.append(target_normalized[i, 0])\n",
    "    y_direction.append(price_direction[i])\n",
    "\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "y_price = np.array(y_price, dtype=np.float32)\n",
    "y_direction = np.array(y_direction, dtype=np.float32)\n",
    "\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_price_train, y_price_test = y_price[:split_idx], y_price[split_idx:]\n",
    "y_dir_train, y_dir_test = y_direction[:split_idx], y_direction[split_idx:]\n",
    "\n",
    "print(f\"데이터 준비 완료 - 학습: {X_train.shape}, 테스트: {X_test.shape}\")\n",
    "\n",
    "print(\"모델 구축 시작\")\n",
    "input_shape = (L, feature_normalized.shape[1])\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "gru1 = layers.GRU(256, return_sequences=True)(input_layer)\n",
    "gru2 = layers.GRU(256, return_sequences=True)(gru1)\n",
    "gru3 = layers.GRU(256, return_sequences=False)(gru2)\n",
    "dense_reg = layers.Dense(128, activation='relu')(gru3)\n",
    "output_price = layers.Dense(1, activation='linear', name='price')(dense_reg)\n",
    "\n",
    "lstm1 = layers.LSTM(256, return_sequences=True)(input_layer)\n",
    "lstm2 = layers.LSTM(256, return_sequences=True)(lstm1)\n",
    "lstm3 = layers.LSTM(256, return_sequences=False)(lstm2)\n",
    "concat = layers.Concatenate()([lstm3, gru3])\n",
    "dense_cls = layers.Dense(128, activation='relu')(concat)\n",
    "dropout = layers.Dropout(0.1)(dense_cls)\n",
    "output_direction = layers.Dense(1, activation='sigmoid', name='direction')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[output_price, output_direction])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LR),\n",
    "    loss={'price': 'mse', 'direction': 'binary_crossentropy'},\n",
    "    loss_weights={'price': 1.0, 'direction': 1.0},\n",
    "    metrics={'price': 'mae', 'direction': 'accuracy'}\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "print(\"모델 구축 완료\")\n",
    "\n",
    "print(f\"학습 시작 (Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE})\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'price': y_price_train, 'direction': y_dir_train},\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"예측 및 평가\")\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "y_price_pred = predictions[0].flatten()\n",
    "y_dir_pred = (predictions[1].flatten() > 0.5).astype(int)\n",
    "\n",
    "y_price_pred_rescaled = scaler_target.inverse_transform(y_price_pred.reshape(-1, 1)).flatten()\n",
    "y_price_test_rescaled = scaler_target.inverse_transform(y_price_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_price_test_rescaled, y_price_pred_rescaled))\n",
    "mape = mean_absolute_percentage_error(y_price_test_rescaled, y_price_pred_rescaled) * 100\n",
    "accuracy = accuracy_score(y_dir_test, y_dir_pred) * 100\n",
    "roc_auc = roc_auc_score(y_dir_test, predictions[1].flatten())\n",
    "\n",
    "print(f\"\\n=== 최종 결과 ===\")\n",
    "print(f\"가격 예측 RMSE: ${rmse:.2f}\")\n",
    "print(f\"가격 예측 MAPE: {mape:.2f}%\")\n",
    "print(f\"방향 예측 Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"방향 예측 ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'actual_price': y_price_test_rescaled,\n",
    "    'predicted_price': y_price_pred_rescaled,\n",
    "    'actual_direction': y_dir_test,\n",
    "    'predicted_direction': y_dir_pred\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b416239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef33c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 데이터 로드\n",
      "1/10 데이터 로드 완료\n",
      "2/10 특성 엔지니어링\n",
      "2/10 특성 엔지니어링 완료\n",
      "3/10 macro 특성 생성\n",
      "3/10 macro 특성 생성 완료\n",
      "4/10 특성 선택\n",
      "4/10 특성 선택 완료: 25개\n",
      "5/10 타깃 생성\n",
      "5/10 타깃 생성 완료 - 상승: 1093, 하락: 643\n",
      "6/10 정규화\n",
      "6/10 정규화 완료\n",
      "7/10 시퀀스 생성\n",
      "7/10 시퀀스 생성 완료 - 학습: (1289, 10, 25), 테스트: (430, 10, 25)\n",
      "8/10 클래스 가중치 계산\n",
      "클래스 가중치: {0: 1.3742004264392325, 1: 0.7859756097560976}\n",
      "9/10 모델 구축\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 10, 25)]             0         []                            \n",
      "                                                                                                  \n",
      " gru_24 (GRU)                (None, 10, 128)              59520     ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_24 (LSTM)              (None, 10, 128)              78848     ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " gru_25 (GRU)                (None, 10, 128)              99072     ['gru_24[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_25 (LSTM)              (None, 10, 128)              131584    ['lstm_24[0][0]']             \n",
      "                                                                                                  \n",
      " gru_26 (GRU)                (None, 128)                  99072     ['gru_25[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)              (None, 128)                  131584    ['lstm_25[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 256)                  0         ['gru_26[0][0]',              \n",
      " )                                                                   'lstm_26[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 64)                   16448     ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 64)                   0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " price (Dense)               (None, 1)                    129       ['gru_26[0][0]']              \n",
      "                                                                                                  \n",
      " direction (Dense)           (None, 1)                    65        ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 616322 (2.35 MB)\n",
      "Trainable params: 616322 (2.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "10/10 학습 시작 (Epochs: 100, Batch: 32)\n",
      "Epoch 1/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4767 - price_loss: 0.0177 - direction_loss: 0.6733 - price_mae: 0.1286 - direction_accuracy: 0.6360 - direction_auc: 0.6114WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 9s 57ms/step - loss: 0.4769 - price_loss: 0.0177 - direction_loss: 0.6738 - price_mae: 0.1282 - direction_accuracy: 0.6356 - direction_auc: 0.6134 - val_loss: 0.4853 - val_price_loss: 0.0041 - val_direction_loss: 0.6916 - val_price_mae: 0.0820 - val_direction_accuracy: 0.4691 - val_direction_auc: 0.7120 - lr: 0.0010\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/invigoworks/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4586 - price_loss: 0.0027 - direction_loss: 0.6540 - price_mae: 0.0556 - direction_accuracy: 0.5978 - direction_auc: 0.6652WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.4573 - price_loss: 0.0027 - direction_loss: 0.6521 - price_mae: 0.0555 - direction_accuracy: 0.6046 - direction_auc: 0.6657 - val_loss: 0.4757 - val_price_loss: 0.0016 - val_direction_loss: 0.6789 - val_price_mae: 0.0424 - val_direction_accuracy: 0.5103 - val_direction_auc: 0.6281 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.4395 - price_loss: 0.0020 - direction_loss: 0.6269 - price_mae: 0.0481 - direction_accuracy: 0.6719 - direction_auc: 0.7120WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.4355 - price_loss: 0.0020 - direction_loss: 0.6213 - price_mae: 0.0483 - direction_accuracy: 0.6721 - direction_auc: 0.7166 - val_loss: 0.5321 - val_price_loss: 0.0013 - val_direction_loss: 0.7596 - val_price_mae: 0.0409 - val_direction_accuracy: 0.5412 - val_direction_auc: 0.5558 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.4216 - price_loss: 0.0021 - direction_loss: 0.6014 - price_mae: 0.0487 - direction_accuracy: 0.7000 - direction_auc: 0.7413WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.4258 - price_loss: 0.0021 - direction_loss: 0.6073 - price_mae: 0.0486 - direction_accuracy: 0.6868 - direction_auc: 0.7345 - val_loss: 0.5364 - val_price_loss: 9.7270e-04 - val_direction_loss: 0.7659 - val_price_mae: 0.0342 - val_direction_accuracy: 0.5412 - val_direction_auc: 0.5527 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4118 - price_loss: 0.0020 - direction_loss: 0.5875 - price_mae: 0.0480 - direction_accuracy: 0.6946 - direction_auc: 0.7561WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.4095 - price_loss: 0.0020 - direction_loss: 0.5841 - price_mae: 0.0479 - direction_accuracy: 0.6886 - direction_auc: 0.7555 - val_loss: 0.6368 - val_price_loss: 0.0012 - val_direction_loss: 0.9092 - val_price_mae: 0.0411 - val_direction_accuracy: 0.4742 - val_direction_auc: 0.5534 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4179 - price_loss: 0.0020 - direction_loss: 0.5961 - price_mae: 0.0484 - direction_accuracy: 0.6825 - direction_auc: 0.7424WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.4155 - price_loss: 0.0021 - direction_loss: 0.5927 - price_mae: 0.0494 - direction_accuracy: 0.6804 - direction_auc: 0.7468 - val_loss: 0.5758 - val_price_loss: 6.3018e-04 - val_direction_loss: 0.8223 - val_price_mae: 0.0270 - val_direction_accuracy: 0.5309 - val_direction_auc: 0.5498 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.3914 - price_loss: 0.0021 - direction_loss: 0.5582 - price_mae: 0.0496 - direction_accuracy: 0.7208 - direction_auc: 0.7881WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.3917 - price_loss: 0.0021 - direction_loss: 0.5586 - price_mae: 0.0496 - direction_accuracy: 0.7151 - direction_auc: 0.7844 - val_loss: 0.7179 - val_price_loss: 0.0014 - val_direction_loss: 1.0250 - val_price_mae: 0.0409 - val_direction_accuracy: 0.4639 - val_direction_auc: 0.5158 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3826 - price_loss: 0.0024 - direction_loss: 0.5456 - price_mae: 0.0526 - direction_accuracy: 0.7067 - direction_auc: 0.8009WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3851 - price_loss: 0.0024 - direction_loss: 0.5491 - price_mae: 0.0521 - direction_accuracy: 0.7041 - direction_auc: 0.7964 - val_loss: 0.7087 - val_price_loss: 0.0017 - val_direction_loss: 1.0116 - val_price_mae: 0.0477 - val_direction_accuracy: 0.5000 - val_direction_auc: 0.4861 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3726 - price_loss: 0.0022 - direction_loss: 0.5313 - price_mae: 0.0501 - direction_accuracy: 0.7087 - direction_auc: 0.8048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3708 - price_loss: 0.0022 - direction_loss: 0.5288 - price_mae: 0.0508 - direction_accuracy: 0.7169 - direction_auc: 0.8086 - val_loss: 0.6462 - val_price_loss: 8.6953e-04 - val_direction_loss: 0.9227 - val_price_mae: 0.0332 - val_direction_accuracy: 0.4536 - val_direction_auc: 0.4927 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3803 - price_loss: 0.0024 - direction_loss: 0.5423 - price_mae: 0.0548 - direction_accuracy: 0.7228 - direction_auc: 0.8006WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3851 - price_loss: 0.0024 - direction_loss: 0.5492 - price_mae: 0.0542 - direction_accuracy: 0.7132 - direction_auc: 0.7943 - val_loss: 0.6587 - val_price_loss: 9.1893e-04 - val_direction_loss: 0.9405 - val_price_mae: 0.0327 - val_direction_accuracy: 0.4485 - val_direction_auc: 0.4367 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3605 - price_loss: 0.0020 - direction_loss: 0.5141 - price_mae: 0.0483 - direction_accuracy: 0.7228 - direction_auc: 0.8238WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3572 - price_loss: 0.0019 - direction_loss: 0.5095 - price_mae: 0.0474 - direction_accuracy: 0.7242 - direction_auc: 0.8259 - val_loss: 0.6587 - val_price_loss: 8.7281e-04 - val_direction_loss: 0.9406 - val_price_mae: 0.0315 - val_direction_accuracy: 0.4536 - val_direction_auc: 0.4886 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.3339 - price_loss: 0.0020 - direction_loss: 0.4761 - price_mae: 0.0483 - direction_accuracy: 0.7708 - direction_auc: 0.8557WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.3375 - price_loss: 0.0020 - direction_loss: 0.4812 - price_mae: 0.0485 - direction_accuracy: 0.7653 - direction_auc: 0.8505 - val_loss: 0.7272 - val_price_loss: 0.0016 - val_direction_loss: 1.0381 - val_price_mae: 0.0443 - val_direction_accuracy: 0.4536 - val_direction_auc: 0.4838 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3178 - price_loss: 0.0017 - direction_loss: 0.4533 - price_mae: 0.0461 - direction_accuracy: 0.7812 - direction_auc: 0.8703WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3177 - price_loss: 0.0017 - direction_loss: 0.4531 - price_mae: 0.0461 - direction_accuracy: 0.7808 - direction_auc: 0.8701 - val_loss: 0.8042 - val_price_loss: 0.0016 - val_direction_loss: 1.1482 - val_price_mae: 0.0433 - val_direction_accuracy: 0.4639 - val_direction_auc: 0.4802 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3280 - price_loss: 0.0020 - direction_loss: 0.4677 - price_mae: 0.0482 - direction_accuracy: 0.7863 - direction_auc: 0.8588WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3280 - price_loss: 0.0020 - direction_loss: 0.4677 - price_mae: 0.0482 - direction_accuracy: 0.7863 - direction_auc: 0.8588 - val_loss: 0.9022 - val_price_loss: 0.0012 - val_direction_loss: 1.2883 - val_price_mae: 0.0383 - val_direction_accuracy: 0.5206 - val_direction_auc: 0.5012 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.3157 - price_loss: 0.0021 - direction_loss: 0.4501 - price_mae: 0.0487 - direction_accuracy: 0.7875 - direction_auc: 0.8693WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3110 - price_loss: 0.0020 - direction_loss: 0.4434 - price_mae: 0.0483 - direction_accuracy: 0.7881 - direction_auc: 0.8736 - val_loss: 0.7768 - val_price_loss: 0.0015 - val_direction_loss: 1.1091 - val_price_mae: 0.0431 - val_direction_accuracy: 0.4897 - val_direction_auc: 0.4897 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3177 - price_loss: 0.0021 - direction_loss: 0.4529 - price_mae: 0.0482 - direction_accuracy: 0.7913 - direction_auc: 0.8636WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.3154 - price_loss: 0.0021 - direction_loss: 0.4497 - price_mae: 0.0491 - direction_accuracy: 0.7909 - direction_auc: 0.8666 - val_loss: 0.7952 - val_price_loss: 0.0018 - val_direction_loss: 1.1352 - val_price_mae: 0.0474 - val_direction_accuracy: 0.5361 - val_direction_auc: 0.4975 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3179 - price_loss: 0.0019 - direction_loss: 0.4534 - price_mae: 0.0472 - direction_accuracy: 0.7792 - direction_auc: 0.8695WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.3222 - price_loss: 0.0021 - direction_loss: 0.4594 - price_mae: 0.0487 - direction_accuracy: 0.7781 - direction_auc: 0.8660 - val_loss: 0.7548 - val_price_loss: 0.0016 - val_direction_loss: 1.0775 - val_price_mae: 0.0468 - val_direction_accuracy: 0.4691 - val_direction_auc: 0.4836 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.3029 - price_loss: 0.0017 - direction_loss: 0.4320 - price_mae: 0.0451 - direction_accuracy: 0.7913 - direction_auc: 0.8798WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3019 - price_loss: 0.0017 - direction_loss: 0.4305 - price_mae: 0.0453 - direction_accuracy: 0.7881 - direction_auc: 0.8803 - val_loss: 0.7998 - val_price_loss: 0.0015 - val_direction_loss: 1.1419 - val_price_mae: 0.0449 - val_direction_accuracy: 0.4845 - val_direction_auc: 0.4839 - lr: 2.5000e-04\n",
      "예측 및 평가\n",
      "\n",
      "=== 최종 결과 ===\n",
      "학습 기간: 2021-01-01 ~ 2025-10-02\n",
      "총 샘플 수: 1719개\n",
      "가격 예측 RMSE: $219.09\n",
      "가격 예측 MAPE: 6.44%\n",
      "방향 예측 Accuracy: 59.53%\n",
      "방향 예측 ROC-AUC: 0.712\n",
      "\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          하락       0.49      0.76      0.60       169\n",
      "          상승       0.76      0.49      0.59       261\n",
      "\n",
      "    accuracy                           0.60       430\n",
      "   macro avg       0.63      0.62      0.60       430\n",
      "weighted avg       0.65      0.60      0.59       430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, mean_absolute_percentage_error, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'\n",
    "ONCHAIN_FILE = 'eth_onchain.csv'\n",
    "DEVICE = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n",
    "\n",
    "START_TIME = '2021-01-01'\n",
    "END_TIME = '2025-10-02'\n",
    "\n",
    "L = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "TOP_N = 10\n",
    "\n",
    "print(\"1/10 데이터 로드\")\n",
    "if not os.path.exists(TARGET_MACRO_FILE):\n",
    "    raise FileNotFoundError(f\"{TARGET_MACRO_FILE} 파일이 필요합니다.\")\n",
    "macro_raw = pd.read_csv(TARGET_MACRO_FILE, parse_dates=['Date'])\n",
    "macro_raw['Date'] = pd.to_datetime(macro_raw['Date']).dt.tz_localize(None).dt.normalize()\n",
    "macro_raw = macro_raw.set_index('Date').sort_index()\n",
    "\n",
    "if not os.path.exists(ONCHAIN_FILE):\n",
    "    raise FileNotFoundError(f\"{ONCHAIN_FILE} 파일이 필요합니다.\")\n",
    "onchain = pd.read_csv(ONCHAIN_FILE, parse_dates=['date']).set_index('date').sort_index()\n",
    "onchain.index = pd.to_datetime(onchain.index)\n",
    "\n",
    "start = max(macro_raw.index.min(), onchain.index.min(), pd.to_datetime(START_TIME))\n",
    "end = min(macro_raw.index.max(), onchain.index.max(), pd.to_datetime(END_TIME))\n",
    "date_index_full = pd.date_range(start, end, freq='D')\n",
    "\n",
    "macro_raw = macro_raw.reindex(date_index_full).ffill().bfill()\n",
    "onchain = onchain.reindex(date_index_full).fillna(0)\n",
    "print(\"1/10 데이터 로드 완료\")\n",
    "\n",
    "print(\"2/10 특성 엔지니어링\")\n",
    "eth_cols = ['ETH_Open','ETH_High','ETH_Low','ETH_Close','ETH_Volume']\n",
    "for c in eth_cols:\n",
    "    if c not in macro_raw.columns:\n",
    "        raise ValueError(f\"{c} 컬럼이 macro 파일에 필요합니다.\")\n",
    "eth_price = macro_raw[eth_cols].rename(columns={'ETH_Open':'open','ETH_High':'high','ETH_Low':'low','ETH_Close':'close','ETH_Volume':'volume'})\n",
    "\n",
    "def compute_all_features(df):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    pt = df['close']\n",
    "    \n",
    "    for period in [7, 14, 30, 60, 90]:\n",
    "        out[f'sma_{period}'] = pt.rolling(period).mean()\n",
    "        out[f'ema_{period}'] = pt.ewm(span=period, adjust=False).mean()\n",
    "        out[f'std_{period}'] = pt.rolling(period).std()\n",
    "        out[f'returns_{period}'] = pt.pct_change(period)\n",
    "    \n",
    "    out['bb_upper'] = pt.rolling(20).mean() + 2 * pt.rolling(20).std()\n",
    "    out['bb_lower'] = pt.rolling(20).mean() - 2 * pt.rolling(20).std()\n",
    "    out['bb_position'] = (pt - out['bb_lower']) / (out['bb_upper'] - out['bb_lower'] + 1e-9)\n",
    "    \n",
    "    N = 14\n",
    "    lowN = df['low'].rolling(N).min()\n",
    "    highN = df['high'].rolling(N).max()\n",
    "    out['stoch_k'] = (pt - lowN) / (highN - lowN + 1e-9) * 100\n",
    "    out['stoch_d'] = out['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    delta = pt.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / (loss + 1e-9)\n",
    "    out['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    exp12 = pt.ewm(span=12, adjust=False).mean()\n",
    "    exp26 = pt.ewm(span=26, adjust=False).mean()\n",
    "    out['macd'] = exp12 - exp26\n",
    "    out['macd_signal'] = out['macd'].ewm(span=9, adjust=False).mean()\n",
    "    out['macd_hist'] = out['macd'] - out['macd_signal']\n",
    "    \n",
    "    out['atr'] = (df['high'] - df['low']).rolling(14).mean()\n",
    "    out['adx'] = 50.0\n",
    "    \n",
    "    out['obv'] = (np.sign(pt.diff()) * df['volume']).cumsum()\n",
    "    out['vwap'] = (pt * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "    \n",
    "    return out.fillna(method='ffill').fillna(0)\n",
    "\n",
    "tech = compute_all_features(eth_price)\n",
    "target_feats = pd.concat([eth_price, tech, onchain], axis=1).fillna(method='ffill').fillna(0)\n",
    "print(\"2/10 특성 엔지니어링 완료\")\n",
    "\n",
    "print(\"3/10 macro 특성 생성\")\n",
    "cols = [c for c in macro_raw.columns if '_' in c]\n",
    "coins = []\n",
    "for c in cols:\n",
    "    coin = c.split('_')[0]\n",
    "    if coin not in coins:\n",
    "        coins.append(coin)\n",
    "coins = [c for c in coins if c.upper() != 'ETH']\n",
    "if len(coins) < TOP_N:\n",
    "    TOP_N = len(coins)\n",
    "selected_coins = coins[:TOP_N]\n",
    "\n",
    "macro_features = []\n",
    "for coin in selected_coins:\n",
    "    needed = [f\"{coin}_Close\", f\"{coin}_Volume\"]\n",
    "    if all(n in macro_raw.columns for n in needed):\n",
    "        coin_data = macro_raw[needed].copy()\n",
    "        coin_data[f'{coin}_returns'] = coin_data[f\"{coin}_Close\"].pct_change()\n",
    "        coin_data[f'{coin}_vol_ma'] = coin_data[f\"{coin}_Volume\"].rolling(7).mean()\n",
    "        macro_features.append(coin_data)\n",
    "\n",
    "if macro_features:\n",
    "    macro_df = pd.concat(macro_features, axis=1).fillna(method='ffill').fillna(0)\n",
    "else:\n",
    "    macro_df = pd.DataFrame(index=date_index_full)\n",
    "print(\"3/10 macro 특성 생성 완료\")\n",
    "\n",
    "print(\"4/10 특성 선택\")\n",
    "all_features = pd.concat([target_feats, macro_df], axis=1)\n",
    "all_features = all_features.loc[:, ~all_features.columns.duplicated()]\n",
    "\n",
    "def adaptive_feature_selection(df, target_col, initial_threshold=0.2, min_features=20):\n",
    "    scores = {}\n",
    "    sample_data = df.iloc[-1000:] if len(df) > 1000 else df\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == target_col or col in ['open', 'high', 'low', 'volume']:\n",
    "            continue\n",
    "        try:\n",
    "            X = sample_data[col].values.reshape(-1, 1)\n",
    "            y = sample_data[target_col].values\n",
    "            dt = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "            score = cross_val_score(dt, X, y, cv=3, scoring='r2').mean()\n",
    "            scores[col] = max(0, score)\n",
    "        except:\n",
    "            scores[col] = 0.0\n",
    "    \n",
    "    sorted_features = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    threshold = initial_threshold\n",
    "    while threshold > 0.01:\n",
    "        selected = [k for k, v in sorted_features if v >= threshold]\n",
    "        if len(selected) >= min_features:\n",
    "            break\n",
    "        threshold -= 0.02\n",
    "    \n",
    "    if len(selected) < min_features:\n",
    "        selected = [k for k, v in sorted_features[:min_features]]\n",
    "    \n",
    "    return selected, scores\n",
    "\n",
    "selected_features, feature_scores = adaptive_feature_selection(all_features, 'close', initial_threshold=0.2, min_features=25)\n",
    "print(f\"4/10 특성 선택 완료: {len(selected_features)}개\")\n",
    "\n",
    "print(\"5/10 타깃 생성\")\n",
    "close_prices = all_features['close'].values\n",
    "returns_future = np.zeros(len(close_prices))\n",
    "for i in range(len(close_prices) - 7):\n",
    "    returns_future[i] = (close_prices[i+7] - close_prices[i]) / close_prices[i]\n",
    "\n",
    "threshold_buy = 0.02\n",
    "threshold_sell = -0.02\n",
    "\n",
    "direction_labels = np.zeros(len(returns_future))\n",
    "direction_labels[returns_future > threshold_buy] = 1\n",
    "direction_labels[returns_future < threshold_sell] = 0\n",
    "direction_labels[(returns_future >= threshold_sell) & (returns_future <= threshold_buy)] = 0.5\n",
    "\n",
    "binary_labels = (direction_labels > 0.25).astype(int)\n",
    "print(f\"5/10 타깃 생성 완료 - 상승: {(binary_labels==1).sum()}, 하락: {(binary_labels==0).sum()}\")\n",
    "\n",
    "print(\"6/10 정규화\")\n",
    "scaler = RobustScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "feature_data = all_features[selected_features].values\n",
    "target_data = all_features[['close']].values\n",
    "\n",
    "feature_normalized = scaler.fit_transform(feature_data)\n",
    "target_normalized = scaler_target.fit_transform(target_data)\n",
    "print(\"6/10 정규화 완료\")\n",
    "\n",
    "print(\"7/10 시퀀스 생성\")\n",
    "X_seq, y_price, y_direction = [], [], []\n",
    "for i in range(L, len(feature_normalized) - 7):\n",
    "    X_seq.append(feature_normalized[i-L:i])\n",
    "    y_price.append(target_normalized[i, 0])\n",
    "    y_direction.append(binary_labels[i])\n",
    "\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "y_price = np.array(y_price, dtype=np.float32)\n",
    "y_direction = np.array(y_direction, dtype=np.float32)\n",
    "\n",
    "split_idx = int(0.75 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_price_train, y_price_test = y_price[:split_idx], y_price[split_idx:]\n",
    "y_dir_train, y_dir_test = y_direction[:split_idx], y_direction[split_idx:]\n",
    "print(f\"7/10 시퀀스 생성 완료 - 학습: {X_train.shape}, 테스트: {X_test.shape}\")\n",
    "\n",
    "print(\"8/10 클래스 가중치 계산\")\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_dir_train), y=y_dir_train)\n",
    "class_weight_map = {int(cls): weight for cls, weight in zip(np.unique(y_dir_train), class_weights_array)}\n",
    "sample_weights_train = np.array([class_weight_map[int(y)] for y in y_dir_train])\n",
    "print(f\"클래스 가중치: {class_weight_map}\")\n",
    "\n",
    "print(\"9/10 모델 구축\")\n",
    "input_shape = (L, feature_normalized.shape[1])\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "x = layers.GRU(128, return_sequences=True, dropout=0.2)(input_layer)\n",
    "x = layers.GRU(128, return_sequences=True, dropout=0.2)(x)\n",
    "x = layers.GRU(128, return_sequences=False, dropout=0.2)(x)\n",
    "price_output = layers.Dense(1, activation='linear', name='price')(x)\n",
    "\n",
    "y = layers.LSTM(128, return_sequences=True, dropout=0.2)(input_layer)\n",
    "y = layers.LSTM(128, return_sequences=True, dropout=0.2)(y)\n",
    "y = layers.LSTM(128, return_sequences=False, dropout=0.2)(y)\n",
    "combined = layers.Concatenate()([x, y])\n",
    "dir_dense = layers.Dense(64, activation='relu')(combined)\n",
    "dir_dense = layers.Dropout(0.3)(dir_dense)\n",
    "direction_output = layers.Dense(1, activation='sigmoid', name='direction')(dir_dense)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[price_output, direction_output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LR),\n",
    "    loss={'price': 'huber', 'direction': 'binary_crossentropy'},\n",
    "    loss_weights={'price': 0.3, 'direction': 0.7},\n",
    "    metrics={'price': 'mae', 'direction': ['accuracy', tf.keras.metrics.AUC(name='auc')]}\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_direction_accuracy', patience=15, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_direction_accuracy', factor=0.5, patience=7, mode='max', min_lr=1e-6),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_direction_accuracy', mode='max', save_best_only=True)\n",
    "]\n",
    "\n",
    "print(f\"10/10 학습 시작 (Epochs: {EPOCHS}, Batch: {BATCH_SIZE})\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'price': y_price_train, 'direction': y_dir_train},\n",
    "    sample_weight={'price': np.ones(len(y_price_train)), 'direction': sample_weights_train},\n",
    "    validation_split=0.15,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"예측 및 평가\")\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "y_price_pred = predictions[0].flatten()\n",
    "y_dir_pred_prob = predictions[1].flatten()\n",
    "y_dir_pred = (y_dir_pred_prob > 0.5).astype(int)\n",
    "\n",
    "y_price_pred_rescaled = scaler_target.inverse_transform(y_price_pred.reshape(-1, 1)).flatten()\n",
    "y_price_test_rescaled = scaler_target.inverse_transform(y_price_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_price_test_rescaled, y_price_pred_rescaled))\n",
    "mape = mean_absolute_percentage_error(y_price_test_rescaled, y_price_pred_rescaled) * 100\n",
    "accuracy = accuracy_score(y_dir_test, y_dir_pred) * 100\n",
    "roc_auc = roc_auc_score(y_dir_test, y_dir_pred_prob)\n",
    "\n",
    "print(f\"\\n=== 최종 결과 ===\")\n",
    "print(f\"학습 기간: {START_TIME} ~ {END_TIME}\")\n",
    "print(f\"총 샘플 수: {len(X_seq)}개\")\n",
    "print(f\"가격 예측 RMSE: ${rmse:.2f}\")\n",
    "print(f\"가격 예측 MAPE: {mape:.2f}%\")\n",
    "print(f\"방향 예측 Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"방향 예측 ROC-AUC: {roc_auc:.3f}\")\n",
    "print(\"\\n분류 리포트:\")\n",
    "print(classification_report(y_dir_test, y_dir_pred, target_names=['하락', '상승']))\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'actual_price': y_price_test_rescaled,\n",
    "    'predicted_price': y_price_pred_rescaled,\n",
    "    'actual_direction': y_dir_test,\n",
    "    'predicted_direction': y_dir_pred,\n",
    "    'predicted_probability': y_dir_pred_prob\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced138d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
