{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91074aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토큰 로드 성공: hf_VQiZxYh...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6925c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23319ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.21.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (0.35.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: psutil in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: filelock in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.64.1)\n",
      "Requirement already satisfied: requests in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: networkx in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: triton==2.3.1 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=2.0.0->accelerate) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d35d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토큰 로드 성공: hf_VQiZxYh...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if token:\n",
    "    print(f\"✅ 토큰 로드 성공: {token[:10]}...\")\n",
    "else:\n",
    "    print(\"❌ 토큰을 불러올 수 없습니다.\")\n",
    "    print(\"   .env 파일 형식을 확인하세요:\")\n",
    "    print(\"   HUGGINGFACE_TOKEN=hf_xxxxx (공백 없이)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392b3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 환경: GPU\n",
      "TensorFlow 버전: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "from binance.client import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import login as hf_login\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "####################################################\n",
    "# 설정\n",
    "####################################################\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'\n",
    "ONCHAIN_FILE = 'eth_onchain.csv'\n",
    "NEWS_DIR = \"./news_data\"\n",
    "START_TIME = '2017-01-01'\n",
    "END_TIME = '2025-10-02'\n",
    "TARGET_COIN = 'ETH'  # 이더리움 타겟\n",
    "DEVICE = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n",
    "SEQUENCE_LENGTH = 30\n",
    "PREDICTION_HORIZON = 1\n",
    "\n",
    "\n",
    "print(f\"실행 환경: {DEVICE}\")\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "# 1. 데이터 로드 함수\n",
    "####################################################\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    \"\"\"파일명에서 날짜 추출\"\"\"\n",
    "    patterns = [\n",
    "        r'(\\d{4})-(\\d{2})-(\\d{2})',\n",
    "        r'(\\d{4})(\\d{2})(\\d{2})',\n",
    "        r'(\\d{2})-(\\d{2})-(\\d{4})',\n",
    "        r'(\\d{2})(\\d{2})(\\d{4})'\n",
    "    ]\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    \"\"\"뉴스 데이터 로드 및 일간 집계\"\"\"\n",
    "    print(\"\\n=== 뉴스 데이터 로드 ===\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"경고: {root_dir} 디렉토리가 존재하지 않습니다. 더미 데이터를 생성합니다.\")\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'news': ['test news'] * len(dates),\n",
    "            'label': np.random.choice([1, 0, -1], len(dates))\n",
    "        })\n",
    "    \n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    print(f\"발견된 뉴스 파일: {len(csv_files)}개\")\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        \n",
    "        # 여러 인코딩 시도\n",
    "        for enc in ['utf-8', 'cp949', 'latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  건너뜀: {filename} (인코딩 실패)\")\n",
    "            continue\n",
    "        \n",
    "        # 날짜 컬럼 처리\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        \n",
    "        # label 컬럼 확인\n",
    "        if 'label' not in df.columns:\n",
    "            print(f\"  경고: {filename}에 'label' 컬럼이 없습니다. 0으로 채웁니다.\")\n",
    "            df['label'] = 0\n",
    "        \n",
    "        # 필요한 컬럼만 선택\n",
    "        if 'news' in df.columns:\n",
    "            df = df[['date', 'news', 'label']]\n",
    "        else:\n",
    "            df = df[['date', 'label']]\n",
    "        \n",
    "        all_data.append(df)\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"경고: 유효한 뉴스 파일을 찾을 수 없습니다. 더미 데이터를 생성합니다.\")\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'news': ['test news'] * len(dates),\n",
    "            'label': np.random.choice([1, 0, -1], len(dates))\n",
    "        })\n",
    "    \n",
    "    # 데이터 결합\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.normalize()\n",
    "    \n",
    "    # 일간 감성 스코어 집계 (평균)\n",
    "    news_agg = combined_df.groupby('date').agg({\n",
    "        'label': 'mean',  # 감성 스코어 평균\n",
    "    }).reset_index()\n",
    "    news_agg.columns = ['date', 'sentiment_score']\n",
    "    \n",
    "    # 추가 통계\n",
    "    news_count = combined_df.groupby('date').size().reset_index(name='news_count')\n",
    "    news_agg = news_agg.merge(news_count, on='date', how='left')\n",
    "    \n",
    "    print(f\" 뉴스 데이터 로드 완료: {len(news_agg)}일치 데이터\")\n",
    "    return news_agg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_macro_data(filepath):\n",
    "    \"\"\"거시경제 + 암호화폐 가격 데이터 로드\"\"\"\n",
    "    print(\"\\n=== Macro 데이터 로드 ===\")\n",
    "    \n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'])\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None).dt.normalize()\n",
    "    df = df.set_index('Date').sort_index()\n",
    "    \n",
    "    # 타겟 코인(ETH) 컬럼 확인\n",
    "    eth_cols = [col for col in df.columns if col.startswith('ETH_')]\n",
    "    print(f\"발견된 ETH 컬럼: {eth_cols}\")\n",
    "    \n",
    "    print(f\"✓ Macro 데이터 로드 완료: {df.shape}\")\n",
    "    print(f\"  기간: {df.index.min()} ~ {df.index.max()}\")\n",
    "    print(f\"  컬럼 수: {len(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_onchain_data(filepath):\n",
    "    \"\"\"온체인 데이터 로드\"\"\"\n",
    "    print(\"\\n=== 온체인 데이터 로드 ===\")\n",
    "    \n",
    "    df = pd.read_csv(filepath, parse_dates=['date'])\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None).dt.normalize()\n",
    "    df = df.set_index('date').sort_index()\n",
    "    \n",
    "    # 컬럼명에 'onchain_' 접두사 추가 (충돌 방지)\n",
    "    df.columns = ['onchain_' + col for col in df.columns]\n",
    "    \n",
    "    print(f\"✓ 온체인 데이터 로드 완료: {df.shape}\")\n",
    "    print(f\"  기간: {df.index.min()} ~ {df.index.max()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33361037",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# 2. 기술적 지표 계산\n",
    "####################################################\n",
    "\n",
    "def calculate_technical_indicators(df, target_coin='ETH'):\n",
    "    \"\"\"\n",
    "    pandas_ta를 사용하여 기술적 지표 계산\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {target_coin} 기술적 지표 계산 ===\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 타겟 코인의 OHLCV 컬럼 추출\n",
    "    prefix = f\"{target_coin}_\"\n",
    "    open_col = f\"{prefix}Open\"\n",
    "    high_col = f\"{prefix}High\"\n",
    "    low_col = f\"{prefix}Low\"\n",
    "    close_col = f\"{prefix}Close\"\n",
    "    volume_col = f\"{prefix}Volume\"\n",
    "    \n",
    "    # 필수 컬럼 확인\n",
    "    required_cols = [open_col, high_col, low_col, close_col, volume_col]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"경고: 필수 컬럼 누락: {missing_cols}\")\n",
    "        return df\n",
    "    \n",
    "    # pandas_ta를 위한 임시 DataFrame 생성 (소문자 컬럼명 사용)\n",
    "    temp_df = pd.DataFrame({\n",
    "        'open': df[open_col],\n",
    "        'high': df[high_col],\n",
    "        'low': df[low_col],\n",
    "        'close': df[close_col],\n",
    "        'volume': df[volume_col]\n",
    "    })\n",
    "    \n",
    "    print(\"기술적 지표 계산 중...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Momentum Indicators\n",
    "        print(\"  - Momentum 지표...\")\n",
    "        temp_df.ta.rsi(length=14, append=True)\n",
    "        temp_df.ta.rsi(length=7, append=True)\n",
    "        temp_df.ta.rsi(length=21, append=True)\n",
    "        temp_df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "        temp_df.ta.stoch(k=14, d=3, append=True)\n",
    "        temp_df.ta.roc(length=10, append=True)\n",
    "        temp_df.ta.roc(length=20, append=True)\n",
    "        temp_df.ta.mom(length=10, append=True)\n",
    "        temp_df.ta.cci(length=20, append=True)\n",
    "        temp_df.ta.willr(length=14, append=True)\n",
    "        \n",
    "        # 2. Volatility Indicators\n",
    "        print(\"  - Volatility 지표...\")\n",
    "        temp_df.ta.bbands(length=20, std=2, append=True)\n",
    "        temp_df.ta.atr(length=14, append=True)\n",
    "        temp_df.ta.natr(length=14, append=True)\n",
    "        temp_df.ta.kc(length=20, append=True)\n",
    "        \n",
    "        # 3. Trend Indicators\n",
    "        print(\"  - Trend 지표...\")\n",
    "        for length in [5, 10, 20, 50, 200]:\n",
    "            temp_df.ta.sma(length=length, append=True)\n",
    "            temp_df.ta.ema(length=length, append=True)\n",
    "        \n",
    "        temp_df.ta.adx(length=14, append=True)\n",
    "        temp_df.ta.aroon(length=25, append=True)\n",
    "        \n",
    "        # 4. Volume Indicators\n",
    "        print(\"  - Volume 지표...\")\n",
    "        temp_df.ta.obv(append=True)\n",
    "        temp_df.ta.ad(append=True)\n",
    "        temp_df.ta.cmf(length=20, append=True)\n",
    "        temp_df.ta.mfi(length=14, append=True)\n",
    "        \n",
    "        # 5. Custom Features\n",
    "        print(\"  - Custom 지표...\")\n",
    "        \n",
    "        # SMA/EMA 차이\n",
    "        if 'SMA_50' in temp_df.columns and 'SMA_200' in temp_df.columns:\n",
    "            temp_df['SMA_diff_50_200'] = temp_df['SMA_50'] - temp_df['SMA_200']\n",
    "        \n",
    "        if 'EMA_50' in temp_df.columns and 'EMA_200' in temp_df.columns:\n",
    "            temp_df['EMA_diff_50_200'] = temp_df['EMA_50'] - temp_df['EMA_200']\n",
    "        \n",
    "        # Bollinger Band %B\n",
    "        if 'BBL_20_2.0' in temp_df.columns and 'BBU_20_2.0' in temp_df.columns:\n",
    "            bb_width = temp_df['BBU_20_2.0'] - temp_df['BBL_20_2.0']\n",
    "            bb_width = bb_width.replace(0, np.nan)  # 0으로 나누기 방지\n",
    "            temp_df['BB_percentB'] = (temp_df['close'] - temp_df['BBL_20_2.0']) / bb_width\n",
    "        \n",
    "        # Price Rate of Change\n",
    "        temp_df['price_roc_1d'] = temp_df['close'].pct_change(1)\n",
    "        temp_df['price_roc_7d'] = temp_df['close'].pct_change(7)\n",
    "        temp_df['price_roc_30d'] = temp_df['close'].pct_change(30)\n",
    "        \n",
    "        # Volume Rate of Change\n",
    "        temp_df['volume_roc_1d'] = temp_df['volume'].pct_change(1)\n",
    "        temp_df['volume_roc_7d'] = temp_df['volume'].pct_change(7)\n",
    "        \n",
    "        # High-Low Range\n",
    "        temp_df['hl_range'] = (temp_df['high'] - temp_df['low']) / temp_df['close']\n",
    "        \n",
    "        # Close position within range\n",
    "        range_val = temp_df['high'] - temp_df['low']\n",
    "        range_val = range_val.replace(0, np.nan)\n",
    "        temp_df['close_position'] = (temp_df['close'] - temp_df['low']) / range_val\n",
    "        \n",
    "        # Moving Average Convergence\n",
    "        if 'SMA_20' in temp_df.columns:\n",
    "            temp_df['price_to_sma20'] = temp_df['close'] / temp_df['SMA_20'] - 1\n",
    "        \n",
    "        if 'EMA_20' in temp_df.columns:\n",
    "            temp_df['price_to_ema20'] = temp_df['close'] / temp_df['EMA_20'] - 1\n",
    "        \n",
    "        # Volume trend\n",
    "        if 'SMA_20' in temp_df.columns:\n",
    "            volume_sma = temp_df['volume'].rolling(window=20).mean()\n",
    "            temp_df['volume_trend'] = temp_df['volume'] / volume_sma\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  경고: 일부 지표 계산 실패: {e}\")\n",
    "    \n",
    "    # 원본 OHLCV 컬럼 제거 (중복 방지)\n",
    "    tech_indicators = temp_df.drop(columns=['open', 'high', 'low', 'close', 'volume'], errors='ignore')\n",
    "    \n",
    "    # 컬럼명에 접두사 추가\n",
    "    tech_indicators.columns = [f'tech_{col}' for col in tech_indicators.columns]\n",
    "    \n",
    "    # 무한대 값 처리\n",
    "    tech_indicators = tech_indicators.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 원본 데이터에 병합\n",
    "    df = pd.concat([df, tech_indicators], axis=1)\n",
    "    \n",
    "    print(f\"✓ 기술적 지표 계산 완료: {len(tech_indicators.columns)}개 지표 추가\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2954f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# 3. 거시경제 지표 추가 (선택적)\n",
    "####################################################\n",
    "\n",
    "def add_macro_indicators(df):\n",
    "    \"\"\"\n",
    "    yfinance와 fredapi를 통해 거시경제 지표 추가\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 거시경제 지표 추가 (선택적) ===\")\n",
    "    \n",
    "    try:\n",
    "        import yfinance as yf\n",
    "        \n",
    "        start_date = df.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = df.index.max().strftime('%Y-%m-%d')\n",
    "        \n",
    "        macro_data = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # DXY (US Dollar Index)\n",
    "        try:\n",
    "            dxy = yf.download('DX-Y.NYB', start=start_date, end=end_date, progress=False)['Close']\n",
    "            macro_data['macro_DXY'] = dxy\n",
    "            print(\"  ✓ DXY 추가\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ DXY 실패: {e}\")\n",
    "        \n",
    "        # Gold\n",
    "        try:\n",
    "            gold = yf.download('GC=F', start=start_date, end=end_date, progress=False)['Close']\n",
    "            macro_data['macro_GOLD'] = gold\n",
    "            print(\"  ✓ GOLD 추가\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ GOLD 실패: {e}\")\n",
    "        \n",
    "        # VIX\n",
    "        try:\n",
    "            vix = yf.download('^VIX', start=start_date, end=end_date, progress=False)['Close']\n",
    "            macro_data['macro_VIX'] = vix\n",
    "            print(\"  ✓ VIX 추가\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ VIX 실패: {e}\")\n",
    "        \n",
    "        # S&P 500\n",
    "        try:\n",
    "            sp500 = yf.download('^GSPC', start=start_date, end=end_date, progress=False)['Close']\n",
    "            macro_data['macro_SP500'] = sp500\n",
    "            print(\"  ✓ S&P500 추가\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ S&P500 실패: {e}\")\n",
    "        \n",
    "        # Forward fill missing values\n",
    "        macro_data = macro_data.ffill()\n",
    "        \n",
    "        # 원본 데이터에 병합\n",
    "        df = df.join(macro_data, how='left')\n",
    "        \n",
    "        print(f\"✓ 거시경제 지표 추가 완료: {len([col for col in df.columns if col.startswith('macro_')])}개\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  경고: yfinance가 설치되지 않았습니다. 거시경제 지표를 건너뜁니다.\")\n",
    "        print(\"  설치: pip install yfinance\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "####################################################\n",
    "# 4. 선물 시장 데이터 추가 (선택적)\n",
    "####################################################\n",
    "\n",
    "def add_futures_data(df, symbol='ETHUSDT'):\n",
    "    \"\"\"\n",
    "    Binance API를 통해 선물 시장 데이터 추가\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 선물 시장 데이터 추가 (선택적) ===\")\n",
    "    \n",
    "    try:\n",
    "        from binance.client import Client\n",
    "        \n",
    "        client = Client(\"\", \"\")  # API Key 없이 공개 데이터 접근\n",
    "        \n",
    "        start_date = df.index.min()\n",
    "        end_date = df.index.max()\n",
    "        \n",
    "        print(\"  Funding Rate 수집 중...\")\n",
    "        funding_rates = []\n",
    "        \n",
    "        start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "        end_ts = int(pd.Timestamp(end_date).timestamp() * 1000)\n",
    "        \n",
    "        current_ts = start_ts\n",
    "        chunk_count = 0\n",
    "        \n",
    "        while current_ts < end_ts and chunk_count < 10:  # 최대 10 청크\n",
    "            try:\n",
    "                rates = client.futures_funding_rate(\n",
    "                    symbol=symbol,\n",
    "                    startTime=current_ts,\n",
    "                    limit=1000\n",
    "                )\n",
    "                if not rates:\n",
    "                    break\n",
    "                funding_rates.extend(rates)\n",
    "                current_ts = rates[-1]['fundingTime'] + 1\n",
    "                chunk_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"    청크 수집 실패: {e}\")\n",
    "                break\n",
    "        \n",
    "        if funding_rates:\n",
    "            df_funding = pd.DataFrame(funding_rates)\n",
    "            df_funding['fundingTime'] = pd.to_datetime(df_funding['fundingTime'], unit='ms')\n",
    "            df_funding.set_index('fundingTime', inplace=True)\n",
    "            df_funding['fundingRate'] = df_funding['fundingRate'].astype(float)\n",
    "            \n",
    "            # 일간 평균으로 리샘플링\n",
    "            funding_daily = df_funding['fundingRate'].resample('D').mean()\n",
    "            df['futures_funding_rate'] = funding_daily\n",
    "            \n",
    "            print(f\"  ✓ Funding Rate 추가: {len(df_funding)} 개 데이터\")\n",
    "        else:\n",
    "            print(\"  ✗ Funding Rate 데이터 없음\")\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"  경고: python-binance가 설치되지 않았습니다. 선물 데이터를 건너뜁니다.\")\n",
    "        print(\"  설치: pip install python-binance\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ 선물 데이터 수집 실패: {e}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99950643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf83599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89405f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d65319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 15:47:32.275910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-04 15:47:32.275954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-04 15:47:32.277181: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-04 15:47:32.283868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 15:47:33.031437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Enhanced Directional Prediction Model\n",
      "======================================================================\n",
      "\n",
      "Period: 2022-09-15 to 2025-10-02\n",
      "\n",
      "Step 1: Loading Data\n",
      "Samples: 1115\n",
      "\n",
      "Step 2: Feature Selection\n",
      "Clean samples: 1115\n",
      "Initial features: 86\n",
      "Selected features: 40\n",
      "\n",
      "Top features:\n",
      " 1. BTC_Open\n",
      " 2. BTC_Volume\n",
      " 3. BNB_Volume\n",
      " 4. XRP_Open\n",
      " 5. XRP_Volume\n",
      " 6. SOL_Volume\n",
      " 7. ADA_Open\n",
      " 8. ADA_Volume\n",
      " 9. DOGE_Open\n",
      "10. DOGE_Volume\n",
      "11. AVAX_Open\n",
      "12. AVAX_Volume\n",
      "13. DOT_Open\n",
      "14. DOT_Volume\n",
      "15. MATIC_Open\n",
      "\n",
      "Step 3: Scaling\n",
      "\n",
      "Step 4: Creating Sequences\n",
      "Sequences: (1104, 10, 40)\n",
      "Direction balance: 50.91% up\n",
      "\n",
      "Step 5: Train/Val/Test Split\n",
      "Train: 772, Val: 165, Test: 167\n",
      "\n",
      "Step 6: Building Model with Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 15:47:46.307079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46689 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Training\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 15:47:51.726765: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2025-10-04 15:47:53.558460: I external/local_xla/xla/service/service.cc:168] XLA service 0x2bc7cec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-04 15:47:53.558493: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-10-04 15:47:53.565650: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759560473.719739  180510 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 8s 27ms/step - loss: 0.3549 - price_loss: 0.1252 - direction_loss: 0.6993 - price_mae: 0.3770 - direction_accuracy: 0.4870 - val_loss: 0.3515 - val_price_loss: 0.1195 - val_direction_loss: 0.6995 - val_price_mae: 0.4271 - val_direction_accuracy: 0.4970 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.3101 - price_loss: 0.0511 - direction_loss: 0.6986 - price_mae: 0.2467 - direction_accuracy: 0.4780 - val_loss: 0.3656 - val_price_loss: 0.1414 - val_direction_loss: 0.7018 - val_price_mae: 0.4599 - val_direction_accuracy: 0.4909 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.3004 - price_loss: 0.0407 - direction_loss: 0.6899 - price_mae: 0.2197 - direction_accuracy: 0.5337 - val_loss: 0.3506 - val_price_loss: 0.1195 - val_direction_loss: 0.6972 - val_price_mae: 0.4315 - val_direction_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.3029 - price_loss: 0.0420 - direction_loss: 0.6941 - price_mae: 0.2143 - direction_accuracy: 0.5181 - val_loss: 0.3424 - val_price_loss: 0.1053 - val_direction_loss: 0.6980 - val_price_mae: 0.4010 - val_direction_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2999 - price_loss: 0.0379 - direction_loss: 0.6929 - price_mae: 0.2065 - direction_accuracy: 0.5181 - val_loss: 0.3494 - val_price_loss: 0.1166 - val_direction_loss: 0.6986 - val_price_mae: 0.4209 - val_direction_accuracy: 0.4727 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2977 - price_loss: 0.0355 - direction_loss: 0.6911 - price_mae: 0.2040 - direction_accuracy: 0.5168 - val_loss: 0.3575 - val_price_loss: 0.1288 - val_direction_loss: 0.7005 - val_price_mae: 0.4449 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2978 - price_loss: 0.0350 - direction_loss: 0.6920 - price_mae: 0.1946 - direction_accuracy: 0.5376 - val_loss: 0.3420 - val_price_loss: 0.1027 - val_direction_loss: 0.7009 - val_price_mae: 0.4031 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2934 - price_loss: 0.0287 - direction_loss: 0.6905 - price_mae: 0.1822 - direction_accuracy: 0.5194 - val_loss: 0.3406 - val_price_loss: 0.0973 - val_direction_loss: 0.7055 - val_price_mae: 0.3910 - val_direction_accuracy: 0.4364 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2940 - price_loss: 0.0298 - direction_loss: 0.6904 - price_mae: 0.1857 - direction_accuracy: 0.5376 - val_loss: 0.3385 - val_price_loss: 0.0960 - val_direction_loss: 0.7021 - val_price_mae: 0.3921 - val_direction_accuracy: 0.4788 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.2930 - price_loss: 0.0290 - direction_loss: 0.6889 - price_mae: 0.1837 - direction_accuracy: 0.5207 - val_loss: 0.3289 - val_price_loss: 0.0793 - val_direction_loss: 0.7032 - val_price_mae: 0.3530 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2924 - price_loss: 0.0296 - direction_loss: 0.6865 - price_mae: 0.1834 - direction_accuracy: 0.5440 - val_loss: 0.3465 - val_price_loss: 0.1084 - val_direction_loss: 0.7036 - val_price_mae: 0.4183 - val_direction_accuracy: 0.4424 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2913 - price_loss: 0.0286 - direction_loss: 0.6853 - price_mae: 0.1772 - direction_accuracy: 0.5427 - val_loss: 0.3381 - val_price_loss: 0.0947 - val_direction_loss: 0.7031 - val_price_mae: 0.3843 - val_direction_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2894 - price_loss: 0.0256 - direction_loss: 0.6850 - price_mae: 0.1724 - direction_accuracy: 0.5764 - val_loss: 0.3519 - val_price_loss: 0.1181 - val_direction_loss: 0.7025 - val_price_mae: 0.4307 - val_direction_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2937 - price_loss: 0.0326 - direction_loss: 0.6853 - price_mae: 0.1916 - direction_accuracy: 0.5609 - val_loss: 0.3321 - val_price_loss: 0.0823 - val_direction_loss: 0.7068 - val_price_mae: 0.3509 - val_direction_accuracy: 0.4485 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2935 - price_loss: 0.0267 - direction_loss: 0.6938 - price_mae: 0.1754 - direction_accuracy: 0.5337 - val_loss: 0.3530 - val_price_loss: 0.1173 - val_direction_loss: 0.7066 - val_price_mae: 0.4236 - val_direction_accuracy: 0.4727 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.2878 - price_loss: 0.0254 - direction_loss: 0.6813 - price_mae: 0.1722 - direction_accuracy: 0.5518 - val_loss: 0.3591 - val_price_loss: 0.1296 - val_direction_loss: 0.7034 - val_price_mae: 0.4327 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2886 - price_loss: 0.0265 - direction_loss: 0.6818 - price_mae: 0.1754 - direction_accuracy: 0.5622 - val_loss: 0.3615 - val_price_loss: 0.1339 - val_direction_loss: 0.7029 - val_price_mae: 0.4487 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2880 - price_loss: 0.0235 - direction_loss: 0.6847 - price_mae: 0.1651 - direction_accuracy: 0.5699 - val_loss: 0.3693 - val_price_loss: 0.1454 - val_direction_loss: 0.7052 - val_price_mae: 0.4573 - val_direction_accuracy: 0.4727 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2853 - price_loss: 0.0217 - direction_loss: 0.6808 - price_mae: 0.1573 - direction_accuracy: 0.5803 - val_loss: 0.3653 - val_price_loss: 0.1424 - val_direction_loss: 0.6996 - val_price_mae: 0.4514 - val_direction_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2857 - price_loss: 0.0230 - direction_loss: 0.6798 - price_mae: 0.1608 - direction_accuracy: 0.5777 - val_loss: 0.4116 - val_price_loss: 0.2196 - val_direction_loss: 0.6996 - val_price_mae: 0.5340 - val_direction_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2893 - price_loss: 0.0256 - direction_loss: 0.6849 - price_mae: 0.1700 - direction_accuracy: 0.5738 - val_loss: 0.3800 - val_price_loss: 0.1613 - val_direction_loss: 0.7080 - val_price_mae: 0.4750 - val_direction_accuracy: 0.4788 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2833 - price_loss: 0.0223 - direction_loss: 0.6748 - price_mae: 0.1588 - direction_accuracy: 0.5868 - val_loss: 0.3820 - val_price_loss: 0.1659 - val_direction_loss: 0.7062 - val_price_mae: 0.4841 - val_direction_accuracy: 0.4970 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2866 - price_loss: 0.0238 - direction_loss: 0.6809 - price_mae: 0.1667 - direction_accuracy: 0.5764 - val_loss: 0.3695 - val_price_loss: 0.1363 - val_direction_loss: 0.7192 - val_price_mae: 0.4419 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2878 - price_loss: 0.0245 - direction_loss: 0.6829 - price_mae: 0.1655 - direction_accuracy: 0.5803 - val_loss: 0.3326 - val_price_loss: 0.0762 - val_direction_loss: 0.7173 - val_price_mae: 0.3347 - val_direction_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2829 - price_loss: 0.0229 - direction_loss: 0.6728 - price_mae: 0.1629 - direction_accuracy: 0.5777 - val_loss: 0.3513 - val_price_loss: 0.1060 - val_direction_loss: 0.7194 - val_price_mae: 0.3998 - val_direction_accuracy: 0.4788 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2840 - price_loss: 0.0267 - direction_loss: 0.6701 - price_mae: 0.1745 - direction_accuracy: 0.6023 - val_loss: 0.3587 - val_price_loss: 0.1192 - val_direction_loss: 0.7180 - val_price_mae: 0.4176 - val_direction_accuracy: 0.4848 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2801 - price_loss: 0.0209 - direction_loss: 0.6689 - price_mae: 0.1572 - direction_accuracy: 0.5907 - val_loss: 0.3474 - val_price_loss: 0.0970 - val_direction_loss: 0.7228 - val_price_mae: 0.3726 - val_direction_accuracy: 0.4606 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2807 - price_loss: 0.0214 - direction_loss: 0.6697 - price_mae: 0.1528 - direction_accuracy: 0.5933 - val_loss: 0.3553 - val_price_loss: 0.1067 - val_direction_loss: 0.7282 - val_price_mae: 0.3935 - val_direction_accuracy: 0.4727 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2796 - price_loss: 0.0236 - direction_loss: 0.6636 - price_mae: 0.1618 - direction_accuracy: 0.6075 - val_loss: 0.3469 - val_price_loss: 0.0895 - val_direction_loss: 0.7331 - val_price_mae: 0.3558 - val_direction_accuracy: 0.4545 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2803 - price_loss: 0.0261 - direction_loss: 0.6617 - price_mae: 0.1676 - direction_accuracy: 0.6256 - val_loss: 0.3609 - val_price_loss: 0.1078 - val_direction_loss: 0.7405 - val_price_mae: 0.3941 - val_direction_accuracy: 0.4606 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2777 - price_loss: 0.0216 - direction_loss: 0.6619 - price_mae: 0.1598 - direction_accuracy: 0.6049 - val_loss: 0.3587 - val_price_loss: 0.1068 - val_direction_loss: 0.7365 - val_price_mae: 0.3901 - val_direction_accuracy: 0.4424 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2745 - price_loss: 0.0202 - direction_loss: 0.6558 - price_mae: 0.1508 - direction_accuracy: 0.6179 - val_loss: 0.3552 - val_price_loss: 0.0879 - val_direction_loss: 0.7562 - val_price_mae: 0.3537 - val_direction_accuracy: 0.4545 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2741 - price_loss: 0.0203 - direction_loss: 0.6548 - price_mae: 0.1511 - direction_accuracy: 0.6412 - val_loss: 0.3545 - val_price_loss: 0.0940 - val_direction_loss: 0.7453 - val_price_mae: 0.3751 - val_direction_accuracy: 0.4545 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2780 - price_loss: 0.0208 - direction_loss: 0.6637 - price_mae: 0.1543 - direction_accuracy: 0.5997 - val_loss: 0.3512 - val_price_loss: 0.0852 - val_direction_loss: 0.7502 - val_price_mae: 0.3522 - val_direction_accuracy: 0.4848 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2733 - price_loss: 0.0194 - direction_loss: 0.6542 - price_mae: 0.1488 - direction_accuracy: 0.6308 - val_loss: 0.3501 - val_price_loss: 0.0863 - val_direction_loss: 0.7457 - val_price_mae: 0.3536 - val_direction_accuracy: 0.4667 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2741 - price_loss: 0.0200 - direction_loss: 0.6553 - price_mae: 0.1483 - direction_accuracy: 0.6218 - val_loss: 0.3518 - val_price_loss: 0.0859 - val_direction_loss: 0.7508 - val_price_mae: 0.3501 - val_direction_accuracy: 0.4667 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2732 - price_loss: 0.0224 - direction_loss: 0.6493 - price_mae: 0.1600 - direction_accuracy: 0.6192 - val_loss: 0.3540 - val_price_loss: 0.0854 - val_direction_loss: 0.7568 - val_price_mae: 0.3512 - val_direction_accuracy: 0.4788 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2748 - price_loss: 0.0232 - direction_loss: 0.6523 - price_mae: 0.1588 - direction_accuracy: 0.6088 - val_loss: 0.3598 - val_price_loss: 0.1011 - val_direction_loss: 0.7478 - val_price_mae: 0.3706 - val_direction_accuracy: 0.4848 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2737 - price_loss: 0.0202 - direction_loss: 0.6538 - price_mae: 0.1523 - direction_accuracy: 0.6373 - val_loss: 0.3574 - val_price_loss: 0.0971 - val_direction_loss: 0.7478 - val_price_mae: 0.3775 - val_direction_accuracy: 0.4909 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2741 - price_loss: 0.0206 - direction_loss: 0.6544 - price_mae: 0.1559 - direction_accuracy: 0.6166 - val_loss: 0.3649 - val_price_loss: 0.1028 - val_direction_loss: 0.7581 - val_price_mae: 0.3825 - val_direction_accuracy: 0.4667 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2688 - price_loss: 0.0199 - direction_loss: 0.6421 - price_mae: 0.1515 - direction_accuracy: 0.6412 - val_loss: 0.3672 - val_price_loss: 0.1057 - val_direction_loss: 0.7596 - val_price_mae: 0.3883 - val_direction_accuracy: 0.4727 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2755 - price_loss: 0.0192 - direction_loss: 0.6599 - price_mae: 0.1470 - direction_accuracy: 0.5972 - val_loss: 0.3668 - val_price_loss: 0.0962 - val_direction_loss: 0.7727 - val_price_mae: 0.3620 - val_direction_accuracy: 0.4667 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2739 - price_loss: 0.0233 - direction_loss: 0.6500 - price_mae: 0.1597 - direction_accuracy: 0.6295 - val_loss: 0.3720 - val_price_loss: 0.1027 - val_direction_loss: 0.7759 - val_price_mae: 0.3817 - val_direction_accuracy: 0.4606 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.2705 - price_loss: 0.0208 - direction_loss: 0.6450 - price_mae: 0.1547 - direction_accuracy: 0.6425 - val_loss: 0.3933 - val_price_loss: 0.1126 - val_direction_loss: 0.8145 - val_price_mae: 0.4017 - val_direction_accuracy: 0.4667 - lr: 5.0000e-04\n",
      "\n",
      "Step 8: Evaluation\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "\n",
      "Price Metrics:\n",
      "RMSE: $720.07\n",
      "MAPE: 17.44%\n",
      "R2: 0.4386\n",
      "\n",
      "Trading Performance:\n",
      "Direction Accuracy: 56.29%\n",
      "Total Return: 105.86%\n",
      "Sharpe Ratio: 2.09\n",
      "Max Drawdown: 25.50%\n",
      "Win Rate: 52.41%\n",
      "\n",
      "======================================================================\n",
      "Complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'\n",
    "ONCHAIN_FILE = 'eth_onchain.csv'\n",
    "NEWS_DIR = \"./news_data\"\n",
    "START_TIME = '2022-09-15'\n",
    "END_TIME = '2025-10-02'\n",
    "TARGET_COIN = 'ETH'\n",
    "SEQUENCE_LENGTH = 10\n",
    "PREDICTION_HORIZON = 1\n",
    "TOP_K_FEATURES = 40\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    patterns = [\n",
    "        r'(\\d{4})-(\\d{2})-(\\d{2})',\n",
    "        r'(\\d{4})(\\d{2})(\\d{2})',\n",
    "        r'(\\d{2})-(\\d{2})-(\\d{4})',\n",
    "        r'(\\d{2})(\\d{2})(\\d{4})'\n",
    "    ]\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    all_data = []\n",
    "    \n",
    "    if not os.path.exists(root_dir):\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'sentiment_score': np.zeros(len(dates)),\n",
    "            'news_count': np.ones(len(dates))\n",
    "        })\n",
    "    \n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        \n",
    "        for enc in ['utf-8', 'cp949', 'latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        \n",
    "        if 'label' not in df.columns:\n",
    "            df['label'] = 0\n",
    "        \n",
    "        all_data.append(df[['date', 'label']])\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'sentiment_score': np.zeros(len(dates)),\n",
    "            'news_count': np.ones(len(dates))\n",
    "        })\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.normalize()\n",
    "    \n",
    "    news_agg = combined_df.groupby('date').agg({\n",
    "        'label': 'mean',\n",
    "    }).reset_index()\n",
    "    news_agg.columns = ['date', 'sentiment_score']\n",
    "    \n",
    "    news_count = combined_df.groupby('date').size().reset_index(name='news_count')\n",
    "    news_agg = news_agg.merge(news_count, on='date', how='left')\n",
    "    \n",
    "    return news_agg\n",
    "\n",
    "def load_macro_data(filepath):\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'])\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None).dt.normalize()\n",
    "    df = df.set_index('Date').sort_index()\n",
    "    df = df[df.index >= START_TIME]\n",
    "    return df\n",
    "\n",
    "def load_onchain_data(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    df = pd.read_csv(filepath, parse_dates=['date'])\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None).dt.normalize()\n",
    "    df = df.set_index('date').sort_index()\n",
    "    df = df[df.index >= START_TIME]\n",
    "    df.columns = ['onchain_' + col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def calculate_directional_features(df, target_coin='ETH'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    prefix = f\"{target_coin}_\"\n",
    "    close_col = f\"{prefix}Close\"\n",
    "    volume_col = f\"{prefix}Volume\"\n",
    "    high_col = f\"{prefix}High\"\n",
    "    low_col = f\"{prefix}Low\"\n",
    "    \n",
    "    if close_col not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    df[f'{prefix}returns_1d'] = df[close_col].pct_change(1)\n",
    "    df[f'{prefix}returns_3d'] = df[close_col].pct_change(3)\n",
    "    df[f'{prefix}returns_7d'] = df[close_col].pct_change(7)\n",
    "    \n",
    "    df[f'{prefix}momentum_5'] = df[close_col] - df[close_col].shift(5)\n",
    "    df[f'{prefix}momentum_10'] = df[close_col] - df[close_col].shift(10)\n",
    "    \n",
    "    for period in [5, 10, 20]:\n",
    "        sma = df[close_col].rolling(period).mean()\n",
    "        df[f'{prefix}sma_{period}'] = sma\n",
    "        df[f'{prefix}price_to_sma_{period}'] = (df[close_col] / sma - 1) * 100\n",
    "        df[f'{prefix}sma_{period}_slope'] = sma.diff(1)\n",
    "    \n",
    "    ema_fast = df[close_col].ewm(span=12).mean()\n",
    "    ema_slow = df[close_col].ewm(span=26).mean()\n",
    "    df[f'{prefix}macd'] = ema_fast - ema_slow\n",
    "    df[f'{prefix}macd_signal'] = df[f'{prefix}macd'].ewm(span=9).mean()\n",
    "    df[f'{prefix}macd_diff'] = df[f'{prefix}macd'] - df[f'{prefix}macd_signal']\n",
    "    \n",
    "    delta = df[close_col].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df[f'{prefix}rsi'] = 100 - (100 / (1 + rs))\n",
    "    df[f'{prefix}rsi_oversold'] = (df[f'{prefix}rsi'] < 30).astype(int)\n",
    "    df[f'{prefix}rsi_overbought'] = (df[f'{prefix}rsi'] > 70).astype(int)\n",
    "    \n",
    "    df[f'{prefix}volatility_5'] = df[close_col].pct_change().rolling(5).std()\n",
    "    df[f'{prefix}volatility_10'] = df[close_col].pct_change().rolling(10).std()\n",
    "    \n",
    "    if volume_col in df.columns:\n",
    "        df[f'{prefix}volume_change'] = df[volume_col].pct_change(1)\n",
    "        df[f'{prefix}volume_sma_10'] = df[volume_col].rolling(10).mean()\n",
    "        df[f'{prefix}volume_ratio'] = df[volume_col] / df[f'{prefix}volume_sma_10']\n",
    "        \n",
    "        price_change = df[close_col].diff()\n",
    "        df[f'{prefix}obv'] = (np.sign(price_change) * df[volume_col]).fillna(0).cumsum()\n",
    "        df[f'{prefix}obv_slope'] = df[f'{prefix}obv'].diff(1)\n",
    "    \n",
    "    if high_col in df.columns and low_col in df.columns:\n",
    "        df[f'{prefix}atr'] = ((df[high_col] - df[low_col]) / df[close_col]).rolling(14).mean()\n",
    "        df[f'{prefix}price_position'] = (df[close_col] - df[low_col]) / (df[high_col] - df[low_col])\n",
    "    \n",
    "    for i in [1, 2, 3, 5, 7]:\n",
    "        df[f'{prefix}higher_than_{i}d_ago'] = (df[close_col] > df[close_col].shift(i)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_all_data():\n",
    "    macro_df = load_macro_data(TARGET_MACRO_FILE)\n",
    "    \n",
    "    onchain_df = load_onchain_data(ONCHAIN_FILE)\n",
    "    if onchain_df is not None:\n",
    "        macro_df = macro_df.join(onchain_df, how='left')\n",
    "    \n",
    "    news_df = load_all_news_data(NEWS_DIR)\n",
    "    news_df = news_df.set_index('date')\n",
    "    macro_df = macro_df.join(news_df, how='left')\n",
    "    \n",
    "    macro_df = calculate_directional_features(macro_df, TARGET_COIN)\n",
    "    \n",
    "    macro_df = macro_df.replace([np.inf, -np.inf], np.nan)\n",
    "    macro_df = macro_df.ffill().bfill()\n",
    "    \n",
    "    return macro_df\n",
    "\n",
    "def remove_correlated_features(X, threshold=0.90):\n",
    "    corr_matrix = np.corrcoef(X.T)\n",
    "    corr_matrix = np.abs(corr_matrix)\n",
    "    \n",
    "    upper_triangle = np.triu(corr_matrix, k=1)\n",
    "    \n",
    "    to_remove = set()\n",
    "    for i in range(upper_triangle.shape[0]):\n",
    "        for j in range(i+1, upper_triangle.shape[1]):\n",
    "            if upper_triangle[i, j] > threshold:\n",
    "                to_remove.add(j)\n",
    "    \n",
    "    keep_indices = [i for i in range(X.shape[1]) if i not in to_remove]\n",
    "    return keep_indices\n",
    "\n",
    "def select_features(X, y, feature_names, k=40):\n",
    "    keep_indices = remove_correlated_features(X, threshold=0.90)\n",
    "    X_filtered = X[:, keep_indices]\n",
    "    feature_names_filtered = [feature_names[i] for i in keep_indices]\n",
    "    \n",
    "    selector = SelectKBest(score_func=mutual_info_regression, k=min(k, X_filtered.shape[1]))\n",
    "    selector.fit(X_filtered, y)\n",
    "    selected_mask = selector.get_support()\n",
    "    \n",
    "    final_indices = [keep_indices[i] for i, selected in enumerate(selected_mask) if selected]\n",
    "    final_features = [feature_names[i] for i in final_indices]\n",
    "    \n",
    "    return final_indices, final_features\n",
    "\n",
    "def create_directional_sequences(data, target, seq_length=10):\n",
    "    X, y_price, y_direction = [], [], []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        \n",
    "        current_price = target[i+seq_length-1]\n",
    "        next_price = target[i+seq_length]\n",
    "        \n",
    "        y_price.append(next_price)\n",
    "        y_direction.append(1 if next_price > current_price else 0)\n",
    "    \n",
    "    return np.array(X), np.array(y_price), np.array(y_direction)\n",
    "\n",
    "def build_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.LSTM(128, return_sequences=True)(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    attention = layers.RepeatVector(128)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "    \n",
    "    x_attended = layers.multiply([x, attention])\n",
    "    x_attended = layers.Lambda(lambda xin: tf.reduce_sum(xin, axis=1))(x_attended)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x_attended)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    direction_branch = layers.Dense(32, activation='relu')(x)\n",
    "    direction_branch = layers.Dropout(0.2)(direction_branch)\n",
    "    direction_output = layers.Dense(1, activation='sigmoid', name='direction')(direction_branch)\n",
    "    \n",
    "    price_branch = layers.Dense(32, activation='relu')(x)\n",
    "    price_branch = layers.Dropout(0.2)(price_branch)\n",
    "    price_output = layers.Dense(1, name='price')(price_branch)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[price_output, direction_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_trading_performance(y_true_price, y_pred_price, y_true_dir, y_pred_dir_prob):\n",
    "    y_pred_dir = (y_pred_dir_prob > 0.5).astype(int).flatten()\n",
    "    y_true_dir = y_true_dir.flatten()\n",
    "    \n",
    "    direction_acc = accuracy_score(y_true_dir, y_pred_dir)\n",
    "    \n",
    "    returns_true = np.diff(y_true_price.flatten()) / y_true_price.flatten()[:-1]\n",
    "    \n",
    "    signals = y_pred_dir[:-1]\n",
    "    signals = np.where(signals == 0, -1, 1)\n",
    "    \n",
    "    strategy_returns = signals * returns_true\n",
    "    \n",
    "    cumulative_returns = np.cumprod(1 + strategy_returns) - 1\n",
    "    total_return = cumulative_returns[-1] if len(cumulative_returns) > 0 else 0\n",
    "    \n",
    "    sharpe = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10) * np.sqrt(252)\n",
    "    \n",
    "    cumulative_max = np.maximum.accumulate(1 + cumulative_returns)\n",
    "    drawdown = (cumulative_max - (1 + cumulative_returns)) / cumulative_max\n",
    "    max_dd = np.max(drawdown) if len(drawdown) > 0 else 0\n",
    "    \n",
    "    win_rate = np.mean(strategy_returns > 0) if len(strategy_returns) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'direction_accuracy': direction_acc,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': max_dd,\n",
    "        'win_rate': win_rate\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Enhanced Directional Prediction Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nPeriod: {START_TIME} to {END_TIME}\")\n",
    "    \n",
    "    print(\"\\nStep 1: Loading Data\")\n",
    "    df = merge_all_data()\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    \n",
    "    target_col = f'{TARGET_COIN}_Close'\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Error: {target_col} not found\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStep 2: Feature Selection\")\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col] and not col.endswith('_Close')]\n",
    "    \n",
    "    df_clean = df[[target_col] + feature_cols].dropna()\n",
    "    print(f\"Clean samples: {len(df_clean)}\")\n",
    "    print(f\"Initial features: {len(feature_cols)}\")\n",
    "    \n",
    "    selected_indices, selected_features = select_features(\n",
    "        df_clean[feature_cols].values,\n",
    "        df_clean[target_col].values,\n",
    "        feature_cols,\n",
    "        k=TOP_K_FEATURES\n",
    "    )\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    print(\"\\nTop features:\")\n",
    "    for i, feat in enumerate(selected_features[:15], 1):\n",
    "        print(f\"{i:2d}. {feat}\")\n",
    "    \n",
    "    print(\"\\nStep 3: Scaling\")\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_scaled = scaler_X.fit_transform(df_clean[selected_features])\n",
    "    y_values = df_clean[target_col].values.reshape(-1, 1)\n",
    "    y_scaled = scaler_y.fit_transform(y_values).flatten()\n",
    "    \n",
    "    print(\"\\nStep 4: Creating Sequences\")\n",
    "    X_seq, y_price, y_direction = create_directional_sequences(X_scaled, y_scaled, SEQUENCE_LENGTH)\n",
    "    print(f\"Sequences: {X_seq.shape}\")\n",
    "    print(f\"Direction balance: {np.mean(y_direction):.2%} up\")\n",
    "    \n",
    "    print(\"\\nStep 5: Train/Val/Test Split\")\n",
    "    train_size = int(len(X_seq) * 0.7)\n",
    "    val_size = int(len(X_seq) * 0.15)\n",
    "    \n",
    "    X_train = X_seq[:train_size]\n",
    "    y_train_price = y_price[:train_size]\n",
    "    y_train_dir = y_direction[:train_size]\n",
    "    \n",
    "    X_val = X_seq[train_size:train_size+val_size]\n",
    "    y_val_price = y_price[train_size:train_size+val_size]\n",
    "    y_val_dir = y_direction[train_size:train_size+val_size]\n",
    "    \n",
    "    X_test = X_seq[train_size+val_size:]\n",
    "    y_test_price = y_price[train_size+val_size:]\n",
    "    y_test_dir = y_direction[train_size+val_size:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    print(\"\\nStep 6: Building Model with Attention\")\n",
    "    model = build_attention_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'price': 'huber',\n",
    "            'direction': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'price': 0.6,\n",
    "            'direction': 0.4\n",
    "        },\n",
    "        metrics={\n",
    "            'price': ['mae'],\n",
    "            'direction': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 7: Training\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {'price': y_train_price, 'direction': y_train_dir},\n",
    "        validation_data=(\n",
    "            X_val,\n",
    "            {'price': y_val_price, 'direction': y_val_dir}\n",
    "        ),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                patience=40,\n",
    "                restore_best_weights=True,\n",
    "                monitor='val_direction_accuracy',\n",
    "                mode='max'\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                factor=0.5,\n",
    "                patience=20,\n",
    "                min_lr=1e-7,\n",
    "                monitor='val_direction_accuracy',\n",
    "                mode='max'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 8: Evaluation\")\n",
    "    y_pred_price, y_pred_dir = model.predict(X_test)\n",
    "    \n",
    "    y_test_price_original = scaler_y.inverse_transform(y_test_price.reshape(-1, 1))\n",
    "    y_pred_price_original = scaler_y.inverse_transform(y_pred_price)\n",
    "    \n",
    "    mse = mean_squared_error(y_test_price_original, y_pred_price_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test_price_original, y_pred_price_original)\n",
    "    r2 = r2_score(y_test_price_original, y_pred_price_original)\n",
    "    \n",
    "    print(f\"\\nPrice Metrics:\")\n",
    "    print(f\"RMSE: ${rmse:.2f}\")\n",
    "    print(f\"MAPE: {mape*100:.2f}%\")\n",
    "    print(f\"R2: {r2:.4f}\")\n",
    "    \n",
    "    trading_perf = calculate_trading_performance(\n",
    "        y_test_price_original,\n",
    "        y_pred_price_original,\n",
    "        y_test_dir,\n",
    "        y_pred_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrading Performance:\")\n",
    "    print(f\"Direction Accuracy: {trading_perf['direction_accuracy']*100:.2f}%\")\n",
    "    print(f\"Total Return: {trading_perf['total_return']*100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {trading_perf['sharpe_ratio']:.2f}\")\n",
    "    print(f\"Max Drawdown: {trading_perf['max_drawdown']*100:.2f}%\")\n",
    "    print(f\"Win Rate: {trading_perf['win_rate']*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Complete\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return model, scaler_X, scaler_y, selected_features, df_clean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler_X, scaler_y, selected_features, df_clean = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
