{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc722ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 23:24:58.912222: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-19 23:24:58.912268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-19 23:24:58.913655: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-19 23:24:58.920709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-19 23:25:00.128363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING\n",
      "================================================================================\n",
      "(2978, 41) macro_crypto_data.csv\n",
      "(2233, 2) SP500.csv\n",
      "(2234, 2) VIX.csv\n",
      "(2235, 2) GOLD.csv\n",
      "(2236, 2) DXY.csv\n",
      "(2845, 2) fear_greed.csv\n",
      "(2185, 2) eth_funding_rate.csv\n",
      "(2913, 6) usdt_eth_mcap.csv\n",
      "(2009, 2) aave_eth_tvl.csv\n",
      "(1795, 2) lido_eth_tvl.csv\n",
      "(2511, 2) makerdao_eth_tvl.csv\n",
      "(2570, 2) uniswap_eth_tvl.csv\n",
      "(2103, 2) curve-dex_eth_tvl.csv\n",
      "(2974, 2) eth_chain_tvl.csv\n",
      "(1603, 5) layer2_tvl.csv\n",
      "Loaded 10 files\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT FEATURES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATA MERGING\n",
      "================================================================================\n",
      "Merged shape: (2346, 62)\n",
      "Missing before fill: 23,712\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUE HANDLING\n",
      "================================================================================\n",
      "Missing after fill: 0\n",
      "Shape: (2346, 62)\n",
      "Period: 2019-06-15 ~ 2025-11-14\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "import glob\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from collections import Counter\n",
    "from numba import jit\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE,\n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "    ,log_loss\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier, StackingClassifier, VotingClassifier,\n",
    "    AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor, RandomForestRegressor,\n",
    "    StackingRegressor, VotingRegressor\n",
    ")\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm.callback import early_stopping\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Flatten, Dropout, Activation,\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    Attention, MultiHeadAttention,\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    pass\n",
    "# ============================================================================\n",
    "# 환경 설정 및 경고 무시\n",
    "# ============================================================================\n",
    "\n",
    "# GPU 메모리 증가 허용 설정\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DATA_DIR_MAIN = './macro_data'\n",
    "DATA_DIR_NEW = './macro_data/macro_data'\n",
    "\n",
    "TRAIN_START_DATE = pd.to_datetime('2020-01-01')\n",
    "LOOKBACK_DAYS = 200\n",
    "LOOKBACK_START_DATE = TRAIN_START_DATE - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "\n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    print(df.shape,file_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"[Warning] {filename} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    return standardize_date_column(df, filename)\n",
    "\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [f\"{prefix}_{col}\" if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sentiment_features(news_df):\n",
    "    if news_df.empty:\n",
    "        return pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index().fillna(0)\n",
    "    \n",
    "    agg['sentiment_polarity'] = agg['positive_ratio'] - agg['negative_ratio']\n",
    "    agg['sentiment_intensity'] = agg['positive_ratio'] + agg['negative_ratio']\n",
    "    agg['sentiment_disagreement'] = agg['positive_ratio'] * agg['negative_ratio']\n",
    "    agg['bull_bear_ratio'] = agg['positive_ratio'] / (agg['negative_ratio'] + 1e-10)\n",
    "    agg['weighted_sentiment'] = agg['sentiment_mean'] * np.log1p(agg['news_count'])\n",
    "    agg['extremity_index'] = (agg['extreme_positive_count'] + agg['extreme_negative_count']) / (agg['news_count'] + 1e-10)\n",
    "    \n",
    "    for window in [3,7]:\n",
    "        agg[f'sentiment_ma{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        agg[f'sentiment_volatility_{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    agg['sentiment_trend'] = agg['sentiment_mean'].diff()\n",
    "    agg['sentiment_acceleration'] = agg['sentiment_trend'].diff()\n",
    "    agg['news_volume_change'] = agg['news_count'].pct_change()\n",
    "    \n",
    "    for window in [7, 14]:\n",
    "        agg[f'news_volume_ma{window}'] = agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return agg.fillna(0)\n",
    "\n",
    "\n",
    "def smart_fill_missing(df_merged):\n",
    "    REFERENCE_START_DATE = pd.to_datetime('2020-01-01')\n",
    "    \n",
    "    for col in df_merged.columns:\n",
    "        if col == 'date':\n",
    "            continue\n",
    "        \n",
    "        if df_merged[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        non_null_idx = df_merged[col].first_valid_index()\n",
    "        \n",
    "        if non_null_idx is None:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "            continue\n",
    "        \n",
    "        first_date = df_merged.loc[non_null_idx, 'date']\n",
    "        \n",
    "        before_mask = df_merged['date'] < first_date\n",
    "        after_mask = df_merged['date'] >= first_date\n",
    "        \n",
    "        df_merged.loc[before_mask, col] = df_merged.loc[before_mask, col].fillna(0)\n",
    "        df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(method='ffill')\n",
    "        \n",
    "        remaining = df_merged.loc[after_mask, col].isnull().sum()\n",
    "        if remaining > 0:\n",
    "            df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(0)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#news_df = load_csv(DATA_DIR_MAIN, 'news_data.csv')\n",
    "#eth_onchain_df = load_csv(DATA_DIR_MAIN, 'eth_onchain.csv')\n",
    "macro_df = load_csv(DATA_DIR_NEW, 'macro_crypto_data.csv')\n",
    "sp500_df = load_csv(DATA_DIR_NEW, 'SP500.csv')\n",
    "vix_df = load_csv(DATA_DIR_NEW, 'VIX.csv')\n",
    "gold_df = load_csv(DATA_DIR_NEW, 'GOLD.csv')\n",
    "dxy_df = load_csv(DATA_DIR_NEW, 'DXY.csv')\n",
    "fear_greed_df = load_csv(DATA_DIR_NEW, 'fear_greed.csv')\n",
    "eth_funding_df = load_csv(DATA_DIR_NEW, 'eth_funding_rate.csv')\n",
    "usdt_eth_mcap_df = load_csv(DATA_DIR_NEW, 'usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_csv(DATA_DIR_NEW, 'aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_csv(DATA_DIR_NEW, 'lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_csv(DATA_DIR_NEW, 'makerdao_eth_tvl.csv')\n",
    "uniswap_tvl_df = load_csv(DATA_DIR_NEW, 'uniswap_eth_tvl.csv')\n",
    "curve_tvl_df = load_csv(DATA_DIR_NEW, 'curve-dex_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_csv(DATA_DIR_NEW, 'eth_chain_tvl.csv')\n",
    "layer2_tvl_df = load_csv(DATA_DIR_NEW, 'layer2_tvl.csv')\n",
    "\n",
    "print(f\"Loaded {len([df for df in [fear_greed_df, eth_funding_df, usdt_eth_mcap_df, aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df, eth_chain_tvl_df, layer2_tvl_df] if not df.empty])} files\")\n",
    "\n",
    "all_dataframes = [\n",
    "    macro_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df, \n",
    "    sp500_df, vix_df, gold_df, dxy_df#,news_df, eth_onchain_df\n",
    "]\n",
    "\n",
    "last_dates = [\n",
    "    pd.to_datetime(df['date']).max() \n",
    "    for df in all_dataframes \n",
    "    if not df.empty and 'date' in df.columns\n",
    "]\n",
    "\n",
    "end_date = min(last_dates) if last_dates else pd.Timestamp.today()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "#sentiment_features = create_sentiment_features(news_df)\n",
    "#print(f\"Generated {sentiment_features.shape[1]-1} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA MERGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "uniswap_tvl_df = add_prefix(uniswap_tvl_df, 'uniswap')\n",
    "curve_tvl_df = add_prefix(curve_tvl_df, 'curve')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "layer2_tvl_df = add_prefix(layer2_tvl_df, 'l2')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=LOOKBACK_START_DATE, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df,  fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df,\n",
    "    sp500_df, vix_df, gold_df, dxy_df#,sentiment_features,eth_onchain_df,\n",
    "]\n",
    "\n",
    "for df in dataframes_to_merge:\n",
    "    if not df.empty:\n",
    "        df_merged = pd.merge(df_merged, df, on='date', how='left')\n",
    "\n",
    "print(f\"Merged shape: {df_merged.shape}\")\n",
    "print(f\"Missing before fill: {df_merged.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUE HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_merged = smart_fill_missing(df_merged)\n",
    "\n",
    "missing_after = df_merged.isnull().sum().sum()\n",
    "print(f\"Missing after fill: {missing_after:,}\")\n",
    "\n",
    "if missing_after > 0:\n",
    "    df_merged = df_merged.fillna(0)\n",
    "    print(f\"Remaining filled with 0\")\n",
    "\n",
    "lookback_df = df_merged[df_merged['date'] < TRAIN_START_DATE]\n",
    "cols_to_drop = [\n",
    "    col for col in lookback_df.columns \n",
    "    if lookback_df[col].isnull().all() and col != 'date'\n",
    "]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping {len(cols_to_drop)} fully missing columns\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Shape: {df_merged.shape}\")\n",
    "print(f\"Period: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"Missing: {df_merged.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e25a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_14'] > 100).astype(int)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "\n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "\n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)\n",
    "\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "\n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 5]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df,target_type,method):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        for period in [5, 20]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        \n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['eth_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        \n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['eth_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = df_transformed[col_name].rolling(30, min_periods=10).std().fillna(0)\n",
    "            \n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(btc_volume.pct_change())\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (eth_volume / (btc_volume + 1e-8)).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "            \n",
    "    ## raw_data 저장하기\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    base_dir=os.path.join('model_results',timestamp,'raw_data',target_type,method)\n",
    "    os.makedirs(base_dir,exist_ok=True)\n",
    "    df.to_csv(os.path.join(base_dir,\"raw_data_all_features.csv\"),index=False)        \n",
    "            \n",
    "            \n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [col for col in df_transformed.columns if 'return' in col.lower() and 'next' not in col]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650f6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = ['sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio', 'negative_ratio']\n",
    "    sentiment_ma_cols = [col for col in df.columns if 'sentiment' in col and ('_ma7' in col or '_volatility_7' in col)]\n",
    "    no_lag_patterns = ['_trend', '_acceleration', '_volume_change', 'news_volume_change', 'news_volume_ma']\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', \n",
    "                                  'eth_contract', 'eth_avg_gas', 'eth_total_gas', 'eth_avg_block'])]\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col or '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            if not any(pattern in col for pattern in no_lag_patterns):\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' not in df.columns:\n",
    "        return df_norm\n",
    "    \n",
    "    current_close = df['ETH_Close']\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_non_stationary_features(df):\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    prefixes_to_transform = [\n",
    "        'eth_', 'aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_',\n",
    "        'l2_', 'sp500_', 'gold_', 'dxy_', 'vix_', 'usdt_'\n",
    "    ]\n",
    "    \n",
    "    exclude_prefixes = ['fg_', 'funding_']\n",
    "    \n",
    "    exclude_keywords = [\n",
    "        '_pct_', '_ratio', '_lag', '_volatility', '_corr', '_beta', '_spread',\n",
    "        'eth_return', 'btc_return', 'eth_log_return' \n",
    "    ]\n",
    "    \n",
    "    cols_to_transform = []\n",
    "    for col in df_proc.columns:\n",
    "        if col.startswith(tuple(prefixes_to_transform)):\n",
    "            if not col.startswith(tuple(exclude_prefixes)):\n",
    "                if not any(keyword in col for keyword in exclude_keywords):\n",
    "                    cols_to_transform.append(col)\n",
    "                    \n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        df_proc[col] = df_proc[col].fillna(method='ffill').replace(0, 1e-9)\n",
    "\n",
    "        df_proc[f'{col}_pct_1d'] = df_proc[col].pct_change(1)\n",
    "        df_proc[f'{col}_pct_5d'] = df_proc[col].pct_change(5)\n",
    "        \n",
    "        ma_30 = df_proc[col].rolling(window=30, min_periods=10).mean()\n",
    "        df_proc[f'{col}_ma30_ratio'] = df_proc[col] / (ma_30 + 1e-9)\n",
    "        \n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "    df_proc = df_proc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    df_proc = df_proc.replace([np.inf, -np.inf], np.nan)\n",
    "    df_proc = df_proc.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "   \n",
    "    return df_proc    \n",
    "    \n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_triple_barrier_targets(\n",
    "    prices_close,\n",
    "    prices_high,\n",
    "    prices_low,\n",
    "    atr,\n",
    "    lookahead_candles,\n",
    "    atr_multiplier_profit,\n",
    "    atr_multiplier_stop\n",
    "):\n",
    "    n = len(prices_close)\n",
    "    targets_raw = np.zeros(n, dtype=np.int32)\n",
    "    upper_barriers_arr = np.zeros(n, dtype=np.float64)\n",
    "    lower_barriers_arr = np.zeros(n, dtype=np.float64)\n",
    "    exit_reasons = np.zeros(n, dtype=np.int32)\n",
    "\n",
    "    for i in range(n - lookahead_candles):\n",
    "        current_atr = max(atr[i], 1e-8)\n",
    "        current_price = prices_close[i]\n",
    "        \n",
    "        if np.isnan(current_atr) or current_price <= 0:\n",
    "            continue\n",
    "        \n",
    "        upper_barrier = current_price + (current_atr * atr_multiplier_profit)\n",
    "        lower_barrier = current_price - (current_atr * atr_multiplier_stop)\n",
    "        \n",
    "        upper_barriers_arr[i] = upper_barrier\n",
    "        lower_barriers_arr[i] = lower_barrier\n",
    "\n",
    "        profit_hit = False\n",
    "        stop_hit = False\n",
    "        \n",
    "        for j in range(1, lookahead_candles + 1):\n",
    "            future_high = prices_high[i + j]\n",
    "            future_low = prices_low[i + j]\n",
    "            \n",
    "            if future_high >= upper_barrier:\n",
    "                profit_hit = True\n",
    "                targets_raw[i] = 1\n",
    "                exit_reasons[i] = 1\n",
    "                break\n",
    "            \n",
    "            if future_low <= lower_barrier:\n",
    "                stop_hit = True\n",
    "                targets_raw[i] = 2\n",
    "                exit_reasons[i] = 2\n",
    "                break\n",
    "        \n",
    "        if not profit_hit and not stop_hit:\n",
    "            targets_raw[i] = 0\n",
    "            exit_reasons[i] = 3\n",
    "\n",
    "    return targets_raw, upper_barriers_arr, lower_barriers_arr, exit_reasons\n",
    "\n",
    "def create_targets(df, lookahead_candles=3, atr_multiplier_profit=1.2, atr_multiplier_stop=0.8, trend_window=20, trend_analysis_points=None):\n",
    "    \n",
    "    df_target = df.copy()\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "    if atr_col_name not in df.columns:\n",
    "        raise ValueError(f\"'{atr_col_name}' feature is missing. Run calculate_technical_indicators first.\")\n",
    "\n",
    "    sma_short = ta.sma(df_target['ETH_Close'], length=5)\n",
    "    sma_long = ta.sma(df_target['ETH_Close'], length=trend_window)\n",
    "    \n",
    "    trend_directions = np.where(sma_short > sma_long, 1, -1).astype(int)\n",
    "    trend_strengths = (df_target['ETH_Close'] - sma_long).abs() / (sma_long + 1e-8)\n",
    "    trend_strengths = trend_strengths.fillna(0)\n",
    "\n",
    "    df_target['trend_direction'] = trend_directions\n",
    "    df_target['trend_strength'] = trend_strengths\n",
    "\n",
    "    prices_close = df_target['ETH_Close'].to_numpy()\n",
    "    prices_high = df_target['ETH_High'].to_numpy()\n",
    "    prices_low = df_target['ETH_Low'].to_numpy()\n",
    "    atr = pd.Series(df_target[atr_col_name]).fillna(method='ffill').fillna(0).to_numpy()\n",
    "\n",
    "    targets_raw, upper_barriers, lower_barriers, exit_reasons = compute_triple_barrier_targets(\n",
    "        prices_close, prices_high, prices_low, atr,\n",
    "        lookahead_candles, atr_multiplier_profit, atr_multiplier_stop\n",
    "    )\n",
    "\n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "\n",
    "    df_target['next_open'] = next_open\n",
    "    df_target['next_close'] = next_close\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "\n",
    "    df_target['next_direction'] = targets_raw\n",
    "    df_target['take_profit_price'] = pd.Series(upper_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['stop_loss_price'] = pd.Series(lower_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['exit_reason'] = exit_reasons\n",
    "\n",
    "    df_target['next_direction'].iloc[-lookahead_candles:] = np.nan\n",
    "\n",
    "    return df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38918df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================\n",
    "# 특징 선택 함수 (Feature Selection)\n",
    "# ==========================================================================\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=30, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        # 상호 정보량 (Mutual Information)\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    # RFE Estimator (LGBM)\n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    # Random Forest Importance\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            class_weight='balanced', # 클래스 불균형 고려\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    # 앙상블 투표\n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=30):\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "\n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train,  \n",
    "            y_train['next_direction'],  \n",
    "            task='class',  \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        # ATR_14가 제외되었을 경우 필수적으로 포함 (안정성 확보)\n",
    "        if atr_col_name not in selected and atr_col_name in X_train.columns:\n",
    "            if len(selected) > 0:\n",
    "                 # 가장 덜 중요한 요소를 제거하고 ATR_14를 추가\n",
    "                selected.pop() \n",
    "            selected.insert(0, atr_col_name)\n",
    "    \n",
    "    \n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "# ==========================================================================\n",
    "# 데이터 분할 및 처리 함수\n",
    "# ==========================================================================\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date,\n",
    "                             final_test_start='2025-01-01',\n",
    "                             initial_train_size=800,\n",
    "                             val_size=150,\n",
    "                             test_size=150,\n",
    "                             step=150,\n",
    "                             gap_size=20,\n",
    "                             apply_gap_to_train_val=True):\n",
    "    \n",
    "    df_full_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_full_period = df_full_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "        \n",
    "    final_test_df = df_full_period[df_full_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    rolling_cutoff_date = final_test_start - pd.Timedelta(days=gap_size)\n",
    "    df_rolling_period = df_full_period[df_full_period['date'] < rolling_cutoff_date].copy().reset_index(drop=True)\n",
    "\n",
    "    total_rolling_days = len(df_rolling_period)\n",
    "    \n",
    "    if apply_gap_to_train_val:\n",
    "        min_required_days = initial_train_size + gap_size + val_size + gap_size + test_size\n",
    "    else:\n",
    "        min_required_days = initial_train_size + val_size + gap_size + test_size\n",
    "    \n",
    "    if total_rolling_days < min_required_days:\n",
    "        n_splits = 0\n",
    "    else:\n",
    "        n_splits = (total_rolling_days - min_required_days) // step + 1\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        test_end_idx = total_rolling_days - (fold_idx * step)\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        \n",
    "        if test_start_idx < 0: break\n",
    "        \n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        \n",
    "        if apply_gap_to_train_val:\n",
    "            train_end_idx = val_start_idx - gap_size\n",
    "        else:\n",
    "            train_end_idx = val_start_idx\n",
    "            \n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0: break\n",
    "        \n",
    "        train_fold = df_rolling_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_rolling_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_rolling_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold, 'val': val_fold, 'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1, 'fold_type': 'walk_forward_rolling'\n",
    "        })\n",
    "        \n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "\n",
    "    if len(final_test_df) > 0 and len(df_rolling_period) > 0:\n",
    "        \n",
    "        final_val_end_idx = len(df_rolling_period) \n",
    "        final_val_start_idx = final_val_end_idx - val_size\n",
    "        \n",
    "        if apply_gap_to_train_val:\n",
    "            final_train_end_idx = final_val_start_idx - gap_size\n",
    "        else:\n",
    "            final_train_end_idx = final_val_start_idx\n",
    "            \n",
    "        final_train_start_idx = max(0, final_train_end_idx - initial_train_size)\n",
    "        \n",
    "        final_train_data = df_rolling_period.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "        final_val_data = df_rolling_period.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None, trend_params=None):\n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(), train_start_date=train_df['date'].min(), is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(), train_start_date=val_df['date'].min(),\n",
    "        is_train=False, train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(), train_start_date=test_df['date'].min(),\n",
    "        is_train=False, train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_direction','next_log_return', 'next_close','next_open', \n",
    "                   'take_profit_price', 'stop_loss_price',\n",
    "                   'trend_direction', 'trend_strength', 'exit_reason']\n",
    "    \n",
    "    dropna_cols = ['next_direction','next_log_return', 'next_close','next_open', \n",
    "                   'take_profit_price', 'stop_loss_price']\n",
    "\n",
    "    train_processed = train_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                    if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 클래스 불균형 분석 코드 추가\n",
    "    # ==========================================================================\n",
    "    def analyze_class_balance(y_df, name):\n",
    "        target_col = 'next_direction'\n",
    "        if target_col not in y_df.columns:\n",
    "            return \"\"\n",
    "        \n",
    "        counts = y_df[target_col].value_counts(normalize=True).sort_index()\n",
    "        total_n = len(y_df)\n",
    "        \n",
    "        # 클래스 0, 1, 2의 비율을 포맷팅\n",
    "        ratio_0 = counts.get(0, 0)\n",
    "        ratio_1 = counts.get(1, 0)\n",
    "        ratio_2 = counts.get(2, 0)\n",
    "\n",
    "        # 타겟 클래스 맵핑: 0:Hold, 1:Long, 2:Short\n",
    "        output = f\" {name} Set (N={total_n}): Hold(0): {ratio_0:.2%} | Long(1): {ratio_1:.2%} | Short(2): {ratio_2:.2%}\"\n",
    "        \n",
    "        return output\n",
    "\n",
    "    print(f\"\\n--- Fold {fold_idx} ({fold_type}) Class Balance Analysis (top_n={top_n}) ---\")\n",
    "    print(analyze_class_balance(y_train, 'Train'))\n",
    "    print(analyze_class_balance(y_val, 'Validation'))\n",
    "    print(analyze_class_balance(y_test, 'Test'))\n",
    "    print(\"-\" * 60)\n",
    "    # ==========================================================================\n",
    "\n",
    "\n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, y_train, target_type=target_type, top_n=top_n\n",
    "    )\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    if trend_params is None:\n",
    "        trend_params = {}\n",
    "        \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_processed['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),\n",
    "            'dates': test_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'scaler': robust_scaler, \n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'missing_stats': missing_stats,  \n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx,\n",
    "            'trend_window': trend_params.get('trend_window', 120),\n",
    "            'trend_analysis_points': trend_params.get('trend_analysis_points', 5)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                      final_test_start='2025-01-01',\n",
    "                                      method='tvt', target_type='direction',\n",
    "                                      lookahead_candles=8, atr_multiplier_profit=1.5, \n",
    "                                      atr_multiplier_stop=1.0, **kwargs):\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = preprocess_non_stationary_features(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "    \n",
    "    trend_params = {\n",
    "        'trend_window': kwargs.get('trend_window', 120),\n",
    "        'trend_analysis_points': kwargs.get('trend_analysis_points', 5)\n",
    "    }\n",
    "    \n",
    "    df = create_targets(\n",
    "        df, \n",
    "        lookahead_candles=lookahead_candles,\n",
    "        atr_multiplier_profit=atr_multiplier_profit, \n",
    "        atr_multiplier_stop=atr_multiplier_stop,\n",
    "        **trend_params  \n",
    "    )\n",
    "    \n",
    "    df = remove_raw_prices_and_transform(df, target_type, method)\n",
    "    df = df.iloc[:-lookahead_candles]\n",
    "    \n",
    "    split_kwargs = {\n",
    "        'final_test_start': final_test_start,\n",
    "        'gap_size': lookahead_candles, \n",
    "        'apply_gap_to_train_val': True\n",
    "    }\n",
    "    \n",
    "    for key in ['initial_train_size', 'val_size', 'test_size', 'step']:\n",
    "        if key in kwargs:\n",
    "            split_kwargs[key] = kwargs[key]\n",
    "    \n",
    "    splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "\n",
    "    # top_n을 20으로 설정하여 특징 개수 축소 (기존 30/40에서 조정)\n",
    "    top_n_features = kwargs.get('top_n', 20) \n",
    "    \n",
    "    result = [\n",
    "        process_single_split(\n",
    "            fold, \n",
    "            target_type=target_type,  \n",
    "            top_n=top_n_features,\n",
    "            fold_idx=fold['fold_idx'],\n",
    "            trend_params=trend_params\n",
    "        ) \n",
    "        for fold in splits\n",
    "    ]\n",
    "    \n",
    "    summary_data = []\n",
    "    for fold in result:\n",
    "        f_idx = fold['stats']['fold_idx']\n",
    "        f_type = fold['stats']['fold_type']\n",
    "        tr_dates = fold['train']['dates']\n",
    "        val_dates = fold['val']['dates']\n",
    "        te_dates = fold['test']['dates']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Fold': f_idx,\n",
    "            'Type': f_type,\n",
    "            'Train_N': len(fold['train']['y']),\n",
    "            'Train_Start': tr_dates.min().date() if len(tr_dates)>0 else '-',\n",
    "            'Train_End': tr_dates.max().date() if len(tr_dates)>0 else '-',\n",
    "            'Val_N': len(fold['val']['y']),\n",
    "            'Val_Start': val_dates.min().date() if len(val_dates)>0 else '-',\n",
    "            'Val_End': val_dates.max().date() if len(val_dates)>0 else '-',\n",
    "            'Test_N': len(fold['test']['y']),\n",
    "            'Test_Start': te_dates.min().date() if len(te_dates)>0 else '-',\n",
    "            'Test_End': te_dates.max().date() if len(te_dates)>0 else '-'\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\" [FINAL PIPELINE SUMMARY] dropna() 이후 데이터 생존 현황\")\n",
    "    print(\"=\"*100)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee94dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionModels:\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Random Forest (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 10, 40),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 4, 20),\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            val_pred_proba = model.predict_proba(X_val)\n",
    "            return log_loss(y_val, val_pred_proba)\n",
    "        \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=15)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # 과적합 확인용 출력\n",
    "        train_acc = accuracy_score(y_train, best_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, best_model.predict(X_val))\n",
    "        print(f\"[RandomForest] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. LightGBM (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 60),\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': 3,\n",
    "                'metric': 'multi_logloss',\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': 42,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[early_stopping(20, verbose=False)])\n",
    "            return model.best_score_['valid_0']['multi_logloss']\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params.update({'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss', 'class_weight': 'balanced', 'random_state': 42, 'verbose': -1, 'force_col_wise': True})\n",
    "        \n",
    "        final_model = LGBMClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, final_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, final_model.predict(X_val))\n",
    "        print(f\"[LightGBM] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. XGBoost (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': 3,\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], sample_weight=sample_weights, verbose=False)\n",
    "            return model.evals_result()['validation_0']['mlogloss'][-1]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params.update({'objective': 'multi:softprob', 'num_class': 3, 'eval_metric': 'mlogloss', 'tree_method': 'hist', 'random_state': 42, 'n_jobs': -1})\n",
    "        \n",
    "        final_model = XGBClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, final_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, final_model.predict(X_val))\n",
    "        print(f\"[XGBoost] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 5. Logistic Regression (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'lbfgs',\n",
    "                'multi_class': 'multinomial',\n",
    "                'class_weight': 'balanced',\n",
    "                'max_iter': 2000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = LogisticRegression(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_pred_proba = model.predict_proba(X_val)\n",
    "            return log_loss(y_val, val_pred_proba)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=15)\n",
    "        \n",
    "        best_model = LogisticRegression(**study.best_params, penalty='l2', solver='lbfgs', multi_class='multinomial', class_weight='balanced', max_iter=2000, random_state=42, n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, best_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, best_model.predict(X_val))\n",
    "        print(f\"[LogisticReg] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 9. CatBoost (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'depth': trial.suggest_int('depth', 3, 8),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'loss_function': 'MultiClass',\n",
    "                'eval_metric': 'MultiClass',\n",
    "                'auto_class_weights': 'Balanced',\n",
    "                'random_seed': 42,\n",
    "                'verbose': False,\n",
    "                'early_stopping_rounds': 20,\n",
    "                'allow_writing_files': False\n",
    "            }\n",
    "            \n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "            \n",
    "            val_pred_proba = model.predict_proba(X_val)\n",
    "            \n",
    "            \n",
    "            if val_pred_proba.shape[1] < 3:\n",
    "                padded = np.zeros((val_pred_proba.shape[0], 3))\n",
    "\n",
    "                padded[:, :val_pred_proba.shape[1]] = val_pred_proba\n",
    "                val_pred_proba = padded\n",
    "\n",
    "            try:\n",
    "                return log_loss(y_val, val_pred_proba, labels=[0, 1, 2])\n",
    "            except ValueError:\n",
    "\n",
    "                acc = accuracy_score(y_val, model.predict(X_val))\n",
    "                return 1.0 - acc\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'loss_function': 'MultiClass', \n",
    "            'eval_metric': 'MultiClass', \n",
    "            'auto_class_weights': 'Balanced', \n",
    "            'random_seed': 42, \n",
    "            'verbose': False, \n",
    "            'allow_writing_files': False\n",
    "        })\n",
    "        \n",
    "        final_model = CatBoostClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "        train_pred = final_model.predict(X_train).ravel()\n",
    "        val_pred = final_model.predict(X_val).ravel()\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        \n",
    "        print(f\"[CatBoost] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 13. Gradient Boosting (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "                'validation_fraction': 0.1,\n",
    "                'n_iter_no_change': 10,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            val_pred_proba = model.predict_proba(X_val)\n",
    "            return log_loss(y_val, val_pred_proba)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=15)\n",
    "        \n",
    "        best_model = GradientBoostingClassifier(**study.best_params, random_state=42)\n",
    "        best_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, best_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, best_model.predict(X_val))\n",
    "        print(f\"[GradientBoost] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 14. HistGradientBoosting (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def histgradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "                'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 10.0, log=True),\n",
    "                'early_stopping': True,\n",
    "                'validation_fraction': 0.1,\n",
    "                'n_iter_no_change': 10,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = HistGradientBoostingClassifier(**params)\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            val_pred_proba = model.predict_proba(X_val)\n",
    "            return log_loss(y_val, val_pred_proba)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=15)\n",
    "        \n",
    "        best_model = HistGradientBoostingClassifier(**study.best_params, early_stopping=True, random_state=42)\n",
    "        best_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, best_model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, best_model.predict(X_val))\n",
    "        print(f\"[HistGradient] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 15. Stacking Ensemble (ML)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        base_learners = [\n",
    "            ('xgb', XGBClassifier(n_estimators=100, max_depth=3, objective='multi:softprob', num_class=3, eval_metric='mlogloss', n_jobs=-1, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, max_depth=3, objective='multiclass', num_class=3, class_weight='balanced', n_jobs=-1, random_state=42, verbose=-1)),\n",
    "            ('hist', HistGradientBoostingClassifier(max_iter=100, max_depth=3, random_state=42))\n",
    "        ]\n",
    "        \n",
    "        meta_learner = LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=42)\n",
    "        \n",
    "        model = StackingClassifier(\n",
    "            estimators=base_learners,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        print(f\"[Stacking] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 19. LSTM (DL)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units = trial.suggest_int('units', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                LSTM(units, input_shape=input_shape, return_sequences=False),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dropout(dropout),\n",
    "                Dense(3, activation='softmax') # 3-Class\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "            return history.history['val_loss'][-1]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=10)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units'], input_shape=input_shape, return_sequences=False),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['lr']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # 재학습\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "        \n",
    "        _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        _, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[LSTM] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 20. BiLSTM (DL)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units = trial.suggest_int('units', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(units, return_sequences=False), input_shape=input_shape),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dropout(dropout),\n",
    "                Dense(3, activation='softmax')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "            return history.history['val_loss'][-1]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=10)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(best_params['units'], return_sequences=False), input_shape=input_shape),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['lr']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "        \n",
    "        _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        _, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[BiLSTM] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 21. GRU (DL)\n",
    "    # ==========================================================================\n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units = trial.suggest_int('units', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                GRU(units, input_shape=input_shape, return_sequences=False),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dropout(dropout),\n",
    "                Dense(3, activation='softmax')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "            return history.history['val_loss'][-1]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=10)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            GRU(best_params['units'], input_shape=input_shape, return_sequences=False),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['lr']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, class_weight=class_weight_dict, verbose=0)\n",
    "        \n",
    "        _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        _, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[GRU] Acc: {train_acc:.4f}/{val_acc:.4f} (Gap: {train_acc-val_acc:.4f})\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc00393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models (8개) \n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 6, 'name': 'HistGradientBoosting', 'func': DirectionModels.histgradient_boosting, 'needs_val': True},\n",
    "    {'index': 7, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    {'index': 8, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# DL Models (3개)\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 9, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 10, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 11, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3bfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.predictions = {}\n",
    "        self.models = {} if save_models else None\n",
    "        self.save_models = save_models\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for p in pred:\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return np.array(cleaned)\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "\n",
    "    def _ensure_3class_proba(self, proba):\n",
    "        if proba is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(proba, list):\n",
    "            proba = np.array(proba)\n",
    "            \n",
    "        if proba.ndim == 1:\n",
    "            proba = proba.reshape(-1, 1)\n",
    "\n",
    "        n_samples, n_classes = proba.shape\n",
    "        \n",
    "        if n_classes >= 3:\n",
    "            return proba\n",
    "        \n",
    "        padded = np.zeros((n_samples, 3))\n",
    "        padded[:, :n_classes] = proba\n",
    "        return padded\n",
    "\n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                    X_test, y_test_df, test_dates, model_name,\n",
    "                                    is_deep_learning=False):\n",
    "        \n",
    "        if is_deep_learning:\n",
    "            train_pred_proba = self._predict_model(model, X_train)\n",
    "            val_pred_proba = self._predict_model(model, X_val)\n",
    "            test_pred_proba = self._predict_model(model, X_test)\n",
    "            \n",
    "            if isinstance(test_pred_proba, list): test_pred_proba = test_pred_proba[0]\n",
    "            if isinstance(train_pred_proba, list): train_pred_proba = train_pred_proba[0]\n",
    "            if isinstance(val_pred_proba, list): val_pred_proba = val_pred_proba[0]\n",
    "\n",
    "        else:\n",
    "            train_pred = self._predict_model(model, X_train)\n",
    "            val_pred = self._predict_model(model, X_val)\n",
    "            test_pred = self._predict_model(model, X_test)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                train_pred_proba = model.predict_proba(X_train)\n",
    "                val_pred_proba = model.predict_proba(X_val)\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "            else:\n",
    "                n_classes = 3\n",
    "                train_pred_proba = np.eye(n_classes)[train_pred.astype(int)]\n",
    "                val_pred_proba = np.eye(n_classes)[val_pred.astype(int)]\n",
    "                test_pred_proba = np.eye(n_classes)[test_pred.astype(int)]\n",
    "\n",
    "        train_pred_proba = self._ensure_3class_proba(train_pred_proba)\n",
    "        val_pred_proba = self._ensure_3class_proba(val_pred_proba)\n",
    "        test_pred_proba = self._ensure_3class_proba(test_pred_proba)\n",
    "\n",
    "        train_pred = np.argmax(train_pred_proba, axis=1)\n",
    "        val_pred = np.argmax(val_pred_proba, axis=1)\n",
    "        test_pred = np.argmax(test_pred_proba, axis=1)\n",
    "        \n",
    "        y_test_direction = y_test_df['next_direction'].values.astype(int)\n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "\n",
    "        # 1. 기본 지표 계산\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test_direction, test_pred)\n",
    "        \n",
    "\n",
    "        report = classification_report(y_test_direction, test_pred, output_dict=True, zero_division=0, labels=[0, 1, 2])\n",
    "        \n",
    "        # 0: 관망, 1: 롱, 2: 숏\n",
    "        prec_neutral = report['0']['precision']\n",
    "        prec_long = report['1']['precision']   # 여기가 진짜 롱 승률\n",
    "        prec_short = report['2']['precision']  # 여기가 진짜 숏 승률\n",
    "        \n",
    "        rec_long = report['1']['recall']\n",
    "        rec_short = report['2']['recall']\n",
    "        \n",
    "        f1_long = report['1']['f1-score']\n",
    "        \n",
    "        # Weighted Average (참고용으로 유지)\n",
    "        test_prec_weighted = precision_score(y_test_direction, test_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            test_roc_auc = roc_auc_score(y_test_direction, test_pred_proba, multi_class='ovr')\n",
    "        except:\n",
    "            test_roc_auc = 0.0\n",
    "\n",
    "        self._save_predictions(model_name, test_pred, test_pred_proba, y_test_df, test_dates)\n",
    "        \n",
    "        if self.save_models and self.models is not None:\n",
    "            self.models[model_name] = model\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name, \n",
    "            'Train_Accuracy': train_acc, \n",
    "            'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc, \n",
    "            \n",
    "            'Precision_Long': prec_long,\n",
    "            'Precision_Short': prec_short,\n",
    "            'Precision_Neutral': prec_neutral,\n",
    "            \n",
    "            'Recall_Long': rec_long,\n",
    "            'Recall_Short': rec_short,\n",
    "            'F1_Long': f1_long,\n",
    "            \n",
    "            # 전체 지표 (참고용)\n",
    "            'Test_Precision_Weighted': test_prec_weighted, \n",
    "            'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        # 4. 터미널 출력도 상세하게 변경\n",
    "        print(f\"[{model_name}] Total Acc: {test_acc:.4f} | Long(1) Prec: {prec_long:.4f} | Short(2) Prec: {prec_short:.4f}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        return self.results[-1]\n",
    "    \n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba, y_test_df, dates):\n",
    "        pred_proba = self._ensure_3class_proba(pred_proba)\n",
    "\n",
    "        prob_neutral = pred_proba[:, 0]\n",
    "        prob_long = pred_proba[:, 1]\n",
    "        prob_short = pred_proba[:, 2]\n",
    "        \n",
    "        trade_confidence = np.where(pred_direction == 1, prob_long, \n",
    "                                  np.where(pred_direction == 2, prob_short, 0.0))\n",
    "\n",
    "        y_test_direction = y_test_df['next_direction'].values.astype(int)\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': y_test_direction,\n",
    "            'actual_return': y_test_df['next_log_return'].values,\n",
    "            'take_profit_price': y_test_df['take_profit_price'].values,\n",
    "            'stop_loss_price': y_test_df['stop_loss_price'].values,\n",
    "            'pred_direction': pred_direction,\n",
    "            'prob_neutral': prob_neutral,\n",
    "            'prob_long': prob_long,\n",
    "            'prob_short': prob_short,\n",
    "            'trade_confidence': trade_confidence,\n",
    "            'is_correct': (pred_direction == y_test_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        self.predictions[model_name] = predictions_df\n",
    "    \n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_predictions_dict(self):\n",
    "        return self.predictions\n",
    "    \n",
    "    def get_models_dict(self):\n",
    "        return self.models if self.models is not None else {}\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        X_arr = np.array(Xs)\n",
    "        y_arr = np.array(ys)\n",
    "        del Xs, ys\n",
    "        gc.collect()\n",
    "        return X_arr, y_arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        tf.keras.backend.clear_session()\n",
    "        try:\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "    \n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train, y_train, X_val, y_val, X_test, y_test_df,\n",
    "                test_dates, model_config['name'], is_deep_learning=is_mlp\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                if is_mlp:\n",
    "                    self.clear_memory()\n",
    "                else:\n",
    "                    gc.collect()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            if model_config.get('name') == 'MLP':\n",
    "                self.clear_memory()\n",
    "            else:\n",
    "                gc.collect()\n",
    "    \n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            self.clear_memory()\n",
    "            \n",
    "            model = model_config['func'](X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape)\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                X_test_seq, y_test_df_seq, test_dates_seq,\n",
    "                model_config['name'], is_deep_learning=True\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                self.clear_memory()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, evaluator, lookback=30, ml_models=None, dl_models=None):\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    ml_success = 0\n",
    "    for model_config in ml_models:\n",
    "        if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "            ml_success += 1\n",
    "        gc.collect()\n",
    "    \n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test_direction, lookback)\n",
    "    \n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    y_test_df_seq = y_test_df.iloc[lookback:].reset_index(drop=True)\n",
    "    \n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    dl_success = 0\n",
    "    for model_config in dl_models:\n",
    "        trainer.clear_memory()\n",
    "        \n",
    "        if model_config['name'] in ['TabNet', 'StackingEnsemble', 'VotingHard', 'VotingSoft']:\n",
    "            if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "                dl_success += 1\n",
    "        else:\n",
    "            if trainer.train_dl_model(model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape):\n",
    "                dl_success += 1\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    del X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, y_test_df_seq, test_dates_seq\n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    return ml_success + dl_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8736210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data_once(pipeline_result, target_name, split_method):\n",
    "    raw_dir = os.path.join(RESULT_DIR, \"raw_data\", target_name, split_method)\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    \n",
    "    for fold_data in pipeline_result:\n",
    "        fold_idx = fold_data['stats']['fold_idx']\n",
    "        fold_type = fold_data['stats']['fold_type']\n",
    "        fold_dir = os.path.join(raw_dir, f\"fold_{fold_idx}_{fold_type}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        features = fold_data['stats']['selected_features']\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if 'X_raw' in fold_data[split]:\n",
    "                df = pd.DataFrame(fold_data[split]['X_raw'], columns=features)\n",
    "                df['date'] = fold_data[split]['dates']\n",
    "                \n",
    "                y_df = fold_data[split]['y']\n",
    "                df = pd.concat([df, y_df], axis=1)\n",
    "                \n",
    "                df.to_csv(os.path.join(fold_dir, f\"{split}_raw.csv\"), index=False)\n",
    "\n",
    "def check_fold_completed(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    \n",
    "    if not os.path.isdir(fold_dir):\n",
    "        return False\n",
    "    \n",
    "    required_files = [\n",
    "        \"fold_summary.csv\",\n",
    "        \"robust_scaler.pkl\",\n",
    "        \"standard_scaler.pkl\",\n",
    "        \"selected_features.pkl\",\n",
    "        \"inference_config.pkl\"\n",
    "    ]\n",
    "    \n",
    "    for file in required_files:\n",
    "        if not os.path.exists(os.path.join(fold_dir, file)):\n",
    "            return False\n",
    "    \n",
    "    model_files = [f for f in os.listdir(fold_dir) if f.endswith(('.pkl', '.h5')) \n",
    "                   and not any(x in f for x in ['scaler', 'features', 'stats', 'config'])]\n",
    "    \n",
    "    return len(model_files) > 0\n",
    "\n",
    "def save_fold_results(fold_idx, fold_type, evaluator, target_name, fold_data):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, model_obj in evaluator.get_models_dict().items():\n",
    "        try:\n",
    "            is_dl = isinstance(model_obj, tf.keras.Model)\n",
    "            ext = \".h5\" if is_dl else \".pkl\"\n",
    "            path = os.path.join(fold_dir, f\"{model_name}{ext}\")\n",
    "            \n",
    "            if is_dl: \n",
    "                model_obj.save(path)\n",
    "            else:\n",
    "                with open(path, 'wb') as f:\n",
    "                    pickle.dump(model_obj, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {model_name}: {e}\")\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"robust_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['robust_scaler'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"standard_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['standard_scaler'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"selected_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['selected_features'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"missing_stats.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['missing_stats'], f)\n",
    "    \n",
    "    inference_config = {\n",
    "        'selected_features': fold_data['stats']['selected_features'],\n",
    "        'feature_order': fold_data['stats']['selected_features'], \n",
    "        'target_cols': fold_data['stats']['target_cols'],\n",
    "        'target_type': fold_data['stats']['target_type'],\n",
    "        'fold_type': fold_type,\n",
    "        'fold_idx': fold_idx,\n",
    "        'trend_params': {\n",
    "            'trend_window': fold_data['stats'].get('trend_window', 120),\n",
    "            'trend_analysis_points': fold_data['stats'].get('trend_analysis_points', 5)\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(fold_dir, \"inference_config.pkl\"), 'wb') as f:\n",
    "        pickle.dump(inference_config, f)\n",
    "    \n",
    "    for model_name, pred_df in evaluator.get_predictions_dict().items():\n",
    "        pred_df.to_csv(os.path.join(fold_dir, f\"{model_name}_predictions.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "        \n",
    "    summary = evaluator.get_summary_dataframe()\n",
    "    summary.to_csv(os.path.join(fold_dir, \"fold_summary.csv\"), \n",
    "                   index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    return summary, evaluator.get_predictions_dict()\n",
    "\n",
    "def load_fold_results(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    if not os.path.exists(summary_path): \n",
    "        return None, None\n",
    "    summary = pd.read_csv(summary_path)\n",
    "    predictions = {f.replace('_predictions.csv', ''): pd.read_csv(os.path.join(fold_dir, f)) \n",
    "                   for f in os.listdir(fold_dir) if f.endswith('_predictions.csv')}\n",
    "    return summary, predictions\n",
    "\n",
    "def save_walk_forward_summary(all_fold_results, target_name):\n",
    "    if not all_fold_results: \n",
    "        return\n",
    "    detailed_df = pd.concat([df.assign(Fold=i+1, fold_type=ft) \n",
    "                             for i, (df, ft) in enumerate(all_fold_results)], ignore_index=True)\n",
    "    detailed_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_all_folds_detailed.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward_rolling'].copy()\n",
    "    if wf_data.empty: \n",
    "        return\n",
    "\n",
    "    numeric_cols = wf_data.select_dtypes(include=np.number).columns.drop('Fold', errors='ignore')\n",
    "    avg_results = wf_data.groupby('Model')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "    avg_results.columns = ['_'.join(col).strip() if col[1] else col[0] for col in avg_results.columns.values]\n",
    "    \n",
    "    sort_col = 'Test_Precision_mean' if 'Test_Precision_mean' in avg_results.columns else 'Test_Accuracy_mean'\n",
    "    avg_results = avg_results.sort_values(by=sort_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    avg_results.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_walk_forward_average.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "\n",
    "def load_all_fold_results_from_disk(target_name):\n",
    "    fold_results_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name)\n",
    "    if not os.path.isdir(fold_results_dir):\n",
    "        return []\n",
    "    \n",
    "    fold_dirs = sorted([d for d in os.listdir(fold_results_dir) \n",
    "                        if os.path.isdir(os.path.join(fold_results_dir, d))])\n",
    "    \n",
    "    fold_results = []\n",
    "    for fold_dir_name in fold_dirs:\n",
    "        fold_parts = fold_dir_name.split('_')\n",
    "        fold_idx = int(fold_parts[1])\n",
    "        fold_type = '_'.join(fold_parts[2:])\n",
    "        \n",
    "        summary, _ = load_fold_results(target_name, fold_idx, fold_type)\n",
    "        if summary is not None:\n",
    "            fold_results.append((summary, fold_type))\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "def run_and_save_master_summary(result_dir):\n",
    "    summary_files = glob.glob(os.path.join(result_dir, \"*_walk_forward_average.csv\"))\n",
    "    if not summary_files: \n",
    "        return\n",
    "\n",
    "    master_list = []\n",
    "    for f in summary_files:\n",
    "        filename = os.path.basename(f)\n",
    "        parts = filename.replace('trial_', '').replace('_walk_forward_average.csv', '').split('_')\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        for p in parts:\n",
    "            if p.startswith('l'):\n",
    "                df['lookahead'] = int(p[1:])\n",
    "            elif p.startswith('p'):\n",
    "                df['profit_mult'] = float(p[1:])\n",
    "            elif p.startswith('s'):\n",
    "                df['stop_mult'] = float(p[1:])\n",
    "            elif p.startswith('tw'):\n",
    "                df['trend_window'] = int(p[2:])\n",
    "            elif p.startswith('tap'):\n",
    "                df['trend_analysis_points'] = int(p[3:])\n",
    "        \n",
    "        master_list.append(df)\n",
    "        \n",
    "    master_df = pd.concat(master_list, ignore_index=True)\n",
    "    \n",
    "    sort_col = 'Test_Precision_mean' if 'Test_Precision_mean' in master_df.columns else 'Test_Accuracy_mean'\n",
    "    master_df = master_df.sort_values(by=sort_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    master_df.to_csv(os.path.join(result_dir, \"_MASTER_SUMMARY_RESULTS.csv\"), \n",
    "                     index=False, encoding='utf-8-sig')\n",
    "\n",
    "def check_experiment_completed(target_name, result_dir):\n",
    "    fold_results_dir = os.path.join(result_dir, \"fold_results\", target_name)\n",
    "    if os.path.isdir(fold_results_dir):\n",
    "        fold_dirs = [d for d in os.listdir(fold_results_dir) if os.path.isdir(os.path.join(fold_results_dir, d))]\n",
    "        has_final_holdout = any('final_holdout' in d for d in fold_dirs)\n",
    "        \n",
    "        if has_final_holdout and len(fold_dirs) >= 2:\n",
    "            summary_path = os.path.join(result_dir, f\"{target_name}_walk_forward_average.csv\")\n",
    "            if os.path.exists(summary_path):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def record_optuna_result(trial_num, target_name, score, params):\n",
    "    summary_file = os.path.join(RESULT_DIR, \"optuna_trials_summary.csv\")\n",
    "    \n",
    "    row = {\n",
    "        'trial_number': trial_num,\n",
    "        'target_name': target_name,\n",
    "        'score': score,\n",
    "        **params\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(summary_file):\n",
    "        df.to_csv(summary_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(summary_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91342413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     lookahead = trial.suggest_int('lookahead', 3, 5)\n",
    "#     p_mult = trial.suggest_float('profit_mult', 0.8, 1.5, step=0.1)\n",
    "#     s_mult = trial.suggest_float('stop_mult', 0.5, 1.0, step=0.1)\n",
    "#     trend_window = trial.suggest_int('trend_window', 10, 30, step=5)\n",
    "#     trend_analysis_points = trial.suggest_int('trend_analysis_points', 3, 5)\n",
    "    \n",
    "#     param_suffix = f\"l{lookahead}_p{p_mult:.1f}_s{s_mult:.1f}_tw{trend_window}_tap{trend_analysis_points}\"\n",
    "#     trial_target_name = f\"trial_{trial.number}_{param_suffix}\"\n",
    "    \n",
    "#     if check_experiment_completed(trial_target_name, RESULT_DIR):\n",
    "#         print(f\"Trial {trial.number} already completed - skipping\")\n",
    "#         summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "#         existing_summary = pd.read_csv(summary_path)\n",
    "        \n",
    "#         if 'Test_Precision_mean' in existing_summary.columns:\n",
    "#             return existing_summary['Test_Precision_mean'].max()\n",
    "#         return 0.0\n",
    "    \n",
    "#     try:\n",
    "#         pipeline_result = build_complete_pipeline_corrected(\n",
    "#             df_raw=df_merged,\n",
    "#             train_start_date=TRAIN_START_DATE,\n",
    "#             final_test_start='2025-01-01',\n",
    "#             method='walk_forward',\n",
    "#             target_type='direction',\n",
    "#             lookahead_candles=lookahead,\n",
    "#             atr_multiplier_profit=p_mult,\n",
    "#             atr_multiplier_stop=s_mult,\n",
    "#             trend_window=trend_window,\n",
    "#             trend_analysis_points=trend_analysis_points,\n",
    "#             top_n=40\n",
    "#         )\n",
    "        \n",
    "#         save_raw_data_once(pipeline_result, trial_target_name, 'walk_forward')\n",
    "        \n",
    "#         fold_results = []\n",
    "        \n",
    "#         for fold_data in pipeline_result:\n",
    "#             fold_idx = fold_data['stats']['fold_idx']\n",
    "#             fold_type = fold_data['stats']['fold_type']\n",
    "            \n",
    "#             if check_fold_completed(trial_target_name, fold_idx, fold_type):\n",
    "#                 print(f\"Fold {fold_idx} already completed\")\n",
    "#                 fold_summary, _ = load_fold_results(trial_target_name, fold_idx, fold_type)\n",
    "#                 if fold_summary is not None:\n",
    "#                     fold_results.append((fold_summary, fold_type))\n",
    "#                 continue\n",
    "            \n",
    "#             print(f\"Training Fold {fold_idx} ({fold_type})...\")\n",
    "            \n",
    "#             evaluator = ModelEvaluator(save_models=True)\n",
    "            \n",
    "#             train_all_models(\n",
    "#                 fold_data['train']['X_robust'], \n",
    "#                 fold_data['train']['y']['next_direction'].values.astype(int),\n",
    "#                 fold_data['val']['X_robust'], \n",
    "#                 fold_data['val']['y']['next_direction'].values.astype(int),\n",
    "#                 fold_data['test']['X_robust'], \n",
    "#                 fold_data['test']['y'],\n",
    "#                 fold_data['test']['dates'].values,\n",
    "#                 evaluator,\n",
    "#                 ml_models=ML_MODELS_CLASSIFICATION,\n",
    "#                 dl_models=DL_MODELS_CLASSIFICATION\n",
    "#             )\n",
    "            \n",
    "#             fold_summary, _ = save_fold_results(fold_idx, fold_type, evaluator, trial_target_name, fold_data)\n",
    "#             fold_results.append((fold_summary, fold_type))\n",
    "            \n",
    "#             del evaluator\n",
    "#             tf.keras.backend.clear_session()\n",
    "#             gc.collect()\n",
    "        \n",
    "#         save_walk_forward_summary(fold_results, trial_target_name)\n",
    "        \n",
    "#         summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "#         if os.path.exists(summary_path):\n",
    "#             wf_avg = pd.read_csv(summary_path)\n",
    "#             best_score = wf_avg['Test_Precision_mean'].max()\n",
    "            \n",
    "#             record_optuna_result(trial.number, trial_target_name, best_score, trial.params)\n",
    "#             return best_score\n",
    "        \n",
    "#         return 0.0\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Trial {trial.number} Failed: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return 0.0\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "# os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     try:\n",
    "#         for gpu in tf.config.list_physical_devices('GPU'):\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "# study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 23:25:04,111] A new study created in memory with name: no-name-0dc0e4a6-ce5d-4b42-8dc3-ffacb2dc5913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: trial_5_l5_p1.2_s0.7_tw20_tap4 (Optuna Index: 0)\n",
      "\n",
      "--- Fold 1 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 14.12% | Long(1): 32.88% | Short(2): 53.00%\n",
      " Validation Set (N=150): Hold(0): 19.33% | Long(1): 25.33% | Short(2): 55.33%\n",
      " Test Set (N=150): Hold(0): 13.33% | Long(1): 34.00% | Short(2): 52.67%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, DMP_14, DMN_14, btc_eth_strength_ratio_7d, sp500_SP500_pct_5d_lag1, vix_VIX_pct_5d_lag1, gold_GOLD_pct_5d_lag1, btc_volatility_7d, VWAP, ROLLING_MIN_20, ISB_26, SMA_50, IKS_26, ISA_9, ROLLING_MAX_20, VWMA_20, BBL_20, AD, WMA_20\n",
      "\n",
      "--- Fold 2 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 14.75% | Long(1): 31.25% | Short(2): 54.00%\n",
      " Validation Set (N=150): Hold(0): 14.67% | Long(1): 32.00% | Short(2): 53.33%\n",
      " Test Set (N=150): Hold(0): 18.67% | Long(1): 24.00% | Short(2): 57.33%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, DMN_14, eth_btc_corr_7d, VIX_ETH_Vol_Cross_lag1, gold_GOLD_pct_5d_lag1, btc_volatility_7d, VWAP, IKS_26, ROLLING_MIN_20, ISB_26, ISA_9, xrp_volatility_30d, SMA_50, eth_btc_corr_highvol, ada_volatility_30d, BBL_20, avax_volatility_30d, ITS_9, VWMA_20\n",
      "\n",
      "--- Fold 3 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 15.62% | Long(1): 29.50% | Short(2): 54.87%\n",
      " Validation Set (N=150): Hold(0): 18.67% | Long(1): 24.67% | Short(2): 56.67%\n",
      " Test Set (N=150): Hold(0): 8.00% | Long(1): 50.67% | Short(2): 41.33%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, ADX_14, vol_percentile_90d, RSI_percentile_60d, usdt_totalCirculating_pct_5d_lag1, aave_aave_eth_tvl_ma30_ratio_lag1, VWAP, ROLLING_MIN_20, IKS_26, ISB_26, ISA_9, eth_btc_corr_highvol, xrp_volatility_30d, ada_volatility_30d, ITS_9, SMA_50, doge_volatility_30d, ROLLING_MAX_20, BBL_20\n",
      "\n",
      "--- Fold 4 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 16.50% | Long(1): 27.50% | Short(2): 56.00%\n",
      " Validation Set (N=150): Hold(0): 8.00% | Long(1): 49.33% | Short(2): 42.67%\n",
      " Test Set (N=150): Hold(0): 22.00% | Long(1): 21.33% | Short(2): 56.67%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, usdt_totalBridgedToUSD_ma30_ratio_lag1, btc_volatility_7d, BB_POSITION, eth_btc_volcorr_sq_30d_ma30_ratio, l2_optimism_tvl_pct_5d_lag1, VWAP, eth_btc_corr_highvol, ROLLING_MAX_20, IKS_26, ROLLING_MIN_20, SMA_50, ISB_26, ITS_9, ISA_9, SMA_20, BBL_20, KCB_20, BBM_20\n",
      "\n",
      "--- Fold 5 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 15.88% | Long(1): 29.88% | Short(2): 54.25%\n",
      " Validation Set (N=150): Hold(0): 21.33% | Long(1): 23.33% | Short(2): 55.33%\n",
      " Test Set (N=150): Hold(0): 16.00% | Long(1): 30.00% | Short(2): 54.00%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, MACDH_12_26_9, BB_WIDTH, eth_btc_corr_lowvol, RSI_percentile_60d, dxy_DXY_ma30_ratio_lag1, xrp_volatility_30d, VWAP, ROLLING_MIN_20, ITS_9, ROLLING_MAX_20, eth_btc_corr_highvol, IKS_26, SMA_50, ISA_9, ISB_26, SMA_20, DEMA_10, ada_volatility_30d\n",
      "\n",
      "--- Fold 6 (final_holdout) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 17.00% | Long(1): 30.25% | Short(2): 52.75%\n",
      " Validation Set (N=150): Hold(0): 16.00% | Long(1): 30.00% | Short(2): 54.00%\n",
      " Test Set (N=314): Hold(0): 21.02% | Long(1): 28.34% | Short(2): 50.64%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, xrp_volatility_30d, ada_volatility_30d, btc_volatility_14d, btc_volatility_30d, VOLATILITY_20, BB_POSITION, VWAP, ROLLING_MIN_20, IKS_26, ITS_9, eth_btc_corr_highvol, ISA_9, doge_volatility_30d, SMA_20, BBM_20, KCB_20, EMA_26, KCL_20\n",
      "\n",
      "====================================================================================================\n",
      " [FINAL PIPELINE SUMMARY] dropna() 이후 데이터 생존 현황\n",
      "====================================================================================================\n",
      " Fold                 Type  Train_N Train_Start  Train_End  Val_N  Val_Start    Val_End  Test_N Test_Start   Test_End\n",
      "    1 walk_forward_rolling      800  2020-04-22 2022-06-30    150 2022-07-06 2022-12-02     150 2022-12-08 2023-05-06\n",
      "    2 walk_forward_rolling      800  2020-09-19 2022-11-27    150 2022-12-03 2023-05-01     150 2023-05-07 2023-10-03\n",
      "    3 walk_forward_rolling      800  2021-02-16 2023-04-26    150 2023-05-02 2023-09-28     150 2023-10-04 2024-03-01\n",
      "    4 walk_forward_rolling      800  2021-07-16 2023-09-23    150 2023-09-29 2024-02-25     150 2024-03-02 2024-07-29\n",
      "    5 walk_forward_rolling      800  2021-12-13 2024-02-20    150 2024-02-26 2024-07-24     150 2024-07-30 2024-12-26\n",
      "    6        final_holdout      800  2022-05-17 2024-07-24    150 2024-07-30 2024-12-26     314 2025-01-01 2025-11-10\n",
      "====================================================================================================\n",
      "\n",
      "Training Fold 1 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7925/0.5600 (Gap: 0.2325)\n",
      "[CatBoost] Total Acc: 0.6067 | Long(1) Prec: 0.5362 | Short(2) Prec: 0.7361\n",
      "[RandomForest] Acc: 0.7950/0.5533 (Gap: 0.2417)\n",
      "[RandomForest] Total Acc: 0.6067 | Long(1) Prec: 0.5362 | Short(2) Prec: 0.7222\n",
      "[LightGBM] Acc: 0.8313/0.5467 (Gap: 0.2846)\n",
      "[LightGBM] Total Acc: 0.6267 | Long(1) Prec: 0.5789 | Short(2) Prec: 0.6782\n",
      "[XGBoost] Acc: 0.9738/0.6000 (Gap: 0.3738)\n",
      "[XGBoost] Total Acc: 0.6200 | Long(1) Prec: 0.5741 | Short(2) Prec: 0.6489\n",
      "[GradientBoost] Acc: 0.9387/0.5867 (Gap: 0.3521)\n",
      "[GradientBoosting] Total Acc: 0.6067 | Long(1) Prec: 0.5455 | Short(2) Prec: 0.6452\n",
      "[HistGradient] Acc: 0.7937/0.5600 (Gap: 0.2337)\n",
      "[HistGradientBoosting] Total Acc: 0.5800 | Long(1) Prec: 0.5156 | Short(2) Prec: 0.7123\n",
      "[LogisticReg] Acc: 0.5850/0.6067 (Gap: -0.0217)\n",
      "[LogisticRegression] Total Acc: 0.5400 | Long(1) Prec: 0.4471 | Short(2) Prec: 0.6615\n",
      "[Stacking] Acc: 0.9163/0.5200 (Gap: 0.3962)\n",
      "[StackingEnsemble] Total Acc: 0.5600 | Long(1) Prec: 0.6296 | Short(2) Prec: 0.6559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 23:28:12.314358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46227 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1d:00.0, compute capability: 8.6\n",
      "2025-11-19 23:28:15.954840: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2025-11-19 23:28:17.182058: I external/local_xla/xla/service/service.cc:168] XLA service 0x3fc3f290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-19 23:28:17.182100: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-11-19 23:28:17.188457: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763562497.298098  383758 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Acc: 0.6416/0.6083 (Gap: 0.0332)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.5750 | Long(1) Prec: 0.4833 | Short(2) Prec: 0.6667\n",
      "[BiLSTM] Acc: 0.6506/0.5167 (Gap: 0.1340)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.6167 | Long(1) Prec: 0.5263 | Short(2) Prec: 0.7692\n",
      "[GRU] Acc: 0.6143/0.5750 (Gap: 0.0393)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.5583 | Long(1) Prec: 0.4444 | Short(2) Prec: 0.7949\n",
      "Training Fold 2 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.6212/0.6200 (Gap: 0.0012)\n",
      "[CatBoost] Total Acc: 0.5733 | Long(1) Prec: 0.3714 | Short(2) Prec: 0.7595\n",
      "[RandomForest] Acc: 0.7588/0.6333 (Gap: 0.1254)\n",
      "[RandomForest] Total Acc: 0.6200 | Long(1) Prec: 0.4423 | Short(2) Prec: 0.7143\n",
      "[LightGBM] Acc: 0.7625/0.6600 (Gap: 0.1025)\n",
      "[LightGBM] Total Acc: 0.5933 | Long(1) Prec: 0.4098 | Short(2) Prec: 0.7273\n",
      "[XGBoost] Acc: 0.9688/0.5800 (Gap: 0.3888)\n",
      "[XGBoost] Total Acc: 0.5000 | Long(1) Prec: 0.3455 | Short(2) Prec: 0.5895\n",
      "[GradientBoost] Acc: 0.7812/0.6400 (Gap: 0.1412)\n",
      "[GradientBoosting] Total Acc: 0.5267 | Long(1) Prec: 0.3220 | Short(2) Prec: 0.6667\n",
      "[HistGradient] Acc: 0.6450/0.6267 (Gap: 0.0183)\n",
      "[HistGradientBoosting] Total Acc: 0.5667 | Long(1) Prec: 0.3538 | Short(2) Prec: 0.7470\n",
      "[LogisticReg] Acc: 0.5813/0.5867 (Gap: -0.0054)\n",
      "[LogisticRegression] Total Acc: 0.5067 | Long(1) Prec: 0.3059 | Short(2) Prec: 0.7692\n",
      "[Stacking] Acc: 0.6913/0.5533 (Gap: 0.1379)\n",
      "[StackingEnsemble] Total Acc: 0.5533 | Long(1) Prec: 0.3000 | Short(2) Prec: 0.5969\n",
      "[LSTM] Acc: 0.6130/0.6333 (Gap: -0.0203)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.6167 | Long(1) Prec: 0.5769 | Short(2) Prec: 0.6277\n",
      "[BiLSTM] Acc: 0.7013/0.4000 (Gap: 0.3013)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.2583 | Long(1) Prec: 0.2583 | Short(2) Prec: 0.0000\n",
      "[GRU] Acc: 0.6974/0.5833 (Gap: 0.1141)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.6500 | Long(1) Prec: 0.6250 | Short(2) Prec: 0.6591\n",
      "Training Fold 3 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7837/0.6400 (Gap: 0.1437)\n",
      "[CatBoost] Total Acc: 0.6467 | Long(1) Prec: 0.7436 | Short(2) Prec: 0.6852\n",
      "[RandomForest] Acc: 0.8113/0.6400 (Gap: 0.1713)\n",
      "[RandomForest] Total Acc: 0.6800 | Long(1) Prec: 0.7176 | Short(2) Prec: 0.6721\n",
      "[LightGBM] Acc: 1.0000/0.6333 (Gap: 0.3667)\n",
      "[LightGBM] Total Acc: 0.6267 | Long(1) Prec: 0.7538 | Short(2) Prec: 0.5789\n",
      "[XGBoost] Acc: 0.9812/0.6333 (Gap: 0.3479)\n",
      "[XGBoost] Total Acc: 0.6533 | Long(1) Prec: 0.7576 | Short(2) Prec: 0.5732\n",
      "[GradientBoost] Acc: 0.8325/0.6333 (Gap: 0.1992)\n",
      "[GradientBoosting] Total Acc: 0.6867 | Long(1) Prec: 0.7317 | Short(2) Prec: 0.6324\n",
      "[HistGradient] Acc: 0.7037/0.5667 (Gap: 0.1371)\n",
      "[HistGradientBoosting] Total Acc: 0.6067 | Long(1) Prec: 0.7632 | Short(2) Prec: 0.7714\n",
      "[LogisticReg] Acc: 0.6100/0.5267 (Gap: 0.0833)\n",
      "[LogisticRegression] Total Acc: 0.5533 | Long(1) Prec: 0.7432 | Short(2) Prec: 0.8000\n",
      "[Stacking] Acc: 0.8438/0.5600 (Gap: 0.2837)\n",
      "[StackingEnsemble] Total Acc: 0.5933 | Long(1) Prec: 0.6970 | Short(2) Prec: 0.7143\n",
      "[LSTM] Acc: 0.6714/0.4917 (Gap: 0.1798)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.3833 | Long(1) Prec: 0.6897 | Short(2) Prec: 0.7308\n",
      "[BiLSTM] Acc: 0.7494/0.5667 (Gap: 0.1827)\n",
      "25/25 [==============================] - 3s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.3333 | Long(1) Prec: 0.6538 | Short(2) Prec: 0.5938\n",
      "[GRU] Acc: 0.6857/0.5000 (Gap: 0.1857)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.3750 | Long(1) Prec: 0.7419 | Short(2) Prec: 0.7500\n",
      "Training Fold 4 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.8175/0.6733 (Gap: 0.1442)\n",
      "[CatBoost] Total Acc: 0.6400 | Long(1) Prec: 0.5833 | Short(2) Prec: 0.7113\n",
      "[RandomForest] Acc: 0.7900/0.6467 (Gap: 0.1433)\n",
      "[RandomForest] Total Acc: 0.6067 | Long(1) Prec: 0.6316 | Short(2) Prec: 0.6435\n",
      "[LightGBM] Acc: 0.9788/0.7000 (Gap: 0.2788)\n",
      "[LightGBM] Total Acc: 0.6600 | Long(1) Prec: 0.5385 | Short(2) Prec: 0.6903\n",
      "[XGBoost] Acc: 0.9525/0.6733 (Gap: 0.2792)\n",
      "[XGBoost] Total Acc: 0.6133 | Long(1) Prec: 0.5789 | Short(2) Prec: 0.6435\n",
      "[GradientBoost] Acc: 0.9862/0.6667 (Gap: 0.3196)\n",
      "[GradientBoosting] Total Acc: 0.6267 | Long(1) Prec: 0.6875 | Short(2) Prec: 0.6349\n",
      "[HistGradient] Acc: 0.9075/0.7000 (Gap: 0.2075)\n",
      "[HistGradientBoosting] Total Acc: 0.6333 | Long(1) Prec: 0.4828 | Short(2) Prec: 0.6789\n",
      "[LogisticReg] Acc: 0.4175/0.4200 (Gap: -0.0025)\n",
      "[LogisticRegression] Total Acc: 0.3533 | Long(1) Prec: 0.0000 | Short(2) Prec: 0.5789\n",
      "[Stacking] Acc: 0.7975/0.5000 (Gap: 0.2975)\n",
      "[StackingEnsemble] Total Acc: 0.5133 | Long(1) Prec: 0.6111 | Short(2) Prec: 0.8103\n",
      "[LSTM] Acc: 0.6260/0.5583 (Gap: 0.0676)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[LSTM] Total Acc: 0.5167 | Long(1) Prec: 0.8000 | Short(2) Prec: 0.7167\n",
      "[BiLSTM] Acc: 0.7143/0.4417 (Gap: 0.2726)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5333 | Long(1) Prec: 0.8571 | Short(2) Prec: 0.6582\n",
      "[GRU] Acc: 0.4896/0.4000 (Gap: 0.0896)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.5833 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.6598\n",
      "Training Fold 5 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7963/0.6067 (Gap: 0.1896)\n",
      "[CatBoost] Total Acc: 0.6000 | Long(1) Prec: 0.5385 | Short(2) Prec: 0.7969\n",
      "[RandomForest] Acc: 0.8237/0.6000 (Gap: 0.2238)\n",
      "[RandomForest] Total Acc: 0.6000 | Long(1) Prec: 0.5102 | Short(2) Prec: 0.7108\n",
      "[LightGBM] Acc: 0.8575/0.5533 (Gap: 0.3042)\n",
      "[LightGBM] Total Acc: 0.6133 | Long(1) Prec: 0.4795 | Short(2) Prec: 0.7869\n",
      "[XGBoost] Acc: 0.9912/0.6000 (Gap: 0.3912)\n",
      "[XGBoost] Total Acc: 0.6400 | Long(1) Prec: 0.5283 | Short(2) Prec: 0.7159\n",
      "[GradientBoost] Acc: 0.9838/0.6200 (Gap: 0.3638)\n",
      "[GradientBoosting] Total Acc: 0.6333 | Long(1) Prec: 0.5208 | Short(2) Prec: 0.6768\n",
      "[HistGradient] Acc: 0.8725/0.6067 (Gap: 0.2658)\n",
      "[HistGradientBoosting] Total Acc: 0.5867 | Long(1) Prec: 0.4746 | Short(2) Prec: 0.7536\n",
      "[LogisticReg] Acc: 0.6112/0.5800 (Gap: 0.0312)\n",
      "[LogisticRegression] Total Acc: 0.5267 | Long(1) Prec: 0.4789 | Short(2) Prec: 0.9167\n",
      "[Stacking] Acc: 0.7562/0.5733 (Gap: 0.1829)\n",
      "[StackingEnsemble] Total Acc: 0.5400 | Long(1) Prec: 0.5455 | Short(2) Prec: 0.7833\n",
      "[LSTM] Acc: 0.7351/0.5500 (Gap: 0.1851)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.3333 | Long(1) Prec: 0.6667 | Short(2) Prec: 0.6552\n",
      "[BiLSTM] Acc: 0.7286/0.3167 (Gap: 0.4119)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5167 | Long(1) Prec: 0.4535 | Short(2) Prec: 0.7586\n",
      "[GRU] Acc: 0.6987/0.6167 (Gap: 0.0820)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.2667 | Long(1) Prec: 0.6364 | Short(2) Prec: 0.7647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 6 (final_holdout)...\n",
      "[CatBoost] Acc: 0.7800/0.5333 (Gap: 0.2467)\n",
      "[CatBoost] Total Acc: 0.5478 | Long(1) Prec: 0.5652 | Short(2) Prec: 0.7456\n",
      "[RandomForest] Acc: 0.8688/0.5800 (Gap: 0.2888)\n",
      "[RandomForest] Total Acc: 0.5955 | Long(1) Prec: 0.5750 | Short(2) Prec: 0.6398\n",
      "[LightGBM] Acc: 0.9988/0.5600 (Gap: 0.4387)\n",
      "[LightGBM] Total Acc: 0.6083 | Long(1) Prec: 0.5775 | Short(2) Prec: 0.7105\n",
      "[XGBoost] Acc: 0.9700/0.5467 (Gap: 0.4233)\n",
      "[XGBoost] Total Acc: 0.5860 | Long(1) Prec: 0.5283 | Short(2) Prec: 0.6747\n",
      "[GradientBoost] Acc: 0.9750/0.5467 (Gap: 0.4283)\n",
      "[GradientBoosting] Total Acc: 0.5255 | Long(1) Prec: 0.4706 | Short(2) Prec: 0.6095\n",
      "[HistGradient] Acc: 0.8000/0.5000 (Gap: 0.3000)\n",
      "[HistGradientBoosting] Total Acc: 0.5223 | Long(1) Prec: 0.5455 | Short(2) Prec: 0.6864\n",
      "[LogisticReg] Acc: 0.6112/0.4267 (Gap: 0.1846)\n",
      "[LogisticRegression] Total Acc: 0.3790 | Long(1) Prec: 0.7931 | Short(2) Prec: 0.8824\n",
      "[Stacking] Acc: 0.7325/0.5467 (Gap: 0.1858)\n",
      "[StackingEnsemble] Total Acc: 0.5350 | Long(1) Prec: 0.4194 | Short(2) Prec: 0.7239\n",
      "[LSTM] Acc: 0.6922/0.3167 (Gap: 0.3755)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.3697 | Long(1) Prec: 0.8571 | Short(2) Prec: 0.9189\n",
      "[BiLSTM] Acc: 0.6078/0.5000 (Gap: 0.1078)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5246 | Long(1) Prec: 0.4884 | Short(2) Prec: 0.8736\n",
      "[GRU] Acc: 0.6338/0.4083 (Gap: 0.2254)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.6162 | Long(1) Prec: 0.6923 | Short(2) Prec: 0.7521\n",
      "Trial 5 Failed: 'Test_Precision_mean'\n",
      "Processing: trial_6_l4_p2.0_s0.9_tw30_tap3 (Optuna Index: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'Test_Precision_mean'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_381508/2192209421.py\", line 103, in objective\n",
      "    best_score = wf_avg['Test_Precision_mean'].max()\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'Test_Precision_mean'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 41.88% | Long(1): 14.62% | Short(2): 43.50%\n",
      " Validation Set (N=150): Hold(0): 49.33% | Long(1): 11.33% | Short(2): 39.33%\n",
      " Test Set (N=150): Hold(0): 42.67% | Long(1): 20.67% | Short(2): 36.67%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, EMA_26, MOM_10, eth_btc_volcorr_sq_14d_pct_5d, btc_volatility_7d, VWAP, SMA_50, IKS_26, ROLLING_MIN_20, ITS_9, VWMA_20, ISB_26, KCB_20, SMA_20, WMA_20, ISA_9, eth_btc_corr_highvol, KCU_20, AD\n",
      "\n",
      "--- Fold 2 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 43.88% | Long(1): 12.62% | Short(2): 43.50%\n",
      " Validation Set (N=150): Hold(0): 44.67% | Long(1): 20.00% | Short(2): 35.33%\n",
      " Test Set (N=150): Hold(0): 42.67% | Long(1): 14.67% | Short(2): 42.67%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, eth_btc_corr_highvol, xrp_volatility_30d, MOM_10, eth_btc_corr_lowvol, eth_btc_volcorr_sq_7d_ma30_ratio, btc_volatility_7d, VWAP, ROLLING_MIN_20, IKS_26, KCB_20, SMA_50, ISB_26, SMA_20, VWMA_20, ITS_9, KCU_20, AD, ISA_9\n",
      "\n",
      "--- Fold 3 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 44.25% | Long(1): 12.00% | Short(2): 43.75%\n",
      " Validation Set (N=150): Hold(0): 42.00% | Long(1): 15.33% | Short(2): 42.67%\n",
      " Test Set (N=150): Hold(0): 40.00% | Long(1): 26.00% | Short(2): 34.00%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, VOLUME_SMA_20, CCI_14, VWAP, ROLLING_MIN_20, IKS_26, eth_btc_corr_highvol, eth_btc_beta_90d, xrp_volatility_30d, ISA_9, ITS_9, SMA_50, ISB_26, doge_volatility_30d, SMA_20, ada_volatility_30d, BBL_20, VWMA_20, ROLLING_MAX_20\n",
      "\n",
      "--- Fold 4 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 44.88% | Long(1): 12.38% | Short(2): 42.75%\n",
      " Validation Set (N=150): Hold(0): 39.33% | Long(1): 24.67% | Short(2): 36.00%\n",
      " Test Set (N=150): Hold(0): 44.00% | Long(1): 9.33% | Short(2): 46.67%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, SMA_50, VOLUME_SMA_20, SMA_20, VWMA_20, vol_regime_duration, btc_volatility_7d, xrp_volatility_30d, VWAP, eth_btc_corr_highvol, ROLLING_MIN_20, eth_btc_beta_90d, IKS_26, ITS_9, BBL_20, KCB_20, ISA_9, KCL_20, btc_volatility_30d\n",
      "\n",
      "--- Fold 5 (walk_forward_rolling) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 43.62% | Long(1): 14.12% | Short(2): 42.25%\n",
      " Validation Set (N=150): Hold(0): 44.67% | Long(1): 10.67% | Short(2): 44.67%\n",
      " Test Set (N=150): Hold(0): 48.00% | Long(1): 12.67% | Short(2): 39.33%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, KCU_20, VOLATILITY_20, btc_volatility_30d, eth_btc_corr_highvol, VOLUME_SMA_20, eth_btc_spread_std7, usdt_totalBridgedToUSD_ma30_ratio_lag1, xrp_volatility_30d, VWAP, SMA_50, IKS_26, ROLLING_MAX_20, ROLLING_MIN_20, SMA_20, ITS_9, BBL_20, BBM_20, KCL_20\n",
      "\n",
      "--- Fold 6 (final_holdout) Class Balance Analysis (top_n=20) ---\n",
      " Train Set (N=800): Hold(0): 44.38% | Long(1): 15.50% | Short(2): 40.12%\n",
      " Validation Set (N=150): Hold(0): 48.00% | Long(1): 12.67% | Short(2): 39.33%\n",
      " Test Set (N=315): Hold(0): 42.22% | Long(1): 15.24% | Short(2): 42.54%\n",
      "------------------------------------------------------------\n",
      "ATR_14, DPO_20, btc_volatility_30d, VOLATILITY_20, DISTANCE_FROM_HIGH, ada_volatility_30d, VWAP, IKS_26, SMA_50, ISA_9, ITS_9, VWMA_20, BBM_20, KCU_20, SMA_20, eth_btc_corr_highvol, ROLLING_MIN_20, ROLLING_MAX_20, BBL_20, KCB_20\n",
      "\n",
      "====================================================================================================\n",
      " [FINAL PIPELINE SUMMARY] dropna() 이후 데이터 생존 현황\n",
      "====================================================================================================\n",
      " Fold                 Type  Train_N Train_Start  Train_End  Val_N  Val_Start    Val_End  Test_N Test_Start   Test_End\n",
      "    1 walk_forward_rolling      800  2020-04-25 2022-07-03    150 2022-07-08 2022-12-04     150 2022-12-09 2023-05-07\n",
      "    2 walk_forward_rolling      800  2020-09-22 2022-11-30    150 2022-12-05 2023-05-03     150 2023-05-08 2023-10-04\n",
      "    3 walk_forward_rolling      800  2021-02-19 2023-04-29    150 2023-05-04 2023-09-30     150 2023-10-05 2024-03-02\n",
      "    4 walk_forward_rolling      800  2021-07-19 2023-09-26    150 2023-10-01 2024-02-27     150 2024-03-03 2024-07-30\n",
      "    5 walk_forward_rolling      800  2021-12-16 2024-02-23    150 2024-02-28 2024-07-26     150 2024-07-31 2024-12-27\n",
      "    6        final_holdout      800  2022-05-19 2024-07-26    150 2024-07-31 2024-12-27     315 2025-01-01 2025-11-11\n",
      "====================================================================================================\n",
      "\n",
      "Training Fold 1 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7163/0.5533 (Gap: 0.1629)\n",
      "[CatBoost] Total Acc: 0.5133 | Long(1) Prec: 0.4490 | Short(2) Prec: 0.5972\n",
      "[RandomForest] Acc: 0.9200/0.5400 (Gap: 0.3800)\n",
      "[RandomForest] Total Acc: 0.5933 | Long(1) Prec: 0.5385 | Short(2) Prec: 0.6104\n",
      "[LightGBM] Acc: 0.9350/0.5133 (Gap: 0.4217)\n",
      "[LightGBM] Total Acc: 0.4733 | Long(1) Prec: 0.4444 | Short(2) Prec: 0.5176\n",
      "[XGBoost] Acc: 0.9712/0.5267 (Gap: 0.4446)\n",
      "[XGBoost] Total Acc: 0.5133 | Long(1) Prec: 0.4419 | Short(2) Prec: 0.5465\n",
      "[GradientBoost] Acc: 0.7725/0.5133 (Gap: 0.2592)\n",
      "[GradientBoosting] Total Acc: 0.4800 | Long(1) Prec: 0.4286 | Short(2) Prec: 0.5217\n",
      "[HistGradient] Acc: 0.8387/0.5667 (Gap: 0.2721)\n",
      "[HistGradientBoosting] Total Acc: 0.4867 | Long(1) Prec: 0.6923 | Short(2) Prec: 0.4948\n",
      "[LogisticReg] Acc: 0.5813/0.5800 (Gap: 0.0013)\n",
      "[LogisticRegression] Total Acc: 0.5067 | Long(1) Prec: 0.7778 | Short(2) Prec: 0.4624\n",
      "[Stacking] Acc: 0.3912/0.3533 (Gap: 0.0379)\n",
      "[StackingEnsemble] Total Acc: 0.5133 | Long(1) Prec: 0.6000 | Short(2) Prec: 0.5301\n",
      "[LSTM] Acc: 0.6519/0.5417 (Gap: 0.1103)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[LSTM] Total Acc: 0.4500 | Long(1) Prec: 0.3425 | Short(2) Prec: 0.6857\n",
      "[BiLSTM] Acc: 0.6636/0.5750 (Gap: 0.0886)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.4083 | Long(1) Prec: 0.3043 | Short(2) Prec: 0.6429\n",
      "[GRU] Acc: 0.6468/0.5750 (Gap: 0.0718)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.5083 | Long(1) Prec: 0.4571 | Short(2) Prec: 0.5507\n",
      "Training Fold 2 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7825/0.5267 (Gap: 0.2558)\n",
      "[CatBoost] Total Acc: 0.5533 | Long(1) Prec: 0.4667 | Short(2) Prec: 0.5862\n",
      "[RandomForest] Acc: 0.7688/0.5200 (Gap: 0.2488)\n",
      "[RandomForest] Total Acc: 0.5133 | Long(1) Prec: 0.3333 | Short(2) Prec: 0.5783\n",
      "[LightGBM] Acc: 0.9700/0.4667 (Gap: 0.5033)\n",
      "[LightGBM] Total Acc: 0.4533 | Long(1) Prec: 0.3750 | Short(2) Prec: 0.4673\n",
      "[XGBoost] Acc: 0.8050/0.4667 (Gap: 0.3383)\n",
      "[XGBoost] Total Acc: 0.4933 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.5000\n",
      "[GradientBoost] Acc: 0.8150/0.4933 (Gap: 0.3217)\n",
      "[GradientBoosting] Total Acc: 0.4667 | Long(1) Prec: 0.6250 | Short(2) Prec: 0.4677\n",
      "[HistGradient] Acc: 0.6850/0.5000 (Gap: 0.1850)\n",
      "[HistGradientBoosting] Total Acc: 0.5267 | Long(1) Prec: 0.3182 | Short(2) Prec: 0.6441\n",
      "[LogisticReg] Acc: 0.6312/0.5600 (Gap: 0.0712)\n",
      "[LogisticRegression] Total Acc: 0.4933 | Long(1) Prec: 0.3200 | Short(2) Prec: 0.5667\n",
      "[Stacking] Acc: 0.5687/0.4600 (Gap: 0.1087)\n",
      "[StackingEnsemble] Total Acc: 0.4800 | Long(1) Prec: 0.4000 | Short(2) Prec: 0.5439\n",
      "[LSTM] Acc: 0.6429/0.5417 (Gap: 0.1012)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.5000 | Long(1) Prec: 0.7500 | Short(2) Prec: 0.4737\n",
      "[BiLSTM] Acc: 0.6857/0.5250 (Gap: 0.1607)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5083 | Long(1) Prec: 0.6667 | Short(2) Prec: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Acc: 0.6351/0.5417 (Gap: 0.0934)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.4500 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.4464\n",
      "Training Fold 3 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7338/0.5467 (Gap: 0.1871)\n",
      "[CatBoost] Total Acc: 0.5667 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.6304\n",
      "[RandomForest] Acc: 0.9062/0.4867 (Gap: 0.4196)\n",
      "[RandomForest] Total Acc: 0.5933 | Long(1) Prec: 0.5926 | Short(2) Prec: 0.5965\n",
      "[LightGBM] Acc: 0.9175/0.4933 (Gap: 0.4242)\n",
      "[LightGBM] Total Acc: 0.5067 | Long(1) Prec: 0.5323 | Short(2) Prec: 0.5132\n",
      "[XGBoost] Acc: 0.7588/0.4800 (Gap: 0.2788)\n",
      "[XGBoost] Total Acc: 0.5267 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.5469\n",
      "[GradientBoost] Acc: 0.7050/0.5133 (Gap: 0.1917)\n",
      "[GradientBoosting] Total Acc: 0.5400 | Long(1) Prec: 0.4615 | Short(2) Prec: 0.6000\n",
      "[HistGradient] Acc: 0.7412/0.4800 (Gap: 0.2612)\n",
      "[HistGradientBoosting] Total Acc: 0.5267 | Long(1) Prec: 0.4930 | Short(2) Prec: 0.5469\n",
      "[LogisticReg] Acc: 0.6362/0.4533 (Gap: 0.1829)\n",
      "[LogisticRegression] Total Acc: 0.5667 | Long(1) Prec: 0.5143 | Short(2) Prec: 0.5714\n",
      "[Stacking] Acc: 0.7250/0.4467 (Gap: 0.2783)\n",
      "[StackingEnsemble] Total Acc: 0.4867 | Long(1) Prec: 0.5769 | Short(2) Prec: 0.4659\n",
      "[LSTM] Acc: 0.7299/0.5833 (Gap: 0.1465)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.5583 | Long(1) Prec: 0.5952 | Short(2) Prec: 0.5238\n",
      "[BiLSTM] Acc: 0.6494/0.4417 (Gap: 0.2077)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5500 | Long(1) Prec: 0.5088 | Short(2) Prec: 0.5667\n",
      "[GRU] Acc: 0.6896/0.5583 (Gap: 0.1313)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.5667 | Long(1) Prec: 0.6410 | Short(2) Prec: 0.5610\n",
      "Training Fold 4 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.8600/0.5733 (Gap: 0.2867)\n",
      "[CatBoost] Total Acc: 0.5733 | Long(1) Prec: 0.5833 | Short(2) Prec: 0.5914\n",
      "[RandomForest] Acc: 0.9038/0.5333 (Gap: 0.3704)\n",
      "[RandomForest] Total Acc: 0.4733 | Long(1) Prec: 0.0000 | Short(2) Prec: 0.4758\n",
      "[LightGBM] Acc: 0.7225/0.4933 (Gap: 0.2292)\n",
      "[LightGBM] Total Acc: 0.5067 | Long(1) Prec: 0.3548 | Short(2) Prec: 0.5652\n",
      "[XGBoost] Acc: 0.9775/0.5600 (Gap: 0.4175)\n",
      "[XGBoost] Total Acc: 0.5133 | Long(1) Prec: 0.0000 | Short(2) Prec: 0.5085\n",
      "[GradientBoost] Acc: 0.8638/0.4933 (Gap: 0.3704)\n",
      "[GradientBoosting] Total Acc: 0.5733 | Long(1) Prec: 0.0000 | Short(2) Prec: 0.6136\n",
      "[HistGradient] Acc: 0.8250/0.5533 (Gap: 0.2717)\n",
      "[HistGradientBoosting] Total Acc: 0.5667 | Long(1) Prec: 0.3929 | Short(2) Prec: 0.5895\n",
      "[LogisticReg] Acc: 0.5663/0.5333 (Gap: 0.0329)\n",
      "[LogisticRegression] Total Acc: 0.5467 | Long(1) Prec: 0.4000 | Short(2) Prec: 0.5574\n",
      "[Stacking] Acc: 0.6700/0.5000 (Gap: 0.1700)\n",
      "[StackingEnsemble] Total Acc: 0.4533 | Long(1) Prec: 0.1944 | Short(2) Prec: 0.5714\n",
      "[LSTM] Acc: 0.6182/0.5417 (Gap: 0.0765)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.5250 | Long(1) Prec: 0.5556 | Short(2) Prec: 0.5243\n",
      "[BiLSTM] Acc: 0.6623/0.5833 (Gap: 0.0790)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5917 | Long(1) Prec: 0.6667 | Short(2) Prec: 0.6133\n",
      "[GRU] Acc: 0.6442/0.5667 (Gap: 0.0775)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.6250 | Long(1) Prec: 0.7143 | Short(2) Prec: 0.6667\n",
      "Training Fold 5 (walk_forward_rolling)...\n",
      "[CatBoost] Acc: 0.7125/0.5467 (Gap: 0.1658)\n",
      "[CatBoost] Total Acc: 0.5933 | Long(1) Prec: 0.3939 | Short(2) Prec: 0.5942\n",
      "[RandomForest] Acc: 0.8063/0.4067 (Gap: 0.3996)\n",
      "[RandomForest] Total Acc: 0.5000 | Long(1) Prec: 0.3824 | Short(2) Prec: 0.4805\n",
      "[LightGBM] Acc: 0.9925/0.6067 (Gap: 0.3858)\n",
      "[LightGBM] Total Acc: 0.6133 | Long(1) Prec: 0.4815 | Short(2) Prec: 0.5753\n",
      "[XGBoost] Acc: 0.9900/0.5600 (Gap: 0.4300)\n",
      "[XGBoost] Total Acc: 0.5867 | Long(1) Prec: 0.4615 | Short(2) Prec: 0.5645\n",
      "[GradientBoost] Acc: 0.9975/0.4867 (Gap: 0.5108)\n",
      "[GradientBoosting] Total Acc: 0.5467 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.5156\n",
      "[HistGradient] Acc: 0.7738/0.4867 (Gap: 0.2871)\n",
      "[HistGradientBoosting] Total Acc: 0.5800 | Long(1) Prec: 0.4286 | Short(2) Prec: 0.5667\n",
      "[LogisticReg] Acc: 0.5725/0.5267 (Gap: 0.0458)\n",
      "[LogisticRegression] Total Acc: 0.6533 | Long(1) Prec: 0.5000 | Short(2) Prec: 0.6500\n",
      "[Stacking] Acc: 0.4562/0.4400 (Gap: 0.0162)\n",
      "[StackingEnsemble] Total Acc: 0.3267 | Long(1) Prec: 0.0923 | Short(2) Prec: 0.5538\n",
      "[LSTM] Acc: 0.7078/0.5667 (Gap: 0.1411)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[LSTM] Total Acc: 0.6000 | Long(1) Prec: 0.7857 | Short(2) Prec: 0.6800\n",
      "[BiLSTM] Acc: 0.6506/0.5083 (Gap: 0.1423)\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Total Acc: 0.5167 | Long(1) Prec: 0.6000 | Short(2) Prec: 0.0000\n",
      "[GRU] Acc: 0.6792/0.5667 (Gap: 0.1126)\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[GRU] Total Acc: 0.5083 | Long(1) Prec: 0.7500 | Short(2) Prec: 0.5600\n",
      "Training Fold 6 (final_holdout)...\n",
      "[CatBoost] Acc: 0.7250/0.6267 (Gap: 0.0983)\n",
      "[CatBoost] Total Acc: 0.6444 | Long(1) Prec: 0.5581 | Short(2) Prec: 0.6815\n",
      "[RandomForest] Acc: 0.8575/0.6267 (Gap: 0.2308)\n",
      "[RandomForest] Total Acc: 0.6317 | Long(1) Prec: 0.6190 | Short(2) Prec: 0.6618\n",
      "[LightGBM] Acc: 0.9725/0.5800 (Gap: 0.3925)\n",
      "[LightGBM] Total Acc: 0.6000 | Long(1) Prec: 0.4894 | Short(2) Prec: 0.6480\n",
      "[XGBoost] Acc: 0.8225/0.6000 (Gap: 0.2225)\n",
      "[XGBoost] Total Acc: 0.6159 | Long(1) Prec: 0.5294 | Short(2) Prec: 0.6515\n",
      "[GradientBoost] Acc: 0.8163/0.5933 (Gap: 0.2229)\n",
      "[GradientBoosting] Total Acc: 0.5905 | Long(1) Prec: 0.5556 | Short(2) Prec: 0.5860\n",
      "[HistGradient] Acc: 0.7538/0.6067 (Gap: 0.1471)\n",
      "[HistGradientBoosting] Total Acc: 0.5937 | Long(1) Prec: 0.5088 | Short(2) Prec: 0.6269\n",
      "[LogisticReg] Acc: 0.5450/0.6067 (Gap: -0.0617)\n",
      "[LogisticRegression] Total Acc: 0.5524 | Long(1) Prec: 0.5128 | Short(2) Prec: 0.8718\n",
      "[Stacking] Acc: 0.7350/0.4333 (Gap: 0.3017)\n",
      "[StackingEnsemble] Total Acc: 0.5429 | Long(1) Prec: 0.2840 | Short(2) Prec: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import traceback\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import traceback\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "TRIAL_START_OFFSET = 5\n",
    "\n",
    "def objective(trial):\n",
    "    lookahead = trial.suggest_int('lookahead', 3, 7)\n",
    "    p_mult = trial.suggest_float('profit_mult', 0.8, 2.0, step=0.1)\n",
    "    s_mult = trial.suggest_float('stop_mult', 0.4, 1.0, step=0.1)\n",
    "    trend_window = trial.suggest_int('trend_window', 10, 40, step=5)\n",
    "    trend_analysis_points = trial.suggest_int('trend_analysis_points', 3, 7)\n",
    "    \n",
    "    actual_trial_number = trial.number + TRIAL_START_OFFSET\n",
    "    \n",
    "    param_suffix = f\"l{lookahead}_p{p_mult:.1f}_s{s_mult:.1f}_tw{trend_window}_tap{trend_analysis_points}\"\n",
    "    trial_target_name = f\"trial_{actual_trial_number}_{param_suffix}\"\n",
    "    \n",
    "    print(f\"Processing: {trial_target_name} (Optuna Index: {trial.number})\")\n",
    "    \n",
    "    if check_experiment_completed(trial_target_name, RESULT_DIR):\n",
    "        print(f\"Trial {actual_trial_number} already completed - skipping\")\n",
    "        summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "        if os.path.exists(summary_path):\n",
    "            existing_summary = pd.read_csv(summary_path)\n",
    "            if 'Test_Precision_mean' in existing_summary.columns:\n",
    "                return existing_summary['Test_Precision_mean'].max()\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        pipeline_result = build_complete_pipeline_corrected(\n",
    "            df_raw=df_merged,\n",
    "            train_start_date=TRAIN_START_DATE,\n",
    "            final_test_start='2025-01-01',\n",
    "            method='walk_forward',\n",
    "            target_type='direction',\n",
    "            lookahead_candles=lookahead,\n",
    "            atr_multiplier_profit=p_mult,\n",
    "            atr_multiplier_stop=s_mult,\n",
    "            trend_window=trend_window,\n",
    "            trend_analysis_points=trend_analysis_points,\n",
    "            top_n=20\n",
    "        )\n",
    "        \n",
    "        save_raw_data_once(pipeline_result, trial_target_name, 'walk_forward')\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold_data in pipeline_result:\n",
    "            fold_idx = fold_data['stats']['fold_idx']\n",
    "            fold_type = fold_data['stats']['fold_type']\n",
    "            \n",
    "            if check_fold_completed(trial_target_name, fold_idx, fold_type):\n",
    "                print(f\"Fold {fold_idx} already completed in {trial_target_name}\")\n",
    "                fold_summary, _ = load_fold_results(trial_target_name, fold_idx, fold_type)\n",
    "                if fold_summary is not None:\n",
    "                    fold_results.append((fold_summary, fold_type))\n",
    "                continue\n",
    "            \n",
    "            print(f\"Training Fold {fold_idx} ({fold_type})...\")\n",
    "            \n",
    "            evaluator = ModelEvaluator(save_models=True)\n",
    "            \n",
    "            train_all_models(\n",
    "                fold_data['train']['X_robust'], \n",
    "                fold_data['train']['y']['next_direction'].values.astype(int),\n",
    "                fold_data['val']['X_robust'], \n",
    "                fold_data['val']['y']['next_direction'].values.astype(int),\n",
    "                fold_data['test']['X_robust'], \n",
    "                fold_data['test']['y'],\n",
    "                fold_data['test']['dates'].values,\n",
    "                evaluator,\n",
    "                ml_models=ML_MODELS_CLASSIFICATION,\n",
    "                dl_models=DL_MODELS_CLASSIFICATION\n",
    "            )\n",
    "            \n",
    "            fold_summary, _ = save_fold_results(fold_idx, fold_type, evaluator, trial_target_name, fold_data)\n",
    "            fold_results.append((fold_summary, fold_type))\n",
    "            \n",
    "            del evaluator\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "        \n",
    "        save_walk_forward_summary(fold_results, trial_target_name)\n",
    "        \n",
    "        summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "        if os.path.exists(summary_path):\n",
    "            wf_avg = pd.read_csv(summary_path)\n",
    "            best_score = wf_avg['Test_Precision_mean'].max()\n",
    "            \n",
    "            record_optuna_result(actual_trial_number, trial_target_name, best_score, trial.params)\n",
    "            return best_score\n",
    "        \n",
    "        return 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {actual_trial_number} Failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "\n",
    "study.enqueue_trial({\n",
    "    'lookahead': 5,\n",
    "    'profit_mult': 1.2,\n",
    "    'stop_mult': 0.7,\n",
    "    'trend_window': 20,\n",
    "    'trend_analysis_points': 4\n",
    "})\n",
    "\n",
    "study.optimize(objective, n_trials=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
