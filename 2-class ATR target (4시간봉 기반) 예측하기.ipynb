{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203e373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 08:25:18.414914: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-29 08:25:18.414955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-29 08:25:18.416335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-29 08:25:18.423837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-29 08:25:19.186236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ GPU Detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import pandas_ta as ta\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from scipy.signal import argrelextrema\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import joblib\n",
    "import json\n",
    "import gc\n",
    "import traceback\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Numba (For Fast Backtesting)\n",
    "from numba import jit\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"üöÄ GPU Detected: {tf.config.list_physical_devices('GPU')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Using CPU.\")\n",
    "    \n",
    "\n",
    "def remove_raw_prices_and_transform(df, target_type, method):\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Î°úÍ∑∏ ÏàòÏùµÎ•† ÏÉùÏÑ±\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        close_col = 'ETH_Close' if 'ETH_Close' in df_transformed.columns else 'close'\n",
    "        if close_col in df_transformed.columns:\n",
    "            df_transformed['eth_log_return'] = np.log(df_transformed[close_col] / df_transformed[close_col].shift(1))\n",
    "    \n",
    "    # 2. ATR Î≥ÄÌôò (Îã¨Îü¨ -> ÎπÑÏú®)\n",
    "    if 'ATR_84' in df_transformed.columns:\n",
    "        close_col = 'ETH_Close' if 'ETH_Close' in df_transformed.columns else 'close'\n",
    "        if close_col in df_transformed.columns:\n",
    "            df_transformed['ATR_84'] = (df_transformed['ATR_84'] / df_transformed[close_col]) * 100\n",
    "            df_transformed['ATR_84'] = df_transformed['ATR_84'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    remove_patterns = [\n",
    "        '_Close', '_Open', '_High', '_Low', '_Volume', \n",
    "        'tvl', 'mcap', 'sp500', 'gold', 'dxy', 'vwap' \n",
    "    ]\n",
    "    \n",
    "    keep_keywords = [\n",
    "        '_pct', '_ratio', '_chg', '_diff', '_dist', '_score', 'zscore', \n",
    "        'natr', 'rsi', 'mfi', 'adx', 'cci', 'vr', 'shadow', \n",
    "        'div', 'spike', 'climax', 'absorption', 'breakout', 'price_vs',\n",
    "        'ATR_84'\n",
    "    ]\n",
    "    \n",
    "    cols_to_remove = []\n",
    "    for col in df_transformed.columns:\n",
    "        if col in ['date', 'target', 'next_direction', 'next_close', 'next_log_return', 'take_profit_price', 'stop_loss_price', 'real_entry_price', 'ATR_84']:\n",
    "            continue\n",
    "            \n",
    "        is_dangerous = any(p in col.lower() for p in remove_patterns) or col in ['EMA_72', 'SMA_120', 'OBV']\n",
    "        is_safe = any(k in col.lower() for k in keep_keywords)\n",
    "        \n",
    "        if is_dangerous and not is_safe:\n",
    "            cols_to_remove.append(col)\n",
    "\n",
    "    if cols_to_remove:\n",
    "        df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "        \n",
    "    return df_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801ac771",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def compute_targets_with_hourly_numba_4h(\n",
    "    base_dates_ts, base_atr, hourly_dates_ts, hourly_open,\n",
    "    hourly_high, hourly_low, lookahead_periods, profit_mult, stop_mult\n",
    "):\n",
    "    n = len(base_dates_ts)\n",
    "    # Ï¥àÍ∏∞Ìôî: Í∏∞Î≥∏Í∞íÏùÄ -1 (Ïú†Ìö®ÌïòÏßÄ ÏïäÏùå) ÎòêÎäî NaN\n",
    "    targets = np.full(n, -1, dtype=np.int32)\n",
    "    entry_prices = np.full(n, np.nan, dtype=np.float64) # Î™ÖÏãúÏ†Å float64\n",
    "    tp_prices = np.full(n, np.nan, dtype=np.float64)\n",
    "    sl_prices = np.full(n, np.nan, dtype=np.float64)\n",
    "    \n",
    "    four_hour_ms = 14400000\n",
    "    lookahead_ms = lookahead_periods * four_hour_ms\n",
    "    MIN_PROFIT_THRESHOLD = 0.0025 \n",
    "    \n",
    "    h_start = 0\n",
    "    \n",
    "    # ÎßàÏßÄÎßâ Îç∞Ïù¥ÌÑ∞Îäî lookahead Í≥ÑÏÇ∞ Î∂àÍ∞ÄÌïòÎØÄÎ°ú n-1ÍπåÏßÄÎßå Î£®ÌîÑ\n",
    "    for i in range(n - 1):\n",
    "        atr = base_atr[i]\n",
    "        # ATRÏù¥ ÏóÜÍ±∞ÎÇò 0Ïù¥Î©¥ Í≥ÑÏÇ∞ Î∂àÍ∞Ä -> continue (Ï¥àÍ∏∞Í∞í -1/NaN Ïú†ÏßÄ)\n",
    "        if np.isnan(atr) or atr <= 0: \n",
    "            continue\n",
    "        \n",
    "        entry_start_ts = base_dates_ts[i] + four_hour_ms\n",
    "        entry_end_ts = entry_start_ts + lookahead_ms\n",
    "        \n",
    "        first_entry_idx = -1\n",
    "        \n",
    "        # 1ÏãúÍ∞ÑÎ¥â Îß§Ïπ≠ (h_startÎ∂ÄÌÑ∞ Í≤ÄÏÉâÌïòÏó¨ ÏÜçÎèÑ ÏµúÏ†ÅÌôî)\n",
    "        for h in range(h_start, len(hourly_dates_ts)):\n",
    "            if hourly_dates_ts[h] >= entry_start_ts:\n",
    "                first_entry_idx = h\n",
    "                h_start = h # Îã§Ïùå Î£®ÌîÑÎ•º ÏúÑÌï¥ ÏãúÏûëÏ†ê ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                break\n",
    "        \n",
    "        # Îß§Ïπ≠ÎêòÎäî 1ÏãúÍ∞ÑÎ¥âÏù¥ ÏóÜÏúºÎ©¥(Îç∞Ïù¥ÌÑ∞ ÎÅùÎ∂ÄÎ∂Ñ Îì±) Í≥ÑÏÇ∞ Î∂àÍ∞Ä -> continue\n",
    "        if first_entry_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        # ÏßÑÏûÖ Í∞ÄÍ≤© ÌôïÏ†ï\n",
    "        entry_price = hourly_open[first_entry_idx]\n",
    "        tp = entry_price + (atr * profit_mult)\n",
    "        sl = entry_price - (atr * stop_mult)\n",
    "        \n",
    "        # [Ï§ëÏöî] Î∞∞Ïó¥Ïóê Í∞í Ìï†Îãπ (i Ïù∏Îç±Ïä§ ÏúÑÏπòÏóê Ï†ïÌôïÌûà!)\n",
    "        entry_prices[i] = entry_price\n",
    "        tp_prices[i] = tp\n",
    "        sl_prices[i] = sl\n",
    "        \n",
    "        result = 0 # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ïã§Ìå®(0)Î°ú ÏãúÏûë\n",
    "        final_idx = -1 \n",
    "        \n",
    "        for h in range(first_entry_idx, len(hourly_dates_ts)):\n",
    "            # 1. ÏãúÍ∞Ñ Ï¥àÍ≥º (Lookahead Í∏∞Í∞Ñ Ï¢ÖÎ£å)\n",
    "            if hourly_dates_ts[h] >= entry_end_ts:\n",
    "                final_idx = h\n",
    "                break\n",
    "            \n",
    "            # 2. ÏÜêÏ†à (Stop Loss) - ÏµúÏö∞ÏÑ† ÌôïÏù∏\n",
    "            if hourly_low[h] <= sl:\n",
    "                result = 0\n",
    "                final_idx = -1 # ÏÜêÏ†à ÎãπÌñàÏúºÎØÄÎ°ú Timeout ÏàòÏùµ Ï≤¥ÌÅ¨ Î∂àÌïÑÏöî\n",
    "                break\n",
    "                \n",
    "            # 3. ÏùµÏ†à (Take Profit)\n",
    "            if hourly_high[h] >= tp:\n",
    "                result = 1\n",
    "                final_idx = -1 # ÏùµÏ†à ÌñàÏúºÎØÄÎ°ú Ï¢ÖÎ£å\n",
    "                break\n",
    "        \n",
    "        # 4. Timeout Ï≤òÎ¶¨ (ÏïÑÏßÅ Ìè¨ÏßÄÏÖò Î≥¥Ïú† Ï§ëÏùº Îïå ÏàòÏùµÎ•† Ï≤¥ÌÅ¨)\n",
    "        if final_idx != -1:\n",
    "            exit_price = hourly_open[final_idx]\n",
    "            return_rate = (exit_price - entry_price) / entry_price\n",
    "            \n",
    "            if return_rate >= MIN_PROFIT_THRESHOLD:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "\n",
    "        # Í≤∞Í≥º Ìï†Îãπ\n",
    "        targets[i] = result\n",
    "    \n",
    "    return targets, entry_prices, tp_prices, sl_prices\n",
    "\n",
    "\n",
    "def create_targets_4h(df_4h, df_1h, lookahead=30, profit_mult=1.5, stop_mult=1.0, **kwargs):\n",
    "    df_target = df_4h.copy()\n",
    "    hourly_df = df_1h.copy()\n",
    "    \n",
    "    df_target['date'] = pd.to_datetime(df_target['date'])\n",
    "    hourly_df['datetime'] = pd.to_datetime(hourly_df['datetime'])\n",
    "    \n",
    "    hourly_df.columns = hourly_df.columns.str.lower()\n",
    "    \n",
    "    if 'ATR_84' not in df_target.columns:\n",
    "        df_target['ATR_84'] = ta.atr(\n",
    "            df_target['ETH_High'], df_target['ETH_Low'], df_target['ETH_Close'], length=84\n",
    "        )\n",
    "    \n",
    "    df_target = df_target.sort_values('date').reset_index(drop=True)\n",
    "    hourly_df = hourly_df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    base_dates_ts = df_target['date'].astype(np.int64).values // 10**6\n",
    "    base_atr = df_target['ATR_84'].fillna(method='ffill').fillna(0).to_numpy()\n",
    "    \n",
    "    hourly_dates_ts = hourly_df['datetime'].astype(np.int64).values // 10**6\n",
    "    hourly_open = hourly_df['open'].astype(np.float64).to_numpy()\n",
    "    hourly_high = hourly_df['high'].astype(np.float64).to_numpy()\n",
    "    hourly_low = hourly_df['low'].astype(np.float64).to_numpy()\n",
    "    \n",
    "    targets, entry_prices, tp_prices, sl_prices = compute_targets_with_hourly_numba_4h(\n",
    "        base_dates_ts, base_atr, hourly_dates_ts, hourly_open, hourly_high, hourly_low,\n",
    "        lookahead, profit_mult, stop_mult\n",
    "    )\n",
    "    \n",
    "    df_target['next_direction'] = targets\n",
    "    df_target['real_entry_price'] = entry_prices\n",
    "    df_target['take_profit_price'] = tp_prices\n",
    "    df_target['stop_loss_price'] = sl_prices\n",
    "    \n",
    "    df_target['next_close'] = df_target['ETH_Close'].shift(-1)\n",
    "    df_target['next_open'] = df_target['ETH_Open'].shift(-1)\n",
    "    df_target['next_log_return'] = np.log(df_target['next_close'] / (df_target['next_open'] + 1e-9))\n",
    "    \n",
    "    if lookahead > 0:\n",
    "        df_target = df_target.iloc[:-lookahead]\n",
    "    \n",
    "    valid_before = len(df_target)\n",
    "    df_target = df_target[df_target['next_direction'] != -1].reset_index(drop=True)\n",
    "    valid_after = len(df_target)\n",
    "    \n",
    "    print(f\"Valid Samples: {valid_after}/{valid_before} (Removed: {valid_before - valid_after})\")\n",
    "    \n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d34a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_verified(X_train, y_train, task='class', top_n=20, verbose=True):\n",
    "    if len(X_train) > 10000:\n",
    "        idx = np.random.choice(len(X_train), 10000, replace=False)\n",
    "        X_sub = X_train.iloc[idx]\n",
    "        y_sub = y_train.iloc[idx]\n",
    "    else:\n",
    "        X_sub, y_sub = X_train, y_train\n",
    "\n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_sub, y_sub, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_sub, y_sub, random_state=42, n_neighbors=3)\n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    estimator = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1) if task == 'class' else LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=top_n, step=0.1, verbose=0)\n",
    "    rfe.fit(X_sub, y_sub)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1) if task == 'class' else RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_sub, y_sub)\n",
    "    rf_idx = np.argsort(rf_model.feature_importances_)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "    \n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0: break\n",
    "    \n",
    "    return selected_features, {}\n",
    "\n",
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=20):\n",
    "    atr_col_name = 'ATR_84'\n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(X_train, y_train['next_direction'], task='class', top_n=top_n)\n",
    "        \n",
    "        if atr_col_name not in selected and atr_col_name in X_train.columns:\n",
    "            if len(selected) > 0: selected.pop()\n",
    "            selected.insert(0, atr_col_name)\n",
    "            \n",
    "    print(f\"\\n[Feature Selection] Top {len(selected)} Features Selected:\")\n",
    "    print(f\" -> {', '.join(selected)}\")\n",
    "    return selected, stats\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=20, fold_idx=None, atr_col_name='ATR_84'): \n",
    "    train_df = split_data['train'] \n",
    "    val_df = split_data['val'] \n",
    "    test_df = split_data['test'] \n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" Processing Fold {fold_idx} ({fold_type})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\" Train Period: {train_df['date'].min()} ~ {train_df['date'].max()} (N={len(train_df)})\")\n",
    "    print(f\" Val   Period: {val_df['date'].min()} ~ {val_df['date'].max()} (N={len(val_df)})\")\n",
    "    print(f\" Test  Period: {test_df['date'].min()} ~ {test_df['date'].max()} (N={len(test_df)})\")\n",
    "\n",
    "    target_cols = [\n",
    "        'next_direction', 'next_log_return', 'next_close', 'next_open', \n",
    "        'take_profit_price', 'stop_loss_price', \n",
    "        'ATR_84', 'real_entry_price' \n",
    "    ]\n",
    "\n",
    "    train_processed = train_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    exclude_cols = [col for col in target_cols if col != atr_col_name] + ['date']\n",
    "    feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    balance = y_train['next_direction'].value_counts(normalize=True).to_dict()\n",
    "    print(f\"[Class Balance] Train Set: {balance}\")\n",
    "\n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, y_train, target_type=target_type, top_n=top_n\n",
    "    )\n",
    "\n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_sel), columns=selected_features)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val_sel), columns=selected_features)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_sel), columns=selected_features)\n",
    "\n",
    "    return {\n",
    "        'train': {'X': X_train_scaled, 'y': y_train.reset_index(drop=True), 'dates': train_processed['date'].reset_index(drop=True)},\n",
    "        'val': {'X': X_val_scaled, 'y': y_val.reset_index(drop=True), 'dates': val_processed['date'].reset_index(drop=True)},\n",
    "        'test': {'X': X_test_scaled, 'y': y_test.reset_index(drop=True), 'dates': test_processed['date'].reset_index(drop=True)},\n",
    "        'scaler': scaler, \n",
    "        'selected_features': selected_features,\n",
    "        'stats': {\n",
    "            'fold_idx': fold_idx if fold_idx is not None else split_data.get('fold_idx', 0),\n",
    "            'fold_type': split_data.get('fold_type', 'unknown')\n",
    "        }\n",
    "    }\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, final_test_start='2025-01-01', \n",
    "                              initial_train_size=800, val_size=150, test_size=150, \n",
    "                              step=150, gap_size=30):\n",
    "    \n",
    "    df_period = df[df['date'] >= pd.to_datetime(train_start_date)].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    total_len = len(df_period)\n",
    "    folds = []\n",
    "    current_test_end = total_len\n",
    "    \n",
    "    while True:\n",
    "        test_end_idx = current_test_end\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        train_end_idx = val_start_idx - gap_size\n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0: break\n",
    "        \n",
    "        train_fold = df_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_type': 'walk_forward_rolling_reverse'\n",
    "        })\n",
    "        current_test_end = test_start_idx - gap_size\n",
    "    \n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "        \n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    if len(final_test_df) > 0:\n",
    "        pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "        if len(pre_final_df) >= (initial_train_size + val_size + gap_size):\n",
    "            final_val_end_idx = len(pre_final_df)\n",
    "            final_val_start_idx = final_val_end_idx - val_size\n",
    "            final_train_end_idx = final_val_start_idx - gap_size\n",
    "            final_train_start_idx = final_train_end_idx - initial_train_size\n",
    "            \n",
    "            final_train_data = pre_final_df.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "            final_val_data = pre_final_df.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "            \n",
    "            folds.append({\n",
    "                'train': final_train_data,\n",
    "                'val': final_val_data,\n",
    "                'test': final_test_df,\n",
    "                'fold_idx': len(folds) + 1,\n",
    "                'fold_type': 'final_holdout'\n",
    "            })\n",
    "            \n",
    "    return folds\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, df_hour, train_start_date, **kwargs): \n",
    "    print(f\"\\n Pipeline Started... (Train Start: {train_start_date})\")\n",
    "\n",
    "\n",
    "    lookahead = kwargs.get('lookahead_periods', 30) \n",
    "    profit_mult = kwargs.get('profit_mult', 2.0)\n",
    "    stop_mult = kwargs.get('stop_mult', 1.0)\n",
    "\n",
    "    df = create_targets_4h(df_raw, df_hour,\n",
    "        lookahead=lookahead, \n",
    "        profit_mult=profit_mult, \n",
    "        stop_mult=stop_mult\n",
    "    )\n",
    "\n",
    "    df = remove_raw_prices_and_transform(df, 'direction', 'tvt')\n",
    "    print(f\"Final Data Shape: {df.shape}\")\n",
    "    \n",
    "    initial_train_size = kwargs.get('initial_train_days', 800) * 6\n",
    "    val_size = 150 * 6\n",
    "    test_size = 150 * 6\n",
    "    gap_size = lookahead\n",
    "\n",
    "    splits = split_walk_forward_method(\n",
    "        df, \n",
    "        train_start_date=train_start_date,\n",
    "        final_test_start=kwargs.get('final_test_start', '2025-01-01'),\n",
    "        initial_train_size=initial_train_size,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size,\n",
    "        step=150 * 6,\n",
    "        gap_size=gap_size\n",
    "    )\n",
    "    print(f\"Data Split Completed. Total {len(splits)} folds generated.\")\n",
    "\n",
    "    result = []\n",
    "    for fold in splits:\n",
    "        res = process_single_split(\n",
    "            fold, \n",
    "            top_n=kwargs.get('top_n', 20), \n",
    "            fold_idx=fold['fold_idx'],\n",
    "            atr_col_name='ATR_84'\n",
    "        )\n",
    "        result.append(res)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3896bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import gc\n",
    "import traceback\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_class_weights(y_train):\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "        return dict(zip(classes, weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_expectancy_percentile(y_true, y_prob, rr_ratio=1.5, min_trades=20):\n",
    "        \"\"\"\n",
    "        [ÏàòÏ†ïÎê®] Í≥†Ï†ï ÏûÑÍ≥ÑÍ∞í ÎåÄÏã† Percentile Í∏∞Î∞òÏúºÎ°ú ÏµúÏ†ÅÏùò Expectancy Í≥ÑÏÇ∞\n",
    "        \"\"\"\n",
    "        best_exp = -999.0\n",
    "        best_th = 0.5\n",
    "        best_metrics = {'win_rate': 0.0, 'trades': 0}\n",
    "        \n",
    "        # ÏÉÅÏúÑ 1% ~ 50% ÏÇ¨Ïù¥Î•º ÌÉêÏÉâ (ÎÑàÎ¨¥ ÎÑêÎÑêÌïòÎ©¥ ÏùòÎØ∏ ÏóÜÏùå)\n",
    "        percentiles = np.arange(50, 99, 2) \n",
    "        \n",
    "        for pct in percentiles:\n",
    "            th = np.percentile(y_prob, pct)\n",
    "            if th < 0.5: continue # ÏµúÏÜå ÌôïÎ•† Ï°∞Í±¥\n",
    "            \n",
    "            preds = (y_prob >= th).astype(int)\n",
    "            n_trades = preds.sum()\n",
    "            \n",
    "            if n_trades < min_trades: continue\n",
    "            \n",
    "            wins = ((preds == 1) & (y_true == 1)).sum()\n",
    "            losses = n_trades - wins\n",
    "            \n",
    "            win_rate = wins / n_trades\n",
    "            exp = (wins * rr_ratio - losses * 1.0) / n_trades\n",
    "            \n",
    "            # ÏäπÎ•† ÌéòÎÑêÌã∞ (ÏïàÏ†ïÏÑ± ÌôïÎ≥¥)\n",
    "            if win_rate < 0.4: exp -= 0.5\n",
    "            \n",
    "            # Í±∞Îûò ÌöüÏàò Í∞ÄÏÇ∞Ï†ê (Ïã†Î¢∞ÎèÑ ÌôïÎ≥¥)\n",
    "            score = exp * np.log1p(n_trades)\n",
    "            \n",
    "            if score > best_exp: # Score Í∏∞Ï§ÄÏúºÎ°ú Í∞±Ïã†ÌïòÎêò, Ï†ÄÏû•ÏùÄ ÏõêÎ≥∏ ExpÎ°ú\n",
    "                best_exp = score\n",
    "                best_th = th\n",
    "                best_metrics = {'win_rate': win_rate, 'trades': n_trades, 'real_exp': exp}\n",
    "                \n",
    "        # Î∞òÌôòÍ∞í: ÏµúÏ†ÅÌôîÏö© Ï†êÏàò(Score), ÏûÑÍ≥ÑÍ∞í, Î©îÌÉÄÎç∞Ïù¥ÌÑ∞(Ïã§Ï†ú Exp Ìè¨Ìï®)\n",
    "        return best_exp, best_th, best_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val, rr_ratio=1.5):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 800), # Í≥ºÏ†ÅÌï© Î∞©ÏßÄ ÏúÑÌï¥ ÏÉÅÌïú Ï∂ïÏÜå\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 50, 200),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 50),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            # [ÏàòÏ†ï] Percentile Í∏∞Î∞ò ÌèâÍ∞Ä Ìï®Ïàò Ìò∏Ï∂ú\n",
    "            score, _, _ = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=15) # Trial ÌöüÏàò Ï°∞Ï†à\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Î°úÍπÖÏö©\n",
    "        train_prob = best_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        _, _, train_meta = DirectionModels._calculate_expectancy_percentile(y_train, train_prob, rr_ratio=rr_ratio)\n",
    "        _, _, val_meta = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "        \n",
    "        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïã§Ï†ú Exp Í∫ºÎÇ¥Í∏∞ (ÏóÜÏúºÎ©¥ 0)\n",
    "        tr_exp = train_meta.get('real_exp', 0.0)\n",
    "        val_exp = val_meta.get('real_exp', 0.0)\n",
    "\n",
    "        print(f\"  [RandomForest] Train Exp: {tr_exp:.4f}R | Val Exp: {val_exp:.4f}R (Trades: {val_meta['trades']})\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val, rr_ratio=1.5):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 60),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 50, 300), # Í≥ºÏ†ÅÌï© Î∞©ÏßÄ ÏÉÅÌñ•\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'class_weight': 'balanced',\n",
    "                'verbosity': -1,\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            # Early StoppingÏùÑ ÏúÑÌïú ÏΩúÎ∞±\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=callbacks)\n",
    "            \n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            score, _, _ = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'n_estimators': 1000, 'objective': 'binary', 'metric': 'binary_logloss', \n",
    "            'class_weight': 'balanced', 'verbosity': -1, 'n_jobs': -1, 'random_state': 42\n",
    "        })\n",
    "        \n",
    "        final_model = lgb.LGBMClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "        \n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        _, _, train_meta = DirectionModels._calculate_expectancy_percentile(y_train, train_prob, rr_ratio=rr_ratio)\n",
    "        _, _, val_meta = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "        \n",
    "        tr_exp = train_meta.get('real_exp', 0.0)\n",
    "        val_exp = val_meta.get('real_exp', 0.0)\n",
    "        \n",
    "        print(f\"  [LightGBM]     Train Exp: {tr_exp:.4f}R | Val Exp: {val_exp:.4f}R (Trades: {val_meta['trades']})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val, rr_ratio=1.5):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        neg, pos = np.bincount(y_train.astype(int))\n",
    "        scale_pos_weight = neg / pos\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 10, 50), # Í≥ºÏ†ÅÌï© Î∞©ÏßÄ ÏÉÅÌñ•\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 5.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "                'scale_pos_weight': scale_pos_weight,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'logloss',\n",
    "                'tree_method': 'hist',\n",
    "                'early_stopping_rounds': 30,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            \n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            score, _, _ = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'n_estimators': 1000, 'objective': 'binary:logistic', 'eval_metric': 'logloss', \n",
    "            'tree_method': 'hist', 'scale_pos_weight': scale_pos_weight,\n",
    "            'early_stopping_rounds': 50, 'random_state': 42, 'n_jobs': -1\n",
    "        })\n",
    "        \n",
    "        final_model = xgb.XGBClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        _, _, train_meta = DirectionModels._calculate_expectancy_percentile(y_train, train_prob, rr_ratio=rr_ratio)\n",
    "        _, _, val_meta = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "        \n",
    "        tr_exp = train_meta.get('real_exp', 0.0)\n",
    "        val_exp = val_meta.get('real_exp', 0.0)\n",
    "        \n",
    "        print(f\"  [XGBoost]      Train Exp: {tr_exp:.4f}R | Val Exp: {val_exp:.4f}R (Trades: {val_meta['trades']})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val, rr_ratio=1.5):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'iterations': 1000,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 8),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 5, 30), # Ï†ïÍ∑úÌôî Í∞ïÌôî\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'Logloss',\n",
    "                'auto_class_weights': 'Balanced',\n",
    "                'logging_level': 'Silent',\n",
    "                'random_seed': 42,\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 30,\n",
    "                'allow_writing_files': False\n",
    "            }\n",
    "            \n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "            \n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            score, _, _ = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'iterations': 1000, 'loss_function': 'Logloss', 'eval_metric': 'Logloss',\n",
    "            'auto_class_weights': 'Balanced', 'logging_level': 'Silent',\n",
    "            'random_seed': 42, 'od_type': 'Iter', 'od_wait': 50, 'allow_writing_files': False\n",
    "        })\n",
    "        \n",
    "        final_model = cb.CatBoostClassifier(**best_params)\n",
    "        final_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        \n",
    "        train_prob = final_model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        _, _, train_meta = DirectionModels._calculate_expectancy_percentile(y_train, train_prob, rr_ratio=rr_ratio)\n",
    "        _, _, val_meta = DirectionModels._calculate_expectancy_percentile(y_val, val_prob, rr_ratio=rr_ratio)\n",
    "        \n",
    "        tr_exp = train_meta.get('real_exp', 0.0)\n",
    "        val_exp = val_meta.get('real_exp', 0.0)\n",
    "        \n",
    "        print(f\"  [CatBoost]     Train Exp: {tr_exp:.4f}R | Val Exp: {val_exp:.4f}R (Trades: {val_meta['trades']})\")\n",
    "        \n",
    "        return final_model\n",
    "\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b420447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.best_thresholds = {}\n",
    "        self.save_models = save_models\n",
    "        self.models = {} if save_models else None\n",
    "        self.prediction_logs = {}\n",
    "        \n",
    "    def optimize_threshold(self, y_true, buy_prob, min_trades=20, reward_risk_ratio=1.5):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            pct = trial.suggest_float('percentile', 50, 99)\n",
    "            th = np.percentile(buy_prob, pct)\n",
    "            \n",
    "            if th < 0.5: return -999.0\n",
    "            \n",
    "            preds = (buy_prob >= th).astype(int)\n",
    "            n_trades = np.sum(preds == 1)\n",
    "            \n",
    "            if n_trades < min_trades: return -10.0\n",
    "            \n",
    "            wins = np.sum((preds == 1) & (y_true == 1))\n",
    "            losses = n_trades - wins\n",
    "            \n",
    "            win_rate = wins / n_trades\n",
    "            expectancy = ((wins * reward_risk_ratio) - (losses * 1.0)) / n_trades\n",
    "            \n",
    "            if win_rate < 0.4: expectancy -= 0.5\n",
    "            \n",
    "            score = expectancy * np.log1p(n_trades)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        best_pct = study.best_params['percentile']\n",
    "        best_th = np.percentile(buy_prob, best_pct)\n",
    "        \n",
    "        return max(best_th, 0.5)\n",
    "\n",
    "    def evaluate_model(self, model, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                       model_name, profit_mult=1.5, stop_mult=1.0,\n",
    "                       test_dates=None, test_prices=None, y_test_df=None):  \n",
    "        \n",
    "        def calculate_all_metrics(y_true, probs, threshold, prefix):\n",
    "            preds = (probs >= threshold).astype(int)\n",
    "            n_trades = np.sum(preds == 1)\n",
    "\n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            prec = precision_score(y_true, preds, zero_division=0)\n",
    "            rec = recall_score(y_true, preds, zero_division=0)\n",
    "            f1 = f1_score(y_true, preds, zero_division=0)\n",
    "            try: auc = roc_auc_score(y_true, probs)\n",
    "            except: auc = 0.5\n",
    "\n",
    "            if n_trades > 0:\n",
    "                wins = np.sum((preds == 1) & (y_true == 1))\n",
    "                losses = n_trades - wins\n",
    "                win_rate = wins / n_trades\n",
    "                expectancy = ((wins * rr_ratio) - (losses * 1.0)) / n_trades\n",
    "            else:\n",
    "                win_rate, expectancy = 0.0, 0.0\n",
    "\n",
    "            return {\n",
    "                f\"{prefix}_Acc\": acc, f\"{prefix}_Prec\": prec, f\"{prefix}_Rec\": rec,\n",
    "                f\"{prefix}_F1\": f1, f\"{prefix}_AUC\": auc,\n",
    "                f\"{prefix}_Exp\": expectancy, f\"{prefix}_WinRate\": win_rate, f\"{prefix}_Trades\": n_trades\n",
    "                }\n",
    "        \n",
    "        rr_ratio = profit_mult / stop_mult if stop_mult > 0 else 1.5\n",
    "        \n",
    "        tr_prob = model.predict_proba(X_train)[:, 1]\n",
    "        val_prob = model.predict_proba(X_val)[:, 1]\n",
    "        te_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        min_trades_val = max(20, int(len(y_val) * 0.02))\n",
    "        \n",
    "        best_th = self.optimize_threshold(y_val.astype(int), val_prob, min_trades=min_trades_val, reward_risk_ratio=rr_ratio)\n",
    "        self.best_thresholds[model_name] = best_th\n",
    "\n",
    "        tr_pos_ratio = np.mean(y_train)\n",
    "        val_pos_ratio = np.mean(y_val)\n",
    "        te_pos_ratio = np.mean(y_test)\n",
    "        \n",
    "        metrics_train = calculate_all_metrics(y_train.astype(int), tr_prob, best_th, \"Train\")\n",
    "        metrics_val = calculate_all_metrics(y_val.astype(int), val_prob, best_th, \"Val\")\n",
    "        metrics_test = calculate_all_metrics(y_test.astype(int), te_prob, best_th, \"Test\")\n",
    "        \n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Threshold': best_th,\n",
    "            'Train_PosRatio': tr_pos_ratio, \n",
    "            'Val_PosRatio': val_pos_ratio, \n",
    "            'Test_PosRatio': te_pos_ratio,\n",
    "            **metrics_train,\n",
    "            **metrics_val,\n",
    "            **metrics_test\n",
    "        }\n",
    "        \n",
    "        print(f\"    -> [EVAL] {model_name:<12} | Te_Exp: {result['Test_Exp']:.3f}R | Te_Trades: {result['Test_Trades']} | Th: {best_th:.4f}\")\n",
    "\n",
    "        if test_dates is not None:\n",
    "            close_vals = test_prices['ETH_Close'].values if isinstance(test_prices, pd.DataFrame) else test_prices\n",
    "            open_vals = test_prices['ETH_Open'].values if isinstance(test_prices, pd.DataFrame) else np.zeros(len(te_prob))\n",
    "            high_vals = test_prices['ETH_High'].values if isinstance(test_prices, pd.DataFrame) else np.zeros(len(te_prob))\n",
    "            low_vals = test_prices['ETH_Low'].values if isinstance(test_prices, pd.DataFrame) else np.zeros(len(te_prob))\n",
    "\n",
    "            tp_vals = y_test_df['take_profit_price'].values if y_test_df is not None else np.zeros(len(te_prob))\n",
    "            sl_vals = y_test_df['stop_loss_price'].values if y_test_df is not None else np.zeros(len(te_prob))\n",
    "            entry_vals = y_test_df['real_entry_price'].values if y_test_df is not None else np.zeros(len(te_prob))\n",
    "\n",
    "            pred_df = pd.DataFrame({\n",
    "                'timestamp': test_dates,\n",
    "                'open': open_vals, 'high': high_vals, 'low': low_vals, 'close': close_vals,\n",
    "                'prob': te_prob, 'threshold': best_th, 'signal': (te_prob >= best_th).astype(int),\n",
    "                'actual_target': y_test.astype(int),\n",
    "                'take_profit_price': tp_vals, 'stop_loss_price': sl_vals, 'real_entry_price': entry_vals\n",
    "            })\n",
    "            if not isinstance(pred_df.index, pd.DatetimeIndex):\n",
    "                pred_df.set_index('timestamp', inplace=True)\n",
    "            self.prediction_logs[model_name] = pred_df\n",
    "\n",
    "        self.results.append(result)\n",
    "        if self.save_models: self.models[model_name] = model\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_summary_dataframe(self): return pd.DataFrame(self.results)\n",
    "    def get_models_dict(self): return self.models or {}\n",
    "    def get_prediction_logs(self): return self.prediction_logs\n",
    "    def get_best_thresholds(self): return self.best_thresholds\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    def _prepare_target(self, y_data):\n",
    "        # Extracts the first column (target) and flattens it to 1D array\n",
    "        if isinstance(y_data, pd.DataFrame):\n",
    "            y_data = y_data.iloc[:, 0].values\n",
    "        elif isinstance(y_data, pd.Series):\n",
    "            y_data = y_data.values\n",
    "        y_data = np.array(y_data).flatten()\n",
    "        y_data = np.nan_to_num(y_data, nan=0.0)\n",
    "        return np.round(y_data).astype(int)\n",
    "\n",
    "    def train_all_models(self, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                         profit_mult, stop_mult, ml_models,\n",
    "                         test_dates=None, test_prices=None):\n",
    "        \n",
    "        # 1. Prepare numeric arrays for training\n",
    "        y_train_arr = self._prepare_target(y_train)\n",
    "        y_val_arr = self._prepare_target(y_val)\n",
    "        y_test_arr = self._prepare_target(y_test)\n",
    "        \n",
    "        real_rr_ratio = profit_mult / stop_mult if stop_mult > 0 else 1.5\n",
    "        print(f\"  [Info] Training with RR_Ratio: {real_rr_ratio:.2f}\") \n",
    "        \n",
    "        for config in ml_models:\n",
    "            try:\n",
    "                # Train Model\n",
    "                if config.get('needs_val', False):\n",
    "                    model = config['func'](X_train, y_train_arr, X_val, y_val_arr, rr_ratio=real_rr_ratio)\n",
    "                else:\n",
    "                    model = config['func'](X_train, y_train_arr)\n",
    "                \n",
    "                # Evaluate Model\n",
    "                self.evaluator.evaluate_model(\n",
    "                    model=model, \n",
    "                    X_train=X_train, \n",
    "                    y_train=y_train_arr, \n",
    "                    X_val=X_val, \n",
    "                    y_val=y_val_arr, \n",
    "                    X_test=X_test, \n",
    "                    y_test=y_test_arr,         \n",
    "                    model_name=config['name'], \n",
    "                    profit_mult=profit_mult, \n",
    "                    stop_mult=stop_mult,\n",
    "                    test_dates=test_dates, \n",
    "                    test_prices=test_prices,\n",
    "                    y_test_df=y_test         \n",
    "                )\n",
    "                del model\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed {config['name']}: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    \n",
    "    \n",
    "def save_fold_results(fold_idx, fold_type, evaluator, trial_name, fold_data, result_dir):\n",
    "    base_dir = f\"{result_dir}/{trial_name}/fold_{fold_idx}_{fold_type}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\" Saving results to: {base_dir}\")\n",
    "    \n",
    "    # 1. Í≤∞Í≥º ÏöîÏïΩ Ï†ÄÏû• \n",
    "    summary = evaluator.get_summary_dataframe()\n",
    "    summary.to_csv(f\"{base_dir}/fold_summary.csv\", index=False)\n",
    "    \n",
    "    # 2. ÏÉÅÏÑ∏ ÏòàÏ∏° Î°úÍ∑∏ & ÎàÑÏ†Å ÏàòÏùµÎ•† Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± \n",
    "    pred_logs = evaluator.get_prediction_logs()\n",
    "    \n",
    "    for model_name, df_log in pred_logs.items():\n",
    "        # (1) Í∏∞Î≥∏ ÏòàÏ∏° Î°úÍ∑∏ Ï†ÄÏû•\n",
    "        df_log.to_csv(f\"{base_dir}/predictions_{model_name}.csv\")\n",
    "        \n",
    "        # (2) [Ï∂îÍ∞Ä] ÎàÑÏ†Å ÏàòÏùµÎ•† Í≥°ÏÑ† Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (Equity Curve)\n",
    "\n",
    "        if 'actual_return' in df_log.columns: # ÎßåÏïΩ ÏàòÏùµÎ•† Ïª¨ÎüºÏù¥ ÏûàÎã§Î©¥\n",
    "            df_log['strategy_return'] = df_log['signal'] * df_log['actual_return']\n",
    "            df_log['cumulative_return'] = (1 + df_log['strategy_return']).cumprod()\n",
    "            df_log[['cumulative_return']].to_csv(f\"{base_dir}/equity_curve_{model_name}.csv\")\n",
    "\n",
    "    # 3. Î™®Îç∏ Í∞ùÏ≤¥ Ï†ÄÏû• \n",
    "    for name, model in evaluator.get_models_dict().items():\n",
    "        joblib.dump(model, f\"{base_dir}/model_{name}.pkl\")\n",
    "            \n",
    "    # 4. Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû• \n",
    "    if 'scaler' in fold_data:\n",
    "        joblib.dump(fold_data['scaler'], f\"{base_dir}/scaler.pkl\")\n",
    "        \n",
    "    # 5. [Ï§ëÏöî] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "    feature_names = fold_data.get('selected_features', [])\n",
    "    if isinstance(feature_names, pd.Index): feature_names = feature_names.tolist()\n",
    "    \n",
    "    meta_data = {\n",
    "        'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'fold_idx': fold_idx,\n",
    "        'model_thresholds': evaluator.get_best_thresholds(), \n",
    "        'input_features': feature_names,                   \n",
    "        'scaling_needed': True if 'scaler' in fold_data else False,\n",
    "        'parameters': {\n",
    "            'profit_mult': fold_data.get('profit_mult'), \n",
    "            'stop_mult': fold_data.get('stop_mult'),     \n",
    "            'lookahead': fold_data.get('lookahead')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{base_dir}/metadata.json\", 'w') as f:\n",
    "        json.dump(meta_data, f, indent=4)\n",
    "            \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b83e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_optimization(df_merged, df_hour, ml_models, n_trials=30):\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    RESULT_DIR = f\"model_results/{TIMESTAMP}_Sniper\"\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "    \n",
    "    LOG_PATH = f\"{RESULT_DIR}/optuna_logs.csv\"\n",
    "\n",
    "    ohlc_cols = ['date', 'ETH_Open', 'ETH_High', 'ETH_Low', 'ETH_Close'] \n",
    "    \n",
    "    lookup_df = df_merged[ohlc_cols].copy()\n",
    "    lookup_df['date'] = pd.to_datetime(lookup_df['date'])\n",
    "    lookup_df = lookup_df.set_index('date').sort_index()\n",
    "    \n",
    "    # [Resume] Í∏∞Ï°¥ Î°úÍ∑∏ Î°úÎìú\n",
    "    existing_history = pd.DataFrame()\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        try:\n",
    "            existing_history = pd.read_csv(LOG_PATH)\n",
    "            print(f\"\\n[Resume] Loaded {len(existing_history)} existing trials.\")\n",
    "        except: pass\n",
    "\n",
    "    if not os.path.exists(LOG_PATH):\n",
    "        with open(LOG_PATH, \"w\") as f:\n",
    "            f.write(\"trial,lookahead,profit_mult,stop_mult,top_n,train_days,score\\n\")\n",
    "\n",
    "    def objective(trial):\n",
    "        nonlocal existing_history \n",
    "        \n",
    "        train_days = trial.suggest_int('train_days', 700, 1000, step=50)\n",
    "        s_mult = trial.suggest_float('stop_mult', 1.2, 1.8, step=0.1)\n",
    "        p_mult = trial.suggest_float('profit_mult', 2.0, 3.5, step=0.3)\n",
    "        lookahead = trial.suggest_int('lookahead', 6, 18, step=2)\n",
    "        top_n = trial.suggest_int('top_n', 15, 25, step=2)\n",
    "\n",
    "        # Ï§ëÎ≥µ Ïã§Ìñâ Î∞©ÏßÄ \n",
    "        if not existing_history.empty:\n",
    "            mask = (\n",
    "                (existing_history['lookahead'] == lookahead) &\n",
    "                (np.isclose(existing_history['profit_mult'], p_mult, atol=1e-5)) &\n",
    "                (np.isclose(existing_history['stop_mult'], s_mult, atol=1e-5)) &\n",
    "                (existing_history['top_n'] == top_n) &\n",
    "                (existing_history['train_days'] == train_days)\n",
    "            )\n",
    "            if mask.any():\n",
    "                prev_score = existing_history.loc[mask, 'score'].values[0]\n",
    "                print(f\"[Skip] Already done. Score: {prev_score}\")\n",
    "                return prev_score\n",
    "\n",
    "        trial_name = f\"T{trial.number}_L{lookahead}_P{p_mult:.1f}_S{s_mult:.1f}_N{top_n}\"\n",
    "        print(f\"\\n{'='*60}\\n Starting {trial_name}\\n{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Build Pipeline (Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ)\n",
    "            pipeline_result = build_complete_pipeline_corrected(\n",
    "                df_raw=df_merged,\n",
    "                df_hour=df_hour,\n",
    "                train_start_date='2020-01-01',\n",
    "                final_test_start='2025-01-01',\n",
    "                lookahead_periods=lookahead,\n",
    "                profit_mult=p_mult,\n",
    "                stop_mult=s_mult,\n",
    "                top_n=top_n,\n",
    "                initial_train_days=train_days\n",
    "            )\n",
    "            \n",
    "            fold_scores = []\n",
    "            \n",
    "            # 2. Walk-Forward Loop\n",
    "            for fold_data in pipeline_result:\n",
    "                stats = fold_data.get('stats', {}) \n",
    "                fold_idx = stats.get('fold_idx', 0)\n",
    "                fold_type = stats.get('fold_type', 'unknown')\n",
    "                \n",
    "                print(f\"   >> Running Fold {fold_idx} ({fold_type})\")\n",
    "                \n",
    "                fold_data['profit_mult'] = p_mult\n",
    "                fold_data['stop_mult'] = s_mult\n",
    "                fold_data['lookahead'] = lookahead \n",
    "\n",
    "                test_dates = pd.to_datetime(fold_data['test']['dates'])\n",
    "                \n",
    "                test_ohlc = lookup_df.reindex(test_dates).fillna(0)\n",
    "                \n",
    "                # =============================================================\n",
    "\n",
    "                evaluator = ModelEvaluator(save_models=True) \n",
    "                trainer = ModelTrainer(evaluator)\n",
    "                \n",
    "                # Î™®Îç∏ ÌïôÏäµ\n",
    "                trainer.train_all_models(\n",
    "                    fold_data['train']['X'], fold_data['train']['y'],\n",
    "                    fold_data['val']['X'], fold_data['val']['y'],\n",
    "                    fold_data['test']['X'], fold_data['test']['y'],\n",
    "                    p_mult, s_mult, ml_models,\n",
    "                    test_dates=test_dates, \n",
    "                    test_prices=test_ohlc\n",
    "                )\n",
    "\n",
    "                # Í≤∞Í≥º Î∞è Î™®Îç∏ Ï†ÄÏû•\n",
    "                summary = save_fold_results(fold_idx, fold_type, evaluator, trial_name, fold_data, RESULT_DIR)\n",
    "                \n",
    "                # [Ï§ëÏöî ÏàòÏ†ï Î∂ÄÎ∂Ñ] Ï†êÏàò ÏßëÍ≥Ñ (Expectancy)\n",
    "                # ModelEvaluator.evaluate_modelÏóêÏÑú Î∞òÌôòÌïòÎäî ÌÇ§Îäî \"Test_Exp\"ÏûÖÎãàÎã§.\n",
    "                if 'Test_Exp' in summary.columns:\n",
    "                    best_fold_score = summary['Test_Exp'].max()\n",
    "                    fold_scores.append(best_fold_score)\n",
    "                \n",
    "                del evaluator, trainer\n",
    "                gc.collect()\n",
    "            \n",
    "            # fold_scoresÍ∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ -99.0 Ï≤òÎ¶¨ (Ïó¨Ï†ÑÌûà ÏïàÏ†ÑÏû•ÏπòÎäî Ïú†ÏßÄ)\n",
    "            final_score = np.mean(fold_scores) if fold_scores else -99.0\n",
    "            print(f\"\\n === Trial Score: {final_score:.4f}R ===\")\n",
    "            \n",
    "            # Î°úÍ∑∏ Í∏∞Î°ù\n",
    "            with open(LOG_PATH, \"a\") as f:\n",
    "                f.write(f\"{trial.number},{lookahead},{p_mult},{s_mult},{top_n},{train_days},{final_score}\\n\")\n",
    "                f.flush() \n",
    "                os.fsync(f.fileno())\n",
    "            \n",
    "            # Î©îÎ™®Î¶¨ DB ÏóÖÎç∞Ïù¥Ìä∏\n",
    "            new_row = pd.DataFrame([[trial.number, lookahead, p_mult, s_mult, top_n, train_days, final_score]], \n",
    "                                   columns=['trial','lookahead','profit_mult','stop_mult','top_n','train_days','score'])\n",
    "            if existing_history.empty: existing_history = new_row\n",
    "            else: existing_history = pd.concat([existing_history, new_row], ignore_index=True)\n",
    "                \n",
    "            return final_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" [Error] Trial Failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return -99.0\n",
    "\n",
    "    # Optuna Ïã§Ìñâ\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"\\n[Optuna] Best Params: {study.best_params}\")\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05555c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 08:25:23,076] A new study created in memory with name: no-name-6cfc1ea3-686d-4741-bf35-71ef8ebaa43c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Resume] Loaded 0 existing trials.\n",
      "\n",
      "============================================================\n",
      " Starting T0_L8_P3.5_S1.4_N21\n",
      "============================================================\n",
      "\n",
      " Pipeline Started... (Train Start: 2020-01-01)\n",
      "Valid Samples: 17818/17901 (Removed: 83)\n",
      "Final Data Shape: (17818, 221)\n",
      "Data Split Completed. Total 9 folds generated.\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-04-05 13:00:00 ~ 2022-03-06 09:00:00 (N=4200)\n",
      " Val   Period: 2022-03-07 21:00:00 ~ 2022-08-04 17:00:00 (N=900)\n",
      " Test  Period: 2022-08-06 05:00:00 ~ 2023-01-03 01:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.5311904761904762, 1: 0.4688095238095238}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> curve-dex_eth_tvl_1d_chg, uniswap_eth_tvl_1d_chg, usdt_eth_mcap_pct_chg_24h, GOLD_pct_chg_24h, ATR_84, MACDS_72_156_54, PRICE_VS_LOW_30p, PRICE_VS_LOW_360p, usdt_eth_mcap_ma180_ratio, DXY_ma180_ratio, curve-dex_eth_tvl_ma180_ratio, makerdao_eth_tvl_ma180_ratio, SP500_ma180_ratio, GOLD_ma180_ratio, SMA_120_ma180_ratio, eth_chain_tvl_pct_chg_24h, makerdao_eth_tvl_1d_chg, curve-dex_eth_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, eth_chain_tvl_1d_chg, uniswap_eth_tvl_pct_chg_24h\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 2 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-09-03 21:00:00 ~ 2022-08-04 17:00:00 (N=4200)\n",
      " Val   Period: 2022-08-06 05:00:00 ~ 2023-01-03 01:00:00 (N=900)\n",
      " Test  Period: 2023-01-04 13:00:00 ~ 2023-06-03 09:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.5430952380952381, 1: 0.4569047619047619}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> uniswap_eth_tvl_1d_chg, SP500_pct_chg_24h, ATR_84, RSI_84, MACDH_72_156_54, PRICE_VS_LOW_30p, DXY_ma180_ratio, VIX_ma180_ratio, makerdao_eth_tvl_ma180_ratio, uniswap_eth_tvl_ma180_ratio, GOLD_ma180_ratio, SMA_120_ma180_ratio, OBV_ma180_ratio, eth_chain_tvl_pct_chg_24h, usdt_eth_mcap_pct_chg_24h, aave_eth_tvl_1d_chg, makerdao_eth_tvl_1d_chg, uniswap_eth_tvl_pct_chg_24h, eth_chain_tvl_1d_chg, makerdao_eth_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 3 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-02-02 05:00:00 ~ 2023-01-03 01:00:00 (N=4200)\n",
      " Val   Period: 2023-01-04 13:00:00 ~ 2023-06-03 09:00:00 (N=900)\n",
      " Test  Period: 2023-06-04 21:00:00 ~ 2023-11-01 17:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.5654761904761905, 1: 0.43452380952380953}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> BB_WIDTH, usdt_eth_mcap_pct_chg_24h, SP500_pct_chg_24h, ATR_84, MACDH_72_156_54, PRICE_VS_LOW_30p, aave_eth_tvl_ma180_ratio, DXY_ma180_ratio, SP500_ma180_ratio, GOLD_ma180_ratio, SMA_120_ma180_ratio, EMA_72_ma180_ratio, uniswap_eth_tvl_pct_chg_24h, aave_eth_tvl_pct_chg_24h, uniswap_eth_tvl_1d_chg, lido_eth_tvl_pct_chg_24h, aave_eth_tvl_1d_chg, makerdao_eth_tvl_1d_chg, curve-dex_eth_tvl_1d_chg, curve-dex_eth_tvl_pct_chg_24h, lido_eth_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 4 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-07-03 13:00:00 ~ 2023-06-03 09:00:00 (N=4200)\n",
      " Val   Period: 2023-06-04 21:00:00 ~ 2023-11-01 17:00:00 (N=900)\n",
      " Test  Period: 2023-11-03 05:00:00 ~ 2024-04-01 05:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.5804761904761905, 1: 0.4195238095238095}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> eth_chain_tvl_pct_chg_24h, usdt_eth_mcap_pct_chg_24h, uniswap_eth_tvl_1d_chg, makerdao_eth_tvl_1d_chg, ATR_84, RSI_84, MACDH_72_156_54, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Corr_ETH_XRP_24h, usdt_eth_mcap_ma180_ratio, DXY_ma180_ratio, SP500_ma180_ratio, arbitrum_tvl_ma180_ratio, optimism_tvl_ma180_ratio, eth_chain_tvl_1d_chg, lido_eth_tvl_1d_chg, curve-dex_eth_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg, lido_eth_tvl_pct_chg_24h, aave_eth_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 5 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2021-12-01 21:00:00 ~ 2023-11-01 17:00:00 (N=4200)\n",
      " Val   Period: 2023-11-03 05:00:00 ~ 2024-04-01 05:00:00 (N=900)\n",
      " Test  Period: 2024-04-02 17:00:00 ~ 2024-08-30 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6140476190476191, 1: 0.38595238095238094}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> ATR_84, MACDH_72_156_54, BREAKOUT_STR_30p, Price_div_VWMA_20d, return_lag_12p, Corr_ETH_XRP_24h, Corr_ETH_ADA_24h, DXY_ma180_ratio, VIX_ma180_ratio, arbitrum_tvl_ma180_ratio, GOLD_ma180_ratio, SMA_300_pct_chg_24h, uniswap_eth_tvl_1d_chg, eth_chain_tvl_1d_chg, lido_eth_tvl_1d_chg, optimism_tvl_pct_chg_24h, lido_eth_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, eth_chain_tvl_pct_chg_24h, usdt_eth_mcap_pct_chg_24h\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 6 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2022-05-02 05:00:00 ~ 2024-04-01 05:00:00 (N=4200)\n",
      " Val   Period: 2024-04-02 17:00:00 ~ 2024-08-30 13:00:00 (N=900)\n",
      " Test  Period: 2024-09-01 01:00:00 ~ 2025-01-28 21:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6035714285714285, 1: 0.3964285714285714}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> ATR_84, usdt_eth_mcap_pct_chg_24h, uniswap_eth_tvl_1d_chg, MFI_84, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, Price_div_VWMA_20d, DXY_ma180_ratio, VIX_ma180_ratio, curve-dex_eth_tvl_ma180_ratio, SP500_ma180_ratio, GOLD_ma180_ratio, SMA_120_ma180_ratio, SMA_300_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, arbitrum_tvl_pct_chg_24h, curve-dex_eth_tvl_pct_chg_24h, makerdao_eth_tvl_1d_chg, eth_chain_tvl_1d_chg, curve-dex_eth_tvl_1d_chg, optimism_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 7 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2022-09-30 13:00:00 ~ 2024-08-30 13:00:00 (N=4200)\n",
      " Val   Period: 2024-09-01 01:00:00 ~ 2025-01-28 21:00:00 (N=900)\n",
      " Test  Period: 2025-01-30 09:00:00 ~ 2025-06-29 05:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6111904761904762, 1: 0.3888095238095238}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> ATR_84, usdt_eth_mcap_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, MFI_84, PRICE_VS_LOW_30p, BREAKOUT_STR_120p, Price_div_VWMA_50d, Corr_ETH_XRP_24h, DXY_ma180_ratio, VIX_ma180_ratio, SP500_ma180_ratio, arbitrum_tvl_ma180_ratio, SMA_120_ma180_ratio, OBV_ma180_ratio, eth_chain_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg, optimism_tvl_pct_chg_24h, makerdao_eth_tvl_pct_chg_24h, aave_eth_tvl_1d_chg, makerdao_eth_tvl_1d_chg, eth_chain_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 8 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2023-02-28 21:00:00 ~ 2025-01-28 21:00:00 (N=4200)\n",
      " Val   Period: 2025-01-30 09:00:00 ~ 2025-06-29 05:00:00 (N=900)\n",
      " Test  Period: 2025-06-30 17:00:00 ~ 2025-11-27 13:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.6061904761904762, 1: 0.39380952380952383}\n",
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> usdt_eth_mcap_pct_chg_24h, arbitrum_tvl_1d_chg, base_tvl_1d_chg, ETH_Close, ATR_84, MACDH_72_156_54, PRICE_VS_HIGH_120p, BREAKOUT_STR_120p, DXY_ma180_ratio, VIX_ma180_ratio, curve-dex_eth_tvl_ma180_ratio, GOLD_ma180_ratio, zksync_era_tvl_1d_chg, uniswap_eth_tvl_pct_chg_24h, curve-dex_eth_tvl_pct_chg_24h, uniswap_eth_tvl_1d_chg, eth_chain_tvl_1d_chg, eth_chain_tvl_pct_chg_24h, lido_eth_tvl_pct_chg_24h, lido_eth_tvl_1d_chg, optimism_tvl_1d_chg\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 9 (final_holdout)\n",
      "================================================================================\n",
      " Train Period: 2022-09-02 13:00:00 ~ 2024-08-02 13:00:00 (N=4200)\n",
      " Val   Period: 2024-08-04 01:00:00 ~ 2024-12-31 21:00:00 (N=900)\n",
      " Test  Period: 2025-01-01 01:00:00 ~ 2025-11-27 13:00:00 (N=1984)\n",
      "[Class Balance] Train Set: {0: 0.6085714285714285, 1: 0.3914285714285714}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Feature Selection] Top 21 Features Selected:\n",
      " -> usdt_eth_mcap_pct_chg_24h, lido_eth_tvl_1d_chg, DXY_pct_chg_24h, ATR_84, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, BREAKOUT_STR_120p, DXY_ma180_ratio, VIX_ma180_ratio, curve-dex_eth_tvl_ma180_ratio, SP500_ma180_ratio, GOLD_ma180_ratio, OBV_ma180_ratio, curve-dex_eth_tvl_pct_chg_24h, eth_chain_tvl_1d_chg, makerdao_eth_tvl_1d_chg, eth_chain_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg, uniswap_eth_tvl_pct_chg_24h, aave_eth_tvl_pct_chg_24h, optimism_tvl_pct_chg_24h\n",
      "   >> Running Fold 1 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.3229R | Val Exp: 1.8378R (Trades: 37)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: -0.327R | Te_Trades: 26 | Th: 0.5536\n",
      "  [RandomForest] Train Exp: 2.3526R | Val Exp: 2.0139R (Trades: 36)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.149R | Te_Trades: 67 | Th: 0.5854\n",
      "  [LightGBM]     Train Exp: 1.6398R | Val Exp: 1.6047R (Trades: 43)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.867R | Te_Trades: 30 | Th: 0.5113\n",
      "  [XGBoost]      Train Exp: 1.8049R | Val Exp: 1.0385R (Trades: 91)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.207R | Te_Trades: 58 | Th: 0.5038\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_1_walk_forward_rolling_reverse\n",
      "   >> Running Fold 2 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.0486R | Val Exp: 0.6538R (Trades: 91)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.647R | Te_Trades: 17 | Th: 0.5399\n",
      "  [RandomForest] Train Exp: 2.2257R | Val Exp: -0.2556R (Trades: 180)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.307R | Te_Trades: 75 | Th: 0.5162\n",
      "  [LightGBM]     Train Exp: 1.5882R | Val Exp: 0.4771R (Trades: 218)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.488R | Te_Trades: 214 | Th: 0.5072\n",
      "  [XGBoost]      Train Exp: 2.1386R | Val Exp: 0.5135R (Trades: 37)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.431R | Te_Trades: 203 | Th: 0.5025\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_2_walk_forward_rolling_reverse\n",
      "   >> Running Fold 3 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.1305R | Val Exp: 1.3085R (Trades: 94)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: -1.000R | Te_Trades: 2 | Th: 0.5098\n",
      "  [RandomForest] Train Exp: 2.1611R | Val Exp: 1.3981R (Trades: 54)\n",
      "    -> [EVAL] RandomForest | Te_Exp: -0.239R | Te_Trades: 23 | Th: 0.5290\n",
      "  [LightGBM]     Train Exp: 2.0179R | Val Exp: 1.0872R (Trades: 109)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.074R | Te_Trades: 101 | Th: 0.5043\n",
      "  [XGBoost]      Train Exp: 2.0994R | Val Exp: 1.1304R (Trades: 92)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.061R | Te_Trades: 66 | Th: 0.5578\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_3_walk_forward_rolling_reverse\n",
      "   >> Running Fold 4 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.1542R | Val Exp: 0.5273R (Trades: 55)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.615R | Te_Trades: 13 | Th: 0.5993\n",
      "  [RandomForest] Train Exp: 2.1319R | Val Exp: 1.2361R (Trades: 36)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.029R | Te_Trades: 17 | Th: 0.6221\n",
      "  [LightGBM]     Train Exp: 1.7634R | Val Exp: 1.3649R (Trades: 37)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.511R | Te_Trades: 44 | Th: 0.6015\n",
      "  [XGBoost]      Train Exp: 2.0844R | Val Exp: 1.1389R (Trades: 36)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 1.059R | Te_Trades: 17 | Th: 0.6256\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_4_walk_forward_rolling_reverse\n",
      "   >> Running Fold 5 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.1923R | Val Exp: 1.7364R (Trades: 55)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 1.333R | Te_Trades: 15 | Th: 0.5390\n",
      "  [RandomForest] Train Exp: 2.2708R | Val Exp: 0.7306R (Trades: 180)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.335R | Te_Trades: 118 | Th: 0.5530\n",
      "  [LightGBM]     Train Exp: 1.6344R | Val Exp: 1.8269R (Trades: 26)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.375R | Te_Trades: 28 | Th: 0.5137\n",
      "  [XGBoost]      Train Exp: 1.0127R | Val Exp: 1.5345R (Trades: 58)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.225R | Te_Trades: 100 | Th: 0.5070\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_5_walk_forward_rolling_reverse\n",
      "   >> Running Fold 6 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.3704R | Val Exp: 1.7222R (Trades: 36)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.231R | Te_Trades: 54 | Th: 0.6442\n",
      "  [RandomForest] Train Exp: 2.2130R | Val Exp: 1.4306R (Trades: 36)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.190R | Te_Trades: 50 | Th: 0.6008\n",
      "  [LightGBM]     Train Exp: 1.9634R | Val Exp: 1.9474R (Trades: 38)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.451R | Te_Trades: 41 | Th: 0.6228\n",
      "  [XGBoost]      Train Exp: 2.0952R | Val Exp: 1.9167R (Trades: 36)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: -0.284R | Te_Trades: 44 | Th: 0.5909\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_6_walk_forward_rolling_reverse\n",
      "   >> Running Fold 7 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 1.2593R | Val Exp: 1.3333R (Trades: 36)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.330R | Te_Trades: 271 | Th: 0.5075\n",
      "  [RandomForest] Train Exp: 2.1856R | Val Exp: 0.5799R (Trades: 288)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.339R | Te_Trades: 583 | Th: 0.5009\n",
      "  [LightGBM]     Train Exp: 1.2621R | Val Exp: 1.3545R (Trades: 55)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.085R | Te_Trades: 71 | Th: 0.5181\n",
      "  [XGBoost]      Train Exp: 2.1806R | Val Exp: 1.0030R (Trades: 166)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.403R | Te_Trades: 459 | Th: 0.5120\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_7_walk_forward_rolling_reverse\n",
      "   >> Running Fold 8 (walk_forward_rolling_reverse)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.1250R | Val Exp: 1.5455R (Trades: 22)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.754R | Te_Trades: 391 | Th: 0.5040\n",
      "  [RandomForest] Train Exp: 2.0208R | Val Exp: 1.5278R (Trades: 54)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.400R | Te_Trades: 45 | Th: 0.6369\n",
      "  [LightGBM]     Train Exp: 0.9945R | Val Exp: 0.5303R (Trades: 478)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.483R | Te_Trades: 203 | Th: 0.5038\n",
      "  [XGBoost]      Train Exp: 1.6526R | Val Exp: 1.0903R (Trades: 72)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.577R | Te_Trades: 91 | Th: 0.5076\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_8_walk_forward_rolling_reverse\n",
      "   >> Running Fold 9 (final_holdout)\n",
      "  [Info] Training with RR_Ratio: 2.50\n",
      "  [CatBoost]     Train Exp: 2.0000R | Val Exp: 1.3947R (Trades: 57)\n",
      "    -> [EVAL] CatBoost     | Te_Exp: 0.615R | Te_Trades: 260 | Th: 0.5383\n",
      "  [RandomForest] Train Exp: 1.9896R | Val Exp: 0.7500R (Trades: 54)\n",
      "    -> [EVAL] RandomForest | Te_Exp: 0.804R | Te_Trades: 163 | Th: 0.5916\n",
      "  [LightGBM]     Train Exp: 2.2276R | Val Exp: 1.1477R (Trades: 44)\n",
      "    -> [EVAL] LightGBM     | Te_Exp: 0.802R | Te_Trades: 202 | Th: 0.5642\n",
      "  [XGBoost]      Train Exp: 1.8041R | Val Exp: 0.7823R (Trades: 271)\n",
      "    -> [EVAL] XGBoost      | Te_Exp: 0.546R | Te_Trades: 917 | Th: 0.5108\n",
      " Saving results to: model_results/2025-11-29_Sniper/T0_L8_P3.5_S1.4_N21/fold_9_final_holdout\n",
      "\n",
      " === Trial Score: 0.7103R ===\n",
      "\n",
      "============================================================\n",
      " Starting T1_L6_P2.0_S1.6_N23\n",
      "============================================================\n",
      "\n",
      " Pipeline Started... (Train Start: 2020-01-01)\n",
      "Valid Samples: 17820/17903 (Removed: 83)\n",
      "Final Data Shape: (17820, 221)\n",
      "Data Split Completed. Total 9 folds generated.\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-04-08 21:00:00 ~ 2022-03-09 17:00:00 (N=4200)\n",
      " Val   Period: 2022-03-10 21:00:00 ~ 2022-08-07 17:00:00 (N=900)\n",
      " Test  Period: 2022-08-08 21:00:00 ~ 2023-01-05 17:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.503095238095238, 1: 0.4969047619047619}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Feature Selection] Top 23 Features Selected:\n",
      " -> usdt_eth_mcap_pct_chg_24h, aave_eth_tvl_1d_chg, ATR_84, MACDH_72_156_54, PRICE_VS_LOW_30p, BREAKOUT_STR_30p, usdt_eth_mcap_ma180_ratio, DXY_ma180_ratio, curve-dex_eth_tvl_ma180_ratio, SP500_ma180_ratio, GOLD_ma180_ratio, SMA_120_ma180_ratio, SMA_300_ma180_ratio, eth_chain_tvl_pct_chg_24h, uniswap_eth_tvl_pct_chg_24h, curve-dex_eth_tvl_pct_chg_24h, curve-dex_eth_tvl_1d_chg, eth_chain_tvl_1d_chg, makerdao_eth_tvl_1d_chg, uniswap_eth_tvl_1d_chg, makerdao_eth_tvl_pct_chg_24h, lido_eth_tvl_1d_chg, SP500_pct_chg_24h\n",
      "\n",
      "================================================================================\n",
      " Processing Fold 2 (walk_forward_rolling_reverse)\n",
      "================================================================================\n",
      " Train Period: 2020-09-06 21:00:00 ~ 2022-08-07 17:00:00 (N=4200)\n",
      " Val   Period: 2022-08-08 21:00:00 ~ 2023-01-05 17:00:00 (N=900)\n",
      " Test  Period: 2023-01-06 21:00:00 ~ 2023-06-05 17:00:00 (N=900)\n",
      "[Class Balance] Train Set: {0: 0.5095238095238095, 1: 0.49047619047619045}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Execution Block\n",
    "# -----------------------------------------------------------------------------\n",
    "df_merged=pd.read_csv(\"eth_4hour_cal.csv\")\n",
    "df_hour = pd.read_csv(\"eth_hour.csv\")\n",
    "# [Ïã§Ï†ú Ïã§Ìñâ]\n",
    "study = run_optuna_optimization(df_merged, df_hour, ML_MODELS_CLASSIFICATION, n_trials=60)\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\" Best Expectancy: {study.best_value:.4f}\")\n",
    "print(\"==================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7257f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
