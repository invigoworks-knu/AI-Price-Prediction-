{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fb3116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install pycoingecko ccxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354b91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Ethereum Price Prediction - Data Collection Module\n",
    "# 수집 데이터:\n",
    "# - Cryptocurrency OHLCV (yfinance + CCXT)\n",
    "# - Macro indicators (SP500, VIX, Gold, DXY)\n",
    "# - Fear & Greed Index\n",
    "# - DeFi TVL (Protocols, L2 chains)\n",
    "# - ETH Funding Rate\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import warnings\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# import yfinance as yf\n",
    "# import ccxt\n",
    "# from binance.client import Client\n",
    "# from defillama2 import DefiLlama\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# pyupbit_url = \"https://api.upbit.com/v1/candles/days\"\n",
    "# # ============================================================================\n",
    "# # Configuration\n",
    "# # ============================================================================\n",
    "# START_DATE = \"2017-01-01\"\n",
    "# END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "# OUTPUT_DIR = \"./macro_data\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# CRYPTO_TICKERS = {\n",
    "#     'BTC-USD': ('BTC', 'BTC/USDT'),\n",
    "#     'ETH-USD': ('ETH', 'ETH/USDT'),\n",
    "#     'BNB-USD': ('BNB', 'BNB/USDT'),\n",
    "#     'XRP-USD': ('XRP', 'XRP/USDT'),\n",
    "#     'SOL-USD': ('SOL', 'SOL/USDT'),\n",
    "#     'ADA-USD': ('ADA', 'ADA/USDT'),\n",
    "#     'DOGE-USD': ('DOGE', 'DOGE/USDT'),\n",
    "#     'AVAX-USD': ('AVAX', 'AVAX/USDT'),\n",
    "#     'DOT-USD': ('DOT', 'DOT/USDT')\n",
    "# }\n",
    "\n",
    "# MACRO_TICKERS = {\n",
    "#     'DX-Y.NYB': 'DXY',\n",
    "#     'GC=F': 'GOLD',\n",
    "#     '^VIX': 'VIX',\n",
    "#     '^GSPC': 'SP500'\n",
    "# }\n",
    "\n",
    "# DEFI_PROTOCOLS = ['makerdao', 'lido', 'aave', 'uniswap', 'curve-dex']\n",
    "# L2_CHAINS = ['Arbitrum', 'Optimism', 'Base', 'zkSync Era']\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 1. Cryptocurrency Price Data (yfinance + CCXT)\n",
    "# # ============================================================================\n",
    "# def collect_crypto_prices():\n",
    "#     \"\"\"암호화폐 가격 데이터 수집 (yfinance 우선, CCXT 보완)\"\"\"\n",
    "#     print(f\"\\n[1/6] Collecting cryptocurrency prices...\")\n",
    "    \n",
    "#     start_dt = pd.to_datetime(START_DATE)\n",
    "#     end_dt = pd.Timestamp.today()\n",
    "#     date_range = pd.date_range(start=start_dt, end=end_dt, freq='D')\n",
    "#     merged_df = pd.DataFrame(date_range, columns=['date'])\n",
    "    \n",
    "#     for ticker, (symbol, ccxt_symbol) in CRYPTO_TICKERS.items():\n",
    "#         try:\n",
    "#             df = yf.Ticker(ticker).history(start=START_DATE)\n",
    "#             if df.empty:\n",
    "#                 print(f\"  - {symbol}: No data\")\n",
    "#                 continue\n",
    "                \n",
    "#             df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "#             df.columns = [f\"{symbol}_{col}\" for col in df.columns]\n",
    "#             df = df.reset_index()\n",
    "#             df['date'] = df['Date'].dt.tz_localize(None)\n",
    "#             df = df.drop(columns=['Date'])\n",
    "            \n",
    "#             merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "#             print(f\"  - {symbol}: {len(df)} days\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  - {symbol}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "#     merged_df = merged_df.sort_values('date')\n",
    "#     output_path = os.path.join(OUTPUT_DIR, \"macro_crypto_data.csv\")\n",
    "#     merged_df.to_csv(output_path, index=False)\n",
    "#     print(f\"  Saved: {output_path}\")\n",
    "#     return merged_df\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 2. Macro Indicators (SP500, VIX, Gold, DXY)\n",
    "# # ============================================================================\n",
    "# def collect_macro_indicators():\n",
    "#     \"\"\"거시경제 지표 수집\"\"\"\n",
    "#     print(f\"\\n[2/6] Collecting macro indicators...\")\n",
    "    \n",
    "#     for ticker, name in MACRO_TICKERS.items():\n",
    "#         try:\n",
    "#             df = yf.download(ticker, start=START_DATE, end=END_DATE, \n",
    "#                            progress=False, group_by='column')\n",
    "#             df = df[['Close']].copy()\n",
    "#             df.columns = [name]\n",
    "#             df.index.name = 'date'\n",
    "            \n",
    "#             output_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "#             df.to_csv(output_path)\n",
    "#             print(f\"  - {name}: {len(df)} days\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  - {name}: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 3. Fear & Greed Index\n",
    "# # ============================================================================\n",
    "# def collect_fear_greed():\n",
    "#     \"\"\"Fear & Greed Index 수집\"\"\"\n",
    "#     print(f\"\\n[3/6] Collecting Fear & Greed Index...\")\n",
    "    \n",
    "#     try:\n",
    "#         url = \"https://api.alternative.me/fng/?limit=4000&format=json\"\n",
    "#         response = requests.get(url)\n",
    "#         data = response.json()['data']\n",
    "        \n",
    "#         df = pd.DataFrame(data)\n",
    "#         df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "#         df = df[['date', 'value']].rename(columns={'value': 'fear_greed'})\n",
    "#         df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "#         output_path = os.path.join(OUTPUT_DIR, \"fear_greed.csv\")\n",
    "#         df.to_csv(output_path, index=False)\n",
    "#         print(f\"  - Fear & Greed: {len(df)} days\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  - Fear & Greed: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 4. ETH Funding Rate\n",
    "# # ============================================================================\n",
    "# def collect_funding_rate():\n",
    "#     \"\"\"이더리움 펀딩비 수집\"\"\"\n",
    "#     print(f\"\\n[4/6] Collecting ETH funding rate...\")\n",
    "    \n",
    "#     try:\n",
    "#         client = Client(\"\", \"\")\n",
    "#         funding_rates = []\n",
    "        \n",
    "#         start_time = int(datetime.strptime(START_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "#         end_time = int(datetime.strptime(END_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "#         current_ts = start_time\n",
    "        \n",
    "#         while current_ts < end_time:\n",
    "#             rates = client.futures_funding_rate(\n",
    "#                 symbol='ETHUSDT',\n",
    "#                 startTime=current_ts,\n",
    "#                 limit=1000\n",
    "#             )\n",
    "#             if not rates:\n",
    "#                 break\n",
    "#             funding_rates.extend(rates)\n",
    "#             current_ts = rates[-1]['fundingTime'] + 1\n",
    "        \n",
    "#         df = pd.DataFrame(funding_rates)\n",
    "#         df['date'] = pd.to_datetime(df['fundingTime'], unit='ms').dt.date\n",
    "#         df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "#         df = df.groupby('date')['fundingRate'].mean().reset_index()\n",
    "        \n",
    "#         output_path = os.path.join(OUTPUT_DIR, \"eth_funding_rate.csv\")\n",
    "#         df.to_csv(output_path, index=False)\n",
    "#         print(f\"  - Funding Rate: {len(df)} days\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  - Funding Rate: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 5. DeFi Protocol TVL\n",
    "# # ============================================================================\n",
    "# def collect_defi_tvl():\n",
    "#     \"\"\"DeFi 프로토콜 TVL 수집\"\"\"\n",
    "#     print(f\"\\n[5/6] Collecting DeFi protocol TVL...\")\n",
    "    \n",
    "#     obj = DefiLlama()\n",
    "    \n",
    "#     # Ethereum Chain TVL\n",
    "#     try:\n",
    "#         df = obj.get_chain_hist_tvl('Ethereum')\n",
    "#         df = df.reset_index().rename(columns={'tvl': 'eth_chain_tvl'})\n",
    "#         output_path = os.path.join(OUTPUT_DIR, 'eth_chain_tvl.csv')\n",
    "#         df.to_csv(output_path, index=False)\n",
    "#         print(f\"  - ETH Chain TVL: {len(df)} days\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  - ETH Chain TVL: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "#     # Protocol TVL (Ethereum only)\n",
    "#     for protocol in DEFI_PROTOCOLS:\n",
    "#         try:\n",
    "#             tvl_dict = obj.get_protocol_hist_tvl_by_chain(protocol)\n",
    "#             if 'Ethereum' in tvl_dict:\n",
    "#                 df = tvl_dict['Ethereum'].reset_index()\n",
    "#                 df = df.rename(columns={'tvl': f'{protocol}_eth_tvl'})\n",
    "#                 output_path = os.path.join(OUTPUT_DIR, f'{protocol}_eth_tvl.csv')\n",
    "#                 df.to_csv(output_path, index=False)\n",
    "#                 print(f\"  - {protocol}: {len(df)} days\")\n",
    "#             else:\n",
    "#                 print(f\"  - {protocol}: No Ethereum data\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  - {protocol}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "#     # USDT Market Cap\n",
    "#     try:\n",
    "#         df = obj.get_stablecoin_hist_mcap_on_a_chain(1, 'ethereum')\n",
    "#         df = df.reset_index().rename(columns={'mcap': 'usdt_eth_mcap'})\n",
    "#         output_path = os.path.join(OUTPUT_DIR, 'usdt_eth_mcap.csv')\n",
    "#         df.to_csv(output_path, index=False)\n",
    "#         print(f\"  - USDT ETH Mcap: {len(df)} days\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  - USDT ETH Mcap: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 6. Layer 2 TVL\n",
    "# # ============================================================================\n",
    "# def collect_l2_tvl():\n",
    "#     \"\"\"Layer 2 TVL 수집\"\"\"\n",
    "#     print(f\"\\n[6/6] Collecting Layer 2 TVL...\")\n",
    "    \n",
    "#     all_data = []\n",
    "    \n",
    "#     for chain in L2_CHAINS:\n",
    "#         try:\n",
    "#             url = f\"https://api.llama.fi/v2/historicalChainTvl/{chain}\"\n",
    "#             response = requests.get(url)\n",
    "            \n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 df = pd.DataFrame(data)\n",
    "#                 df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "#                 df = df.rename(columns={'tvl': f'{chain.lower()}_tvl'})\n",
    "#                 all_data.append(df[['date', f'{chain.lower()}_tvl']])\n",
    "#                 print(f\"  - {chain}: {len(df)} days\")\n",
    "#             else:\n",
    "#                 print(f\"  - {chain}: API error\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  - {chain}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "#     if all_data:\n",
    "#         result = all_data[0]\n",
    "#         for df in all_data[1:]:\n",
    "#             result = pd.merge(result, df, on='date', how='outer')\n",
    "        \n",
    "#         result = result.sort_values('date').reset_index(drop=True)\n",
    "#         output_path = os.path.join(OUTPUT_DIR, 'layer2_tvl.csv')\n",
    "#         result.to_csv(output_path, index=False)\n",
    "#         print(f\"  Saved: {output_path}\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # Main Execution\n",
    "# # ============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"=\" * 80)\n",
    "#     print(\"ETH Price Prediction - Data Collection Pipeline\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     collect_crypto_prices()\n",
    "#     collect_macro_indicators()\n",
    "#     collect_fear_greed()\n",
    "#     collect_funding_rate()\n",
    "#     collect_defi_tvl()\n",
    "#     collect_l2_tvl()\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"Data collection completed!\")\n",
    "#     print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f8a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import ccxt\n",
    "from binance.client import Client\n",
    "from defillama2 import DefiLlama\n",
    "NEWS_DIR = '../news_data'\n",
    "\n",
    "####################################################\n",
    "# 1. 데이터 로드 함수\n",
    "####################################################\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    \"\"\"파일명에서 날짜 추출\"\"\"\n",
    "    patterns = [\n",
    "        r'(\\d{4})-(\\d{2})-(\\d{2})',\n",
    "        r'(\\d{4})(\\d{2})(\\d{2})',\n",
    "        r'(\\d{2})-(\\d{2})-(\\d{4})',\n",
    "        r'(\\d{2})(\\d{2})(\\d{4})'\n",
    "    ]\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    \"\"\"뉴스 데이터 로드 및 일간 집계\"\"\"\n",
    "    print(\"\\n=== 뉴스 데이터 로드 ===\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"경고: {root_dir} 디렉토리가 존재하지 않습니다. 더미 데이터를 생성합니다.\")\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'news': ['test news'] * len(dates),\n",
    "            'label': np.random.choice([1, 0, -1], len(dates))\n",
    "        })\n",
    "    \n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    print(f\"발견된 뉴스 파일: {len(csv_files)}개\")\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        \n",
    "        # 여러 인코딩 시도\n",
    "        for enc in ['utf-8', 'cp949', 'latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  건너뜀: {filename} (인코딩 실패)\")\n",
    "            continue\n",
    "        \n",
    "        # 날짜 컬럼 처리\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        \n",
    "        # label 컬럼 확인\n",
    "        if 'label' not in df.columns:\n",
    "            print(f\"  경고: {filename}에 'label' 컬럼이 없습니다. 0으로 채웁니다.\")\n",
    "            df['label'] = 0\n",
    "        \n",
    "        # 필요한 컬럼만 선택\n",
    "        if 'news' in df.columns:\n",
    "            df = df[['date', 'news', 'label']]\n",
    "        else:\n",
    "            df = df[['date', 'label']]\n",
    "        \n",
    "        all_data.append(df)\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"경고: 유효한 뉴스 파일을 찾을 수 없습니다. 더미 데이터를 생성합니다.\")\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'news': ['test news'] * len(dates),\n",
    "            'label': np.random.choice([1, 0, -1], len(dates))\n",
    "        })\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a962d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 뉴스 데이터 로드 ===\n",
      "발견된 뉴스 파일: 2135개\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26341</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>익명 고래 트레이더가 자오창펑(CZ) 관련 발언 직후 알트코인 공매도 포지션으로 대...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26342</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>마치 빅 브라더(Machi Big Brother)로 알려진 유명 트레이더 제프리 황...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26343</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>리플(Ripple)의 스테이블코인 RLUSD가 출시 1년 만에 시가총액 10억달러를...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26344</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>시바이누(Shiba Inu, SHIB)가 미국에서 현물 상장지수펀드(ETF) 출시 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26345</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>엑스알피(XRP)와 리플의 스테이블코인 RLUSD가 글로벌 결제 혁신에 속도를 내며...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               news label\n",
       "26341 2025-11-05  익명 고래 트레이더가 자오창펑(CZ) 관련 발언 직후 알트코인 공매도 포지션으로 대...     0\n",
       "26342 2025-11-05  마치 빅 브라더(Machi Big Brother)로 알려진 유명 트레이더 제프리 황...    -1\n",
       "26343 2025-11-05  리플(Ripple)의 스테이블코인 RLUSD가 출시 1년 만에 시가총액 10억달러를...     1\n",
       "26344 2025-11-05  시바이누(Shiba Inu, SHIB)가 미국에서 현물 상장지수펀드(ETF) 출시 ...     1\n",
       "26345 2025-11-05  엑스알피(XRP)와 리플의 스테이블코인 RLUSD가 글로벌 결제 혁신에 속도를 내며...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_all_news_data(NEWS_DIR)\n",
    "df.to_csv(\"news_data.csv\",index=False)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68131da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ETH Price Prediction - Data Collection Pipeline\n",
      "================================================================================\n",
      "\n",
      "[1/6] Collecting cryptocurrency prices from Upbit...\n",
      "  - BTC: 2980 days\n",
      "  - ETH: 2977 days\n",
      "  - XRP: 2977 days\n",
      "  - SOL: 1499 days\n",
      "  - ADA: 2966 days\n",
      "  - DOGE: 1732 days\n",
      "  - AVAX: 1383 days\n",
      "  - DOT: 1874 days\n",
      "  Saved: ./macro_data/macro_crypto_data.csv\n",
      "\n",
      "[2/6] Collecting macro indicators...\n",
      "  - DXY: 2238 days\n",
      "  - GOLD: 2237 days\n",
      "  - VIX: 2236 days\n",
      "  - SP500: 2235 days\n",
      "\n",
      "[3/6] Collecting Fear & Greed Index...\n",
      "  - Fear & Greed: 2847 days\n",
      "\n",
      "[4/6] Collecting ETH funding rate...\n",
      "  - Funding Rate: 2187 days\n",
      "\n",
      "[5/6] Collecting DeFi protocol TVL...\n",
      "  - ETH Chain TVL: 2978 days\n",
      "  - makerdao: 2515 days\n",
      "  - lido: 1799 days\n",
      "  - aave: 2013 days\n",
      "  - uniswap: 2577 days\n",
      "  - curve-dex: 2107 days\n",
      "  - USDT ETH Mcap: 2915 days\n",
      "\n",
      "[6/6] Collecting Layer 2 TVL...\n",
      "  - Arbitrum: 1571 days\n",
      "  - Optimism: 1592 days\n",
      "  - Base: 891 days\n",
      "  - zkSync Era: 1290 days\n",
      "  Saved: ./macro_data/layer2_tvl.csv\n",
      "\n",
      "================================================================================\n",
      "Data collection completed!\n",
      "Output directory: ./macro_data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import ccxt\n",
    "from binance.client import Client\n",
    "from defillama2 import DefiLlama\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from defillama2 import DefiLlama\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "START_DATE = \"2017-01-01\"\n",
    "END_DATE = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "OUTPUT_DIR = \"./macro_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "UPBIT_CRYPTO_TICKERS = {\n",
    "    'KRW-BTC': ('BTC', 'BTC'),\n",
    "    'KRW-ETH': ('ETH', 'ETH'),\n",
    "\n",
    "    'KRW-XRP': ('XRP', 'XRP'),\n",
    "    'KRW-SOL': ('SOL', 'SOL'),\n",
    "    'KRW-ADA': ('ADA', 'ADA'),\n",
    "    'KRW-DOGE': ('DOGE', 'DOGE'),\n",
    "    'KRW-AVAX': ('AVAX', 'AVAX'),\n",
    "    'KRW-DOT': ('DOT', 'DOT')\n",
    "}\n",
    "\n",
    "MACRO_TICKERS = {\n",
    "    'DX-Y.NYB': 'DXY',\n",
    "    'GC=F': 'GOLD',\n",
    "    '^VIX': 'VIX',\n",
    "    '^GSPC': 'SP500'\n",
    "}\n",
    "\n",
    "DEFI_PROTOCOLS = ['makerdao', 'lido', 'aave', 'uniswap', 'curve-dex']\n",
    "L2_CHAINS = ['Arbitrum', 'Optimism', 'Base', 'zkSync Era']\n",
    "\n",
    "\n",
    "def collect_upbit_crypto_prices():\n",
    "    print(f\"\\n[1/6] Collecting cryptocurrency prices from Upbit...\")\n",
    "    \n",
    "    start_dt = pd.to_datetime(START_DATE)\n",
    "    merged_df = None\n",
    "    \n",
    "    for market, (symbol, _) in UPBIT_CRYPTO_TICKERS.items():\n",
    "        try:\n",
    "            all_candles = []\n",
    "            to_date = None\n",
    "            \n",
    "            while True:\n",
    "                url = f\"https://api.upbit.com/v1/candles/days\"\n",
    "                params = {\n",
    "                    'market': market,\n",
    "                    'count': 200\n",
    "                }\n",
    "                if to_date:\n",
    "                    params['to'] = to_date\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                candles = response.json()\n",
    "                \n",
    "                if not candles:\n",
    "                    break\n",
    "                \n",
    "                all_candles.extend(candles)\n",
    "                oldest_date = pd.to_datetime(candles[-1]['candle_date_time_kst'])\n",
    "                \n",
    "                if oldest_date <= start_dt:\n",
    "                    break\n",
    "                \n",
    "                to_date = candles[-1]['candle_date_time_utc']\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            if not all_candles:\n",
    "                print(f\"  - {symbol}: No data\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame(all_candles)\n",
    "            df['date'] = pd.to_datetime(df['candle_date_time_kst']).dt.normalize()\n",
    "            \n",
    "            df = df.rename(columns={\n",
    "                'opening_price': f'{symbol}_Open',\n",
    "                'high_price': f'{symbol}_High',\n",
    "                'low_price': f'{symbol}_Low',\n",
    "                'trade_price': f'{symbol}_Close',\n",
    "                'candle_acc_trade_volume': f'{symbol}_Volume'\n",
    "            })\n",
    "            \n",
    "            df = df[['date', f'{symbol}_Open', f'{symbol}_High', \n",
    "                    f'{symbol}_Low', f'{symbol}_Close', f'{symbol}_Volume']]\n",
    "            df = df.sort_values('date')\n",
    "            df = df[df['date'] >= start_dt]\n",
    "            \n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "            \n",
    "            print(f\"  - {symbol}: {len(df)} days\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  - {symbol}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.sort_values('date')\n",
    "        output_path = os.path.join(OUTPUT_DIR, \"macro_crypto_data.csv\")\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "        print(f\"  Saved: {output_path}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Macro Indicators (SP500, VIX, Gold, DXY)\n",
    "# ============================================================================\n",
    "def collect_macro_indicators():\n",
    "    \"\"\"거시경제 지표 수집\"\"\"\n",
    "    print(f\"\\n[2/6] Collecting macro indicators...\")\n",
    "    \n",
    "    for ticker, name in MACRO_TICKERS.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=START_DATE, end=END_DATE, \n",
    "                           progress=False, group_by='column')\n",
    "            df = df[['Close']].copy()\n",
    "            df.columns = [name]\n",
    "            df.index.name = 'date'\n",
    "            \n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "            df.to_csv(output_path)\n",
    "            print(f\"  - {name}: {len(df)} days\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {name}: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Fear & Greed Index\n",
    "# ============================================================================\n",
    "def collect_fear_greed():\n",
    "    \"\"\"Fear & Greed Index 수집\"\"\"\n",
    "    print(f\"\\n[3/6] Collecting Fear & Greed Index...\")\n",
    "    \n",
    "    try:\n",
    "        url = \"https://api.alternative.me/fng/?limit=4000&format=json\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()['data']\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df = df[['date', 'value']].rename(columns={'value': 'fear_greed'})\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_DIR, \"fear_greed.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  - Fear & Greed: {len(df)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Fear & Greed: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ETH Funding Rate\n",
    "# ============================================================================\n",
    "def collect_funding_rate():\n",
    "    \"\"\"이더리움 펀딩비 수집\"\"\"\n",
    "    print(f\"\\n[4/6] Collecting ETH funding rate...\")\n",
    "    \n",
    "    try:\n",
    "        client = Client(\"\", \"\")\n",
    "        funding_rates = []\n",
    "        \n",
    "        start_time = int(datetime.strptime(START_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "        end_time = int(datetime.strptime(END_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "        current_ts = start_time\n",
    "        \n",
    "        while current_ts < end_time:\n",
    "            rates = client.futures_funding_rate(\n",
    "                symbol='ETHUSDT',\n",
    "                startTime=current_ts,\n",
    "                limit=1000\n",
    "            )\n",
    "            if not rates:\n",
    "                break\n",
    "            funding_rates.extend(rates)\n",
    "            current_ts = rates[-1]['fundingTime'] + 1\n",
    "        \n",
    "        df = pd.DataFrame(funding_rates)\n",
    "        df['date'] = pd.to_datetime(df['fundingTime'], unit='ms').dt.date\n",
    "        df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "        df = df.groupby('date')['fundingRate'].mean().reset_index()\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_DIR, \"eth_funding_rate.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  - Funding Rate: {len(df)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Funding Rate: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DeFi Protocol TVL\n",
    "# ============================================================================\n",
    "def collect_defi_tvl():\n",
    "    \"\"\"DeFi 프로토콜 TVL 수집\"\"\"\n",
    "    print(f\"\\n[5/6] Collecting DeFi protocol TVL...\")\n",
    "    \n",
    "    obj = DefiLlama()\n",
    "    \n",
    "#     # Ethereum Chain TVL\n",
    "    try:\n",
    "        df = obj.get_chain_hist_tvl('Ethereum')\n",
    "        df = df.reset_index().rename(columns={'tvl': 'eth_chain_tvl'})\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'eth_chain_tvl.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  - ETH Chain TVL: {len(df)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - ETH Chain TVL: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "    # Protocol TVL (Ethereum only)\n",
    "    for protocol in DEFI_PROTOCOLS:\n",
    "        try:\n",
    "            tvl_dict = obj.get_protocol_hist_tvl_by_chain(protocol)\n",
    "            if 'Ethereum' in tvl_dict:\n",
    "                df = tvl_dict['Ethereum'].reset_index()\n",
    "                df = df.rename(columns={'tvl': f'{protocol}_eth_tvl'})\n",
    "                output_path = os.path.join(OUTPUT_DIR, f'{protocol}_eth_tvl.csv')\n",
    "                df.to_csv(output_path, index=False)\n",
    "                print(f\"  - {protocol}: {len(df)} days\")\n",
    "            else:\n",
    "                print(f\"  - {protocol}: No Ethereum data\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {protocol}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "    # USDT Market Cap\n",
    "    try:\n",
    "        df = obj.get_stablecoin_hist_mcap_on_a_chain(1, 'ethereum')\n",
    "        df = df.reset_index().rename(columns={'mcap': 'usdt_eth_mcap'})\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'usdt_eth_mcap.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  - USDT ETH Mcap: {len(df)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - USDT ETH Mcap: Failed ({str(e)[:50]})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Layer 2 TVL\n",
    "# ============================================================================\n",
    "def collect_l2_tvl():\n",
    "    \"\"\"Layer 2 TVL 수집\"\"\"\n",
    "    print(f\"\\n[6/6] Collecting Layer 2 TVL...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for chain in L2_CHAINS:\n",
    "        try:\n",
    "            url = f\"https://api.llama.fi/v2/historicalChainTvl/{chain}\"\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                df = pd.DataFrame(data)\n",
    "                df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "                df = df.rename(columns={'tvl': f'{chain.lower()}_tvl'})\n",
    "                all_data.append(df[['date', f'{chain.lower()}_tvl']])\n",
    "                print(f\"  - {chain}: {len(df)} days\")\n",
    "            else:\n",
    "                print(f\"  - {chain}: API error\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {chain}: Failed ({str(e)[:50]})\")\n",
    "    \n",
    "    if all_data:\n",
    "        result = all_data[0]\n",
    "        for df in all_data[1:]:\n",
    "            result = pd.merge(result, df, on='date', how='outer')\n",
    "        \n",
    "        result = result.sort_values('date').reset_index(drop=True)\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'layer2_tvl.csv')\n",
    "        result.to_csv(output_path, index=False)\n",
    "        print(f\"  Saved: {output_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ETH Price Prediction - Data Collection Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    collect_upbit_crypto_prices()\n",
    "    collect_macro_indicators()\n",
    "    collect_fear_greed()\n",
    "    collect_funding_rate()\n",
    "    collect_defi_tvl()\n",
    "    collect_l2_tvl()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Data collection completed!\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec10cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from binance.client import Client\n",
    "from defillama2 import DefiLlama\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fetch_with_retries(url, max_retries=5, timeout=10, sleep_sec=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(sleep_sec)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def get_dates():\n",
    "    today = datetime.today()\n",
    "    start_date = (today - timedelta(days=200)).strftime('%Y-%m-%d')\n",
    "    end_date = (today + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    return start_date, end_date\n",
    "\n",
    "START_DATE, END_DATE = get_dates()\n",
    "OUTPUT_DIR = f\"./macro_data/today/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "UPBIT_CRYPTO_TICKERS = {\n",
    "    'KRW-BTC': ('BTC', 'BTC'),\n",
    "    'KRW-ETH': ('ETH', 'ETH'),\n",
    "    'KRW-XRP': ('XRP', 'XRP'),\n",
    "    'KRW-SOL': ('SOL', 'SOL'),\n",
    "    'KRW-ADA': ('ADA', 'ADA'),\n",
    "    'KRW-DOGE': ('DOGE', 'DOGE'),\n",
    "    'KRW-AVAX': ('AVAX', 'AVAX'),\n",
    "    'KRW-DOT': ('DOT', 'DOT')\n",
    "}\n",
    "\n",
    "MACRO_TICKERS = {\n",
    "    'DX-Y.NYB': 'DXY',\n",
    "    'GC=F': 'GOLD',\n",
    "    '^VIX': 'VIX',\n",
    "    '^GSPC': 'SP500'\n",
    "}\n",
    "\n",
    "DEFI_PROTOCOLS = ['makerdao', 'lido', 'aave', 'uniswap', 'curve-dex']\n",
    "L2_CHAINS = ['Arbitrum', 'Optimism', 'Base', 'zkSync Era']\n",
    "\n",
    "def collect_upbit_crypto_prices(start_date):\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    merged_df = None\n",
    "\n",
    "    for market, (symbol, _) in UPBIT_CRYPTO_TICKERS.items():\n",
    "        try:\n",
    "            all_candles = []\n",
    "            to_date = None\n",
    "            while True:\n",
    "                url = \"https://api.upbit.com/v1/candles/days\"\n",
    "                params = {'market': market, 'count': 200}\n",
    "                if to_date:\n",
    "                    params['to'] = to_date\n",
    "\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                candles = response.json()\n",
    "\n",
    "                if not candles:\n",
    "                    break\n",
    "\n",
    "                all_candles.extend(candles)\n",
    "                oldest_date = pd.to_datetime(candles[-1]['candle_date_time_kst'])\n",
    "                if oldest_date <= start_dt:\n",
    "                    break\n",
    "\n",
    "                to_date = candles[-1]['candle_date_time_utc']\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            if not all_candles:\n",
    "                continue\n",
    "\n",
    "            df = pd.DataFrame(all_candles)\n",
    "            df['date'] = pd.to_datetime(df['candle_date_time_kst']).dt.normalize()\n",
    "\n",
    "            df = df.rename(columns={\n",
    "                'opening_price': f'{symbol}_Open',\n",
    "                'high_price': f'{symbol}_High',\n",
    "                'low_price': f'{symbol}_Low',\n",
    "                'trade_price': f'{symbol}_Close',\n",
    "                'candle_acc_trade_volume': f'{symbol}_Volume'\n",
    "            })\n",
    "\n",
    "            df = df[['date', f'{symbol}_Open', f'{symbol}_High',\n",
    "                     f'{symbol}_Low', f'{symbol}_Close', f'{symbol}_Volume']]\n",
    "            df = df.sort_values('date')\n",
    "            df = df[df['date'] >= start_dt]\n",
    "\n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.sort_values('date')\n",
    "        merged_df.to_csv(os.path.join(OUTPUT_DIR, \"macro_crypto_data.csv\"), index=False)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def collect_macro_indicators(start_date, end_date):\n",
    "    for ticker, name in MACRO_TICKERS.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start_date, end=end_date, progress=False, group_by='column')\n",
    "            df = df[['Close']].copy()\n",
    "            df.columns = [name]\n",
    "            df.index.name = 'date'\n",
    "            df.to_csv(os.path.join(OUTPUT_DIR, f\"{name}.csv\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def collect_fear_greed():\n",
    "    try:\n",
    "        url = \"https://api.alternative.me/fng/?limit=4000&format=json\"\n",
    "        data = fetch_with_retries(url)\n",
    "        df = pd.DataFrame(data['data'])\n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df = df[['date', 'value']].rename(columns={'value': 'fear_greed'})\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, \"fear_greed.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def collect_funding_rate(start_date, end_date):\n",
    "    try:\n",
    "        client = Client(\"\", \"\")\n",
    "        funding_rates = []\n",
    "\n",
    "        start_time = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "        end_time = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "        current_ts = start_time\n",
    "\n",
    "        while current_ts < end_time:\n",
    "            rates = client.futures_funding_rate(symbol='ETHUSDT', startTime=current_ts, limit=1000)\n",
    "            if not rates:\n",
    "                break\n",
    "            funding_rates.extend(rates)\n",
    "            current_ts = rates[-1]['fundingTime'] + 1\n",
    "\n",
    "        df = pd.DataFrame(funding_rates)\n",
    "        df['date'] = pd.to_datetime(df['fundingTime'], unit='ms').dt.date\n",
    "        df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "        df = df.groupby('date')['fundingRate'].mean().reset_index()\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, \"eth_funding_rate.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def collect_defi_tvl():\n",
    "    obj = DefiLlama()\n",
    "    try:\n",
    "        df = obj.get_chain_hist_tvl('Ethereum')\n",
    "        df = df.reset_index().rename(columns={'tvl': 'eth_chain_tvl'})\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, 'eth_chain_tvl.csv'), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for protocol in DEFI_PROTOCOLS:\n",
    "        try:\n",
    "            attempts = 5\n",
    "            for i in range(attempts):\n",
    "                try:\n",
    "                    tvl_dict = obj.get_protocol_hist_tvl_by_chain(protocol)\n",
    "                    if 'Ethereum' in tvl_dict:\n",
    "                        df = tvl_dict['Ethereum'].reset_index()\n",
    "                        df = df.rename(columns={'tvl': f'{protocol}_eth_tvl'})\n",
    "                        df.to_csv(os.path.join(OUTPUT_DIR, f'{protocol}_eth_tvl.csv'), index=False)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if i == attempts - 1:\n",
    "                        raise\n",
    "                    time.sleep(2)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        df = obj.get_stablecoin_hist_mcap_on_a_chain(1, 'ethereum')\n",
    "        df = df.reset_index().rename(columns={'mcap': 'usdt_eth_mcap'})\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, 'usdt_eth_mcap.csv'), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def collect_l2_tvl():\n",
    "    all_data = []\n",
    "    for chain in L2_CHAINS:\n",
    "        try:\n",
    "            attempts = 5\n",
    "            for i in range(attempts):\n",
    "                try:\n",
    "                    url = f\"https://api.llama.fi/v2/historicalChainTvl/{chain}\"\n",
    "                    data = fetch_with_retries(url)\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "                    df = df.rename(columns={'tvl': f'{chain.lower()}_tvl'})\n",
    "                    all_data.append(df[['date', f'{chain.lower()}_tvl']])\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if i == attempts - 1:\n",
    "                        raise\n",
    "                    time.sleep(2)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if all_data:\n",
    "        result = all_data[0]\n",
    "        for df in all_data[1:]:\n",
    "            result = pd.merge(result, df, on='date', how='outer')\n",
    "\n",
    "        result = result.sort_values('date').reset_index(drop=True)\n",
    "        result.to_csv(os.path.join(OUTPUT_DIR, 'layer2_tvl.csv'), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collect_upbit_crypto_prices(START_DATE)\n",
    "    collect_macro_indicators(START_DATE, END_DATE)\n",
    "    collect_fear_greed()\n",
    "    collect_funding_rate(START_DATE, END_DATE)\n",
    "    collect_defi_tvl()\n",
    "    collect_l2_tvl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from defillama2 import DefiLlama\n",
    "from binance.client import Client\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 설정 (Configuration)\n",
    "# ============================================================================\n",
    "START_DATE = \"2017-01-01\"\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "OUTPUT_DIR = \"./dataset\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 업비트 4시간봉 (240분)\n",
    "TIMEFRAME_MINUTES = 240 \n",
    "\n",
    "UPBIT_CRYPTO_TICKERS = {\n",
    "    'KRW-BTC': 'BTC',\n",
    "    'KRW-ETH': 'ETH',\n",
    "    'KRW-XRP': 'XRP',\n",
    "    'KRW-SOL': 'SOL',\n",
    "    'KRW-ADA': 'ADA',\n",
    "    'KRW-DOGE': 'DOGE'\n",
    "}\n",
    "\n",
    "MACRO_TICKERS = {\n",
    "    'DX-Y.NYB': 'DXY',\n",
    "    'GC=F': 'GOLD',\n",
    "    '^VIX': 'VIX',\n",
    "    '^GSPC': 'SP500',\n",
    "    '^IXIC': 'NASDAQ'\n",
    "}\n",
    "\n",
    "DEFI_PROTOCOLS = ['makerdao', 'lido', 'aave', 'uniswap', 'curve-dex']\n",
    "\n",
    "# ============================================================================\n",
    "# 1. 업비트 4시간봉 수집 (KST 기준)\n",
    "# ============================================================================\n",
    "def collect_upbit_4h_candles():\n",
    "    print(f\"\\n[1/6] Collecting Upbit 4-Hour Candles (KST Base)...\")\n",
    "    \n",
    "    start_dt = pd.to_datetime(START_DATE)\n",
    "    merged_df = None\n",
    "    \n",
    "    for market, symbol in UPBIT_CRYPTO_TICKERS.items():\n",
    "        try:\n",
    "            all_candles = []\n",
    "            to_date = None\n",
    "            \n",
    "            while True:\n",
    "                url = f\"https://api.upbit.com/v1/candles/minutes/{TIMEFRAME_MINUTES}\"\n",
    "                params = {'market': market, 'count': 200}\n",
    "                if to_date:\n",
    "                    params['to'] = to_date\n",
    "                \n",
    "                resp = requests.get(url, params=params)\n",
    "                if resp.status_code != 200:\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                    \n",
    "                candles = resp.json()\n",
    "                if not candles:\n",
    "                    break\n",
    "                \n",
    "                all_candles.extend(candles)\n",
    "                \n",
    "                # KST 시간 기준 확인\n",
    "                last_dt_kst = pd.to_datetime(candles[-1]['candle_date_time_kst'])\n",
    "                \n",
    "                if last_dt_kst < start_dt:\n",
    "                    break\n",
    "                \n",
    "                to_date = candles[-1]['candle_date_time_utc']\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            if not all_candles:\n",
    "                continue\n",
    "                \n",
    "            df = pd.DataFrame(all_candles)\n",
    "            # [중요] KST 시간 사용\n",
    "            df['datetime'] = pd.to_datetime(df['candle_date_time_kst'])\n",
    "            \n",
    "            cols = {\n",
    "                'opening_price': f'{symbol}_Open',\n",
    "                'high_price': f'{symbol}_High',\n",
    "                'low_price': f'{symbol}_Low',\n",
    "                'trade_price': f'{symbol}_Close',\n",
    "                'candle_acc_trade_volume': f'{symbol}_Volume'\n",
    "            }\n",
    "            df = df.rename(columns=cols)[['datetime'] + list(cols.values())]\n",
    "            \n",
    "            # 정렬 및 중복 제거\n",
    "            df = df.sort_values('datetime').drop_duplicates('datetime')\n",
    "            df = df[df['datetime'] >= start_dt]\n",
    "            \n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = pd.merge(merged_df, df, on='datetime', how='outer')\n",
    "                \n",
    "            print(f\"  - {symbol}: {len(df)} rows collected\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  - {symbol} Error: {e}\")\n",
    "    \n",
    "    # 4시간 정각 인덱스 리샘플링 (중간 누락 채우기)\n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.sort_values('datetime').set_index('datetime')\n",
    "        merged_df = merged_df.resample('4H').first() \n",
    "        merged_df = merged_df.reset_index()\n",
    "        \n",
    "    return merged_df\n",
    "\n",
    "# ============================================================================\n",
    "# 2. 거시/온체인 일봉 데이터 (KST 09:00 기준 매핑용)\n",
    "# ============================================================================\n",
    "def collect_daily_metrics():\n",
    "    print(f\"\\n[2/6] Collecting Daily Metrics (Fees, Macro, TVL)...\")\n",
    "    daily_data_map = {}\n",
    "    \n",
    "    # 2-1. Macro\n",
    "    for ticker, name in MACRO_TICKERS.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=START_DATE, progress=False)\n",
    "            if isinstance(df.columns, pd.MultiIndex): df = df['Close']\n",
    "            else: df = df[['Close']]\n",
    "            \n",
    "            df.columns = [name]\n",
    "            df.index = pd.to_datetime(df.index).normalize()\n",
    "            daily_data_map[name] = df\n",
    "        except: pass\n",
    "\n",
    "    # 2-2. DefiLlama (TVL & Fees)\n",
    "    obj = DefiLlama()\n",
    "    \n",
    "    # Fees (우리가 중요하게 여긴 데이터)\n",
    "    for proto in DEFI_PROTOCOLS:\n",
    "        try:\n",
    "            # 1. TVL\n",
    "            tvl = obj.get_protocol_hist_tvl_by_chain(proto)\n",
    "            if 'Ethereum' in tvl:\n",
    "                df_tvl = tvl['Ethereum']\n",
    "                df_tvl.index = pd.to_datetime(df_tvl.index).normalize()\n",
    "                df_tvl.columns = [f'{proto}_tvl']\n",
    "                daily_data_map[f'{proto}_tvl'] = df_tvl\n",
    "            \n",
    "            # 2. Fees (Revenue) - 핵심 데이터\n",
    "            url = f\"https://api.llama.fi/summary/fees/{proto}?dataType=dailyFees\"\n",
    "            res = requests.get(url).json()\n",
    "            if 'totalDataChart' in res:\n",
    "                df_fees = pd.DataFrame(res['totalDataChart'])\n",
    "                # Unix Timestamp -> Date\n",
    "                df_fees['date'] = pd.to_datetime(df_fees['date'], unit='s').dt.normalize()\n",
    "                df_fees = df_fees.set_index('date')[['value']].rename(columns={'value': f'{proto}_fees'})\n",
    "                daily_data_map[f'{proto}_fees'] = df_fees\n",
    "                print(f\"  - {proto} Fees collected\")\n",
    "        except: pass\n",
    "        \n",
    "    return daily_data_map\n",
    "\n",
    "# ============================================================================\n",
    "# 3. [추가됨] Binance Open Interest & Funding Rate (4H)\n",
    "# ============================================================================\n",
    "def collect_binance_oi_funding():\n",
    "    print(f\"\\n[3/6] Collecting Binance OI & Funding Rate...\")\n",
    "    \n",
    "    try:\n",
    "        client = Client() \n",
    "        \n",
    "        # 1. Funding Rate (8시간 단위)\n",
    "        # API 제한 때문에 최근 1000개만 가져옴 (실전용 루프 생략)\n",
    "        fr = client.futures_funding_rate(symbol='ETHUSDT', limit=1000)\n",
    "        df_fr = pd.DataFrame(fr)\n",
    "        df_fr['datetime'] = pd.to_datetime(df_fr['fundingTime'], unit='ms')\n",
    "        df_fr['fundingRate'] = df_fr['fundingRate'].astype(float)\n",
    "        \n",
    "        # KST 변환 (UTC+9)\n",
    "        df_fr['datetime'] = df_fr['datetime'] + timedelta(hours=9)\n",
    "        \n",
    "        # 4H Resample\n",
    "        df_fr = df_fr.set_index('datetime').resample('4H').ffill()\n",
    "        df_fr = df_fr.rename(columns={'fundingRate': 'eth_funding_rate'})\n",
    "        \n",
    "        # 2. Open Interest (Hist) - 핵심 데이터\n",
    "        # 바이낸스는 'futures_open_interest_hist'로 5분~1시간 단위 제공\n",
    "        # 최근 데이터(최대 30일 등)만 제공될 수 있음.\n",
    "        oi = client.futures_open_interest_hist(symbol='ETHUSDT', period='4h', limit=500)\n",
    "        df_oi = pd.DataFrame(oi)\n",
    "        df_oi['datetime'] = pd.to_datetime(df_oi['timestamp'], unit='ms')\n",
    "        \n",
    "        # KST 변환\n",
    "        df_oi['datetime'] = df_oi['datetime'] + timedelta(hours=9)\n",
    "        \n",
    "        df_oi['sumOpenInterest'] = df_oi['sumOpenInterest'].astype(float)\n",
    "        df_oi['sumOpenInterestValue'] = df_oi['sumOpenInterestValue'].astype(float)\n",
    "        \n",
    "        # 4H Resample (이미 4h지만 인덱스 맞춤)\n",
    "        df_oi = df_oi.set_index('datetime').resample('4H').last()\n",
    "        df_oi = df_oi[['sumOpenInterest', 'sumOpenInterestValue']]\n",
    "        df_oi.columns = ['eth_oi_token', 'eth_oi_usdt']\n",
    "        \n",
    "        # Merge FR + OI\n",
    "        df_binance = df_fr.join(df_oi, how='outer')\n",
    "        \n",
    "        print(f\"  - Binance OI & Funding collected\")\n",
    "        return df_binance\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ! Binance Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. 병합 (Merge & Time Alignment)\n",
    "# ============================================================================\n",
    "def merge_all_data(base_df, daily_map, binance_df):\n",
    "    print(f\"\\n[4/6] Merging Datasets...\")\n",
    "    \n",
    "    final_df = base_df.copy()\n",
    "    final_df = final_df.sort_index()\n",
    "    \n",
    "    # 1. 일봉 데이터 병합 (Shift 적용: 어제 데이터 -> 오늘 사용)\n",
    "    # 업비트 KST 09:00 캔들 = 전일 종가 확정 직후\n",
    "    # 따라서 전일(T-1) 일봉 데이터를 T일 09:00 이후 캔들에 매핑\n",
    "    \n",
    "    # Join Key: '날짜' (시간 제외)\n",
    "    # 업비트 캔들의 날짜에서 1일을 뺌 -> 그 날짜의 일봉 데이터를 찾음\n",
    "    final_df['prev_date'] = final_df['datetime'].dt.normalize() - timedelta(days=1)\n",
    "    \n",
    "    for name, df_daily in daily_map.items():\n",
    "        temp = df_daily.reset_index()\n",
    "        temp.columns = ['prev_date', name]\n",
    "        final_df = pd.merge(final_df, temp, on='prev_date', how='left')\n",
    "        final_df[name] = final_df[name].fillna(method='ffill')\n",
    "        \n",
    "    final_df = final_df.drop(columns=['prev_date'])\n",
    "    \n",
    "    # 2. 바이낸스 데이터 병합 (같은 4시간봉끼리 매칭)\n",
    "    if not binance_df.empty:\n",
    "        final_df = final_df.set_index('datetime')\n",
    "        final_df = final_df.join(binance_df, how='left')\n",
    "        final_df = final_df.reset_index()\n",
    "        \n",
    "        # 결측치 처리 (OI는 민감하므로 ffill 조심해야 하나, 데이터 끊김 방지용으로 사용)\n",
    "        final_df['eth_funding_rate'] = final_df['eth_funding_rate'].fillna(method='ffill')\n",
    "        final_df['eth_oi_usdt'] = final_df['eth_oi_usdt'].fillna(method='ffill')\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "# ============================================================================\n",
    "# Main\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Base\n",
    "    df_base = collect_upbit_4h_candles()\n",
    "    \n",
    "    if df_base is not None:\n",
    "        # 2. Daily\n",
    "        daily_data = collect_daily_metrics()\n",
    "        \n",
    "        # 3. Binance (OI + FR)\n",
    "        binance_data = collect_binance_oi_funding()\n",
    "        \n",
    "        # 4. Merge\n",
    "        final_df = merge_all_data(df_base, daily_data, binance_data)\n",
    "        \n",
    "        # 5. Save\n",
    "        final_df = final_df.dropna(subset=['ETH_Close'])\n",
    "        final_df = final_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        save_path = os.path.join(OUTPUT_DIR, \"eth_4hour.csv\")\n",
    "        final_df.to_csv(save_path, index=False)\n",
    "        \n",
    "        print(f\"\\n[Complete] Saved to {save_path}\")\n",
    "        print(f\"Columns: {final_df.columns.tolist()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
