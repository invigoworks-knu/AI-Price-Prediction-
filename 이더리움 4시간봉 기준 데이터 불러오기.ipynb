{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c5148a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Final Pipeline (Safe Cut Rule Applied)...\n",
      "\n",
      "[1/7] Collecting Upbit (Safe Cut: <= NOW)...\n",
      "  - BTC: 17909 rows\n",
      "  - ETH: 17886 rows\n",
      "  - XRP: 17885 rows\n",
      "  - SOL: 9032 rows\n",
      "  - ADA: 17825 rows\n",
      "  - DOGE: 10430 rows\n",
      "  - AVAX: 8335 rows\n",
      "  - DOT: 11282 rows\n",
      "\n",
      "[2/7] Collecting Binance (KST Aligned & Safe Cut)...\n",
      "  - BTCUSDT: 18137 rows\n",
      "  - ETHUSDT: 18137 rows\n",
      "  - XRPUSDT: 16583 rows\n",
      "  - SOLUSDT: 11613 rows\n",
      "  - ADAUSDT: 16686 rows\n",
      "  - DOGEUSDT: 14027 rows\n",
      "  - AVAXUSDT: 11361 rows\n",
      "  - DOTUSDT: 11567 rows\n",
      "\n",
      "[3/7] Collecting Macro (Shifted & Safe Cut)...\n",
      "  - DXY: 19504 rows\n",
      "  - GOLD: 19504 rows\n",
      "  - VIX: 19504 rows\n",
      "  - SP500: 19504 rows\n",
      "\n",
      "[4/7] Collecting Fear & Greed (Shifted & Safe Cut)...\n",
      "  - Fear & Greed: 17140 rows\n",
      "\n",
      "[5/7] Collecting ETH Funding Rate (KST Aligned & Safe Cut)...\n",
      "  - Funding Rate: 13160 rows\n",
      "\n",
      "[6~7/7] Collecting TVL (Shifted & Safe Cut)...\n",
      "  - ETH Chain TVL: 17902 rows\n",
      "  - makerdao: 15118 rows\n",
      "  - lido: 10822 rows\n",
      "  - aave: 12106 rows\n",
      "  - uniswap: 15490 rows\n",
      "  - curve-dex: 12712 rows\n",
      "  - Arbitrum: 9826 rows\n",
      "  - Optimism: 9586 rows\n",
      "  - Base: 5380 rows\n",
      "  - zkSync Era: 7774 rows\n",
      "  - USDT Mcap: 17524 rows\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from binance.client import Client\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "START_DATE = \"2017-01-01\"\n",
    "OUTPUT_DIR = \"./macro_data_4h\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# [ÏõêÏπô] Ïã§Ìñâ ÏãúÏ†êÎ≥¥Îã§ ÎØ∏ÎûòÏù∏ Îç∞Ïù¥ÌÑ∞Îäî Î¨¥Ï°∞Í±¥ ÏÇ≠Ï†ú\n",
    "NOW = pd.Timestamp.now()\n",
    "\n",
    "UPBIT_TICKERS = {\n",
    "    'KRW-BTC': ('BTC', 'BTC'), 'KRW-ETH': ('ETH', 'ETH'), 'KRW-XRP': ('XRP', 'XRP'),\n",
    "    'KRW-SOL': ('SOL', 'SOL'), 'KRW-ADA': ('ADA', 'ADA'), 'KRW-DOGE': ('DOGE', 'DOGE'),\n",
    "    'KRW-AVAX': ('AVAX', 'AVAX'), 'KRW-DOT': ('DOT', 'DOT')\n",
    "}\n",
    "\n",
    "BINANCE_SYMBOLS = {\n",
    "    'BTCUSDT': 'BTC', 'ETHUSDT': 'ETH', 'XRPUSDT': 'XRP', 'SOLUSDT': 'SOL',\n",
    "    'ADAUSDT': 'ADA', 'DOGEUSDT': 'DOGE', 'AVAXUSDT': 'AVAX', 'DOTUSDT': 'DOT'\n",
    "}\n",
    "\n",
    "MACRO_TICKERS = {\n",
    "    'DX-Y.NYB': 'DXY', 'GC=F': 'GOLD', '^VIX': 'VIX', '^GSPC': 'SP500'\n",
    "}\n",
    "\n",
    "DEFI_PROTOCOLS = ['makerdao', 'lido', 'aave', 'uniswap', 'curve-dex']\n",
    "L2_CHAINS = ['Arbitrum', 'Optimism', 'Base', 'zkSync Era']\n",
    "\n",
    "def get_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    })\n",
    "    retry = Retry(total=5, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "    return session\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [1] Upbit (Í∏∞Ï§Ä Îç∞Ïù¥ÌÑ∞)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_upbit_crypto_prices_4h():\n",
    "    print(f\"\\n[1/7] Collecting Upbit (Safe Cut: <= NOW)...\")\n",
    "    session = get_session()\n",
    "    start_dt = pd.to_datetime(START_DATE)\n",
    "    merged_df = None\n",
    "    \n",
    "    for market, (symbol, _) in UPBIT_TICKERS.items():\n",
    "        try:\n",
    "            all_candles = []\n",
    "            to_date = None\n",
    "            while True:\n",
    "                url = \"https://api.upbit.com/v1/candles/minutes/240\"\n",
    "                params = {'market': market, 'count': 200}\n",
    "                if to_date: params['to'] = to_date\n",
    "                resp = session.get(url, params=params, timeout=10)\n",
    "                candles = resp.json()\n",
    "                if not candles: break\n",
    "                all_candles.extend(candles)\n",
    "                if pd.to_datetime(candles[-1]['candle_date_time_kst']) <= start_dt: break\n",
    "                to_date = candles[-1]['candle_date_time_utc']\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            if not all_candles: continue\n",
    "            \n",
    "            df = pd.DataFrame(all_candles)\n",
    "            df['timestamp'] = pd.to_datetime(df['candle_date_time_kst'])\n",
    "            \n",
    "            # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "            df = df[df['timestamp'] <= NOW]\n",
    "            \n",
    "            df = df.rename(columns={\n",
    "                'opening_price': f'{symbol}_Open', 'high_price': f'{symbol}_High',\n",
    "                'low_price': f'{symbol}_Low', 'trade_price': f'{symbol}_Close',\n",
    "                'candle_acc_trade_volume': f'{symbol}_Volume'\n",
    "            })\n",
    "            df = df[['timestamp', f'{symbol}_Open', f'{symbol}_High', f'{symbol}_Low', f'{symbol}_Close', f'{symbol}_Volume']]\n",
    "            df = df.sort_values('timestamp').drop_duplicates('timestamp')\n",
    "            df = df[df['timestamp'] >= start_dt]\n",
    "            \n",
    "            merged_df = df if merged_df is None else pd.merge(merged_df, df, on='timestamp', how='outer')\n",
    "            print(f\"  - {symbol}: {len(df)} rows\")\n",
    "        except Exception as e: print(f\"  - {symbol}: Failed {e}\")\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.sort_values('timestamp')\n",
    "        merged_df.to_csv(os.path.join(OUTPUT_DIR, \"crypto_4h_kst.csv\"), index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [2] Binance (KST Aligned & Safe Cut)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_binance_crypto_prices_4h():\n",
    "    print(f\"\\n[2/7] Collecting Binance (KST Aligned & Safe Cut)...\")\n",
    "    client = Client(\"\", \"\")\n",
    "    start_ms = int(datetime.strptime(START_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "    merged_df = None\n",
    "\n",
    "    for symbol, base in BINANCE_SYMBOLS.items():\n",
    "        try:\n",
    "            klines = client.get_historical_klines(symbol, Client.KLINE_INTERVAL_4HOUR, start_ms)\n",
    "            df = pd.DataFrame(klines, columns=[\"open_time\", \"o\", \"h\", \"l\", \"c\", \"v\", \"ct\", \"qav\", \"nt\", \"tbb\", \"tbq\", \"ig\"])\n",
    "            \n",
    "            # UTC -> KST (+9h)\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\") + pd.Timedelta(hours=9)\n",
    "            \n",
    "            # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "            df = df[df['timestamp'] <= NOW]\n",
    "            \n",
    "            # ÏóÖÎπÑÌä∏ ÏãúÍ∞Ñ(1, 5, 9...) Ï†ïÎ†¨\n",
    "            df = df[df['timestamp'].dt.hour % 4 == 1]\n",
    "            \n",
    "            df = df.rename(columns={\n",
    "                \"o\": f\"{base}_Bin_Open\", \"h\": f\"{base}_Bin_High\",\n",
    "                \"l\": f\"{base}_Bin_Low\", \"c\": f\"{base}_Bin_Close\",\n",
    "                \"v\": f\"{base}_Bin_Vol\"\n",
    "            })\n",
    "            df = df[['timestamp', f'{base}_Bin_Open', f'{base}_Bin_High', f'{base}_Bin_Low', f'{base}_Bin_Close', f'{base}_Bin_Vol']]\n",
    "            df = df[df[\"timestamp\"] >= pd.to_datetime(START_DATE)]\n",
    "            \n",
    "            merged_df = df if merged_df is None else pd.merge(merged_df, df, on=\"timestamp\", how=\"outer\")\n",
    "            print(f\"  - {symbol}: {len(df)} rows\")\n",
    "        except: pass\n",
    "            \n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.sort_values(\"timestamp\")\n",
    "        merged_df.to_csv(os.path.join(OUTPUT_DIR, \"crypto_binance_4h_kst.csv\"), index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [3] Macro (Shifted & Safe Cut)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_macro_indicators_4h():\n",
    "    print(f\"\\n[3/7] Collecting Macro (Shifted & Safe Cut)...\")\n",
    "    for ticker, name in MACRO_TICKERS.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=START_DATE, end=None, progress=False, interval='1d')\n",
    "            if isinstance(df.columns, pd.MultiIndex): df = df.xs('Close', level=0, axis=1)\n",
    "            else: df = df[['Close']]\n",
    "            \n",
    "            df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "            df.columns = [name]\n",
    "            df.index.name = 'timestamp'\n",
    "\n",
    "            # 1Ïùº + 9ÏãúÍ∞Ñ Shift\n",
    "            df.index = df.index + pd.Timedelta(days=1, hours=9)\n",
    "            \n",
    "            # [ÏõêÏπô Ï†ÅÏö©] Shift ÌõÑ ÎØ∏ÎûòÎ°ú ÎÑòÏñ¥Í∞Ñ Îç∞Ïù¥ÌÑ∞(ÎÇ¥Ïùº ÏïÑÏπ® Îì±) ÏÇ≠Ï†ú\n",
    "            # Ïò§Îäò Ïû•Ï§ë Îç∞Ïù¥ÌÑ∞ -> ÎÇ¥Ïùº ÏïÑÏπ®ÏúºÎ°ú Î∞ÄÎ¶º -> Ïó¨Í∏∞ÏÑú ÏûòÎ¶º -> ÏïàÏ†Ñ\n",
    "            df = df[df.index <= NOW]\n",
    "            \n",
    "            # ffillÎ°ú ÌòÑÏû¨ ÏãúÏ†êÍπåÏßÄ Ï±ÑÏõÄ\n",
    "            full_idx = pd.date_range(start=df.index[0], end=NOW, freq='1H')\n",
    "            df = df.reindex(full_idx, method='ffill')\n",
    "            \n",
    "            # ÏóÖÎπÑÌä∏ ÏãúÍ∞Ñ Ï∂îÏ∂ú\n",
    "            df_4h = df[df.index.hour % 4 == 1].copy()\n",
    "            \n",
    "            df_4h = df_4h[df_4h.index >= pd.to_datetime(START_DATE)]\n",
    "            df_4h.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_4h.csv\"))\n",
    "            print(f\"  - {name}: {len(df_4h)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {name}: Failed {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [4] Fear & Greed (Shifted & Safe Cut)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_fear_greed_4h():\n",
    "    print(f\"\\n[4/7] Collecting Fear & Greed (Shifted & Safe Cut)...\")\n",
    "    try:\n",
    "        session = get_session()\n",
    "        resp = session.get(\"https://api.alternative.me/fng/?limit=4000&format=json\", timeout=10)\n",
    "        data = resp.json()['data']\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df = df[['timestamp', 'value']].rename(columns={'value': 'fear_greed'})\n",
    "        df['fear_greed'] = df['fear_greed'].astype(float)\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "        \n",
    "        df.index = df.index + pd.Timedelta(days=1, hours=9)\n",
    "        \n",
    "        # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "        df = df[df.index <= NOW]\n",
    "        \n",
    "        full_idx = pd.date_range(start=df.index[0], end=NOW, freq='1H')\n",
    "        df = df.reindex(full_idx, method='ffill')\n",
    "        \n",
    "        df_4h = df[df.index.hour % 4 == 1].copy()\n",
    "        df_4h = df_4h[df_4h.index >= pd.to_datetime(START_DATE)].reset_index()\n",
    "        \n",
    "        df_4h.to_csv(os.path.join(OUTPUT_DIR, \"fear_greed_4h.csv\"), index=False)\n",
    "        print(f\"  - Fear & Greed: {len(df_4h)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Fear & Greed: Failed {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [5] Funding Rate (KST Aligned & Safe Cut)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_funding_rate_4h():\n",
    "    print(f\"\\n[5/7] Collecting ETH Funding Rate (KST Aligned & Safe Cut)...\")\n",
    "    try:\n",
    "        client = Client(\"\", \"\")\n",
    "        funding_rates = []\n",
    "        start_ts = int(datetime.strptime(START_DATE, \"%Y-%m-%d\").timestamp() * 1000)\n",
    "        end_ts = int(datetime.now().timestamp() * 1000) # ÌòÑÏû¨ÍπåÏßÄÎßå\n",
    "        \n",
    "        while start_ts < end_ts:\n",
    "            rates = client.futures_funding_rate(symbol='ETHUSDT', startTime=start_ts, limit=1000)\n",
    "            if not rates: break\n",
    "            funding_rates.extend(rates)\n",
    "            start_ts = rates[-1]['fundingTime'] + 1\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        df = pd.DataFrame(funding_rates)\n",
    "        df['timestamp'] = pd.to_datetime(df['fundingTime'], unit='ms') + pd.Timedelta(hours=9)\n",
    "        df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "        df = df[['timestamp', 'fundingRate']].sort_values('timestamp').set_index('timestamp')\n",
    "        \n",
    "        # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "        df = df[df.index <= NOW]\n",
    "        \n",
    "        full_idx = pd.date_range(start=df.index[0], end=NOW, freq='1H')\n",
    "        df = df.reindex(full_idx, method='ffill')\n",
    "        \n",
    "        df_4h = df[df.index.hour % 4 == 1].copy()\n",
    "        df_4h = df_4h[df_4h.index >= pd.to_datetime(START_DATE)].reset_index()\n",
    "        \n",
    "        df_4h.to_csv(os.path.join(OUTPUT_DIR, \"eth_funding_rate_4h.csv\"), index=False)\n",
    "        print(f\"  - Funding Rate: {len(df_4h)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Funding Rate: Failed {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [6, 7] TVL (Shifted & Safe Cut)\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_tvl_all():\n",
    "    print(f\"\\n[6~7/7] Collecting TVL (Shifted & Safe Cut)...\")\n",
    "    \n",
    "    def get_data_manual(url):\n",
    "        session = get_session()\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                time.sleep(2) \n",
    "                resp = session.get(url, timeout=30)\n",
    "                if resp.status_code == 200: return resp.json()\n",
    "                time.sleep(5)\n",
    "            except: time.sleep(5)\n",
    "        return None\n",
    "\n",
    "    def process_and_save(df, filename, col_name):\n",
    "        if df is None or df.empty: return\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        if df['timestamp'].dt.tz is not None:\n",
    "            df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "            \n",
    "        # 1Ïùº + 9ÏãúÍ∞Ñ Shift\n",
    "        df['timestamp'] = df['timestamp'] + pd.Timedelta(days=1, hours=9)\n",
    "        \n",
    "        df = df.sort_values('timestamp').drop_duplicates(subset=['timestamp']).set_index('timestamp')\n",
    "        \n",
    "        # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "        df = df[df.index <= NOW]\n",
    "        \n",
    "        # ffillÎ°ú ÌòÑÏû¨ÍπåÏßÄ Ï±ÑÏõÄ\n",
    "        full_idx = pd.date_range(start=df.index[0], end=NOW, freq='1H')\n",
    "        df = df.reindex(full_idx, method='ffill')\n",
    "        \n",
    "        df_4h = df[df.index.hour % 4 == 1].copy()\n",
    "        \n",
    "        df_4h = df_4h[df_4h.index >= pd.to_datetime(START_DATE)].reset_index()\n",
    "        df_4h.to_csv(os.path.join(OUTPUT_DIR, filename), index=False)\n",
    "        print(f\"  - {col_name}: {len(df_4h)} rows\")\n",
    "\n",
    "    # ETH Chain\n",
    "    try:\n",
    "        data = get_data_manual(\"https://api.llama.fi/v2/historicalChainTvl/Ethereum\")\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "            df = df.rename(columns={'tvl': 'eth_chain_tvl'})[['timestamp', 'eth_chain_tvl']]\n",
    "            process_and_save(df, 'eth_chain_tvl_4h.csv', 'ETH Chain TVL')\n",
    "    except: pass\n",
    "\n",
    "    # Protocols\n",
    "    for protocol in DEFI_PROTOCOLS:\n",
    "        try:\n",
    "            data = get_data_manual(f\"https://api.llama.fi/protocol/{protocol}\")\n",
    "            if data:\n",
    "                chain_data = data.get('chainTvls', {}).get('Ethereum', {}).get('tvl', [])\n",
    "                if not chain_data: chain_data = data.get('tvl', [])\n",
    "                if chain_data:\n",
    "                    df = pd.DataFrame(chain_data)\n",
    "                    df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "                    val_col = 'totalLiquidityUSD' if 'totalLiquidityUSD' in df.columns else 'tvl'\n",
    "                    df = df.rename(columns={val_col: f'{protocol}_eth_tvl'})\n",
    "                    process_and_save(df[['timestamp', f'{protocol}_eth_tvl']], f'{protocol}_eth_tvl_4h.csv', protocol)\n",
    "        except: pass\n",
    "\n",
    "    # L2 Chains\n",
    "    for chain in L2_CHAINS:\n",
    "        try:\n",
    "            chain_name = chain.replace(' ', '_').lower()\n",
    "            data = get_data_manual(f\"https://api.llama.fi/v2/historicalChainTvl/{chain}\")\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "                df = df.rename(columns={'tvl': f'{chain_name}_tvl'})\n",
    "                process_and_save(df[['timestamp', f'{chain_name}_tvl']], f'{chain_name}_tvl_4h.csv', chain)\n",
    "        except: pass\n",
    "    \n",
    "    # USDT Mcap\n",
    "    try:\n",
    "        data = get_data_manual(\"https://stablecoins.llama.fi/stablecoincharts/Ethereum?stablecoin=1\")\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            if 'totalCirculatingUSD' in df.columns:\n",
    "                df['usdt_eth_mcap'] = df['totalCirculatingUSD'].apply(\n",
    "                    lambda x: x.get('peggedUSD') if isinstance(x, dict) else x\n",
    "                )\n",
    "                \n",
    "                # 2. ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ï≤òÎ¶¨\n",
    "                df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "                \n",
    "                # 3. Ï†ÄÏû•\n",
    "                process_and_save(df[['timestamp', 'usdt_eth_mcap']], 'usdt_eth_mcap_4h.csv', 'USDT Mcap')\n",
    "    except Exception as e:\n",
    "        print(f\"  - USDT Mcap: Failed ({str(e)[:50]})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run All\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"üöÄ Starting Final Pipeline (Safe Cut Rule Applied)...\")\n",
    "collect_upbit_crypto_prices_4h()\n",
    "collect_binance_crypto_prices_4h()\n",
    "collect_macro_indicators_4h()\n",
    "collect_fear_greed_4h()\n",
    "collect_funding_rate_4h()\n",
    "collect_tvl_all()\n",
    "print(\"‚úÖ Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b57ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - ETH Chain TVL: 17902 rows\n",
      "  - USDT Mcap: 17524 rows\n"
     ]
    }
   ],
   "source": [
    "def get_data_manual(url):\n",
    "    session = get_session()\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            time.sleep(2) \n",
    "            resp = session.get(url, timeout=30)\n",
    "            if resp.status_code == 200: return resp.json()\n",
    "            time.sleep(5)\n",
    "        except: time.sleep(5)\n",
    "    return None\n",
    "\n",
    "def process_and_save(df, filename, col_name):\n",
    "    if df is None or df.empty: return\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    if df['timestamp'].dt.tz is not None:\n",
    "        df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    # 1Ïùº + 9ÏãúÍ∞Ñ Shift\n",
    "    df['timestamp'] = df['timestamp'] + pd.Timedelta(days=1, hours=9)\n",
    "\n",
    "    df = df.sort_values('timestamp').drop_duplicates(subset=['timestamp']).set_index('timestamp')\n",
    "\n",
    "    # [ÏõêÏπô Ï†ÅÏö©] ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú\n",
    "    df = df[df.index <= NOW]\n",
    "\n",
    "    # ffillÎ°ú ÌòÑÏû¨ÍπåÏßÄ Ï±ÑÏõÄ\n",
    "    full_idx = pd.date_range(start=df.index[0], end=NOW, freq='1H')\n",
    "    df = df.reindex(full_idx, method='ffill')\n",
    "\n",
    "    df_4h = df[df.index.hour % 4 == 1].copy()\n",
    "\n",
    "    df_4h = df_4h[df_4h.index >= pd.to_datetime(START_DATE)].reset_index()\n",
    "    df_4h.to_csv(os.path.join(OUTPUT_DIR, filename), index=False)\n",
    "    print(f\"  - {col_name}: {len(df_4h)} rows\")\n",
    "\n",
    "# ETH Chain\n",
    "try:\n",
    "    data = get_data_manual(\"https://api.llama.fi/v2/historicalChainTvl/Ethereum\")\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "        df = df.rename(columns={'tvl': 'eth_chain_tvl'})[['timestamp', 'eth_chain_tvl']]\n",
    "        process_and_save(df, 'eth_chain_tvl_4h.csv', 'ETH Chain TVL')\n",
    "except: pass\n",
    "    \n",
    "data = get_data_manual(\"https://stablecoins.llama.fi/stablecoincharts/Ethereum?stablecoin=1\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "if 'totalCirculatingUSD' in df.columns:\n",
    "    df['usdt_eth_mcap'] = df['totalCirculatingUSD'].apply(\n",
    "        lambda x: x.get('peggedUSD') if isinstance(x, dict) else x)\n",
    "    # 2. ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ï≤òÎ¶¨\n",
    "    df['timestamp'] = pd.to_datetime(df['date'], unit='s')\n",
    "\n",
    "    # 3. Ï†ÄÏû•\n",
    "    process_and_save(df[['timestamp', 'usdt_eth_mcap']], 'usdt_eth_mcap_4h.csv', 'USDT Mcap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05893e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting Final Merge (Target: eth_4hour.csv)...\n",
      "üìç Base Timeline: crypto_4h_kst.csv\n",
      "   -> Start Date: 2017-09-26 17:00:00\n",
      "   -> End Date:   2025-11-28 21:00:00\n",
      "   -> Total Rows: 17909\n",
      "üìÇ Files to merge: 18\n",
      "  ‚úÖ Merged: usdt_eth_mcap_4h.csv\n",
      "  ‚úÖ Merged: zksync_era_tvl_4h.csv\n",
      "  ‚úÖ Merged: base_tvl_4h.csv\n",
      "  ‚úÖ Merged: lido_eth_tvl_4h.csv\n",
      "  ‚úÖ Merged: eth_chain_tvl_4h.csv\n",
      "  ‚úÖ Merged: aave_eth_tvl_4h.csv\n",
      "  ‚úÖ Merged: DXY_4h.csv\n",
      "  ‚úÖ Merged: VIX_4h.csv\n",
      "  ‚úÖ Merged: curve-dex_eth_tvl_4h.csv\n",
      "  ‚úÖ Merged: makerdao_eth_tvl_4h.csv\n",
      "  ‚úÖ Merged: fear_greed_4h.csv\n",
      "  ‚úÖ Merged: SP500_4h.csv\n",
      "  ‚úÖ Merged: arbitrum_tvl_4h.csv\n",
      "  ‚úÖ Merged: uniswap_eth_tvl_4h.csv\n",
      "  ‚úÖ Merged: optimism_tvl_4h.csv\n",
      "  ‚úÖ Merged: eth_funding_rate_4h.csv\n",
      "  ‚úÖ Merged: GOLD_4h.csv\n",
      "  ‚úÖ Merged: crypto_binance_4h_kst.csv\n",
      "üõ†  Handling Missing Values...\n",
      "\n",
      "==================================================\n",
      "üéâ COMPLETE! File created: eth_4hour.csv\n",
      "üìä Final Shape: (17909, 98)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ÏÑ§Ï†ï\n",
    "OUTPUT_DIR = \"./macro_data_4h\"\n",
    "FINAL_FILENAME = \"eth_4hour.csv\"\n",
    "\n",
    "def create_eth_dataset():\n",
    "    print(\"üîÑ Starting Final Merge (Target: eth_4hour.csv)...\")\n",
    "    \n",
    "    # 1. Í∏∞Ï§Ä ÌååÏùº Î°úÎìú (Upbit Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞)\n",
    "    base_path = os.path.join(OUTPUT_DIR, \"crypto_4h_kst.csv\")\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(\"‚ùå Error: Base file 'crypto_4h_kst.csv' not found.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"üìç Base Timeline: {os.path.basename(base_path)}\")\n",
    "    base_df = pd.read_csv(base_path)\n",
    "    \n",
    "    # Timestamp Ï†ïÍ∑úÌôî\n",
    "    if 'timestamp' not in base_df.columns:\n",
    "        base_df.rename(columns={base_df.columns[0]: 'timestamp'}, inplace=True)\n",
    "    base_df['timestamp'] = pd.to_datetime(base_df['timestamp'])\n",
    "    base_df = base_df.sort_values('timestamp')\n",
    "    \n",
    "    print(f\"   -> Start Date: {base_df['timestamp'].min()}\")\n",
    "    print(f\"   -> End Date:   {base_df['timestamp'].max()}\")\n",
    "    print(f\"   -> Total Rows: {len(base_df)}\")\n",
    "\n",
    "    # 2. Î≥ëÌï©Ìï† ÌååÏùº Î¶¨Ïä§Ìä∏ÏóÖ\n",
    "    all_files = glob.glob(os.path.join(OUTPUT_DIR, \"*.csv\"))\n",
    "\n",
    "    exclude_keywords = ['crypto_4h_kst.csv', 'eth_4hour.csv', 'final_', 'merged', 'upbit_']\n",
    "    \n",
    "    files_to_merge = []\n",
    "    for f in all_files:\n",
    "        fname = os.path.basename(f)\n",
    "        if not any(k in fname for k in exclude_keywords):\n",
    "            files_to_merge.append(f)\n",
    "            \n",
    "    print(f\"üìÇ Files to merge: {len(files_to_merge)}\")\n",
    "\n",
    "    # 3. Left Join Î∞òÎ≥µ\n",
    "    # (Left JoinÏùÑ Ïì∞Î©¥ Base(Upbit)Î≥¥Îã§ Í≥ºÍ±∞Ïù∏ Îç∞Ïù¥ÌÑ∞Îäî ÏûêÎèôÏúºÎ°ú ÏûòÎ†§ÎÇòÍ∞ëÎãàÎã§)\n",
    "    for file_path in files_to_merge:\n",
    "        try:\n",
    "            fname = os.path.basename(file_path)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ïª¨ÎüºÎ™Ö Î≥¥Ï†ï (Ï≤´Î≤àÏß∏ Ïª¨Îüº -> timestamp)\n",
    "            if 'timestamp' not in df.columns:\n",
    "                df.rename(columns={df.columns[0]: 'timestamp'}, inplace=True)\n",
    "            \n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.drop_duplicates('timestamp')\n",
    "            \n",
    "            # Î≥ëÌï© ÏàòÌñâ\n",
    "            # on='timestamp' Ïô∏Ïóê Í≤πÏπòÎäî Ïª¨ÎüºÏù¥ ÏóÜÎã§Í≥† Í∞ÄÏ†ï (Left Join)\n",
    "            base_df = pd.merge(base_df, df, on='timestamp', how='left')\n",
    "            print(f\"  ‚úÖ Merged: {fname}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error merging {fname}: {e}\")\n",
    "\n",
    "    # 4. Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "    # (1) ffill: Ï£ºÎßê/Ìú¥Ïùº Îì±ÏúºÎ°ú Ïù∏Ìïú Macro Îç∞Ïù¥ÌÑ∞ Í≥µÎ∞± Î©îÍøà\n",
    "    # (2) fillna(0): ÏÉÅÏû• Ï†ÑÏù¥Îùº ÏïÑÏòà Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÎäî Íµ¨Í∞Ñ 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
    "    print(\"üõ†  Handling Missing Values...\")\n",
    "    base_df = base_df.ffill().fillna(0)\n",
    "\n",
    "    # 5. ÏµúÏ¢Ö Ï†ÄÏû•\n",
    "    save_path = os.path.join(OUTPUT_DIR, FINAL_FILENAME)\n",
    "    base_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üéâ COMPLETE! File created: {FINAL_FILENAME}\")\n",
    "    print(f\"üìä Final Shape: {base_df.shape}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_eth_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421fb8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Upbit KRW-ETH 1H data: 2020-01-01 ~ now\n",
      "  Fetched 200 candles... (oldest: 2025-11-20 15:00:00)\n",
      "  Fetched 400 candles... (oldest: 2025-11-12 16:00:00)\n",
      "  Fetched 600 candles... (oldest: 2025-11-04 17:00:00)\n",
      "  Fetched 800 candles... (oldest: 2025-10-27 18:00:00)\n",
      "  Fetched 1000 candles... (oldest: 2025-10-19 14:00:00)\n",
      "  Fetched 1200 candles... (oldest: 2025-10-11 15:00:00)\n",
      "  Fetched 1400 candles... (oldest: 2025-10-03 16:00:00)\n",
      "  Fetched 1600 candles... (oldest: 2025-09-25 17:00:00)\n",
      "  Fetched 1800 candles... (oldest: 2025-09-17 18:00:00)\n",
      "  Fetched 2000 candles... (oldest: 2025-09-09 19:00:00)\n",
      "  Fetched 2200 candles... (oldest: 2025-09-01 20:00:00)\n",
      "  Fetched 2400 candles... (oldest: 2025-08-24 21:00:00)\n",
      "  Fetched 2600 candles... (oldest: 2025-08-16 22:00:00)\n",
      "  Fetched 2800 candles... (oldest: 2025-08-08 23:00:00)\n",
      "  Fetched 3000 candles... (oldest: 2025-08-01 00:00:00)\n",
      "  Fetched 3200 candles... (oldest: 2025-07-24 01:00:00)\n",
      "  Fetched 3400 candles... (oldest: 2025-07-16 02:00:00)\n",
      "  Fetched 3600 candles... (oldest: 2025-07-08 03:00:00)\n",
      "  Fetched 3800 candles... (oldest: 2025-06-30 00:00:00)\n",
      "  Fetched 4000 candles... (oldest: 2025-06-22 01:00:00)\n",
      "  Fetched 4200 candles... (oldest: 2025-06-14 02:00:00)\n",
      "  Fetched 4400 candles... (oldest: 2025-06-06 03:00:00)\n",
      "  Fetched 4600 candles... (oldest: 2025-05-29 04:00:00)\n",
      "  Fetched 4800 candles... (oldest: 2025-05-21 05:00:00)\n",
      "  Fetched 5000 candles... (oldest: 2025-05-13 06:00:00)\n",
      "  Fetched 5200 candles... (oldest: 2025-05-05 07:00:00)\n",
      "  Fetched 5400 candles... (oldest: 2025-04-27 08:00:00)\n",
      "  Fetched 5600 candles... (oldest: 2025-04-19 09:00:00)\n",
      "  Fetched 5800 candles... (oldest: 2025-04-11 10:00:00)\n",
      "  Fetched 6000 candles... (oldest: 2025-04-03 11:00:00)\n",
      "  Fetched 6200 candles... (oldest: 2025-03-26 07:00:00)\n",
      "  Fetched 6400 candles... (oldest: 2025-03-18 08:00:00)\n",
      "  Fetched 6600 candles... (oldest: 2025-03-10 09:00:00)\n",
      "  Fetched 6800 candles... (oldest: 2025-03-02 10:00:00)\n",
      "  Fetched 7000 candles... (oldest: 2025-02-22 11:00:00)\n",
      "  Fetched 7200 candles... (oldest: 2025-02-14 12:00:00)\n",
      "  Fetched 7400 candles... (oldest: 2025-02-06 13:00:00)\n",
      "  Fetched 7600 candles... (oldest: 2025-01-29 14:00:00)\n",
      "  Fetched 7800 candles... (oldest: 2025-01-21 15:00:00)\n",
      "  Fetched 8000 candles... (oldest: 2025-01-13 16:00:00)\n",
      "  Fetched 8200 candles... (oldest: 2025-01-05 17:00:00)\n",
      "  Fetched 8400 candles... (oldest: 2024-12-28 14:00:00)\n",
      "  Fetched 8600 candles... (oldest: 2024-12-20 15:00:00)\n",
      "  Fetched 8800 candles... (oldest: 2024-12-12 16:00:00)\n",
      "  Fetched 9000 candles... (oldest: 2024-12-04 17:00:00)\n",
      "  Fetched 9200 candles... (oldest: 2024-11-26 18:00:00)\n",
      "  Fetched 9400 candles... (oldest: 2024-11-18 19:00:00)\n",
      "  Fetched 9600 candles... (oldest: 2024-11-10 20:00:00)\n",
      "  Fetched 9800 candles... (oldest: 2024-11-02 21:00:00)\n",
      "  Fetched 10000 candles... (oldest: 2024-10-25 22:00:00)\n",
      "  Fetched 10200 candles... (oldest: 2024-10-17 23:00:00)\n",
      "  Fetched 10400 candles... (oldest: 2024-10-09 20:00:00)\n",
      "  Fetched 10600 candles... (oldest: 2024-10-01 21:00:00)\n",
      "  Fetched 10800 candles... (oldest: 2024-09-23 22:00:00)\n",
      "  Fetched 11000 candles... (oldest: 2024-09-15 23:00:00)\n",
      "  Fetched 11200 candles... (oldest: 2024-09-08 00:00:00)\n",
      "  Fetched 11400 candles... (oldest: 2024-08-31 01:00:00)\n",
      "  Fetched 11600 candles... (oldest: 2024-08-23 02:00:00)\n",
      "  Fetched 11800 candles... (oldest: 2024-08-15 03:00:00)\n",
      "  Fetched 12000 candles... (oldest: 2024-08-07 04:00:00)\n",
      "  Fetched 12200 candles... (oldest: 2024-07-30 05:00:00)\n",
      "  Fetched 12400 candles... (oldest: 2024-07-22 06:00:00)\n",
      "  Fetched 12600 candles... (oldest: 2024-07-14 07:00:00)\n",
      "  Fetched 12800 candles... (oldest: 2024-07-06 08:00:00)\n",
      "  Fetched 13000 candles... (oldest: 2024-06-28 06:00:00)\n",
      "  Fetched 13200 candles... (oldest: 2024-06-20 07:00:00)\n",
      "  Fetched 13400 candles... (oldest: 2024-06-12 08:00:00)\n",
      "  Fetched 13600 candles... (oldest: 2024-06-04 09:00:00)\n",
      "  Fetched 13800 candles... (oldest: 2024-05-27 10:00:00)\n",
      "  Fetched 14000 candles... (oldest: 2024-05-19 11:00:00)\n",
      "  Fetched 14200 candles... (oldest: 2024-05-11 12:00:00)\n",
      "  Fetched 14400 candles... (oldest: 2024-05-03 13:00:00)\n",
      "  Fetched 14600 candles... (oldest: 2024-04-25 14:00:00)\n",
      "  Fetched 14800 candles... (oldest: 2024-04-17 15:00:00)\n",
      "  Fetched 15000 candles... (oldest: 2024-04-09 16:00:00)\n",
      "  Fetched 15200 candles... (oldest: 2024-04-01 17:00:00)\n",
      "  Fetched 15400 candles... (oldest: 2024-03-24 15:00:00)\n",
      "  Fetched 15600 candles... (oldest: 2024-03-16 16:00:00)\n",
      "  Fetched 15800 candles... (oldest: 2024-03-08 17:00:00)\n",
      "  Fetched 16000 candles... (oldest: 2024-02-29 18:00:00)\n",
      "  Fetched 16200 candles... (oldest: 2024-02-21 19:00:00)\n",
      "  Fetched 16400 candles... (oldest: 2024-02-13 20:00:00)\n",
      "  Fetched 16600 candles... (oldest: 2024-02-05 21:00:00)\n",
      "  Fetched 16800 candles... (oldest: 2024-01-28 22:00:00)\n",
      "  Fetched 17000 candles... (oldest: 2024-01-20 23:00:00)\n",
      "  Fetched 17200 candles... (oldest: 2024-01-13 00:00:00)\n",
      "  Fetched 17400 candles... (oldest: 2024-01-05 01:00:00)\n",
      "  Fetched 17600 candles... (oldest: 2023-12-27 23:00:00)\n",
      "  Fetched 17800 candles... (oldest: 2023-12-20 00:00:00)\n",
      "  Fetched 18000 candles... (oldest: 2023-12-12 01:00:00)\n",
      "  Fetched 18200 candles... (oldest: 2023-12-03 21:00:00)\n",
      "  Fetched 18400 candles... (oldest: 2023-11-25 17:00:00)\n",
      "  Fetched 18600 candles... (oldest: 2023-11-17 18:00:00)\n",
      "  Fetched 18800 candles... (oldest: 2023-11-09 19:00:00)\n",
      "  Fetched 19000 candles... (oldest: 2023-11-01 20:00:00)\n",
      "  Fetched 19200 candles... (oldest: 2023-10-24 21:00:00)\n",
      "  Fetched 19400 candles... (oldest: 2023-10-16 22:00:00)\n",
      "  Fetched 19600 candles... (oldest: 2023-10-08 23:00:00)\n",
      "  Fetched 19800 candles... (oldest: 2023-10-01 00:00:00)\n",
      "  Fetched 20000 candles... (oldest: 2023-09-23 01:00:00)\n",
      "  Fetched 20200 candles... (oldest: 2023-09-15 02:00:00)\n",
      "  Fetched 20400 candles... (oldest: 2023-09-07 03:00:00)\n",
      "  Fetched 20600 candles... (oldest: 2023-08-30 04:00:00)\n",
      "  Fetched 20800 candles... (oldest: 2023-08-22 05:00:00)\n",
      "  Fetched 21000 candles... (oldest: 2023-08-14 06:00:00)\n",
      "  Fetched 21200 candles... (oldest: 2023-08-06 04:00:00)\n",
      "  Fetched 21400 candles... (oldest: 2023-07-29 05:00:00)\n",
      "  Fetched 21600 candles... (oldest: 2023-07-21 06:00:00)\n",
      "  Fetched 21800 candles... (oldest: 2023-07-13 07:00:00)\n",
      "  Fetched 22000 candles... (oldest: 2023-07-05 05:00:00)\n",
      "  Fetched 22200 candles... (oldest: 2023-06-27 06:00:00)\n",
      "  Fetched 22400 candles... (oldest: 2023-06-19 07:00:00)\n",
      "  Fetched 22600 candles... (oldest: 2023-06-11 08:00:00)\n",
      "  Fetched 22800 candles... (oldest: 2023-06-03 09:00:00)\n",
      "  Fetched 23000 candles... (oldest: 2023-05-26 10:00:00)\n",
      "  Fetched 23200 candles... (oldest: 2023-05-18 11:00:00)\n",
      "  Fetched 23400 candles... (oldest: 2023-05-10 12:00:00)\n",
      "  Fetched 23600 candles... (oldest: 2023-05-02 13:00:00)\n",
      "  Fetched 23800 candles... (oldest: 2023-04-24 14:00:00)\n",
      "  Fetched 24000 candles... (oldest: 2023-04-16 10:00:00)\n",
      "  Fetched 24200 candles... (oldest: 2023-04-08 11:00:00)\n",
      "  Fetched 24400 candles... (oldest: 2023-03-31 09:00:00)\n",
      "  Fetched 24600 candles... (oldest: 2023-03-23 10:00:00)\n",
      "  Fetched 24800 candles... (oldest: 2023-03-15 11:00:00)\n",
      "  Fetched 25000 candles... (oldest: 2023-03-07 12:00:00)\n",
      "  Fetched 25200 candles... (oldest: 2023-02-27 13:00:00)\n",
      "  Fetched 25400 candles... (oldest: 2023-02-19 14:00:00)\n",
      "  Fetched 25600 candles... (oldest: 2023-02-11 15:00:00)\n",
      "  Fetched 25800 candles... (oldest: 2023-02-03 16:00:00)\n",
      "  Fetched 26000 candles... (oldest: 2023-01-26 17:00:00)\n",
      "  Fetched 26200 candles... (oldest: 2023-01-18 18:00:00)\n",
      "  Fetched 26400 candles... (oldest: 2023-01-10 19:00:00)\n",
      "  Fetched 26600 candles... (oldest: 2023-01-02 20:00:00)\n",
      "  Fetched 26800 candles... (oldest: 2022-12-25 19:00:00)\n",
      "  Fetched 27000 candles... (oldest: 2022-12-17 20:00:00)\n",
      "  Fetched 27200 candles... (oldest: 2022-12-09 21:00:00)\n",
      "  Fetched 27400 candles... (oldest: 2022-12-01 22:00:00)\n",
      "  Fetched 27600 candles... (oldest: 2022-11-23 23:00:00)\n",
      "  Fetched 27800 candles... (oldest: 2022-11-16 00:00:00)\n",
      "  Fetched 28000 candles... (oldest: 2022-11-08 01:00:00)\n",
      "  Fetched 28200 candles... (oldest: 2022-10-31 02:00:00)\n",
      "  Fetched 28400 candles... (oldest: 2022-10-23 03:00:00)\n",
      "  Fetched 28600 candles... (oldest: 2022-10-15 04:00:00)\n",
      "  Fetched 28800 candles... (oldest: 2022-10-07 05:00:00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetched 29000 candles... (oldest: 2022-09-29 05:00:00)\n",
      "  Fetched 29200 candles... (oldest: 2022-09-21 06:00:00)\n",
      "  Fetched 29400 candles... (oldest: 2022-09-13 07:00:00)\n",
      "  Fetched 29600 candles... (oldest: 2022-09-05 08:00:00)\n",
      "  Fetched 29800 candles... (oldest: 2022-08-28 09:00:00)\n",
      "  Fetched 30000 candles... (oldest: 2022-08-20 10:00:00)\n",
      "  Fetched 30200 candles... (oldest: 2022-08-12 11:00:00)\n",
      "  Fetched 30400 candles... (oldest: 2022-08-04 12:00:00)\n",
      "  Fetched 30600 candles... (oldest: 2022-07-27 13:00:00)\n",
      "  Fetched 30800 candles... (oldest: 2022-07-19 14:00:00)\n",
      "  Fetched 31000 candles... (oldest: 2022-07-11 15:00:00)\n",
      "  Fetched 31200 candles... (oldest: 2022-07-03 16:00:00)\n",
      "  Fetched 31400 candles... (oldest: 2022-06-25 14:00:00)\n",
      "  Fetched 31600 candles... (oldest: 2022-06-17 15:00:00)\n",
      "  Fetched 31800 candles... (oldest: 2022-06-09 16:00:00)\n",
      "  Fetched 32000 candles... (oldest: 2022-06-01 17:00:00)\n",
      "  Fetched 32200 candles... (oldest: 2022-05-24 18:00:00)\n",
      "  Fetched 32400 candles... (oldest: 2022-05-16 18:00:00)\n",
      "  Fetched 32600 candles... (oldest: 2022-05-08 19:00:00)\n",
      "  Fetched 32800 candles... (oldest: 2022-04-30 20:00:00)\n",
      "  Fetched 33000 candles... (oldest: 2022-04-22 21:00:00)\n",
      "  Fetched 33200 candles... (oldest: 2022-04-14 22:00:00)\n",
      "  Fetched 33400 candles... (oldest: 2022-04-06 23:00:00)\n",
      "  Fetched 33600 candles... (oldest: 2022-03-30 00:00:00)\n",
      "  Fetched 33800 candles... (oldest: 2022-03-22 01:00:00)\n",
      "  Fetched 34000 candles... (oldest: 2022-03-14 02:00:00)\n",
      "  Fetched 34200 candles... (oldest: 2022-03-06 03:00:00)\n",
      "  Fetched 34400 candles... (oldest: 2022-02-26 04:00:00)\n",
      "  Fetched 34600 candles... (oldest: 2022-02-18 05:00:00)\n",
      "  Fetched 34800 candles... (oldest: 2022-02-10 05:00:00)\n",
      "  Fetched 35000 candles... (oldest: 2022-02-02 06:00:00)\n",
      "  Fetched 35200 candles... (oldest: 2022-01-25 07:00:00)\n",
      "  Fetched 35400 candles... (oldest: 2022-01-17 08:00:00)\n",
      "  Fetched 35600 candles... (oldest: 2022-01-09 09:00:00)\n",
      "  Fetched 35800 candles... (oldest: 2022-01-01 10:00:00)\n",
      "  Fetched 36000 candles... (oldest: 2021-12-24 09:00:00)\n",
      "  Fetched 36200 candles... (oldest: 2021-12-16 10:00:00)\n",
      "  Fetched 36400 candles... (oldest: 2021-12-08 11:00:00)\n",
      "  Fetched 36600 candles... (oldest: 2021-11-30 12:00:00)\n",
      "  Fetched 36800 candles... (oldest: 2021-11-22 13:00:00)\n",
      "  Fetched 37000 candles... (oldest: 2021-11-14 13:00:00)\n",
      "  Fetched 37200 candles... (oldest: 2021-11-06 14:00:00)\n",
      "  Fetched 37400 candles... (oldest: 2021-10-29 15:00:00)\n",
      "  Fetched 37600 candles... (oldest: 2021-10-21 16:00:00)\n",
      "  Fetched 37800 candles... (oldest: 2021-10-13 17:00:00)\n",
      "  Fetched 38000 candles... (oldest: 2021-10-05 17:00:00)\n",
      "  Fetched 38200 candles... (oldest: 2021-09-27 18:00:00)\n",
      "  Fetched 38400 candles... (oldest: 2021-09-19 19:00:00)\n",
      "  Fetched 38600 candles... (oldest: 2021-09-11 20:00:00)\n",
      "  Fetched 38800 candles... (oldest: 2021-09-03 21:00:00)\n",
      "  Fetched 39000 candles... (oldest: 2021-08-26 22:00:00)\n",
      "  Fetched 39200 candles... (oldest: 2021-08-18 23:00:00)\n",
      "  Fetched 39400 candles... (oldest: 2021-08-10 23:00:00)\n",
      "  Fetched 39600 candles... (oldest: 2021-08-03 00:00:00)\n",
      "  Fetched 39800 candles... (oldest: 2021-07-26 01:00:00)\n",
      "  Fetched 40000 candles... (oldest: 2021-07-18 02:00:00)\n",
      "  Fetched 40200 candles... (oldest: 2021-07-10 03:00:00)\n",
      "  Fetched 40400 candles... (oldest: 2021-07-02 04:00:00)\n",
      "  Fetched 40600 candles... (oldest: 2021-06-24 03:00:00)\n",
      "  Fetched 40800 candles... (oldest: 2021-06-16 04:00:00)\n",
      "  Fetched 41000 candles... (oldest: 2021-06-08 05:00:00)\n",
      "  Fetched 41200 candles... (oldest: 2021-05-31 05:00:00)\n",
      "  Fetched 41400 candles... (oldest: 2021-05-23 06:00:00)\n",
      "  Fetched 41600 candles... (oldest: 2021-05-15 07:00:00)\n",
      "  Fetched 41800 candles... (oldest: 2021-05-07 08:00:00)\n",
      "  Fetched 42000 candles... (oldest: 2021-04-29 09:00:00)\n",
      "  Fetched 42200 candles... (oldest: 2021-04-21 10:00:00)\n",
      "  Fetched 42400 candles... (oldest: 2021-04-13 10:00:00)\n",
      "  Fetched 42600 candles... (oldest: 2021-04-05 11:00:00)\n",
      "  Fetched 42800 candles... (oldest: 2021-03-28 12:00:00)\n",
      "  Fetched 43000 candles... (oldest: 2021-03-20 11:00:00)\n",
      "  Fetched 43200 candles... (oldest: 2021-03-12 11:00:00)\n",
      "  Fetched 43400 candles... (oldest: 2021-03-04 12:00:00)\n",
      "  Fetched 43600 candles... (oldest: 2021-02-24 11:00:00)\n",
      "  Fetched 43800 candles... (oldest: 2021-02-16 09:00:00)\n",
      "  Fetched 44000 candles... (oldest: 2021-02-08 10:00:00)\n",
      "  Fetched 44200 candles... (oldest: 2021-01-31 10:00:00)\n",
      "  Fetched 44400 candles... (oldest: 2021-01-23 11:00:00)\n",
      "  Fetched 44600 candles... (oldest: 2021-01-15 12:00:00)\n",
      "  Fetched 44800 candles... (oldest: 2021-01-07 13:00:00)\n",
      "  Fetched 45000 candles... (oldest: 2020-12-30 14:00:00)\n",
      "  Fetched 45200 candles... (oldest: 2020-12-22 15:00:00)\n",
      "  Fetched 45400 candles... (oldest: 2020-12-14 16:00:00)\n",
      "  Fetched 45600 candles... (oldest: 2020-12-06 17:00:00)\n",
      "  Fetched 45800 candles... (oldest: 2020-11-28 18:00:00)\n",
      "  Fetched 46000 candles... (oldest: 2020-11-20 19:00:00)\n",
      "  Fetched 46200 candles... (oldest: 2020-11-12 20:00:00)\n",
      "  Fetched 46400 candles... (oldest: 2020-11-04 21:00:00)\n",
      "  Fetched 46600 candles... (oldest: 2020-10-27 22:00:00)\n",
      "  Fetched 46800 candles... (oldest: 2020-10-19 23:00:00)\n",
      "  Fetched 47000 candles... (oldest: 2020-10-12 00:00:00)\n",
      "  Fetched 47200 candles... (oldest: 2020-10-04 01:00:00)\n",
      "  Fetched 47400 candles... (oldest: 2020-09-26 01:00:00)\n",
      "  Fetched 47600 candles... (oldest: 2020-09-18 02:00:00)\n",
      "  Fetched 47800 candles... (oldest: 2020-09-10 03:00:00)\n",
      "  Fetched 48000 candles... (oldest: 2020-09-02 04:00:00)\n",
      "  Fetched 48200 candles... (oldest: 2020-08-25 05:00:00)\n",
      "  Fetched 48400 candles... (oldest: 2020-08-17 06:00:00)\n",
      "  Fetched 48600 candles... (oldest: 2020-08-09 07:00:00)\n",
      "  Fetched 48800 candles... (oldest: 2020-08-01 08:00:00)\n",
      "  Fetched 49000 candles... (oldest: 2020-07-24 09:00:00)\n",
      "  Fetched 49200 candles... (oldest: 2020-07-16 09:00:00)\n",
      "  Fetched 49400 candles... (oldest: 2020-07-08 10:00:00)\n",
      "  Fetched 49600 candles... (oldest: 2020-06-30 11:00:00)\n",
      "  Fetched 49800 candles... (oldest: 2020-06-22 12:00:00)\n",
      "  Fetched 50000 candles... (oldest: 2020-06-14 09:00:00)\n",
      "  Fetched 50200 candles... (oldest: 2020-06-06 10:00:00)\n",
      "  Fetched 50400 candles... (oldest: 2020-05-29 11:00:00)\n",
      "  Fetched 50600 candles... (oldest: 2020-05-21 12:00:00)\n",
      "  Fetched 50800 candles... (oldest: 2020-05-13 13:00:00)\n",
      "  Fetched 51000 candles... (oldest: 2020-05-05 14:00:00)\n",
      "  Fetched 51200 candles... (oldest: 2020-04-27 15:00:00)\n",
      "  Fetched 51400 candles... (oldest: 2020-04-19 16:00:00)\n",
      "  Fetched 51600 candles... (oldest: 2020-04-11 17:00:00)\n",
      "  Fetched 51800 candles... (oldest: 2020-04-03 18:00:00)\n",
      "  Fetched 52000 candles... (oldest: 2020-03-26 19:00:00)\n",
      "  Fetched 52200 candles... (oldest: 2020-03-18 20:00:00)\n",
      "  Fetched 52400 candles... (oldest: 2020-03-10 21:00:00)\n",
      "  Fetched 52600 candles... (oldest: 2020-03-02 22:00:00)\n",
      "  Fetched 52800 candles... (oldest: 2020-02-23 23:00:00)\n",
      "  Fetched 53000 candles... (oldest: 2020-02-16 00:00:00)\n",
      "  Fetched 53200 candles... (oldest: 2020-02-08 01:00:00)\n",
      "  Fetched 53400 candles... (oldest: 2020-01-31 01:00:00)\n",
      "  Fetched 53600 candles... (oldest: 2020-01-23 01:00:00)\n",
      "  Fetched 53800 candles... (oldest: 2020-01-15 02:00:00)\n",
      "  Fetched 54000 candles... (oldest: 2020-01-07 03:00:00)\n",
      "  Fetched 54200 candles... (oldest: 2019-12-30 04:00:00)\n",
      "Done! Total 51732 hourly candles\n",
      "Period: 2020-01-01 00:00:00 ~ 2025-11-28 22:00:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_upbit_hourly(market='KRW-ETH', start_date='2020-01-01'):\n",
    "    all_data = []\n",
    "    to_date = pd.Timestamp.now()\n",
    "    start_ts = pd.to_datetime(start_date)\n",
    "    \n",
    "    print(f\"Fetching Upbit {market} 1H data: {start_date} ~ now\")\n",
    "    \n",
    "    while to_date > start_ts:\n",
    "        url = \"https://api.upbit.com/v1/candles/minutes/60\"\n",
    "        params = {\n",
    "            'market': market,\n",
    "            'to': to_date.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            'count': 200\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if not data or isinstance(data, dict):\n",
    "                break\n",
    "            \n",
    "            all_data.extend(data)\n",
    "            \n",
    "            oldest = pd.to_datetime(data[-1]['candle_date_time_kst'])\n",
    "            to_date = oldest\n",
    "            \n",
    "            print(f\"  Fetched {len(all_data)} candles... (oldest: {oldest})\")\n",
    "            \n",
    "            time.sleep(0.15)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}, retrying...\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df = df.rename(columns={\n",
    "        'candle_date_time_kst': 'datetime',\n",
    "        'opening_price': 'open',\n",
    "        'high_price': 'high',\n",
    "        'low_price': 'low',\n",
    "        'trade_price': 'close',\n",
    "        'candle_acc_trade_volume': 'volume'\n",
    "    })\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    df = df[['datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    df = df.drop_duplicates(subset='datetime').reset_index(drop=True)\n",
    "    \n",
    "    mask = df['datetime'] >= start_date\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Done! Total {len(df)} hourly candles\")\n",
    "    print(f\"Period: {df['datetime'].min()} ~ {df['datetime'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "eth_hourly_krw = fetch_upbit_hourly('KRW-ETH', '2020-01-01')\n",
    "eth_hourly_krw.to_csv('eth_hour.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
