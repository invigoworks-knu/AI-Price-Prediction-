{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2df265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-dtypes in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (0.2.0)\n",
      "Collecting ml-dtypes\n",
      "  Using cached ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from ml-dtypes) (1.23.5)\n",
      "Installing collected packages: ml-dtypes\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ml-dtypes-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ml-dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155650d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ml_dtypes\n",
      "Version: 0.5.3\n",
      "Summary: ml_dtypes is a stand-alone implementation of several NumPy dtype extensions used in machine learning.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: ml_dtypes authors <ml_dtypes@google.com>\n",
      "License: \n",
      "Location: /raid/invigoworks/anaconda3/lib/python3.10/site-packages\n",
      "Requires: numpy, numpy\n",
      "Required-by: jax, jaxlib, onnx, tensorflow\n",
      "---\n",
      "Name: onnx\n",
      "Version: 1.19.0\n",
      "Summary: Open Neural Network Exchange\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: ONNX Contributors <onnx-technical-discuss@lists.lfaidata.foundation>\n",
      "License: Apache License v2.0\n",
      "Location: /raid/invigoworks/anaconda3/lib/python3.10/site-packages\n",
      "Requires: ml_dtypes, numpy, protobuf, typing_extensions\n",
      "Required-by: onnxmltools, skl2onnx\n",
      "---\n",
      "Name: onnxruntime\n",
      "Version: 1.22.1\n",
      "Summary: ONNX Runtime is a runtime accelerator for Machine Learning models\n",
      "Home-page: https://onnxruntime.ai\n",
      "Author: Microsoft Corporation\n",
      "Author-email: onnxruntime@microsoft.com\n",
      "License: MIT License\n",
      "Location: /raid/invigoworks/anaconda3/lib/python3.10/site-packages\n",
      "Requires: coloredlogs, flatbuffers, numpy, packaging, protobuf, sympy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show ml_dtypes onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563e9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11 뉴스 로드 시작\n",
      "1/11 뉴스 로드 완료: 25947건, 기간 2020-01-01 00:00:00 ~ 2025-10-03 00:00:00\n",
      "2/11 뉴스 감성 집계 시작\n",
      "2/11 뉴스 감성 집계 완료\n",
      "3/11 macro 파일 로드 시작\n",
      "3/11 macro 파일 로드 완료\n",
      "4/11 온체인 로드 시작\n",
      "4/11 온체인 로드 완료\n",
      "5/11 날짜 정렬 및 리인덱스 시작\n",
      "5/11 날짜 정렬 완료\n",
      "6/11 ETH 타깃 및 기술지표 준비 시작\n",
      "6/11 ETH 타깃 및 기술지표 준비 완료\n",
      "7/11 top-n macro 입력 생성 시작\n",
      "7/11 top-n macro 입력 생성 완료\n",
      "8/11 sentiment feature 준비 및 병합\n",
      "8/11 sentiment 준비 완료\n",
      "9/11 정규화 및 데이터 분할 준비\n",
      "9/11 정규화 및 분할 준비 완료\n",
      "10/11 데이터셋 및 DataLoader 생성 완료\n",
      "11/11 모델 초기화 완료\n",
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 - train_mse(scaled):0.006682 val_mae:76.331551 val_mse:11725.232422 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 - train_mse(scaled):0.006639 val_mae:76.915329 val_mse:11743.152344 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 - train_mse(scaled):0.006605 val_mae:76.788208 val_mse:11728.221680 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 - train_mse(scaled):0.006584 val_mae:76.923264 val_mse:11736.145508 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 - train_mse(scaled):0.006573 val_mae:76.940117 val_mse:11734.514648 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 - train_mse(scaled):0.006567 val_mae:76.975281 val_mse:11737.759766 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 - train_mse(scaled):0.006562 val_mae:76.968071 val_mse:11736.279297 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 - train_mse(scaled):0.006561 val_mae:76.974152 val_mse:11736.921875 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 - train_mse(scaled):0.006560 val_mae:76.974068 val_mse:11736.773438 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 - train_mse(scaled):0.006560 val_mae:76.974472 val_mse:11736.765625 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 - train_mse(scaled):0.006559 val_mae:76.975166 val_mse:11736.826172 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 - train_mse(scaled):0.006559 val_mae:76.975174 val_mse:11736.814453 val_corr:0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 결과 - MAE:76.985764 MSE:12178.814453 CORR:0.9908\n",
      "파이프라인 전체 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xg = torch.tensor(xg).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xm = torch.tensor(xm).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s).to(DEVICE)\n",
      "/tmp/ipykernel_176008/2079888074.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
      "/tmp/ipykernel_176008/2079888074.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y).to(DEVICE).float()\n"
     ]
    }
   ],
   "source": [
    "import os, re, math\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'   \n",
    "ONCHAIN_FILE = 'eth_onchain.csv'             \n",
    "NEWS_DIR = \"./news_data\"                       \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "START_TIME='2020-01-01'\n",
    "END_TIME='2025-10-02'\n",
    "\n",
    "L = 7\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "LR = 5e-4\n",
    "TOP_N = 5\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    patterns = [r'(\\d{4})-(\\d{2})-(\\d{2})', r'(\\d{4})(\\d{2})(\\d{2})', r'(\\d{2})-(\\d{2})-(\\d{4})', r'(\\d{2})(\\d{2})(\\d{4})']\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    all_data = []\n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"경고: 디렉토리가 존재하지 않습니다: {root_dir} -> 더미 뉴스 생성\")\n",
    "        dates = pd.date_range(START_TIME, '2025-10-02', freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        for enc in ['utf-8','cp949','latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"읽기 실패: {filepath}\")\n",
    "            continue\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        if 'label' not in df.columns:\n",
    "            raise ValueError(f\"{filepath}에 'label' 컬럼이 필요합니다.\")\n",
    "        if 'news' in df.columns:\n",
    "            df = df[['date','news','label']]\n",
    "        else:\n",
    "            df = df[['date','label']]\n",
    "        all_data.append(df)\n",
    "    if len(all_data) == 0:\n",
    "        print(\"경고: CSV 없음 -> 더미 뉴스 생성\")\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.normalize()\n",
    "    return combined_df\n",
    "\n",
    "print(\"1/11 뉴스 로드 시작\")\n",
    "news_df = load_all_news_data(NEWS_DIR)\n",
    "print(f\"1/11 뉴스 로드 완료: {len(news_df)}건, 기간 {news_df['date'].min()} ~ {news_df['date'].max()}\")\n",
    "\n",
    "print(\"2/11 뉴스 감성 집계 시작\")\n",
    "news_df = news_df.sort_values('date')\n",
    "grouped = news_df.groupby('date')['label']\n",
    "daily = grouped.agg(sent_mean='mean', sent_count='count').reset_index().set_index('date')\n",
    "pos = grouped.apply(lambda x: (x==1).sum())\n",
    "neu = grouped.apply(lambda x: (x==0).sum())\n",
    "neg = grouped.apply(lambda x: (x==-1).sum())\n",
    "props = pd.DataFrame({'pos_cnt': pos, 'neu_cnt': neu, 'neg_cnt': neg})\n",
    "daily = daily.join(props)\n",
    "def day_entropy(row):\n",
    "    counts = np.array([row['pos_cnt'], row['neu_cnt'], row['neg_cnt']], dtype=float)\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / s\n",
    "    p_nonzero = p[p>0]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero))\n",
    "daily['sent_entropy'] = daily.apply(day_entropy, axis=1)\n",
    "daily['sent_majority'] = news_df.groupby('date')['label'].apply(lambda sub: int(np.sign(np.round(sub.mean()))) if len(sub)>0 else 0)\n",
    "daily = daily.sort_index()\n",
    "alpha = 0.4\n",
    "daily['sent_mean_ewma'] = daily['sent_mean'].ewm(alpha=alpha, adjust=False).mean()\n",
    "all_dates_news = pd.date_range(daily.index.min(), daily.index.max(), freq='D')\n",
    "daily = daily.reindex(all_dates_news).fillna({'sent_mean':0.0,'sent_count':0,'pos_cnt':0,'neu_cnt':0,'neg_cnt':0,'sent_entropy':0.0,'sent_majority':0,'sent_mean_ewma':0.0}).fillna(0)\n",
    "print(\"2/11 뉴스 감성 집계 완료\")\n",
    "\n",
    "print(\"3/11 macro 파일 로드 시작\")\n",
    "if not os.path.exists(TARGET_MACRO_FILE):\n",
    "    raise FileNotFoundError(f\"{TARGET_MACRO_FILE} 파일이 필요합니다.\")\n",
    "macro_raw = pd.read_csv(TARGET_MACRO_FILE, parse_dates=['Date'])\n",
    "macro_raw['Date'] = pd.to_datetime(macro_raw['Date']).dt.tz_convert(None).dt.normalize()\n",
    "macro_raw = macro_raw.set_index('Date').sort_index()\n",
    "print(\"3/11 macro 파일 로드 완료\")\n",
    "\n",
    "print(\"4/11 온체인 로드 시작\")\n",
    "if not os.path.exists(ONCHAIN_FILE):\n",
    "    raise FileNotFoundError(f\"{ONCHAIN_FILE} 파일이 필요합니다.\")\n",
    "onchain = pd.read_csv(ONCHAIN_FILE, parse_dates=['date']).set_index('date').sort_index()\n",
    "onchain.index = pd.to_datetime(onchain.index)\n",
    "print(\"4/11 온체인 로드 완료\")\n",
    "\n",
    "start = max(macro_raw.index.min(), onchain.index.min(), daily.index.min())\n",
    "end = min(macro_raw.index.max(), onchain.index.max(), daily.index.max())\n",
    "date_index = pd.date_range(start, end, freq='D')\n",
    "\n",
    "print(\"5/11 날짜 정렬 및 리인덱스 시작\")\n",
    "macro_raw = macro_raw.reindex(date_index).ffill().bfill()\n",
    "onchain = onchain.reindex(date_index).fillna(0)\n",
    "daily = daily.reindex(date_index).fillna(0)\n",
    "print(\"5/11 날짜 정렬 완료\")\n",
    "\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 시작\")\n",
    "eth_cols = ['ETH_Open','ETH_High','ETH_Low','ETH_Close','ETH_Volume']\n",
    "for c in eth_cols:\n",
    "    if c not in macro_raw.columns:\n",
    "        raise ValueError(f\"{c} 컬럼이 macro 파일에 필요합니다.\")\n",
    "eth_price = macro_raw[eth_cols].rename(columns={'ETH_Open':'open','ETH_High':'high','ETH_Low':'low','ETH_Close':'close','ETH_Volume':'volume'})\n",
    "\n",
    "def compute_technical_indicators(df):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    pt = df['close']\n",
    "    N = 14\n",
    "    lowN = df['low'].rolling(N).min()\n",
    "    highN = df['high'].rolling(N).max()\n",
    "    out['stoch_k'] = (pt - lowN) / (highN - lowN + 1e-9) * 100\n",
    "    out['stoch_d'] = out['stoch_k'].rolling(3).mean()\n",
    "    out['williams_r'] = (highN - pt) / (highN - lowN + 1e-9) * 100\n",
    "    out['ad_osc'] = (pt - pt.shift(1)) / (df['high'] - df['low'] + 1e-9)\n",
    "    out['momentum'] = pt - pt.shift(10)\n",
    "    out['disparity7'] = pt / pt.rolling(7).mean() * 100\n",
    "    out['roc'] = pt / pt.shift(12) * 100\n",
    "    return out.fillna(0)\n",
    "\n",
    "tech = compute_technical_indicators(eth_price)\n",
    "target_feats = pd.concat([eth_price, tech, onchain], axis=1).fillna(0)\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 완료\")\n",
    "\n",
    "print(\"7/11 top-n macro 입력 생성 시작\")\n",
    "# macro_raw의 컬럼명에서 코인 접두사 추출\n",
    "cols = [c for c in macro_raw.columns if '_' in c]\n",
    "coins = []\n",
    "for c in cols:\n",
    "    coin = c.split('_')[0]\n",
    "    if coin not in coins:\n",
    "        coins.append(coin)\n",
    "# 타깃 ETH 제외\n",
    "coins = [c for c in coins if c.upper() != 'ETH']\n",
    "if len(coins) < TOP_N:\n",
    "    TOP_N = len(coins)\n",
    "selected_coins = coins[:TOP_N]\n",
    "macro_list = []\n",
    "for coin in selected_coins:\n",
    "    needed = [f\"{coin}_Open\", f\"{coin}_Close\", f\"{coin}_High\", f\"{coin}_Low\", f\"{coin}_Volume\"]\n",
    "    for n in needed:\n",
    "        if n not in macro_raw.columns:\n",
    "            raise ValueError(f\"{n} 컬럼이 macro 파일에 필요합니다.\")\n",
    "    arr = macro_raw[needed].values\n",
    "    macro_list.append(arr)\n",
    "macro_array = np.concatenate(macro_list, axis=1)\n",
    "macro_df = pd.DataFrame(macro_array, index=date_index, columns=[f\"{c}\" for c in range(macro_array.shape[1])])\n",
    "print(\"7/11 top-n macro 입력 생성 완료\")\n",
    "\n",
    "print(\"8/11 sentiment feature 준비 및 병합\")\n",
    "sent_cols = ['sent_mean','sent_count','pos_cnt','neu_cnt','neg_cnt','sent_entropy','sent_mean_ewma']\n",
    "sent_df = daily[sent_cols].fillna(0)\n",
    "print(\"8/11 sentiment 준비 완료\")\n",
    "\n",
    "print(\"9/11 정규화 및 데이터 분할 준비\")\n",
    "xg_all = target_feats.loc[date_index]\n",
    "xm_all = macro_df.loc[date_index]\n",
    "s_all = sent_df.loc[date_index]\n",
    "p_all = eth_price.loc[date_index][['close']]\n",
    "\n",
    "n_total = len(date_index)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "n_test = n_total - n_train - n_val\n",
    "dates = list(date_index)\n",
    "train_dates = dates[L-1 : n_train]\n",
    "val_dates = dates[n_train : n_train + n_val]\n",
    "test_dates = dates[n_train + n_val : ]\n",
    "\n",
    "scaler_xg = StandardScaler().fit(xg_all.iloc[:n_train].values)\n",
    "scaler_xm = StandardScaler().fit(xm_all.iloc[:n_train].values)\n",
    "scaler_s = StandardScaler().fit(s_all.iloc[:n_train].values)\n",
    "scaler_p = StandardScaler().fit(p_all.iloc[:n_train].values)\n",
    "print(\"9/11 정규화 및 분할 준비 완료\")\n",
    "\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, dates_list, xg_df, xm_df, s_df, p_df, L, scalers):\n",
    "        self.dates = dates_list\n",
    "        self.xg = xg_df\n",
    "        self.xm = xm_df\n",
    "        self.s = s_df\n",
    "        self.p = p_df\n",
    "        self.L = L\n",
    "        self.scaler_xg, self.scaler_xm, self.scaler_s, self.scaler_p = scalers\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.dates[idx]\n",
    "        start = t - timedelta(days=self.L-1)\n",
    "        idxs = pd.date_range(start=start, end=t, freq='D')\n",
    "        xg_win = self.xg.loc[idxs].values.astype(np.float32)\n",
    "        xm_win = self.xm.loc[idxs].values.astype(np.float32)\n",
    "        s_win = self.s.loc[idxs].values.astype(np.float32)\n",
    "        p_last = float(self.p.loc[t]['close'])\n",
    "        next_day = t + timedelta(days=1)\n",
    "        y = float(self.p.loc[next_day]['close']) if next_day in self.p.index else p_last\n",
    "        Bxg = self.scaler_xg.transform(xg_win)\n",
    "        Bxm = self.scaler_xm.transform(xm_win)\n",
    "        Bs = self.scaler_s.transform(s_win)\n",
    "        p_last_s = self.scaler_p.transform([[p_last]])[0,0]\n",
    "        y_s = self.scaler_p.transform([[y]])[0,0]\n",
    "        return Bxg, Bxm, Bs, np.float32(p_last_s), np.float32(y_s)\n",
    "\n",
    "train_ds = CryptoDataset(train_dates, xg_all, xm_all, s_all, p_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_p))\n",
    "val_ds = CryptoDataset(val_dates, xg_all, xm_all, s_all, p_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_p))\n",
    "test_ds = CryptoDataset(test_dates, xg_all, xm_all, s_all, p_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_p))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"10/11 데이터셋 및 DataLoader 생성 완료\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        Lx = x.size(1)\n",
    "        return x + self.pe[:Lx, :].unsqueeze(0)\n",
    "\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, in_c, d_model, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv1d(in_channels=in_c, out_channels=d_model, kernel_size=kernel_size, padding=padding)\n",
    "        self.pos = PositionalEncoding(d_model, max_len=500)\n",
    "    def forward(self, x):\n",
    "        x_t = x.transpose(1,2)\n",
    "        y = self.conv(x_t).transpose(1,2)\n",
    "        y = self.pos(y)\n",
    "        return y\n",
    "\n",
    "class Zeta(nn.Module):\n",
    "    def __init__(self, d_model, hidden=128, L=L):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, hidden), nn.ReLU(), nn.Linear(hidden, d_model))\n",
    "        self.linear_time = nn.Linear(L, 1)\n",
    "        self.linear_feat = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        y = self.ff(x)\n",
    "        y_t = self.linear_time(y.transpose(1,2)).squeeze(2)\n",
    "        out = self.linear_feat(y_t)\n",
    "        return out\n",
    "\n",
    "def roll_tensor(x, shift):\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    return torch.cat([x[:, -shift:, :], x[:, :-shift, :]], dim=1)\n",
    "\n",
    "def compute_macro_h(xembg, xembm):\n",
    "    B, Lx, d = xembg.size()\n",
    "    attn_scores = []\n",
    "    rolled = []\n",
    "    for tau in range(Lx):\n",
    "        r = roll_tensor(xembm, tau)\n",
    "        num = (xembg * r).sum(dim=2)\n",
    "        den = (xembg.norm(dim=2) * r.norm(dim=2) + 1e-9)\n",
    "        sim = (num / den).mean(dim=1)\n",
    "        attn_scores.append(sim.unsqueeze(1))\n",
    "        rolled.append(r.unsqueeze(1))\n",
    "    attn = torch.cat(attn_scores, dim=1)\n",
    "    a = F.softmax(attn, dim=1)\n",
    "    rolled_stack = torch.cat(rolled, dim=1)\n",
    "    a_exp = a.unsqueeze(-1).unsqueeze(-1)\n",
    "    hm = (a_exp * rolled_stack).sum(dim=1)\n",
    "    return hm\n",
    "\n",
    "class PriceDynamics(nn.Module):\n",
    "    def __init__(self, in_c, L=L):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_c)\n",
    "        self.lin = nn.Linear(L, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x_t = x.transpose(1,2)\n",
    "        out = self.lin(x_t)\n",
    "        out = out.squeeze(-1)\n",
    "        delta = out.mean(dim=1, keepdim=False)\n",
    "        return delta\n",
    "\n",
    "class CryptoPulseModel(nn.Module):\n",
    "    def __init__(self, in_target_c, in_macro_c, in_sent_c, d_model=64, L=L):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.embed_g = TimeEmbed(in_target_c, d_model)\n",
    "        self.embed_m = TimeEmbed(in_macro_c, d_model)\n",
    "        self.embed_s = TimeEmbed(in_sent_c, d_model)\n",
    "        self.zeta = Zeta(d_model, hidden=128, L=L)\n",
    "        self.price_dyn = PriceDynamics(in_target_c, L=L)\n",
    "        self.macro_pred_head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.ReLU(), nn.Linear(d_model//2, 1))\n",
    "        self.dyn_from_emb = nn.Linear(d_model, 1)\n",
    "        self.gamma_head = nn.Sequential(nn.Linear(2*d_model, 64), nn.ReLU(), nn.Linear(64,1))\n",
    "    def forward(self, xg, xm, s, p_last):\n",
    "        xg = xg.to(DEVICE); xm = xm.to(DEVICE); s = s.to(DEVICE); p_last = p_last.to(DEVICE)\n",
    "        xembg = self.embed_g(xg)\n",
    "        xembm = self.embed_m(xm)\n",
    "        semb = self.embed_s(s)\n",
    "        hm = compute_macro_h(xembg, xembm)\n",
    "        z = self.zeta(hm)\n",
    "        delta_macro = self.macro_pred_head(z).squeeze(-1)\n",
    "        delta_dyn_scale = self.price_dyn(xg)\n",
    "        delta_dyn_emb = self.dyn_from_emb(xembg.mean(dim=1)).squeeze(-1)\n",
    "        delta_dyn = 0.5 * delta_dyn_scale + 0.5 * delta_dyn_emb\n",
    "        kappa_vec = self.zeta(semb)\n",
    "        kappa = torch.tanh(kappa_vec.mean(dim=1))\n",
    "        if kappa.dim() > 1:\n",
    "            kappa = kappa.mean(dim=1)\n",
    "        cat = torch.cat([xembg.mean(dim=1), semb.mean(dim=1)], dim=1)\n",
    "        gamma = torch.sigmoid(self.gamma_head(cat)).squeeze(-1)\n",
    "        p1 = p_last + kappa * delta_macro\n",
    "        p2 = p_last + kappa * delta_dyn\n",
    "        p_hat = gamma * p1 + (1.0 - gamma) * p2\n",
    "        return p_hat, p1, p2, delta_macro, delta_dyn, gamma, kappa\n",
    "\n",
    "in_target_c = xg_all.shape[1]\n",
    "in_macro_c = xm_all.shape[1]\n",
    "in_sent_c = s_all.shape[1]\n",
    "model = CryptoPulseModel(in_target_c, in_macro_c, in_sent_c, d_model=64, L=L).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(\"11/11 모델 초기화 완료\")\n",
    "\n",
    "def inverse_scale_p(x_scaled):\n",
    "    arr = np.array(x_scaled).reshape(-1,1)\n",
    "    return scaler_p.inverse_transform(arr).reshape(-1)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xg,xm,s,p_last,y in loader:\n",
    "            xg = torch.tensor(xg).to(DEVICE)\n",
    "            xm = torch.tensor(xm).to(DEVICE)\n",
    "            s = torch.tensor(s).to(DEVICE)\n",
    "            p_last_t = torch.tensor(p_last).to(DEVICE).float()\n",
    "            y_t = torch.tensor(y).to(DEVICE).float()\n",
    "            p_hat_s, *_ = model(xg, xm, s, p_last_t)\n",
    "            preds.append(p_hat_s.cpu().numpy())\n",
    "            trues.append(y_t.cpu().numpy())\n",
    "    preds = np.concatenate(preds).ravel()\n",
    "    trues = np.concatenate(trues).ravel()\n",
    "    preds_inv = inverse_scale_p(preds)\n",
    "    trues_inv = inverse_scale_p(trues)\n",
    "    mae = np.mean(np.abs(preds_inv - trues_inv))\n",
    "    mse = np.mean((preds_inv - trues_inv)**2)\n",
    "    corr = np.corrcoef(preds_inv, trues_inv)[0,1] if len(preds_inv)>1 and np.std(preds_inv)>0 and np.std(trues_inv)>0 else 0.0\n",
    "    return mae, mse, corr\n",
    "\n",
    "print(\"학습 시작\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for xg,xm,s,p_last,y in train_loader:\n",
    "        xg = xg.to(DEVICE)\n",
    "        xm = xm.to(DEVICE)\n",
    "        s = s.to(DEVICE)\n",
    "        p_last_t = p_last.to(DEVICE).float()\n",
    "        y_t = y.to(DEVICE).float()\n",
    "        optimizer.zero_grad()\n",
    "        p_hat_s, *_ = model(xg, xm, s, p_last_t)\n",
    "        loss = F.mse_loss(p_hat_s, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xg.size(0)\n",
    "        count += xg.size(0)\n",
    "    avg_loss = total_loss / count if count>0 else 0.0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= 0.5\n",
    "    val_mae, val_mse, val_corr = evaluate_model(model, val_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - train_mse(scaled):{avg_loss:.6f} val_mae:{val_mae:.6f} val_mse:{val_mse:.6f} val_corr:{val_corr:.4f}\")\n",
    "\n",
    "test_mae, test_mse, test_corr = evaluate_model(model, test_loader)\n",
    "print(f\"테스트 결과 - MAE:{test_mae:.6f} MSE:{test_mse:.6f} CORR:{test_corr:.4f}\")\n",
    "print(\"파이프라인 전체 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0398e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11 뉴스 로드 시작\n",
      "1/11 뉴스 로드 완료: 25947건, 기간 2020-01-01 00:00:00 ~ 2025-10-03 00:00:00\n",
      "2/11 뉴스 감성 집계 시작\n",
      "2/11 뉴스 감성 집계 완료\n",
      "3/11 macro 파일 로드 시작\n",
      "3/11 macro 파일 로드 완료\n",
      "4/11 온체인 로드 시작\n",
      "4/11 온체인 로드 완료\n",
      "5/11 날짜 정렬 및 리인덱스 시작\n",
      "5/11 날짜 정렬 완료\n",
      "6/11 ETH 타깃 및 기술지표 준비 시작\n",
      "6/11 ETH 타깃 및 기술지표 준비 완료\n",
      "7/11 top-n macro 입력 생성 시작\n",
      "7/11 top-n macro 입력 생성 완료\n",
      "8/11 sentiment feature 준비 및 병합\n",
      "8/11 sentiment 준비 완료\n",
      "9/11 리턴(target) 생성 및 정규화 준비\n",
      "9/11 정규화 및 분할 준비 완료\n",
      "10/11 데이터셋 및 DataLoader 생성 완료\n",
      "11/11 모델 초기화 완료\n",
      "학습 시작\n",
      "Epoch 1/12 - train_mse(scaled):0.966564 val_price_mae:66.650810 val_price_mse:9396.792590 val_corr:0.9799 val_dir_acc:0.5145 val_ret_mae:0.022765\n",
      "Epoch 2/12 - train_mse(scaled):0.966473 val_price_mae:66.543676 val_price_mse:9362.161196 val_corr:0.9799 val_dir_acc:0.5145 val_ret_mae:0.022736\n",
      "Epoch 3/12 - train_mse(scaled):0.965660 val_price_mae:66.420509 val_price_mse:9312.860802 val_corr:0.9800 val_dir_acc:0.5260 val_ret_mae:0.022703\n",
      "Epoch 4/12 - train_mse(scaled):0.963134 val_price_mae:66.296595 val_price_mse:9175.002228 val_corr:0.9800 val_dir_acc:0.4798 val_ret_mae:0.022683\n",
      "Epoch 5/12 - train_mse(scaled):0.960335 val_price_mae:66.381214 val_price_mse:9148.084903 val_corr:0.9801 val_dir_acc:0.4335 val_ret_mae:0.022716\n",
      "Epoch 6/12 - train_mse(scaled):0.958439 val_price_mae:66.422595 val_price_mse:9145.308986 val_corr:0.9801 val_dir_acc:0.4855 val_ret_mae:0.022723\n",
      "Epoch 7/12 - train_mse(scaled):0.956971 val_price_mae:66.421735 val_price_mse:9136.015345 val_corr:0.9801 val_dir_acc:0.4740 val_ret_mae:0.022725\n",
      "Epoch 8/12 - train_mse(scaled):0.955960 val_price_mae:66.436983 val_price_mse:9135.961736 val_corr:0.9801 val_dir_acc:0.4740 val_ret_mae:0.022730\n",
      "Epoch 9/12 - train_mse(scaled):0.955023 val_price_mae:66.583099 val_price_mse:9134.629707 val_corr:0.9801 val_dir_acc:0.4451 val_ret_mae:0.022776\n",
      "Epoch 10/12 - train_mse(scaled):0.954754 val_price_mae:66.515046 val_price_mse:9139.301886 val_corr:0.9801 val_dir_acc:0.4682 val_ret_mae:0.022756\n",
      "Epoch 11/12 - train_mse(scaled):0.953272 val_price_mae:66.638858 val_price_mse:9142.888414 val_corr:0.9801 val_dir_acc:0.4451 val_ret_mae:0.022795\n",
      "Epoch 12/12 - train_mse(scaled):0.953045 val_price_mae:66.835654 val_price_mse:9157.797525 val_corr:0.9801 val_dir_acc:0.4451 val_ret_mae:0.022860\n",
      "테스트 결과 - Price MAE:84.808201 MSE:14430.818113 CORR:0.9905 DirAcc:0.4655 ReturnMAE:0.028515\n",
      "파이프라인 전체 완료\n"
     ]
    }
   ],
   "source": [
    "import os, re, math\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TARGET_MACRO_FILE = 'macro_crypto_data.csv'\n",
    "ONCHAIN_FILE = 'eth_onchain.csv'\n",
    "NEWS_DIR = \"./news_data\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "START_TIME = '2021-01-01'\n",
    "END_TIME = '2025-10-02'\n",
    "\n",
    "L = 7\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "LR = 5e-4\n",
    "TOP_N = 5\n",
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    patterns = [r'(\\d{4})-(\\d{2})-(\\d{2})', r'(\\d{4})(\\d{2})(\\d{2})', r'(\\d{2})-(\\d{2})-(\\d{4})', r'(\\d{2})(\\d{2})(\\d{4})']\n",
    "    basename = os.path.basename(filename)\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            try:\n",
    "                if len(match.group(1)) == 4:\n",
    "                    year, month, day = match.groups()\n",
    "                else:\n",
    "                    day, month, year = match.groups()\n",
    "                return pd.to_datetime(f\"{year}-{month}-{day}\")\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def load_all_news_data(root_dir):\n",
    "    all_data = []\n",
    "    if not os.path.exists(root_dir):\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.csv')])\n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(root_dir, filename)\n",
    "        file_date = parse_date_from_filename(filename)\n",
    "        for enc in ['utf-8','cp949','latin1']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = file_date\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if file_date is not None:\n",
    "                df['date'] = df['date'].fillna(file_date)\n",
    "        if 'label' not in df.columns:\n",
    "            raise ValueError(f\"{filepath}에 'label' 컬럼이 필요합니다.\")\n",
    "        if 'news' in df.columns:\n",
    "            df = df[['date','news','label']]\n",
    "        else:\n",
    "            df = df[['date','label']]\n",
    "        all_data.append(df)\n",
    "    if len(all_data) == 0:\n",
    "        dates = pd.date_range(START_TIME, END_TIME, freq='D')\n",
    "        return pd.DataFrame({'date': dates, 'news': ['test news'] * len(dates), 'label': np.random.choice([1,0,-1], len(dates))})\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.normalize()\n",
    "    return combined_df\n",
    "\n",
    "print(\"1/11 뉴스 로드 시작\")\n",
    "news_df = load_all_news_data(NEWS_DIR)\n",
    "print(f\"1/11 뉴스 로드 완료: {len(news_df)}건, 기간 {news_df['date'].min()} ~ {news_df['date'].max()}\")\n",
    "\n",
    "print(\"2/11 뉴스 감성 집계 시작\")\n",
    "news_df = news_df.sort_values('date')\n",
    "grouped = news_df.groupby('date')['label']\n",
    "daily = grouped.agg(sent_mean='mean', sent_count='count').reset_index().set_index('date')\n",
    "pos = grouped.apply(lambda x: (x==1).sum())\n",
    "neu = grouped.apply(lambda x: (x==0).sum())\n",
    "neg = grouped.apply(lambda x: (x==-1).sum())\n",
    "props = pd.DataFrame({'pos_cnt': pos, 'neu_cnt': neu, 'neg_cnt': neg})\n",
    "daily = daily.join(props)\n",
    "def day_entropy(row):\n",
    "    counts = np.array([row['pos_cnt'], row['neu_cnt'], row['neg_cnt']], dtype=float)\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / s\n",
    "    p_nonzero = p[p>0]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero))\n",
    "daily['sent_entropy'] = daily.apply(day_entropy, axis=1)\n",
    "daily['sent_majority'] = news_df.groupby('date')['label'].apply(lambda sub: int(np.sign(np.round(sub.mean()))) if len(sub)>0 else 0)\n",
    "daily = daily.sort_index()\n",
    "alpha = 0.4\n",
    "daily['sent_mean_ewma'] = daily['sent_mean'].ewm(alpha=alpha, adjust=False).mean()\n",
    "all_dates_news = pd.date_range(daily.index.min(), daily.index.max(), freq='D')\n",
    "daily = daily.reindex(all_dates_news).fillna({'sent_mean':0.0,'sent_count':0,'pos_cnt':0,'neu_cnt':0,'neg_cnt':0,'sent_entropy':0.0,'sent_majority':0,'sent_mean_ewma':0.0}).fillna(0)\n",
    "print(\"2/11 뉴스 감성 집계 완료\")\n",
    "\n",
    "print(\"3/11 macro 파일 로드 시작\")\n",
    "if not os.path.exists(TARGET_MACRO_FILE):\n",
    "    raise FileNotFoundError(f\"{TARGET_MACRO_FILE} 파일이 필요합니다.\")\n",
    "macro_raw = pd.read_csv(TARGET_MACRO_FILE, parse_dates=['Date'])\n",
    "macro_raw['Date'] = pd.to_datetime(macro_raw['Date']).dt.tz_convert(None).dt.normalize()\n",
    "macro_raw = macro_raw.set_index('Date').sort_index()\n",
    "print(\"3/11 macro 파일 로드 완료\")\n",
    "\n",
    "print(\"4/11 온체인 로드 시작\")\n",
    "if not os.path.exists(ONCHAIN_FILE):\n",
    "    raise FileNotFoundError(f\"{ONCHAIN_FILE} 파일이 필요합니다.\")\n",
    "onchain = pd.read_csv(ONCHAIN_FILE, parse_dates=['date']).set_index('date').sort_index()\n",
    "onchain.index = pd.to_datetime(onchain.index)\n",
    "print(\"4/11 온체인 로드 완료\")\n",
    "\n",
    "start = max(macro_raw.index.min(), onchain.index.min(), daily.index.min(), pd.to_datetime(START_TIME))\n",
    "end = min(macro_raw.index.max(), onchain.index.max(), daily.index.max(), pd.to_datetime(END_TIME))\n",
    "date_index_full = pd.date_range(start, end, freq='D')\n",
    "\n",
    "print(\"5/11 날짜 정렬 및 리인덱스 시작\")\n",
    "macro_raw = macro_raw.reindex(date_index_full).ffill().bfill()\n",
    "onchain = onchain.reindex(date_index_full).fillna(0)\n",
    "daily = daily.reindex(date_index_full).fillna(0)\n",
    "print(\"5/11 날짜 정렬 완료\")\n",
    "\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 시작\")\n",
    "eth_cols = ['ETH_Open','ETH_High','ETH_Low','ETH_Close','ETH_Volume']\n",
    "for c in eth_cols:\n",
    "    if c not in macro_raw.columns:\n",
    "        raise ValueError(f\"{c} 컬럼이 macro 파일에 필요합니다.\")\n",
    "eth_price = macro_raw[eth_cols].rename(columns={'ETH_Open':'open','ETH_High':'high','ETH_Low':'low','ETH_Close':'close','ETH_Volume':'volume'})\n",
    "\n",
    "def compute_technical_indicators(df):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    pt = df['close']\n",
    "    N = 14\n",
    "    lowN = df['low'].rolling(N).min()\n",
    "    highN = df['high'].rolling(N).max()\n",
    "    out['stoch_k'] = (pt - lowN) / (highN - lowN + 1e-9) * 100\n",
    "    out['stoch_d'] = out['stoch_k'].rolling(3).mean()\n",
    "    out['williams_r'] = (highN - pt) / (highN - lowN + 1e-9) * 100\n",
    "    out['ad_osc'] = (pt - pt.shift(1)) / (df['high'] - df['low'] + 1e-9)\n",
    "    out['momentum'] = pt - pt.shift(10)\n",
    "    out['disparity7'] = pt / pt.rolling(7).mean() * 100\n",
    "    out['roc'] = pt / pt.shift(12) * 100\n",
    "    return out.fillna(0)\n",
    "\n",
    "tech = compute_technical_indicators(eth_price)\n",
    "target_feats = pd.concat([eth_price, tech, onchain], axis=1).fillna(0)\n",
    "print(\"6/11 ETH 타깃 및 기술지표 준비 완료\")\n",
    "\n",
    "print(\"7/11 top-n macro 입력 생성 시작\")\n",
    "cols = [c for c in macro_raw.columns if '_' in c]\n",
    "coins = []\n",
    "for c in cols:\n",
    "    coin = c.split('_')[0]\n",
    "    if coin not in coins:\n",
    "        coins.append(coin)\n",
    "coins = [c for c in coins if c.upper() != 'ETH']\n",
    "if len(coins) < TOP_N:\n",
    "    TOP_N = len(coins)\n",
    "selected_coins = coins[:TOP_N]\n",
    "macro_list = []\n",
    "for coin in selected_coins:\n",
    "    needed = [f\"{coin}_Open\", f\"{coin}_Close\", f\"{coin}_High\", f\"{coin}_Low\", f\"{coin}_Volume\"]\n",
    "    for n in needed:\n",
    "        if n not in macro_raw.columns:\n",
    "            raise ValueError(f\"{n} 컬럼이 macro 파일에 필요합니다.\")\n",
    "    arr = macro_raw[needed].values\n",
    "    macro_list.append(arr)\n",
    "macro_array = np.concatenate(macro_list, axis=1)\n",
    "feat_suffix = ['Open','Close','High','Low','Volume']\n",
    "feature_names = []\n",
    "for coin in selected_coins:\n",
    "    for sfx in feat_suffix:\n",
    "        feature_names.append(f\"{coin}_{sfx}\")\n",
    "if macro_array.shape[1] != len(feature_names):\n",
    "    feature_names = [f\"m{i}\" for i in range(macro_array.shape[1])]\n",
    "macro_df = pd.DataFrame(macro_array, index=date_index_full, columns=feature_names)\n",
    "print(\"7/11 top-n macro 입력 생성 완료\")\n",
    "\n",
    "print(\"8/11 sentiment feature 준비 및 병합\")\n",
    "sent_cols = ['sent_mean','sent_count','pos_cnt','neu_cnt','neg_cnt','sent_entropy','sent_mean_ewma']\n",
    "sent_df = daily[sent_cols].fillna(0)\n",
    "print(\"8/11 sentiment 준비 완료\")\n",
    "\n",
    "print(\"9/11 리턴(target) 생성 및 정규화 준비\")\n",
    "p_all = eth_price.loc[date_index_full][['close']].copy()\n",
    "r_all = (p_all['close'].shift(-1) - p_all['close']) / (p_all['close'] + 1e-9)\n",
    "# drop last date because no next-day label\n",
    "date_index = date_index_full[:-1]\n",
    "xg_all = target_feats.loc[date_index]\n",
    "xm_all = macro_df.loc[date_index]\n",
    "s_all = sent_df.loc[date_index]\n",
    "p_all = p_all.loc[date_index]\n",
    "r_all = r_all.loc[date_index]\n",
    "\n",
    "n_total = len(date_index)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "n_test = n_total - n_train - n_val\n",
    "dates = list(date_index)\n",
    "train_dates = dates[L-1 : n_train]\n",
    "val_dates = dates[n_train : n_train + n_val]\n",
    "test_dates = dates[n_train + n_val : ]\n",
    "\n",
    "scaler_xg = StandardScaler().fit(xg_all.iloc[:n_train].values)\n",
    "scaler_xm = StandardScaler().fit(xm_all.iloc[:n_train].values)\n",
    "scaler_s = StandardScaler().fit(s_all.iloc[:n_train].values)\n",
    "scaler_r = StandardScaler().fit(r_all.iloc[:n_train].values.reshape(-1,1))\n",
    "print(\"9/11 정규화 및 분할 준비 완료\")\n",
    "\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, dates_list, xg_df, xm_df, s_df, p_df, r_series, L, scalers):\n",
    "        self.dates = dates_list\n",
    "        self.xg = xg_df\n",
    "        self.xm = xm_df\n",
    "        self.s = s_df\n",
    "        self.p = p_df\n",
    "        self.r = r_series\n",
    "        self.L = L\n",
    "        self.scaler_xg, self.scaler_xm, self.scaler_s, self.scaler_r = scalers\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.dates[idx]\n",
    "        start = t - timedelta(days=self.L-1)\n",
    "        idxs = pd.date_range(start=start, end=t, freq='D')\n",
    "        xg_win = self.xg.loc[idxs].values.astype(np.float32)\n",
    "        xm_win = self.xm.loc[idxs].values.astype(np.float32)\n",
    "        s_win = self.s.loc[idxs].values.astype(np.float32)\n",
    "        p_last = float(self.p.loc[t]['close'])\n",
    "        y_raw = float(self.r.loc[t])  # scaled return (or scaled target)\n",
    "\n",
    "        Bxg = self.scaler_xg.transform(xg_win)\n",
    "        Bxm = self.scaler_xm.transform(xm_win)\n",
    "        Bs  = self.scaler_s.transform(s_win)\n",
    "        y_s = self.scaler_r.transform([[y_raw]])[0,0]\n",
    "\n",
    "        # numpy -> torch (batch-window 차원: (L, feat))\n",
    "        Bxg_t = torch.from_numpy(Bxg).float()        # shape (L, in_target_c)\n",
    "        Bxm_t = torch.from_numpy(Bxm).float()        # shape (L, in_macro_c)\n",
    "        Bs_t  = torch.from_numpy(Bs).float()         # shape (L, in_sent_c)\n",
    "        p_last_f = torch.tensor(p_last, dtype=torch.float32)  # scalar tensor\n",
    "        y_s_f = torch.tensor(y_s, dtype=torch.float32)       # scalar tensor\n",
    "\n",
    "        return Bxg_t, Bxm_t, Bs_t, p_last_f, y_s_f\n",
    "\n",
    "train_ds = CryptoDataset(train_dates, xg_all, xm_all, s_all, p_all, r_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_r))\n",
    "val_ds = CryptoDataset(val_dates, xg_all, xm_all, s_all, p_all, r_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_r))\n",
    "test_ds = CryptoDataset(test_dates, xg_all, xm_all, s_all, p_all, r_all, L, (scaler_xg, scaler_xm, scaler_s, scaler_r))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"10/11 데이터셋 및 DataLoader 생성 완료\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        Lx = x.size(1)\n",
    "        return x + self.pe[:Lx, :].unsqueeze(0)\n",
    "\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, in_c, d_model, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv1d(in_channels=in_c, out_channels=d_model, kernel_size=kernel_size, padding=padding)\n",
    "        self.pos = PositionalEncoding(d_model, max_len=500)\n",
    "    def forward(self, x):\n",
    "        x_t = x.transpose(1,2)\n",
    "        y = self.conv(x_t).transpose(1,2)\n",
    "        y = self.pos(y)\n",
    "        return y\n",
    "\n",
    "class Zeta(nn.Module):\n",
    "    def __init__(self, d_model, hidden=128, L=L):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, hidden), nn.ReLU(), nn.Linear(hidden, d_model))\n",
    "        self.linear_time = nn.Linear(L, 1)\n",
    "        self.linear_feat = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        y = self.ff(x)\n",
    "        y_t = self.linear_time(y.transpose(1,2)).squeeze(2)\n",
    "        out = self.linear_feat(y_t)\n",
    "        return out\n",
    "\n",
    "def roll_tensor(x, shift):\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    return torch.cat([x[:, -shift:, :], x[:, :-shift, :]], dim=1)\n",
    "\n",
    "def compute_macro_h(xembg, xembm):\n",
    "    B, Lx, d = xembg.size()\n",
    "    attn_scores = []\n",
    "    rolled = []\n",
    "    for tau in range(Lx):\n",
    "        r = roll_tensor(xembm, tau)\n",
    "        num = (xembg * r).sum(dim=2)\n",
    "        den = (xembg.norm(dim=2) * r.norm(dim=2) + 1e-9)\n",
    "        sim = (num / den).mean(dim=1)\n",
    "        attn_scores.append(sim.unsqueeze(1))\n",
    "        rolled.append(r.unsqueeze(1))\n",
    "    attn = torch.cat(attn_scores, dim=1)\n",
    "    a = F.softmax(attn, dim=1)\n",
    "    rolled_stack = torch.cat(rolled, dim=1)\n",
    "    a_exp = a.unsqueeze(-1).unsqueeze(-1)\n",
    "    hm = (a_exp * rolled_stack).sum(dim=1)\n",
    "    return hm\n",
    "\n",
    "class PriceDynamics(nn.Module):\n",
    "    def __init__(self, in_c, L=L):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_c)\n",
    "        self.lin = nn.Linear(L, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x_t = x.transpose(1,2)\n",
    "        out = self.lin(x_t)\n",
    "        out = out.squeeze(-1)\n",
    "        delta = out.mean(dim=1, keepdim=False)\n",
    "        return delta\n",
    "\n",
    "class CryptoPulseModel(nn.Module):\n",
    "    def __init__(self, in_target_c, in_macro_c, in_sent_c, d_model=64, L=L):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.embed_g = TimeEmbed(in_target_c, d_model)\n",
    "        self.embed_m = TimeEmbed(in_macro_c, d_model)\n",
    "        self.embed_s = TimeEmbed(in_sent_c, d_model)\n",
    "        self.zeta = Zeta(d_model, hidden=128, L=L)\n",
    "        self.price_dyn = PriceDynamics(in_target_c, L=L)\n",
    "        self.macro_pred_head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.ReLU(), nn.Linear(d_model//2, 1))\n",
    "        self.dyn_from_emb = nn.Linear(d_model, 1)\n",
    "        self.gamma_head = nn.Sequential(nn.Linear(2*d_model, 64), nn.ReLU(), nn.Linear(64,1))\n",
    "    def forward(self, xg, xm, s, p_last_raw):\n",
    "        xg = xg.to(DEVICE); xm = xm.to(DEVICE); s = s.to(DEVICE)\n",
    "        xembg = self.embed_g(xg)\n",
    "        xembm = self.embed_m(xm)\n",
    "        semb = self.embed_s(s)\n",
    "        hm = compute_macro_h(xembg, xembm)\n",
    "        z = self.zeta(hm)\n",
    "        delta_macro = self.macro_pred_head(z).squeeze(-1)  # scaled return space\n",
    "        delta_dyn_scale = self.price_dyn(xg)\n",
    "        delta_dyn_emb = self.dyn_from_emb(xembg.mean(dim=1)).squeeze(-1)\n",
    "        delta_dyn = 0.5 * delta_dyn_scale + 0.5 * delta_dyn_emb\n",
    "        kappa_vec = self.zeta(semb)\n",
    "        kappa = torch.tanh(kappa_vec.mean(dim=1))\n",
    "        if kappa.dim() > 1:\n",
    "            kappa = kappa.mean(dim=1)\n",
    "        cat = torch.cat([xembg.mean(dim=1), semb.mean(dim=1)], dim=1)\n",
    "        gamma = torch.sigmoid(self.gamma_head(cat)).squeeze(-1)\n",
    "        # predict scaled return\n",
    "        r_hat_s = gamma * (kappa * delta_macro) + (1.0 - gamma) * (kappa * delta_dyn)\n",
    "        return r_hat_s, delta_macro, delta_dyn, gamma, kappa\n",
    "\n",
    "in_target_c = xg_all.shape[1]\n",
    "in_macro_c = xm_all.shape[1]\n",
    "in_sent_c = s_all.shape[1]\n",
    "model = CryptoPulseModel(in_target_c, in_macro_c, in_sent_c, d_model=64, L=L).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(\"11/11 모델 초기화 완료\")\n",
    "\n",
    "def inverse_scale_r(x_scaled):\n",
    "    arr = np.array(x_scaled).reshape(-1,1)\n",
    "    return scaler_r.inverse_transform(arr).reshape(-1)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds_prices = []\n",
    "    trues_prices = []\n",
    "    direction_hits = []\n",
    "    return_maes = []\n",
    "    with torch.no_grad():\n",
    "        for xg, xm, s, p_last_raw, y_s in loader:\n",
    "            xg = xg.to(DEVICE)\n",
    "            xm = xm.to(DEVICE)\n",
    "            s  = s.to(DEVICE)\n",
    "            p_last_arr = p_last_raw.cpu().numpy().astype(float)  # 필요한 경우 NumPy로 변환\n",
    "            y_s_t = y_s.to(DEVICE).float()\n",
    "            r_hat_s, *_ = model(xg, xm, s, p_last_raw.to(DEVICE).float())\n",
    "            r_hat_s = r_hat_s.detach().cpu().numpy().ravel()\n",
    "            r_true_s = y_s_t.detach().cpu().numpy().ravel()\n",
    "            r_hat = inverse_scale_r(r_hat_s)\n",
    "            r_true = inverse_scale_r(r_true_s)\n",
    "            p_hat = p_last_arr * (1.0 + r_hat)\n",
    "            p_true = p_last_arr * (1.0 + r_true)\n",
    "            preds_prices.append(p_hat)\n",
    "            trues_prices.append(p_true)\n",
    "            direction_hits.append((np.sign(r_hat) == np.sign(r_true)).astype(float))\n",
    "            return_maes.append(np.abs(r_hat - r_true))\n",
    "    preds_prices = np.concatenate(preds_prices)\n",
    "    trues_prices = np.concatenate(trues_prices)\n",
    "    direction_hits = np.concatenate(direction_hits)\n",
    "    return_maes = np.concatenate(return_maes)\n",
    "    mae = np.mean(np.abs(preds_prices - trues_prices))\n",
    "    mse = np.mean((preds_prices - trues_prices)**2)\n",
    "    corr = np.corrcoef(preds_prices, trues_prices)[0,1] if len(preds_prices)>1 and np.std(preds_prices)>0 and np.std(trues_prices)>0 else 0.0\n",
    "    dir_acc = direction_hits.mean() if len(direction_hits)>0 else 0.0\n",
    "    ret_mae = return_maes.mean() if len(return_maes)>0 else 0.0\n",
    "    return mae, mse, corr, dir_acc, ret_mae\n",
    "\n",
    "print(\"학습 시작\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for xg,xm,s,p_last_raw,y_s in train_loader:\n",
    "        xg = xg.to(DEVICE)\n",
    "        xm = xm.to(DEVICE)\n",
    "        s = s.to(DEVICE)\n",
    "        p_last_arr = np.array(p_last_raw).astype(float)\n",
    "        y_t = y_s.to(DEVICE).float()\n",
    "        optimizer.zero_grad()\n",
    "        r_hat_s, *_ = model(xg, xm, s, torch.tensor(p_last_arr).to(DEVICE))\n",
    "        loss = F.mse_loss(r_hat_s, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xg.size(0)\n",
    "        count += xg.size(0)\n",
    "    avg_loss = total_loss / count if count>0 else 0.0\n",
    "    if epoch % 4 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.5\n",
    "    val_mae, val_mse, val_corr, val_dir_acc, val_ret_mae = evaluate_model(model, val_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - train_mse(scaled):{avg_loss:.6f} val_price_mae:{val_mae:.6f} val_price_mse:{val_mse:.6f} val_corr:{val_corr:.4f} val_dir_acc:{val_dir_acc:.4f} val_ret_mae:{val_ret_mae:.6f}\")\n",
    "\n",
    "test_mae, test_mse, test_corr, test_dir_acc, test_ret_mae = evaluate_model(model, test_loader)\n",
    "print(f\"테스트 결과 - Price MAE:{test_mae:.6f} MSE:{test_mse:.6f} CORR:{test_corr:.4f} DirAcc:{test_dir_acc:.4f} ReturnMAE:{test_ret_mae:.6f}\")\n",
    "print(\"파이프라인 전체 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bd94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
