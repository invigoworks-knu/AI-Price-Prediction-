{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2ca4271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading all datasets...\n",
      "Macro: (3199, 51), 2017-01-01 00:00:00 to 2025-10-05 00:00:00\n",
      "\n",
      "[2] Unifying date range...\n",
      "Unified range: 2020-01-01 00:00:00 to 2025-10-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas_ta as ta\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. 데이터 로드\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Loading all datasets...\")\n",
    "# 메인 데이터\n",
    "macro_df = load_and_standardize_data('macro_crypto_data.csv')\n",
    "news_df = load_and_standardize_data('news_data.csv')\n",
    "eth_onchain_df = load_and_standardize_data('eth_onchain.csv')\n",
    "fear_greed_df = load_and_standardize_data('fear_greed.csv')\n",
    "\n",
    "# USDT 데이터\n",
    "usdt_eth_mcap_df = load_and_standardize_data('usdt_eth_mcap.csv')\n",
    "usdt_total_mcap_df = load_and_standardize_data('usdt_total_mcap.csv')\n",
    "\n",
    "# DeFi TVL 데이터\n",
    "aave_tvl_df = load_and_standardize_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_and_standardize_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_and_standardize_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_and_standardize_data('eth_chain_tvl.csv')\n",
    "\n",
    "# 펀딩 레이트\n",
    "eth_funding_df = load_and_standardize_data('eth_funding_rate.csv')\n",
    "\n",
    "# 전통 금융 지표\n",
    "sp500_df = load_and_standardize_data('SP500.csv')\n",
    "vix_df = load_and_standardize_data('VIX.csv')\n",
    "gold_df = load_and_standardize_data('GOLD.csv')\n",
    "dxy_df = load_and_standardize_data('DXY.csv')\n",
    "\n",
    "print(f\"  Macro: {macro_df.shape}\")\n",
    "print(f\"  News: {news_df.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. 날짜 범위 통일\n",
    "# ============================================================================\n",
    "print(\"\\n[2] Unifying date range...\")\n",
    "common_start = macro_df['date'].min()\n",
    "common_end = macro_df['date'].max()\n",
    "\n",
    "for df in [news_df, eth_onchain_df, fear_greed_df]:\n",
    "    common_start = max(common_start, df['date'].min())\n",
    "    common_end = min(common_end, df['date'].max())\n",
    "\n",
    "macro_df = macro_df[(macro_df['date'] >= common_start) & (macro_df['date'] <= common_end)].reset_index(drop=True)\n",
    "print(f\"  Range: {common_start} to {common_end} ({len(macro_df)} days)\")\n",
    "\n",
    "\n",
    "def standardize_date_column(df):\n",
    "    date_cols = [col for col in df.columns if col.lower() == 'date']\n",
    "    if not date_cols:\n",
    "        return df\n",
    "    \n",
    "    date_col = date_cols[0]\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()\n",
    "    \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(file_name, base_dir='./macro_data'):\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df = standardize_date_column(df)\n",
    "    return df\n",
    "\n",
    "def create_sentiment_features_with_lags(news_df):\n",
    "    \"\"\"뉴스 감성 피처 생성\"\"\"\n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x))\n",
    "    ).reset_index()\n",
    "    \n",
    "    sentiment_agg['sentiment_std'] = sentiment_agg['sentiment_std'].fillna(0)\n",
    "    \n",
    "    for lag in [1, 3, 5, 7]:\n",
    "        sentiment_agg[f'sentiment_mean_lag{lag}'] = sentiment_agg['sentiment_mean'].shift(lag)\n",
    "        sentiment_agg[f'positive_ratio_lag{lag}'] = sentiment_agg['positive_ratio'].shift(lag)\n",
    "        sentiment_agg[f'news_count_lag{lag}'] = sentiment_agg['news_count'].shift(lag)\n",
    "    \n",
    "    return sentiment_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b10fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef87a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2277f843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETHEREUM PRICE PREDICTION - CORRECT IMPLEMENTATION\n",
      "======================================================================\n",
      "\n",
      "[1] Loading all datasets...\n",
      "Macro: (3199, 51), 2017-01-01 00:00:00 to 2025-10-05 00:00:00\n",
      "\n",
      "[2] Unifying date range...\n",
      "Unified range: 2020-01-01 00:00:00 to 2025-10-04 00:00:00\n",
      "\n",
      "[3] Calculating technical indicators on FULL dataset...\n",
      "ETH with technical indicators: (2103, 155)\n",
      "\n",
      "[4] Merging external features on FULL dataset...\n",
      "Full dataset with all features: (2110, 263)\n",
      "\n",
      "[5] Creating target variables...\n",
      "\n",
      "[6] Splitting into Train/Val/Test (70-15-15)...\n",
      "Train: (1477, 267), 2020-01-01 00:00:00 to 2024-01-16 00:00:00\n",
      "Val  : (316, 267), 2024-01-17 00:00:00 to 2024-11-27 00:00:00\n",
      "Test : (317, 267), 2024-11-28 00:00:00 to 2025-10-03 00:00:00\n",
      "\n",
      "[7] Removing structural NaN from train set...\n",
      "After structural NaN removal:\n",
      "  Train: (1416, 267), 2020-03-01 00:00:00 to 2024-01-15 00:00:00\n",
      "  Val  : (315, 267), 2024-01-17 00:00:00 to 2024-11-26 00:00:00\n",
      "  Test : (316, 267), 2024-11-28 00:00:00 to 2025-10-03 00:00:00\n",
      "\n",
      "[8] Handling remaining missing values...\n",
      "Total features: 257\n",
      "\n",
      "Missing value statistics:\n",
      "  Train total NaN: 55301\n",
      "  Val total NaN: 11060\n",
      "  Test total NaN: 11452\n",
      "\n",
      "  Columns with missing values (top 10):\n",
      "    usdt_total_totalBridgedToUSD_lag1: 1416 (100.0%)\n",
      "    usdt_total_totalMintedUSD_lag1: 1416 (100.0%)\n",
      "    usdt_total_totalBridgedToUSD: 1416 (100.0%)\n",
      "    usdt_total_totalMintedUSD: 1416 (100.0%)\n",
      "    trends_ethereum_lag14: 1213 (85.7%)\n",
      "    trends_ethereum_lag7: 1213 (85.7%)\n",
      "    trends_ethereum: 1213 (85.7%)\n",
      "    onchain_avg_gas_price_lag3: 948 (66.9%)\n",
      "    onchain_token_transfers_lag1: 948 (66.9%)\n",
      "    onchain_token_transfers_lag2: 948 (66.9%)\n",
      "\n",
      "  Applying time-series appropriate imputation...\n",
      "\n",
      "  Handling Inf values...\n",
      "\n",
      "  After imputation:\n",
      "    Train NaN: 5664\n",
      "    Val NaN: 1260\n",
      "    Test NaN: 1264\n",
      "    Train Inf: 0\n",
      "    Val Inf: 0\n",
      "    Test Inf: 0\n",
      "\n",
      "[9] Target statistics:\n",
      "  Train - Direction: 0.523 (up: 740, down: 676)\n",
      "  Train - Log return: mean=0.001744, std=0.046426\n",
      "  Val   - Direction: 0.524\n",
      "  Test  - Direction: 0.506\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "Final shapes - Train: (1416, 267), Val: (315, 267), Test: (316, 267)\n",
      "Feature count: 257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calculate_technical_indicators(df, price_col='ETH_Close', volume_col='ETH_Volume', \n",
    "                                   high_col=None, low_col=None):\n",
    "    \"\"\"기술적 지표 계산 (과거 데이터만 사용, shift(1) 적용)\"\"\"\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    df['returns'] = df[price_col].pct_change()\n",
    "    df['log_returns'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    \n",
    "    # RSI\n",
    "    for length in [14, 30]:\n",
    "        rsi_series = ta.rsi(df[price_col], length=length)\n",
    "        df[f'rsi_{length}'] = rsi_series.shift(1)\n",
    "    \n",
    "    # MACD\n",
    "    macd_df = ta.macd(df[price_col], fast=12, slow=26, signal=9)\n",
    "    if macd_df is not None and not macd_df.empty:\n",
    "        for col in macd_df.columns:\n",
    "            df[col] = macd_df[col].shift(1)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb_df = ta.bbands(df[price_col], length=20, std=2)\n",
    "    if bb_df is not None and not bb_df.empty:\n",
    "        bb_cols = list(bb_df.columns)\n",
    "        for col in bb_cols:\n",
    "            df[col] = bb_df[col].shift(1)\n",
    "        \n",
    "        bb_middle = bb_df.iloc[:, 1] if len(bb_df.columns) >= 2 else bb_df.iloc[:, 0]\n",
    "        bb_upper = bb_df.iloc[:, 0]\n",
    "        bb_lower = bb_df.iloc[:, 2] if len(bb_df.columns) >= 3 else bb_df.iloc[:, 1]\n",
    "        \n",
    "        df['bb_width_20'] = ((bb_upper - bb_lower) / bb_middle).shift(1)\n",
    "        df['bb_position_20'] = ((df[price_col].shift(1) - bb_lower.shift(1)) / \n",
    "                                (bb_upper.shift(1) - bb_lower.shift(1)))\n",
    "    \n",
    "    # ADX\n",
    "    if high_col and low_col:\n",
    "        adx_df = ta.adx(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if adx_df is not None and not adx_df.empty:\n",
    "            for col in adx_df.columns:\n",
    "                df[col] = adx_df[col].shift(1)\n",
    "    \n",
    "    # ATR\n",
    "    if high_col and low_col:\n",
    "        atr_series = ta.atr(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if atr_series is not None:\n",
    "            df['atr_14'] = atr_series.shift(1)\n",
    "    \n",
    "    # MFI\n",
    "    if high_col and low_col and volume_col:\n",
    "        mfi_series = ta.mfi(df[high_col], df[low_col], df[price_col], df[volume_col], length=14)\n",
    "        if mfi_series is not None:\n",
    "            df['mfi_14'] = mfi_series.shift(1)\n",
    "    \n",
    "    # CCI\n",
    "    if high_col and low_col:\n",
    "        cci_series = ta.cci(df[high_col], df[low_col], df[price_col], length=20)\n",
    "        if cci_series is not None:\n",
    "            df['cci_20'] = cci_series.shift(1)\n",
    "    \n",
    "    # OBV\n",
    "    obv_series = ta.obv(df[price_col], df[volume_col])\n",
    "    if obv_series is not None:\n",
    "        df['obv'] = obv_series.shift(1)\n",
    "    \n",
    "    # VWAP\n",
    "    if high_col and low_col:\n",
    "        vwap_series = ta.vwap(df[high_col], df[low_col], df[price_col], df[volume_col])\n",
    "        if vwap_series is not None:\n",
    "            df['vwap'] = vwap_series.shift(1)\n",
    "    \n",
    "    # SMA/EMA\n",
    "    for window in [7, 14, 30, 60]:\n",
    "        sma = ta.sma(df[price_col], length=window)\n",
    "        ema = ta.ema(df[price_col], length=window)\n",
    "        if sma is not None:\n",
    "            df[f'sma_{window}'] = sma.shift(1)\n",
    "        if ema is not None:\n",
    "            df[f'ema_{window}'] = ema.shift(1)\n",
    "        \n",
    "        df[f'volatility_{window}'] = df['returns'].shift(1).rolling(window=window).std()\n",
    "        df[f'volume_sma_{window}'] = df[volume_col].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_sma_{window}'] = df['returns'].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_ema_{window}'] = df['returns'].shift(1).ewm(span=window, adjust=False).mean()\n",
    "        df[f'cumulative_returns_{window}'] = (1 + df['returns'].shift(1)).rolling(window=window).apply(lambda x: x.prod(), raw=True) - 1\n",
    "    \n",
    "    # Momentum & ROC\n",
    "    for window in [10, 20]:\n",
    "        df[f'momentum_{window}'] = df[price_col].shift(1) - df[price_col].shift(window + 1)\n",
    "        df[f'roc_{window}'] = ((df[price_col].shift(1) - df[price_col].shift(window + 1)) / \n",
    "                               df[price_col].shift(window + 1)) * 100\n",
    "    \n",
    "    # Stochastic & Williams %R\n",
    "    if high_col and low_col:\n",
    "        high_roll = df[high_col].shift(1).rolling(window=14).max()\n",
    "        low_roll = df[low_col].shift(1).rolling(window=14).min()\n",
    "        df['stochastic_14'] = 100 * (df[price_col].shift(1) - low_roll) / (high_roll - low_roll)\n",
    "        df['williams_r_14'] = -100 * (high_roll - df[price_col].shift(1)) / (high_roll - low_roll)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 3, 7, 14]:\n",
    "        df[f'price_lag_{lag}'] = df[price_col].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df[volume_col].shift(lag)\n",
    "        df[f'returns_lag_{lag}'] = df['returns'].shift(lag)\n",
    "    \n",
    "    # Regime indicators\n",
    "    df['volatility_30'] = df['returns'].shift(1).rolling(30).std()\n",
    "    df['volatility_regime'] = (df['volatility_30'] > df['volatility_30'].shift(1).rolling(60).mean()).astype(int)\n",
    "    df['price_trend'] = (df['sma_14'] > df['sma_60']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ETHEREUM PRICE PREDICTION - CORRECT IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "\n",
    "# 3. 전체 데이터에 대해 기술적 지표 계산\n",
    "print(\"\\n[3] Calculating technical indicators on FULL dataset...\")\n",
    "\n",
    "# ETH 지표\n",
    "eth_cols = [col for col in macro_df.columns if col.startswith('ETH_')]\n",
    "eth_df = macro_df[['date'] + eth_cols].copy()\n",
    "has_high_low = 'ETH_High' in eth_df.columns and 'ETH_Low' in eth_df.columns\n",
    "eth_df = calculate_technical_indicators(\n",
    "    eth_df, 'ETH_Close', 'ETH_Volume',\n",
    "    'ETH_High' if has_high_low else None,\n",
    "    'ETH_Low' if has_high_low else None\n",
    ")\n",
    "\n",
    "# BTC 지표\n",
    "btc_cols = [col for col in macro_df.columns if col.startswith('BTC_')]\n",
    "if btc_cols:\n",
    "    btc_df = macro_df[['date'] + btc_cols].copy()\n",
    "    has_btc_hl = 'BTC_High' in btc_df.columns and 'BTC_Low' in btc_df.columns\n",
    "    btc_df = calculate_technical_indicators(\n",
    "        btc_df, 'BTC_Close', 'BTC_Volume',\n",
    "        'BTC_High' if has_btc_hl else None,\n",
    "        'BTC_Low' if has_btc_hl else None\n",
    "    )\n",
    "    btc_df = btc_df.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "    \n",
    "    eth_shifted = eth_df['ETH_Close'].shift(1)\n",
    "    btc_shifted = btc_df['BTC_BTC_Close'].shift(1)\n",
    "    btc_df['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    btc_df['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    btc_df['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    btc_df['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "    \n",
    "    eth_df = eth_df.merge(btc_df, on='date', how='left')\n",
    "\n",
    "# Altcoin 상관관계\n",
    "altcoins = ['BNB', 'ADA']\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in macro_df.columns:\n",
    "        coin_shifted = macro_df[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = eth_df['ETH_Close'].shift(1)\n",
    "        eth_df[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        eth_df[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "print(f\"ETH with technical indicators: {eth_df.shape}\")\n",
    "\n",
    "# 4. External features 병합 (전체 데이터)\n",
    "print(\"\\n[4] Merging external features on FULL dataset...\")\n",
    "\n",
    "# Onchain\n",
    "onchain_cols = [c for c in eth_onchain_df.columns if c != 'date']\n",
    "for col in onchain_cols:\n",
    "    new_col = f'onchain_{col}'\n",
    "    eth_onchain_df.rename(columns={col: new_col}, inplace=True)\n",
    "    for lag in [1, 2, 3]:\n",
    "        eth_onchain_df[f'{new_col}_lag{lag}'] = eth_onchain_df[new_col].shift(lag)\n",
    "\n",
    "eth_df = eth_df.merge(eth_onchain_df, on='date', how='left')\n",
    "\n",
    "# External datasets\n",
    "external_dfs = [\n",
    "    (fear_greed_df, 'fg', [1]),\n",
    "    (usdt_eth_mcap_df, 'usdt_eth', [1]),\n",
    "    (usdt_total_mcap_df, 'usdt_total', [1]),\n",
    "    (aave_tvl_df, 'aave', [1, 3, 7]),\n",
    "    (lido_tvl_df, 'lido', [1, 3, 7]),\n",
    "    (makerdao_tvl_df, 'maker', [1, 3, 7]),\n",
    "    (eth_chain_tvl_df, 'chain_tvl', [1, 3, 7]),\n",
    "    (eth_funding_df, 'funding', [1]),\n",
    "    (sp500_df, 'sp500', [1]),\n",
    "    (vix_df, 'vix', [1]),\n",
    "    (gold_df, 'gold', [1]),\n",
    "    (dxy_df, 'dxy', [1])\n",
    "]\n",
    "\n",
    "for df, prefix, lag_list in external_dfs:\n",
    "    df = standardize_date_column(df)\n",
    "    orig_cols = [c for c in df.columns if c != 'date']\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        df.rename(columns={col: f'{prefix}_{col}'}, inplace=True)\n",
    "    \n",
    "    renamed_cols = [c for c in df.columns if c != 'date']\n",
    "    for col in renamed_cols:\n",
    "        for lag in lag_list:\n",
    "            df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    eth_df = eth_df.merge(df, on='date', how='left')\n",
    "\n",
    "# Sentiment\n",
    "sentiment_features = create_sentiment_features_with_lags(news_df)\n",
    "eth_df = eth_df.merge(sentiment_features, on='date', how='left')\n",
    "\n",
    "\n",
    "print(f\"Full dataset with all features: {eth_df.shape}\")\n",
    "\n",
    "# 5. Target 생성\n",
    "print(\"\\n[5] Creating target variables...\")\n",
    "eth_df['target_next_log_return'] = np.log(eth_df['ETH_Close'].shift(-1) / eth_df['ETH_Close'])\n",
    "eth_df['target_direction'] = (eth_df['target_next_log_return'] > 0).astype(int)\n",
    "eth_df['target_next_price'] = eth_df['ETH_Close'].shift(-1)\n",
    "eth_df['target_next_return_pct'] = ((eth_df['ETH_Close'].shift(-1) - eth_df['ETH_Close']) / eth_df['ETH_Close']) * 100\n",
    "\n",
    "# 6. Train/Val/Test 분할\n",
    "print(\"\\n[6] Splitting into Train/Val/Test (70-15-15)...\")\n",
    "train_size = int(len(eth_df) * 0.7)\n",
    "val_size = int(len(eth_df) * 0.15)\n",
    "\n",
    "train_df = eth_df.iloc[:train_size].copy()\n",
    "val_df = eth_df.iloc[train_size:train_size+val_size].copy()\n",
    "test_df = eth_df.iloc[train_size+val_size:].copy()\n",
    "\n",
    "print(f\"Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Val  : {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "print(f\"Test : {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# 7. 구조적 NaN 제거 (최대 lag window만큼)\n",
    "print(\"\\n[7] Removing structural NaN from train set...\")\n",
    "max_lag = 60\n",
    "train_df = train_df.iloc[max_lag:-1].reset_index(drop=True)  # 마지막 행은 target이 NaN\n",
    "val_df = val_df.iloc[:-1].reset_index(drop=True)\n",
    "test_df = test_df.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "print(f\"After structural NaN removal:\")\n",
    "print(f\"  Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"  Val  : {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "print(f\"  Test : {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# 8. 실제 결측치 처리 (데이터 부재로 인한 NaN)\n",
    "print(\"\\n[8] Handling remaining missing values...\")\n",
    "\n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "                'target_next_log_return', 'target_direction', 'target_next_price', \n",
    "                'target_next_return_pct']\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# 결측치 통계\n",
    "print(f\"\\nMissing value statistics:\")\n",
    "train_nan = train_df[feature_cols].isna().sum()\n",
    "val_nan = val_df[feature_cols].isna().sum()\n",
    "test_nan = test_df[feature_cols].isna().sum()\n",
    "\n",
    "print(f\"  Train total NaN: {train_nan.sum()}\")\n",
    "print(f\"  Val total NaN: {val_nan.sum()}\")\n",
    "print(f\"  Test total NaN: {test_nan.sum()}\")\n",
    "\n",
    "# 결측치가 있는 컬럼 분석\n",
    "cols_with_nan = train_nan[train_nan > 0].sort_values(ascending=False)\n",
    "if len(cols_with_nan) > 0:\n",
    "    print(f\"\\n  Columns with missing values (top 10):\")\n",
    "    for col, count in cols_with_nan.head(10).items():\n",
    "        pct = count / len(train_df) * 100\n",
    "        print(f\"    {col}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# 전략: 시계열 특성 보존 방법 (forward fill -> linear interpolation)\n",
    "print(f\"\\n  Applying time-series appropriate imputation...\")\n",
    "\n",
    "# Step 1: Forward fill (시계열에서 가장 안전)\n",
    "train_df[feature_cols] = train_df[feature_cols].fillna(method='ffill')\n",
    "val_df[feature_cols] = val_df[feature_cols].fillna(method='ffill')\n",
    "test_df[feature_cols] = test_df[feature_cols].fillna(method='ffill')\n",
    "\n",
    "# Step 2: Linear interpolation (forward fill로도 채워지지 않은 경우)\n",
    "train_df[feature_cols] = train_df[feature_cols].interpolate(method='linear', limit_direction='both')\n",
    "val_df[feature_cols] = val_df[feature_cols].interpolate(method='linear', limit_direction='both')\n",
    "test_df[feature_cols] = test_df[feature_cols].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Step 3: 여전히 남은 NaN은 column median으로 (초기 행들)\n",
    "for col in feature_cols:\n",
    "    if train_df[col].isna().any():\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col] = train_df[col].fillna(median_val)\n",
    "        val_df[col] = val_df[col].fillna(median_val)  # train의 median 사용\n",
    "        test_df[col] = test_df[col].fillna(median_val)\n",
    "\n",
    "# Inf 처리\n",
    "print(f\"\\n  Handling Inf values...\")\n",
    "train_df[feature_cols] = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "val_df[feature_cols] = val_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "test_df[feature_cols] = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Inf로 인한 새로운 NaN 처리\n",
    "for col in feature_cols:\n",
    "    if train_df[col].isna().any():\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col] = train_df[col].fillna(median_val)\n",
    "        val_df[col] = val_df[col].fillna(median_val)\n",
    "        test_df[col] = test_df[col].fillna(median_val)\n",
    "\n",
    "print(f\"\\n  After imputation:\")\n",
    "print(f\"    Train NaN: {train_df[feature_cols].isna().sum().sum()}\")\n",
    "print(f\"    Val NaN: {val_df[feature_cols].isna().sum().sum()}\")\n",
    "print(f\"    Test NaN: {test_df[feature_cols].isna().sum().sum()}\")\n",
    "print(f\"    Train Inf: {np.isinf(train_df[feature_cols].select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"    Val Inf: {np.isinf(val_df[feature_cols].select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"    Test Inf: {np.isinf(test_df[feature_cols].select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# 9. Target 검증\n",
    "print(\"\\n[9] Target statistics:\")\n",
    "print(f\"  Train - Direction: {train_df['target_direction'].mean():.3f} \"\n",
    "      f\"(up: {(train_df['target_direction']==1).sum()}, down: {(train_df['target_direction']==0).sum()})\")\n",
    "print(f\"  Train - Log return: mean={train_df['target_next_log_return'].mean():.6f}, \"\n",
    "      f\"std={train_df['target_next_log_return'].std():.6f}\")\n",
    "print(f\"  Val   - Direction: {val_df['target_direction'].mean():.3f}\")\n",
    "print(f\"  Test  - Direction: {test_df['target_direction'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final shapes - Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa648cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2497e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d6911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49da27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69096c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af0f270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volatility_60               1\n",
      "returns_sma_60              1\n",
      "cumulative_returns_60       1\n",
      "BTC_volatility_60           1\n",
      "BTC_returns_sma_60          1\n",
      "                         ... \n",
      "positive_ratio_lag7        39\n",
      "news_count_lag7            39\n",
      "trends_ethereum          1209\n",
      "trends_ethereum_lag7     1209\n",
      "trends_ethereum_lag14    1209\n",
      "Length: 90, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas_ta as ta\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def standardize_date_column(df):\n",
    "    date_cols = [col for col in df.columns if col.lower() == 'date']\n",
    "    if not date_cols:\n",
    "        return df\n",
    "\n",
    "    date_col = date_cols[0]\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()\n",
    "\n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(file_name, base_dir='./macro_data'):\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = standardize_date_column(df)\n",
    "    return df\n",
    "\n",
    "def create_sentiment_features_with_lags(news_df):\n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x))\n",
    "    ).reset_index()\n",
    "\n",
    "    sentiment_agg['sentiment_std'] = sentiment_agg['sentiment_std'].fillna(0)\n",
    "\n",
    "    for lag in [1, 3, 5, 7]:\n",
    "        sentiment_agg[f'sentiment_mean_lag{lag}'] = sentiment_agg['sentiment_mean'].shift(lag)\n",
    "        sentiment_agg[f'positive_ratio_lag{lag}'] = sentiment_agg['positive_ratio'].shift(lag)\n",
    "        sentiment_agg[f'news_count_lag{lag}'] = sentiment_agg['news_count'].shift(lag)\n",
    "\n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "def calculate_technical_indicators(df, price_col='ETH_Close', volume_col='ETH_Volume', high_col=None, low_col=None):\n",
    "    \"\"\"\n",
    "    pandas_ta 기반 기술적 지표 계산 (논문 검증됨)\n",
    "    참고: \"Cryptocurrency Price Forecasting Using XGBoost Regressor\" (2024)\n",
    "    \"\"\"\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    df['returns'] = df[price_col].pct_change()\n",
    "    df['log_returns'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    \n",
    "    \n",
    "    # RSI (Momentum) - 논문에서 중요도 높음\n",
    "    for length in [14, 30]:\n",
    "        rsi_series = ta.rsi(df[price_col], length=length)\n",
    "        df[f'rsi_{length}'] = rsi_series.shift(1)  \n",
    "    \n",
    "    # MACD (Momentum & Trend)\n",
    "    macd_df = ta.macd(df[price_col], fast=12, slow=26, signal=9)\n",
    "    if macd_df is not None and not macd_df.empty:\n",
    "        for col in macd_df.columns:\n",
    "            df[col] = macd_df[col].shift(1)  \n",
    "\n",
    "    bb_df = ta.bbands(df[price_col], length=20, std=2)\n",
    "    if bb_df is not None and not bb_df.empty:\n",
    "        bb_cols = list(bb_df.columns)\n",
    "        for col in bb_cols:\n",
    "            df[col] = bb_df[col].shift(1)  \n",
    "        \n",
    "        # BB 파생 지표 계산 (논문 권장)\n",
    "        bb_middle = bb_df.iloc[:, 1] if len(bb_df.columns) >= 2 else bb_df.iloc[:, 0]\n",
    "        bb_upper = bb_df.iloc[:, 0]\n",
    "        bb_lower = bb_df.iloc[:, 2] if len(bb_df.columns) >= 3 else bb_df.iloc[:, 1]\n",
    "        \n",
    "        df['bb_width_20'] = ((bb_upper - bb_lower) / bb_middle).shift(1)\n",
    "        df['bb_position_20'] = ((df[price_col].shift(1) - bb_lower.shift(1)) / \n",
    "                                (bb_upper.shift(1) - bb_lower.shift(1)))\n",
    "    \n",
    "    # ADX (Trend Strength)\n",
    "    if high_col and low_col:\n",
    "        adx_df = ta.adx(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if adx_df is not None and not adx_df.empty:\n",
    "            for col in adx_df.columns:\n",
    "                df[col] = adx_df[col].shift(1)  \n",
    "    \n",
    "    # ATR (Volatility) - 논문에서 중요도 높음\n",
    "    if high_col and low_col:\n",
    "        atr_series = ta.atr(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if atr_series is not None:\n",
    "            df['atr_14'] = atr_series.shift(1) \n",
    "    \n",
    "    # MFI (Volume)\n",
    "    if high_col and low_col and volume_col:\n",
    "        mfi_series = ta.mfi(df[high_col], df[low_col], df[price_col], df[volume_col], length=14)\n",
    "        if mfi_series is not None:\n",
    "            df['mfi_14'] = mfi_series.shift(1)  \n",
    "    \n",
    "    # CCI (Momentum)\n",
    "    if high_col and low_col:\n",
    "        cci_series = ta.cci(df[high_col], df[low_col], df[price_col], length=20)\n",
    "        if cci_series is not None:\n",
    "            df['cci_20'] = cci_series.shift(1)  \n",
    "    \n",
    "    # OBV (Volume)\n",
    "    obv_series = ta.obv(df[price_col], df[volume_col])\n",
    "    if obv_series is not None:\n",
    "        df['obv'] = obv_series.shift(1) \n",
    "    \n",
    "    # VWAP\n",
    "    if high_col and low_col:\n",
    "        vwap_series = ta.vwap(df[high_col], df[low_col], df[price_col], df[volume_col])\n",
    "        if vwap_series is not None:\n",
    "            df['vwap'] = vwap_series.shift(1)\n",
    "    \n",
    "    # SMA/EMA (Trend) - 다양한 기간\n",
    "    for window in [7, 14, 30, 60]:\n",
    "        sma = ta.sma(df[price_col], length=window)\n",
    "        ema = ta.ema(df[price_col], length=window)\n",
    "        if sma is not None:\n",
    "            df[f'sma_{window}'] = sma.shift(1)  \n",
    "        if ema is not None:\n",
    "            df[f'ema_{window}'] = ema.shift(1) \n",
    "        \n",
    "        # Volatility & Volume indicators\n",
    "        df[f'volatility_{window}'] = df['returns'].shift(1).rolling(window=window).std()\n",
    "        df[f'volume_sma_{window}'] = df[volume_col].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_sma_{window}'] = df['returns'].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_ema_{window}'] = df['returns'].shift(1).ewm(span=window, adjust=False).mean()\n",
    "        df[f'cumulative_returns_{window}'] = (1 + df['returns'].shift(1)).rolling(window=window).apply(lambda x: x.prod(), raw=True) - 1\n",
    "    \n",
    "    # Momentum & ROC\n",
    "    for window in [10, 20]:\n",
    "        df[f'momentum_{window}'] = df[price_col].shift(1) - df[price_col].shift(window + 1)\n",
    "        df[f'roc_{window}'] = ((df[price_col].shift(1) - df[price_col].shift(window + 1)) / \n",
    "                               df[price_col].shift(window + 1)) * 100\n",
    "    \n",
    "    # Stochastic & Williams %R\n",
    "    if high_col in df.columns and low_col in df.columns:\n",
    "        high_roll = df[high_col].shift(1).rolling(window=14).max()\n",
    "        low_roll = df[low_col].shift(1).rolling(window=14).min()\n",
    "        df['stochastic_14'] = 100 * (df[price_col].shift(1) - low_roll) / (high_roll - low_roll)\n",
    "        df['williams_r_14'] = -100 * (high_roll - df[price_col].shift(1)) / (high_roll - low_roll)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 3, 7, 14]:\n",
    "        df[f'price_lag_{lag}'] = df[price_col].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df[volume_col].shift(lag)\n",
    "        df[f'returns_lag_{lag}'] = df['returns'].shift(lag)\n",
    "    \n",
    "    # Regime indicators\n",
    "    df['volatility_30'] = df['returns'].shift(1).rolling(30).std()\n",
    "    df['volatility_regime'] = (df['volatility_30'] > df['volatility_30'].shift(1).rolling(60).mean()).astype(int)\n",
    "    df['price_trend'] = (df['sma_14'] > df['sma_60']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_external_features(base_df, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df):\n",
    "    merged_df = base_df.copy()\n",
    "\n",
    "    eth_onchain_df = standardize_date_column(eth_onchain_df)\n",
    "    onchain_col_orig = [c for c in eth_onchain_df.columns if c != 'date']\n",
    "    for col in onchain_col_orig:\n",
    "        new_col = f'onchain_{col}'\n",
    "        eth_onchain_df.rename(columns={col: new_col}, inplace=True)\n",
    "        for lag in [1,2,3]:\n",
    "            eth_onchain_df[f'{new_col}_lag{lag}'] = eth_onchain_df[new_col].shift(lag)\n",
    "\n",
    "    merged_df = merged_df.merge(eth_onchain_df, on='date', how='left')\n",
    "\n",
    "    for df, prefix, lag_list in data_with_lags:\n",
    "        df_renamed = df.copy()\n",
    "        df_renamed = standardize_date_column(df_renamed)\n",
    "\n",
    "        orig_cols = [c for c in df_renamed.columns if c != 'date']\n",
    "        for col in orig_cols:\n",
    "            df_renamed.rename(columns={col: f'{prefix}_{col}'}, inplace=True)\n",
    "\n",
    "        renamed_cols = [c for c in df_renamed.columns if c != 'date']\n",
    "        for col in renamed_cols:\n",
    "            for lag in lag_list:\n",
    "                df_renamed[f'{col}_lag{lag}'] = df_renamed[col].shift(lag)\n",
    "\n",
    "        merged_df = merged_df.merge(df_renamed, on='date', how='left')\n",
    "\n",
    "    merged_df = merged_df.merge(sentiment_features, on='date', how='left')\n",
    "\n",
    "    google_trends_df = standardize_date_column(google_trends_df)\n",
    "    trend_cols = [c for c in google_trends_df.columns if c != 'date']\n",
    "    for col in trend_cols:\n",
    "        google_trends_df[f'{col}_lag7'] = google_trends_df[col].shift(7)\n",
    "        google_trends_df[f'{col}_lag14'] = google_trends_df[col].shift(14)\n",
    "    merged_df = merged_df.merge(google_trends_df.add_prefix('trends_').rename(columns={'trends_date': 'date'}),\n",
    "                              on='date', how='left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ETHEREUM PRICE PREDICTION - NO LEAKAGE PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nStep 1: Loading raw datasets...\")\n",
    "macro_df = load_and_standardize_data('macro_crypto_data.csv')\n",
    "news_df = load_and_standardize_data('news_data.csv')\n",
    "eth_onchain_df = load_and_standardize_data('eth_onchain.csv')\n",
    "fear_greed_df = load_and_standardize_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_and_standardize_data('usdt_eth_mcap.csv')\n",
    "usdt_total_mcap_df = load_and_standardize_data('usdt_total_mcap.csv')\n",
    "aave_tvl_df = load_and_standardize_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_and_standardize_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_and_standardize_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_and_standardize_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_and_standardize_data('eth_funding_rate.csv')\n",
    "sp500_df = load_and_standardize_data('SP500.csv')\n",
    "vix_df = load_and_standardize_data('VIX.csv')\n",
    "gold_df = load_and_standardize_data('GOLD.csv')\n",
    "dxy_df = load_and_standardize_data('DXY.csv')\n",
    "google_trends_df = load_and_standardize_data('ethereum_google_trends_weekly_2017_2025_scaled.csv')\n",
    "\n",
    "print(f\"Macro: {macro_df.shape}, {macro_df['date'].min()} to {macro_df['date'].max()}\")\n",
    "\n",
    "print(\"\\nStep 2: Creating sentiment features...\")\n",
    "sentiment_features = create_sentiment_features_with_lags(news_df)\n",
    "\n",
    "print(\"\\nStep 3: Unifying date range...\")\n",
    "common_start = macro_df['date'].min()\n",
    "common_end = macro_df['date'].max()\n",
    "for df in [news_df, eth_onchain_df, fear_greed_df]:\n",
    "    common_start = max(common_start, df['date'].min())\n",
    "    common_end = min(common_end, df['date'].max())\n",
    "\n",
    "macro_df = macro_df[(macro_df['date'] >= common_start) & (macro_df['date'] <= common_end)].reset_index(drop=True)\n",
    "print(f\"Unified range: {common_start} to {common_end}\")\n",
    "\n",
    "print(\"\\nStep 4: Split raw data FIRST (70-15-15)...\")\n",
    "train_size = int(len(macro_df) * 0.7)\n",
    "val_size = int(len(macro_df) * 0.15)\n",
    "\n",
    "train_raw = macro_df.iloc[:train_size].copy()\n",
    "val_raw = macro_df.iloc[train_size:train_size+val_size].copy()\n",
    "test_raw = macro_df.iloc[train_size+val_size:].copy()\n",
    "\n",
    "print(f\"Train raw: {train_raw.shape}, {train_raw['date'].min()} to {train_raw['date'].max()}\")\n",
    "print(f\"Val raw: {val_raw.shape}, {val_raw['date'].min()} to {val_raw['date'].max()}\")\n",
    "print(f\"Test raw: {test_raw.shape}, {test_raw['date'].min()} to {test_raw['date'].max()}\")\n",
    "\n",
    "data_with_lags = [\n",
    "    (fear_greed_df, 'fg', [1]),\n",
    "    (usdt_eth_mcap_df, 'usdt_eth', [1]),\n",
    "    (usdt_total_mcap_df, 'usdt_total', [1]),\n",
    "    (aave_tvl_df, 'aave', [1, 3, 7]),\n",
    "    (lido_tvl_df, 'lido', [1, 3, 7]),\n",
    "    (makerdao_tvl_df, 'maker', [1, 3, 7]),\n",
    "    (eth_chain_tvl_df, 'chain_tvl', [1, 3, 7]),\n",
    "    (eth_funding_df, 'funding', [1]),\n",
    "    (sp500_df, 'sp500', [1]),\n",
    "    (vix_df, 'vix', [1]),\n",
    "    (gold_df, 'gold', [1]),\n",
    "    (dxy_df, 'dxy', [1])\n",
    "]\n",
    "\n",
    "print(\"\\nStep 5: Processing TRAIN set independently...\")\n",
    "eth_cols = [col for col in train_raw.columns if col.startswith('ETH_')]\n",
    "train_eth = train_raw[['date'] + eth_cols].copy()\n",
    "has_high_low = 'ETH_High' in train_eth.columns and 'ETH_Low' in train_eth.columns\n",
    "train_eth = calculate_technical_indicators(train_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                          'ETH_High' if has_high_low else None,\n",
    "                                          'ETH_Low' if has_high_low else None)\n",
    "\n",
    "btc_cols = [col for col in train_raw.columns if col.startswith('BTC_')]\n",
    "if btc_cols:\n",
    "    train_btc = train_raw[['date'] + btc_cols].copy()\n",
    "    has_btc_hl = 'BTC_High' in train_btc.columns and 'BTC_Low' in train_btc.columns\n",
    "    train_btc = calculate_technical_indicators(train_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                              'BTC_High' if has_btc_hl else None,\n",
    "                                              'BTC_Low' if has_btc_hl else None)\n",
    "    train_btc = train_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = train_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = train_btc['BTC_BTC_Close'].shift(1)\n",
    "    train_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    train_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    train_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    train_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    train_eth = train_eth.merge(train_btc[['date'] + [col for col in train_btc.columns if col != 'date']],\n",
    "                                on='date', how='left')\n",
    "\n",
    "altcoins = ['BNB', 'ADA']\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in train_raw.columns:\n",
    "        coin_shifted = train_raw[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = train_eth['ETH_Close'].shift(1)\n",
    "        train_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        train_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "train_df = merge_external_features(train_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "train_df['target_next_log_return'] = np.log(train_df['ETH_Close'] / train_df['ETH_Close'].shift(1)).shift(-1)\n",
    "train_df['target_direction'] = (train_df['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "print(f\"Train with features: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 6: Processing VAL set (using train+val for rolling windows)...\")\n",
    "combined_for_val = pd.concat([train_raw, val_raw]).reset_index(drop=True)\n",
    "val_eth = combined_for_val[['date'] + eth_cols].copy()\n",
    "val_eth = calculate_technical_indicators(val_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                        'ETH_High' if has_high_low else None,\n",
    "                                        'ETH_Low' if has_high_low else None)\n",
    "\n",
    "if btc_cols:\n",
    "    val_btc = combined_for_val[['date'] + btc_cols].copy()\n",
    "    val_btc = calculate_technical_indicators(val_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                            'BTC_High' if has_btc_hl else None,\n",
    "                                            'BTC_Low' if has_btc_hl else None)\n",
    "    val_btc = val_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = val_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = val_btc['BTC_BTC_Close'].shift(1)\n",
    "    val_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    val_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    val_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    val_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    val_eth = val_eth.merge(val_btc[['date'] + [col for col in val_btc.columns if col != 'date']],\n",
    "                           on='date', how='left')\n",
    "\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in combined_for_val.columns:\n",
    "        coin_shifted = combined_for_val[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = val_eth['ETH_Close'].shift(1)\n",
    "        val_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        val_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "val_df_full = merge_external_features(val_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "val_df_full['target_next_log_return'] = np.log(val_df_full['ETH_Close'] / val_df_full['ETH_Close'].shift(1)).shift(-1)\n",
    "val_df_full['target_direction'] = (val_df_full['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "val_df = val_df_full.iloc[len(train_raw):].reset_index(drop=True)\n",
    "print(f\"Val with features: {val_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 7: Processing TEST set (using train+val+test for rolling windows)...\")\n",
    "combined_for_test = pd.concat([train_raw, val_raw, test_raw]).reset_index(drop=True)\n",
    "test_eth = combined_for_test[['date'] + eth_cols].copy()\n",
    "test_eth = calculate_technical_indicators(test_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                         'ETH_High' if has_high_low else None,\n",
    "                                         'ETH_Low' if has_high_low else None)\n",
    "\n",
    "if btc_cols:\n",
    "    test_btc = combined_for_test[['date'] + btc_cols].copy()\n",
    "    test_btc = calculate_technical_indicators(test_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                             'BTC_High' if has_btc_hl else None,\n",
    "                                             'BTC_Low' if has_btc_hl else None)\n",
    "    test_btc = test_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = test_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = test_btc['BTC_BTC_Close'].shift(1)\n",
    "    test_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    test_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    test_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    test_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    test_eth = test_eth.merge(test_btc[['date'] + [col for col in test_btc.columns if col != 'date']],\n",
    "                             on='date', how='left')\n",
    "\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in combined_for_test.columns:\n",
    "        coin_shifted = combined_for_test[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = test_eth['ETH_Close'].shift(1)\n",
    "        test_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        test_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "test_df_full = merge_external_features(test_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "test_df_full['target_next_log_return'] = np.log(test_df_full['ETH_Close'] / test_df_full['ETH_Close'].shift(1)).shift(-1)\n",
    "test_df_full['target_direction'] = (test_df_full['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "test_df = test_df_full.iloc[len(train_raw)+len(val_raw):].reset_index(drop=True)\n",
    "print(f\"Test with features: {test_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 8: Removing NaN rows from each set...\")\n",
    "max_lag = 60\n",
    "train_df = train_df.iloc[max_lag:-1].reset_index(drop=True)\n",
    "val_df = val_df.iloc[:-1].reset_index(drop=True)\n",
    "test_df = test_df.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "print(f\"After NaN removal:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7ef828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] Unsupervised filtering (variance/correlation) ...\n",
      "\n",
      "[Phase 2] Nested cross-validation: supervised feature selection & tuning ...\n",
      "\n",
      "Nested CV finished.\n",
      "All folds selected features: [['BTC_DMP_14', 'BTC_bb_position_20', 'volume_sma_7', 'BTC_returns', 'positive_ratio', 'volume_lag_7', 'dxy_DXY', 'returns', 'BTC_returns_lag_7', 'bb_width_20', 'DMN_14', 'BTC_returns_lag_14', 'returns_lag_3', 'returns_ema_7', 'BTC_volume_lag_3', 'BTC_returns_lag_1', 'returns_sma_60', 'obv', 'returns_sma_7', 'stochastic_14', 'BTC_BTC_Open'], ['BTC_returns_ema_14', 'BTC_BTC_Volume', 'volume_lag_1', 'returns_lag_7', 'dxy_DXY', 'BTC_MACD_12_26_9', 'BTC_volume_sma_7', 'BTC_obv', 'BTC_bb_position_20', 'funding_fundingRate', 'volume_sma_7', 'returns', 'negative_ratio', 'bnb_eth_ratio', 'BTC_returns', 'BTC_rsi_14', 'BTC_bb_width_20', 'BTC_returns_ema_7', 'BTC_ADX_14', 'BTC_mfi_14', 'bb_position_20', 'sentiment_mean_lag1', 'vix_VIX', 'BTC_DMN_14', 'BTC_roc_20', 'BTC_atr_14', 'bnb_eth_correlation', 'ada_eth_ratio', 'ADX_14', 'BTC_rsi_30', 'BTC_volume_lag_1', 'lido_lido_eth_tvl', 'bb_width_20', 'news_count_lag5', 'returns_ema_30', 'BTC_momentum_20', 'returns_sma_7', 'BBL_20', 'BTC_volatility_14', 'fg_fear_greed', 'volatility_14', 'BTC_returns_lag_3', 'usdt_total_totalUnreleased', 'volume_lag_3', 'sp500_SP500', 'volume_lag_7'], ['positive_ratio_lag1', 'bb_width_20', 'BTC_returns_ema_7', 'bb_position_20', 'BTC_obv', 'usdt_total_totalUnreleased', 'BTC_bb_position_20', 'btc_eth_correlation', 'sp500_SP500', 'BTC_returns_lag_1', 'volume_sma_7', 'dxy_DXY', 'BTC_rsi_14', 'volume_sma_60', 'BTC_atr_14', 'returns_sma_60', 'BTC_MACD_12_26_9', 'volatility_30', 'returns', 'BTC_returns', 'aave_aave_eth_tvl', 'BTC_returns_sma_7', 'returns_ema_7', 'sentiment_mean', 'news_count', 'BTC_volatility_30', 'BTC_stochastic_14', 'BBL_20', 'returns_sma_7', 'returns_lag_3', 'BTC_returns_sma_60', 'volume_lag_3'], ['bnb_eth_ratio', 'positive_ratio_lag7', 'BTC_returns_lag_1', 'returns', 'volume_sma_60', 'BTC_obv', 'BTC_rsi_14', 'BTC_bb_position_20', 'BTC_cci_20', 'BTC_atr_14', 'DMP_14', 'positive_ratio', 'BTC_ADX_14', 'BTC_returns_sma_30', 'returns_ema_14', 'BTC_returns_sma_14', 'positive_ratio_lag1', 'bb_width_20', 'BTC_returns_ema_30', 'BTC_returns_sma_7', 'returns_ema_7', 'rsi_14', 'mfi_14', 'BTC_returns_ema_7', 'BTC_volume_sma_7', 'BTC_BTC_Volume', 'BTC_DMN_14', 'BTC_stochastic_14', 'momentum_20', 'returns_lag_7', 'bb_position_20', 'volume_lag_1', 'BTC_roc_10', 'BTC_volatility_7', 'sentiment_mean_lag7', 'BTC_MACDH_12_26_9', 'sentiment_mean', 'BBL_20', 'BTC_volume_lag_1', 'BTC_mfi_14', 'aave_aave_eth_tvl', 'btc_eth_correlation', 'sp500_SP500', 'sentiment_std', 'sentiment_mean_lag3', 'negative_ratio', 'sentiment_mean_lag1', 'volume_lag_7', 'BTC_MACD_12_26_9', 'ada_eth_ratio'], ['volume_sma_60', 'BTC_obv', 'bnb_eth_ratio', 'volume_sma_7', 'BTC_atr_14', 'returns', 'volume_lag_1', 'BTC_volatility_7', 'BTC_BTC_Volume', 'BTC_BTC_Open', 'BTC_cci_20', 'returns_ema_14', 'BTC_volume_sma_7', 'bb_width_20', 'BTC_bb_position_20', 'BTC_returns_ema_7', 'BTC_volatility_14', 'BTC_volume_lag_1', 'BTC_returns_sma_30', 'usdt_eth_totalCirculating', 'returns_ema_7', 'volume_lag_14', 'mfi_14', 'BTC_MACDH_12_26_9', 'volume_lag_3', 'BTC_rsi_14', 'gold_GOLD', 'returns_lag_7', 'positive_ratio', 'positive_ratio_lag1', 'dxy_DXY', 'BTC_volume_lag_14', 'news_count', 'obv', 'volume_lag_7', 'BTC_volume_lag_3', 'rsi_14', 'BBL_20', 'BTC_ADX_14', 'sentiment_mean_lag7', 'BTC_returns_sma_14', 'BTC_returns_ema_30', 'stochastic_14', 'lido_lido_eth_tvl', 'volatility_7', 'BTC_returns_sma_60', 'bb_position_20', 'ada_eth_correlation', 'usdt_total_totalUnreleased']]\n",
      "\n",
      "Best params from each fold: [{'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}]\n",
      "\n",
      "Consensus selected features (appearing in majority folds):\n",
      "['BTC_bb_position_20', 'volume_sma_7', 'BTC_returns', 'positive_ratio', 'volume_lag_7', 'dxy_DXY', 'returns', 'bb_width_20', 'returns_ema_7', 'BTC_returns_lag_1', 'returns_sma_7', 'BTC_BTC_Volume', 'volume_lag_1', 'returns_lag_7', 'BTC_MACD_12_26_9', 'BTC_volume_sma_7', 'BTC_obv', 'bnb_eth_ratio', 'BTC_rsi_14', 'BTC_returns_ema_7', 'BTC_ADX_14', 'bb_position_20', 'BTC_atr_14', 'BTC_volume_lag_1', 'BBL_20', 'usdt_total_totalUnreleased', 'volume_lag_3', 'sp500_SP500', 'positive_ratio_lag1', 'volume_sma_60']\n",
      "\n",
      "[Step 3] Scaling selected features only (no leakage)...\n",
      "\n",
      "Final feature selection complete. Model ready: 30 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''\n",
    "최신 논문(특히 2025년도 \"Optimizing Forecast Accuracy in Cryptocurrency Markets: \n",
    "Evaluating Feature Selection Techniques for Technical Indicators\") \n",
    "및 여러 비교 논문들은 supervised+unsupervised 방법을 조합하되 \n",
    "반드시 nested cross-validation과 시계열 보존(roll-forward 방식)으로 \n",
    "데이터 누수를 막는 것을 기준으로 하고 있음. 따라서, 데이터 누수를 막기 위해서\n",
    "\n",
    "'''\n",
    "# [0] 데이터 분할 및 전처리(기술적지표, 외부지표 등 생성 단계는 기존 코드 동일)\n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume', 'target_next_log_return', 'target_direction']\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# [1] Unsupervised 필터 (variance, correlation) - 전체 dataset/validation/test에는 사용 가능\n",
    "print(\"\\n[Phase 1] Unsupervised filtering (variance/correlation) ...\")\n",
    "to_drop = [c for c in feature_cols if train_df[c].isnull().sum()>len(train_df)*0.5 or train_df[c].nunique()<=1]\n",
    "feature_cols = [c for c in feature_cols if c not in to_drop]\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "for col in feature_cols:\n",
    "    train_df[col] = train_df[col].interpolate().fillna(train_df[col].median())\n",
    "    val_df[col] = val_df[col].interpolate().fillna(train_df[col].median())\n",
    "    test_df[col] = test_df[col].interpolate().fillna(train_df[col].median())\n",
    "corr_matrix = train_df[feature_cols].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column]>0.95)]\n",
    "feature_cols = [c for c in feature_cols if c not in to_drop_corr]\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "\n",
    "# [2] Nested CV 기반 supervised feature selection + 모델 튜닝\n",
    "print(\"\\n[Phase 2] Nested cross-validation: supervised feature selection & tuning ...\")\n",
    "\n",
    "X = train_df[feature_cols].values\n",
    "y = train_df['target_next_log_return'].values\n",
    "\n",
    "tscv_outer = TimeSeriesSplit(n_splits=5)\n",
    "final_selected_features = []\n",
    "best_param_list = []\n",
    "\n",
    "for train_index, valid_index in tscv_outer.split(X):\n",
    "    # Split indices for fold\n",
    "    X_train_cv, X_valid_cv = X[train_index], X[valid_index]\n",
    "    y_train_cv, y_valid_cv = y[train_index], y[valid_index]\n",
    "\n",
    "    # [A] Supervised feature selection (MI + RFECV) - 내부CV만 사용\n",
    "    mi_scores = mutual_info_regression(X_train_cv, y_train_cv, random_state=42, n_neighbors=5)\n",
    "    mi_rank_idx = np.argsort(mi_scores)[::-1][:60]  # top 60개\n",
    "    mi_features_idx = [feature_cols[i] for i in mi_rank_idx]\n",
    "    X_train_mi = X_train_cv[:, mi_rank_idx]\n",
    "\n",
    "    # RFECV with GridSearch for best model in fold\n",
    "    xgb = XGBRegressor(tree_method='gpu_hist', n_jobs=-1, random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    rfecv = RFECV(estimator=xgb, step=1, cv=TimeSeriesSplit(3), scoring='neg_mean_squared_error', min_features_to_select=20)\n",
    "    \n",
    "    # RFECV 실행\n",
    "    rfecv.fit(X_train_mi, y_train_cv)\n",
    "    selected_idx = [i for i, s in enumerate(rfecv.support_) if s]\n",
    "    selected_features_this_fold = [mi_features_idx[i] for i in selected_idx]\n",
    "    final_selected_features.append(selected_features_this_fold)\n",
    "\n",
    "    # 최적 파라미터\n",
    "    grid_search = GridSearchCV(xgb, param_grid, scoring='neg_mean_squared_error', cv=TimeSeriesSplit(3), n_jobs=-1)\n",
    "    grid_search.fit(X_train_mi[:,selected_idx], y_train_cv)\n",
    "    best_param_list.append(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nNested CV finished.\\nAll folds selected features: {final_selected_features}\")\n",
    "print(f\"\\nBest params from each fold: {best_param_list}\")\n",
    "\n",
    "# 교차검증에서 가장 많이 선택된 feature만 retain\n",
    "\n",
    "flat_features = [f for featlist in final_selected_features for f in featlist]\n",
    "feature_freq = Counter(flat_features)\n",
    "selected_final_features = [f for f, cnt in feature_freq.items() if cnt > (len(final_selected_features)//2)]\n",
    "print(f\"\\nConsensus selected features (appearing in majority folds):\\n{selected_final_features}\")\n",
    "\n",
    "print(\"\\n[Step 3] Scaling selected features only (no leakage)...\")\n",
    "scaler = StandardScaler()\n",
    "train_df[selected_final_features] = scaler.fit_transform(train_df[selected_final_features])\n",
    "val_df[selected_final_features] = scaler.transform(val_df[selected_final_features])\n",
    "test_df[selected_final_features] = scaler.transform(test_df[selected_final_features])\n",
    "\n",
    "print(f\"\\nFinal feature selection complete. Model ready: {len(selected_final_features)} features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b6da58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>ETH_Volume</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>...</th>\n",
       "      <th>positive_ratio_lag5</th>\n",
       "      <th>news_count_lag5</th>\n",
       "      <th>sentiment_mean_lag7</th>\n",
       "      <th>positive_ratio_lag7</th>\n",
       "      <th>news_count_lag7</th>\n",
       "      <th>trends_ethereum</th>\n",
       "      <th>trends_ethereum_lag7</th>\n",
       "      <th>trends_ethereum_lag14</th>\n",
       "      <th>target_next_log_return</th>\n",
       "      <th>target_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>219.752686</td>\n",
       "      <td>226.677887</td>\n",
       "      <td>214.130432</td>\n",
       "      <td>218.970596</td>\n",
       "      <td>1.817981e+10</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>-0.004001</td>\n",
       "      <td>44.145897</td>\n",
       "      <td>49.621541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.100216</td>\n",
       "      <td>3.100216</td>\n",
       "      <td>1.874549</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>218.711624</td>\n",
       "      <td>232.811584</td>\n",
       "      <td>217.284286</td>\n",
       "      <td>230.569778</td>\n",
       "      <td>2.030559e+10</td>\n",
       "      <td>0.052971</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>43.827643</td>\n",
       "      <td>49.420794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>230.523972</td>\n",
       "      <td>232.325806</td>\n",
       "      <td>221.732666</td>\n",
       "      <td>224.479630</td>\n",
       "      <td>1.985318e+10</td>\n",
       "      <td>-0.026413</td>\n",
       "      <td>-0.026769</td>\n",
       "      <td>49.029309</td>\n",
       "      <td>52.068197</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>224.565338</td>\n",
       "      <td>228.040421</td>\n",
       "      <td>222.088882</td>\n",
       "      <td>224.517975</td>\n",
       "      <td>1.656708e+10</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>46.601665</td>\n",
       "      <td>50.630391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>224.641891</td>\n",
       "      <td>234.364456</td>\n",
       "      <td>224.641891</td>\n",
       "      <td>229.268188</td>\n",
       "      <td>1.820129e+10</td>\n",
       "      <td>0.021157</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>46.619495</td>\n",
       "      <td>50.639259</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>229.168427</td>\n",
       "      <td>243.554977</td>\n",
       "      <td>228.743576</td>\n",
       "      <td>243.525299</td>\n",
       "      <td>1.937477e+10</td>\n",
       "      <td>0.062185</td>\n",
       "      <td>0.060328</td>\n",
       "      <td>48.884896</td>\n",
       "      <td>51.748751</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>243.750198</td>\n",
       "      <td>249.978485</td>\n",
       "      <td>237.551285</td>\n",
       "      <td>237.853088</td>\n",
       "      <td>1.943165e+10</td>\n",
       "      <td>-0.023292</td>\n",
       "      <td>-0.023568</td>\n",
       "      <td>55.023004</td>\n",
       "      <td>54.893194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.169896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>237.780685</td>\n",
       "      <td>237.780685</td>\n",
       "      <td>200.602982</td>\n",
       "      <td>200.689056</td>\n",
       "      <td>2.138182e+10</td>\n",
       "      <td>-0.156248</td>\n",
       "      <td>-0.169896</td>\n",
       "      <td>52.343631</td>\n",
       "      <td>53.460898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.118962</td>\n",
       "      <td>2.090844</td>\n",
       "      <td>1.874549</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>201.318497</td>\n",
       "      <td>207.451401</td>\n",
       "      <td>192.269897</td>\n",
       "      <td>201.986328</td>\n",
       "      <td>2.364543e+10</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>39.008854</td>\n",
       "      <td>45.434656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>202.863953</td>\n",
       "      <td>205.714249</td>\n",
       "      <td>198.064499</td>\n",
       "      <td>200.767242</td>\n",
       "      <td>1.834493e+10</td>\n",
       "      <td>-0.006035</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>39.584482</td>\n",
       "      <td>45.728553</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>200.768036</td>\n",
       "      <td>202.954300</td>\n",
       "      <td>184.362152</td>\n",
       "      <td>194.868530</td>\n",
       "      <td>1.698479e+10</td>\n",
       "      <td>-0.029381</td>\n",
       "      <td>-0.029821</td>\n",
       "      <td>39.211868</td>\n",
       "      <td>45.490626</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.550732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>194.738922</td>\n",
       "      <td>195.147934</td>\n",
       "      <td>111.210709</td>\n",
       "      <td>112.347122</td>\n",
       "      <td>2.213474e+10</td>\n",
       "      <td>-0.423472</td>\n",
       "      <td>-0.550732</td>\n",
       "      <td>37.387363</td>\n",
       "      <td>44.337202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>112.689995</td>\n",
       "      <td>137.429535</td>\n",
       "      <td>95.184303</td>\n",
       "      <td>133.201813</td>\n",
       "      <td>2.786462e+10</td>\n",
       "      <td>0.185627</td>\n",
       "      <td>0.170272</td>\n",
       "      <td>22.025873</td>\n",
       "      <td>32.444952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.077196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>133.582474</td>\n",
       "      <td>134.484375</td>\n",
       "      <td>122.414474</td>\n",
       "      <td>123.306023</td>\n",
       "      <td>1.274078e+10</td>\n",
       "      <td>-0.074292</td>\n",
       "      <td>-0.077196</td>\n",
       "      <td>29.832200</td>\n",
       "      <td>36.867060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>123.246063</td>\n",
       "      <td>132.242142</td>\n",
       "      <td>121.853653</td>\n",
       "      <td>125.214302</td>\n",
       "      <td>1.271925e+10</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>28.387324</td>\n",
       "      <td>35.720552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.109589</td>\n",
       "      <td>3.100216</td>\n",
       "      <td>1.405912</td>\n",
       "      <td>-0.124053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    ETH_Open    ETH_High     ETH_Low   ETH_Close    ETH_Volume  \\\n",
       "0  2020-03-01  219.752686  226.677887  214.130432  218.970596  1.817981e+10   \n",
       "1  2020-03-02  218.711624  232.811584  217.284286  230.569778  2.030559e+10   \n",
       "2  2020-03-03  230.523972  232.325806  221.732666  224.479630  1.985318e+10   \n",
       "3  2020-03-04  224.565338  228.040421  222.088882  224.517975  1.656708e+10   \n",
       "4  2020-03-05  224.641891  234.364456  224.641891  229.268188  1.820129e+10   \n",
       "5  2020-03-06  229.168427  243.554977  228.743576  243.525299  1.937477e+10   \n",
       "6  2020-03-07  243.750198  249.978485  237.551285  237.853088  1.943165e+10   \n",
       "7  2020-03-08  237.780685  237.780685  200.602982  200.689056  2.138182e+10   \n",
       "8  2020-03-09  201.318497  207.451401  192.269897  201.986328  2.364543e+10   \n",
       "9  2020-03-10  202.863953  205.714249  198.064499  200.767242  1.834493e+10   \n",
       "10 2020-03-11  200.768036  202.954300  184.362152  194.868530  1.698479e+10   \n",
       "11 2020-03-12  194.738922  195.147934  111.210709  112.347122  2.213474e+10   \n",
       "12 2020-03-13  112.689995  137.429535   95.184303  133.201813  2.786462e+10   \n",
       "13 2020-03-14  133.582474  134.484375  122.414474  123.306023  1.274078e+10   \n",
       "14 2020-03-15  123.246063  132.242142  121.853653  125.214302  1.271925e+10   \n",
       "\n",
       "     returns  log_returns     rsi_14     rsi_30  ...  positive_ratio_lag5  \\\n",
       "0  -0.003993    -0.004001  44.145897  49.621541  ...             0.500000   \n",
       "1   0.052971     0.051616  43.827643  49.420794  ...             1.000000   \n",
       "2  -0.026413    -0.026769  49.029309  52.068197  ...             1.000000   \n",
       "3   0.000171     0.000171  46.601665  50.630391  ...             0.500000   \n",
       "4   0.021157     0.020937  46.619495  50.639259  ...             1.000000   \n",
       "5   0.062185     0.060328  48.884896  51.748751  ...             1.000000   \n",
       "6  -0.023292    -0.023568  55.023004  54.893194  ...             0.000000   \n",
       "7  -0.156248    -0.169896  52.343631  53.460898  ...             0.000000   \n",
       "8   0.006464     0.006443  39.008854  45.434656  ...             0.000000   \n",
       "9  -0.006035    -0.006054  39.584482  45.728553  ...             1.000000   \n",
       "10 -0.029381    -0.029821  39.211868  45.490626  ...             1.000000   \n",
       "11 -0.423472    -0.550732  37.387363  44.337202  ...             0.000000   \n",
       "12  0.185627     0.170272  22.025873  32.444952  ...             0.000000   \n",
       "13 -0.074292    -0.077196  29.832200  36.867060  ...             0.333333   \n",
       "14  0.015476     0.015357  28.387324  35.720552  ...             0.000000   \n",
       "\n",
       "    news_count_lag5  sentiment_mean_lag7  positive_ratio_lag7  \\\n",
       "0               4.0             1.000000             1.000000   \n",
       "1               1.0             0.666667             0.666667   \n",
       "2               3.0             0.500000             0.500000   \n",
       "3               4.0             1.000000             1.000000   \n",
       "4               2.0             1.000000             1.000000   \n",
       "5               3.0             0.000000             0.500000   \n",
       "6               3.0             1.000000             1.000000   \n",
       "7               3.0             1.000000             1.000000   \n",
       "8               3.0            -0.333333             0.000000   \n",
       "9               2.0            -0.666667             0.000000   \n",
       "10              4.0            -0.666667             0.000000   \n",
       "11              1.0             1.000000             1.000000   \n",
       "12              1.0             1.000000             1.000000   \n",
       "13              6.0            -1.000000             0.000000   \n",
       "14              3.0            -1.000000             0.000000   \n",
       "\n",
       "    news_count_lag7  trends_ethereum  trends_ethereum_lag7  \\\n",
       "0               1.0         3.100216              3.100216   \n",
       "1               3.0              NaN                   NaN   \n",
       "2               4.0              NaN                   NaN   \n",
       "3               1.0              NaN                   NaN   \n",
       "4               3.0              NaN                   NaN   \n",
       "5               4.0              NaN                   NaN   \n",
       "6               2.0              NaN                   NaN   \n",
       "7               3.0         5.118962              2.090844   \n",
       "8               3.0              NaN                   NaN   \n",
       "9               3.0              NaN                   NaN   \n",
       "10              3.0              NaN                   NaN   \n",
       "11              2.0              NaN                   NaN   \n",
       "12              4.0              NaN                   NaN   \n",
       "13              1.0              NaN                   NaN   \n",
       "14              1.0         4.109589              3.100216   \n",
       "\n",
       "    trends_ethereum_lag14  target_next_log_return  target_direction  \n",
       "0                1.874549                0.051616                 1  \n",
       "1                     NaN               -0.026769                 0  \n",
       "2                     NaN                0.000171                 1  \n",
       "3                     NaN                0.020937                 1  \n",
       "4                     NaN                0.060328                 1  \n",
       "5                     NaN               -0.023568                 0  \n",
       "6                     NaN               -0.169896                 0  \n",
       "7                1.874549                0.006443                 1  \n",
       "8                     NaN               -0.006054                 0  \n",
       "9                     NaN               -0.029821                 0  \n",
       "10                    NaN               -0.550732                 0  \n",
       "11                    NaN                0.170272                 1  \n",
       "12                    NaN               -0.077196                 0  \n",
       "13                    NaN                0.015357                 1  \n",
       "14               1.405912               -0.124053                 0  \n",
       "\n",
       "[15 rows x 265 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e699b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4개의 베이스 모델이 초기화되었습니다.\n",
      "베이스 모델 훈련 중...\n",
      "xgb 훈련 완료\n",
      "rf 훈련 완료\n",
      "ridge 훈련 완료\n",
      "svr 훈련 완료\n",
      "교차검증 성능을 기반으로 모델 가중치를 계산하는 중...\n",
      "xgb: MSE = 0.002757, Weight = 362.655052\n",
      "rf: MSE = 0.002543, Weight = 393.269381\n",
      "ridge: MSE = 0.004163, Weight = 240.195143\n",
      "svr: MSE = 0.002556, Weight = 391.217654\n",
      "수동 스태킹 앙상블 훈련 완료\n",
      "블렌딩 앙상블 훈련 완료\n",
      "voting 앙상블 훈련 완료\n",
      "weighted_voting 앙상블 훈련 완료\n",
      "\n",
      "베이스 모델 평가:\n",
      "--------------------------------------------------\n",
      "xgb         : MSE=0.001997, RMSE=0.044683, MAE=0.033155, R2=-0.3194\n",
      "rf          : MSE=0.001618, RMSE=0.040218, MAE=0.028300, R2=-0.0689\n",
      "ridge       : MSE=0.002180, RMSE=0.046693, MAE=0.036519, R2=-0.4408\n",
      "svr         : MSE=0.001602, RMSE=0.040030, MAE=0.028012, R2=-0.0589\n",
      "\n",
      "앙상블 모델 평가:\n",
      "--------------------------------------------------\n",
      "voting      : MSE=0.001635, RMSE=0.040436, MAE=0.029009,MAPE:1586840117626.536377, R2=-0.0805\n",
      "weighted_voting: MSE=0.001613, RMSE=0.040157, MAE=0.028581,MAPE:1273824980317.458496, R2=-0.0656\n",
      "manual_stacking: MSE=0.001523, RMSE=0.039031, MAE=0.027035,MAPE:231547725405.686523, R2=-0.0067\n",
      "blending    : MSE=0.001592, RMSE=0.039896, MAE=0.028632,MAPE:1371893895719.201660, R2=-0.0519\n",
      "\n",
      "최고 성능 모델: ensemble_manual_stacking (MSE: 0.001523)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CryptocurrencyEnsembleFixed:\n",
    "    \"\"\"\n",
    "    수정된 암호화폐 앙상블 프레임워크\n",
    "    TimeSeriesSplit 오류 해결\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.base_models = {}\n",
    "        self.ensemble_models = {}\n",
    "        self.model_weights = {}\n",
    "        self.performance_scores = {}\n",
    "        \n",
    "    def initialize_base_models(self):\n",
    "        \"\"\"베이스 모델 초기화\"\"\"\n",
    "        \n",
    "        # XGBoost 사용 가능한 경우에만 추가\n",
    "        try:\n",
    "            from xgboost import XGBRegressor\n",
    "            self.base_models['xgb'] = XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        except ImportError:\n",
    "            print(\"XGBoost를 사용할 수 없습니다. RandomForest로 대체합니다.\")\n",
    "            \n",
    "        # Random Forest\n",
    "        self.base_models['rf'] = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Ridge Regression\n",
    "        self.base_models['ridge'] = Ridge(\n",
    "            alpha=1.0,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # SVR\n",
    "        self.base_models['svr'] = SVR(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale'\n",
    "        )\n",
    "        \n",
    "        print(f\"{len(self.base_models)}개의 베이스 모델이 초기화되었습니다.\")\n",
    "        \n",
    "    def calculate_model_weights(self, X_train, y_train, cv_folds=5):\n",
    "        \"\"\"시계열 교차검증을 통한 모델 가중치 계산\"\"\"\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "        weights = {}\n",
    "        \n",
    "        print(\"교차검증 성능을 기반으로 모델 가중치를 계산하는 중...\")\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error')\n",
    "            avg_mse = -scores.mean()\n",
    "            weights[name] = 1 / (avg_mse + 1e-8)  # 오류의 역수로 가중치 계산\n",
    "            \n",
    "            print(f\"{name}: MSE = {avg_mse:.6f}, Weight = {weights[name]:.6f}\")\n",
    "            \n",
    "        # 가중치 정규화\n",
    "        total_weight = sum(weights.values())\n",
    "        normalized_weights = [weights[name] / total_weight for name in self.base_models.keys()]\n",
    "        \n",
    "        self.model_weights = dict(zip(self.base_models.keys(), normalized_weights))\n",
    "        return normalized_weights\n",
    "        \n",
    "    def create_voting_ensemble(self):\n",
    "        \"\"\"단순 투표 앙상블 (동일한 가중치)\"\"\"\n",
    "        estimators = [(name, model) for name, model in self.base_models.items()]\n",
    "        self.ensemble_models['voting'] = VotingRegressor(estimators)\n",
    "        \n",
    "    def create_weighted_voting_ensemble(self, weights=None):\n",
    "        \"\"\"가중 투표 앙상블 (성능 기반 가중치)\"\"\"\n",
    "        if weights is None:\n",
    "            weights = list(self.model_weights.values())\n",
    "            \n",
    "        estimators = [(name, model) for name, model in self.base_models.items()]\n",
    "        self.ensemble_models['weighted_voting'] = VotingRegressor(estimators, weights=weights)\n",
    "        \n",
    "    def create_stacking_ensemble_fixed(self, meta_learner=None):\n",
    "        \"\"\"수정된 스태킹 앙상블 (KFold 사용)\"\"\"\n",
    "        if meta_learner is None:\n",
    "            meta_learner = LinearRegression()\n",
    "            \n",
    "        estimators = [(name, model) for name, model in self.base_models.items()]\n",
    "        \n",
    "        # TimeSeriesSplit 대신 KFold 사용 (shuffle=False로 시계열 순서 유지)\n",
    "        cv = KFold(n_splits=3, shuffle=False)\n",
    "        \n",
    "        self.ensemble_models['stacking'] = StackingRegressor(\n",
    "            estimators, \n",
    "            final_estimator=meta_learner,\n",
    "            cv=cv\n",
    "        )\n",
    "        \n",
    "    def create_manual_stacking_ensemble(self, X_train, y_train, meta_learner=None):\n",
    "        \"\"\"수동 스태킹 앙상블 구현 (TimeSeriesSplit 완전 지원)\"\"\"\n",
    "        if meta_learner is None:\n",
    "            meta_learner = LinearRegression()\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        \n",
    "        # 베이스 모델 예측값 저장할 배열\n",
    "        stacking_features = np.zeros((len(X_train), len(self.base_models)))\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train = y_train[train_idx]\n",
    "            \n",
    "            for i, (name, model) in enumerate(self.base_models.items()):\n",
    "                # 폴드별로 모델 복사본 훈련\n",
    "                model_clone = clone(model)\n",
    "                model_clone.fit(X_fold_train, y_fold_train)\n",
    "                \n",
    "                # 검증 세트 예측값 저장\n",
    "                stacking_features[val_idx, i] = model_clone.predict(X_fold_val)\n",
    "        \n",
    "        # 메타 러너 훈련\n",
    "        meta_learner.fit(stacking_features, y_train)\n",
    "        \n",
    "        # 최종 베이스 모델들을 전체 데이터로 재훈련\n",
    "        trained_models = {}\n",
    "        for name, model in self.base_models.items():\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train, y_train)\n",
    "            trained_models[name] = model_clone\n",
    "        \n",
    "        self.ensemble_models['manual_stacking'] = {\n",
    "            'base_models': trained_models,\n",
    "            'meta_learner': meta_learner\n",
    "        }\n",
    "        \n",
    "    def predict_manual_stacking(self, X):\n",
    "        \"\"\"수동 스태킹 예측\"\"\"\n",
    "        if 'manual_stacking' not in self.ensemble_models:\n",
    "            raise ValueError(\"수동 스태킹 앙상블이 훈련되지 않았습니다.\")\n",
    "            \n",
    "        base_predictions = []\n",
    "        for name, model in self.ensemble_models['manual_stacking']['base_models'].items():\n",
    "            pred = model.predict(X).reshape(-1, 1)\n",
    "            base_predictions.append(pred)\n",
    "            \n",
    "        stacking_features = np.hstack(base_predictions)\n",
    "        return self.ensemble_models['manual_stacking']['meta_learner'].predict(stacking_features)\n",
    "        \n",
    "    def create_blending_ensemble(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"블렌딩 앙상블\"\"\"\n",
    "        \n",
    "        # 훈련 세트에서 베이스 모델 훈련\n",
    "        base_predictions_val = {}\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            base_predictions_val[name] = model.predict(X_val).reshape(-1, 1)\n",
    "            \n",
    "        # 블렌딩 특성 생성\n",
    "        blend_features = np.hstack(list(base_predictions_val.values()))\n",
    "        \n",
    "        # 검증 예측값으로 메타 러너 훈련\n",
    "        meta_learner = LinearRegression()\n",
    "        meta_learner.fit(blend_features, y_val)\n",
    "        \n",
    "        self.ensemble_models['blending'] = {\n",
    "            'base_models': self.base_models.copy(),\n",
    "            'meta_learner': meta_learner\n",
    "        }\n",
    "        \n",
    "    def predict_blending(self, X):\n",
    "        \"\"\"블렌딩 앙상블 예측\"\"\"\n",
    "        if 'blending' not in self.ensemble_models:\n",
    "            raise ValueError(\"블렌딩 앙상블이 훈련되지 않았습니다.\")\n",
    "            \n",
    "        base_predictions = []\n",
    "        for name, model in self.ensemble_models['blending']['base_models'].items():\n",
    "            pred = model.predict(X).reshape(-1, 1)\n",
    "            base_predictions.append(pred)\n",
    "            \n",
    "        blend_features = np.hstack(base_predictions)\n",
    "        return self.ensemble_models['blending']['meta_learner'].predict(blend_features)\n",
    "        \n",
    "    def train_all_models(self, X_train, y_train, X_val=None, y_val=None, use_manual_stacking=True):\n",
    "        \"\"\"모든 모델 훈련 (수정된 버전)\"\"\"\n",
    "        \n",
    "        print(\"베이스 모델 훈련 중...\")\n",
    "        \n",
    "        # 베이스 모델 훈련\n",
    "        for name, model in self.base_models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            print(f\"{name} 훈련 완료\")\n",
    "            \n",
    "        # 가중치 계산\n",
    "        weights = self.calculate_model_weights(X_train, y_train)\n",
    "        \n",
    "        # 앙상블 모델 생성 및 훈련\n",
    "        self.create_voting_ensemble()\n",
    "        self.create_weighted_voting_ensemble(weights)\n",
    "        \n",
    "        if use_manual_stacking:\n",
    "            # 수동 스태킹 사용 (TimeSeriesSplit 완전 지원)\n",
    "            self.create_manual_stacking_ensemble(X_train, y_train)\n",
    "            print(\"수동 스태킹 앙상블 훈련 완료\")\n",
    "        else:\n",
    "            # 기본 스태킹 사용 (KFold)\n",
    "            self.create_stacking_ensemble_fixed()\n",
    "            self.ensemble_models['stacking'].fit(X_train, y_train)\n",
    "            print(\"스태킹 앙상블 훈련 완료\")\n",
    "            \n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.create_blending_ensemble(X_train, y_train, X_val, y_val)\n",
    "            print(\"블렌딩 앙상블 훈련 완료\")\n",
    "            \n",
    "        # 일반 앙상블 모델 훈련\n",
    "        for name, model in self.ensemble_models.items():\n",
    "            if name not in ['manual_stacking', 'blending']:\n",
    "                model.fit(X_train, y_train)\n",
    "                print(f\"{name} 앙상블 훈련 완료\")\n",
    "                \n",
    "    def evaluate_all_models(self, X_test, y_test):\n",
    "        \"\"\"모든 모델 성능 평가\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 베이스 모델 평가\n",
    "        print(\"\\n베이스 모델 평가:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            results[f'base_{name}'] = {\n",
    "                'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:12}: MSE={mse:.6f}, RMSE={rmse:.6f}, MAE={mae:.6f}, R2={r2:.4f}\")\n",
    "            \n",
    "        # 앙상블 모델 평가\n",
    "        print(\"\\n앙상블 모델 평가:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, model in self.ensemble_models.items():\n",
    "            if name == 'blending':\n",
    "                y_pred = self.predict_blending(X_test)\n",
    "            elif name == 'manual_stacking':\n",
    "                y_pred = self.predict_manual_stacking(X_test)\n",
    "            else:\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            results[f'ensemble_{name}'] = {\n",
    "                'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:12}: MSE={mse:.6f}, RMSE={rmse:.6f}, MAE={mae:.6f},MAPE:{mape:.6f}, R2={r2:.4f}\")\n",
    "            \n",
    "        # 최고 성능 모델 찾기\n",
    "        best_model = min(results.items(), key=lambda x: x[1]['MSE'])\n",
    "        print(f\"\\n최고 성능 모델: {best_model[0]} (MSE: {best_model[1]['MSE']:.6f})\")\n",
    "        \n",
    "        self.performance_scores = results\n",
    "        return results\n",
    "ensemble = CryptocurrencyEnsembleFixed(random_state=42)\n",
    "ensemble.initialize_base_models()\n",
    "\n",
    "# 데이터 준비 (기존과 동일)\n",
    "X_train = train_df[selected_final_features].values\n",
    "y_train = train_df['target_next_log_return'].values\n",
    "X_val = val_df[selected_final_features].values\n",
    "y_val = val_df['target_next_log_return'].values\n",
    "X_test = test_df[selected_final_features].values\n",
    "y_test = test_df['target_next_log_return'].values\n",
    "\n",
    "# 모든 모델 훈련 (수동 스태킹 사용)\n",
    "ensemble.train_all_models(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    use_manual_stacking=True  \n",
    ")\n",
    "\n",
    "# 성능 평가\n",
    "results = ensemble.evaluate_all_models(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6382c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ca288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc9a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb94c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4746df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2205bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# print(\"\\nStep 9: Advanced Feature Selection (논문 기반)...\")\n",
    "# print(\"Reference: 'Optimizing Forecast Accuracy in Cryptocurrency Markets' (2025)\")\n",
    "\n",
    "# exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "#                 'target_next_log_return', 'target_direction']\n",
    "# feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# # PHASE 1: 결측치 제거\n",
    "# print(\"\\n[Phase 1] Variance-based filtering...\")\n",
    "\n",
    "# cols_to_drop = []\n",
    "# for col in feature_cols:\n",
    "#     if train_df[col].isnull().sum() > len(train_df) * 0.5:\n",
    "#         cols_to_drop.append(col)\n",
    "\n",
    "# if cols_to_drop:\n",
    "#     print(f\"Dropping {len(cols_to_drop)} features with >50% missing\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in cols_to_drop]\n",
    "#     train_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# for col in feature_cols:\n",
    "#     train_df[col] = train_df[col].fillna(method='ffill').fillna(train_df[col].median())\n",
    "#     train_median = train_df[col].median()\n",
    "#     val_df[col] = val_df[col].fillna(method='ffill').fillna(train_median)\n",
    "#     test_df[col] = test_df[col].fillna(method='ffill').fillna(train_median)\n",
    "\n",
    "# print(f\"Features after Phase 1: {len(feature_cols)}\")\n",
    "\n",
    "# # PHASE 2: 상관관계 0.95 이상 중복 제거\n",
    "# print(\"\\n[Phase 2] Correlation-based redundancy removal...\")\n",
    "# corr_matrix = train_df[feature_cols].corr().abs()\n",
    "# upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "\n",
    "# if to_drop_corr:\n",
    "#     print(f\"Dropping {len(to_drop_corr)} highly correlated features (>0.95)\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in to_drop_corr]\n",
    "#     train_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "\n",
    "# print(f\"Features after Phase 2: {len(feature_cols)}\")\n",
    "\n",
    "# # PHASE 3: Mutual Information (Top 50~60)\n",
    "# print(\"\\n[Phase 3] Mutual Information feature selection...\")\n",
    "# print(\"Reference: MI effectively captures non-linear relationships in crypto markets\")\n",
    "\n",
    "# X_train = train_df[feature_cols].values\n",
    "# y_train = train_df['target_next_log_return'].values\n",
    "# X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "# y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=5)\n",
    "# mi_scores_series = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=False)\n",
    "# n_mi_features = min(60, len(feature_cols))\n",
    "# top_mi_features = mi_scores_series.head(n_mi_features).index.tolist()\n",
    "# print(f\"Selected top {len(top_mi_features)} features by MI\")\n",
    "# print(f\"Top 10 MI scores:\\n{mi_scores_series.head(10)}\")\n",
    "\n",
    "# X_train_mi = train_df[top_mi_features].values\n",
    "# X_train_mi = np.nan_to_num(X_train_mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# # ====================하이퍼파라미터 자동 탐색 ====================\n",
    "# print(\"\\nStep 9.1: XGBoost Hyperparameter Optimization (Optuna, 논문 권장)\")\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#         'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1.0),\n",
    "#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 2.0),\n",
    "#         'tree_method': 'gpu_hist',\n",
    "#         'random_state': 42,\n",
    "#         'n_jobs': -1\n",
    "#     }\n",
    "    \n",
    "#     model = XGBRegressor(**params)\n",
    "#     score = cross_val_score(model, X_train_mi, y_train, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#     return -score.mean()\n",
    "\n",
    "\n",
    "# # Pruner 적용\n",
    "# study = optuna.create_study(\n",
    "#     direction='minimize',\n",
    "#     pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "# )\n",
    "# study.optimize(objective, n_trials=50)  \n",
    "\n",
    "# print(\"Best XGBoost params:\", study.best_trial.params)\n",
    "\n",
    "# best_xgb = XGBRegressor(**study.best_trial.params)\n",
    "\n",
    "# # PHASE 4: RFECV with 최적 하이퍼파라미터 적용\n",
    "# print(\"\\n[Phase 4] Recursive Feature Elimination with Cross-Validation... (Optuna 최적 파라미터 적용)\")\n",
    "\n",
    "# selector = RFECV(\n",
    "#     estimator=best_xgb,\n",
    "#     step=1,\n",
    "#     cv=5,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     min_features_to_select=20,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# print(\"Training RFECV... (this may take a few minutes with GPU & Optuna-tuned params)\")\n",
    "# selector.fit(X_train_mi, y_train)\n",
    "# selected_features = [top_mi_features[i] for i in range(len(top_mi_features)) if selector.support_[i]]\n",
    "# print(f\"\\nOptimal number of features: {len(selected_features)}\")\n",
    "# print(f\"Feature reduction: {100 * (1 - len(selected_features) / len(feature_cols)):.1f}%\")\n",
    "# print(f\"\\nSelected features:\\n{selected_features}\")\n",
    "\n",
    "# # PHASE 5: Feature Importance 분석\n",
    "# print(\"\\n[Phase 5] XGBoost Feature Importance analysis...\")\n",
    "# feature_importance = pd.Series(\n",
    "#     selector.estimator_.feature_importances_,\n",
    "#     index=selected_features\n",
    "# ).sort_values(ascending=False)\n",
    "# print(f\"\\nTop 15 most important features:\")\n",
    "# print(feature_importance.head(15))\n",
    "\n",
    "# # ====================  카테고리별 상세 분류 ====================\n",
    "\n",
    "# def categorize_feature(f):\n",
    "#     if any(x in f for x in ['rsi', 'macd', 'momentum', 'roc', 'cci', 'stochastic', 'williams']):\n",
    "#         return 'Momentum'\n",
    "#     if any(x in f for x in ['volatility', 'atr', 'bb_', 'bbands']):\n",
    "#         return 'Volatility'\n",
    "#     if any(x in f for x in ['volume', 'obv', 'mfi']):\n",
    "#         return 'Volume'\n",
    "#     if any(x in f for x in ['sma', 'ema', 'trend']):\n",
    "#         return 'Trend'\n",
    "#     if 'onchain_' in f:\n",
    "#         return 'On-chain'\n",
    "#     if any(x in f for x in ['sp500', 'vix', 'gold', 'dxy']):\n",
    "#         return 'Macro'\n",
    "#     if any(x in f for x in ['sentiment', 'news', 'positive_ratio', 'negative_ratio']):\n",
    "#         return 'Sentiment'\n",
    "#     if any(x in f for x in ['aave', 'lido', 'maker', 'chain_tvl', 'funding']):\n",
    "#         return 'External'\n",
    "#     return 'Other'\n",
    "\n",
    "# category_map = {f: categorize_feature(f) for f in selected_features}\n",
    "\n",
    "# category_counts = Counter(category_map.values())\n",
    "# print(\"\\nFeature category breakdown (detailed):\")\n",
    "# for cat, count in category_counts.items():\n",
    "#     print(f\"{cat}: {count}\")\n",
    "\n",
    "# print(\"\\nDetailed feature list by category:\")\n",
    "# for cat in sorted(set(category_map.values())):\n",
    "#     print(f\"\\n[{cat}]\")\n",
    "#     for f in [k for k, v in category_map.items() if v == cat]:\n",
    "#         print(f\"  - {f}\")\n",
    "\n",
    "# # ==================== Step 10: Scaling ====================\n",
    "# print(\"\\n[Step 10] Scaling selected features only...\")\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# train_df[selected_features] = scaler.fit_transform(train_df[selected_features])\n",
    "# val_df[selected_features] = scaler.transform(val_df[selected_features])\n",
    "# test_df[selected_features] = scaler.transform(test_df[selected_features])\n",
    "\n",
    "# print(f\"\\n{'='*70}\")\n",
    "# print(f\"FINAL PREPROCESSED DATASETS\")\n",
    "# print(f\"{'='*70}\")\n",
    "# print(f\"Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "# print(f\"Val: {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "# print(f\"Test: {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "# print(f\"Selected Features: {len(selected_features)}\")\n",
    "# print(f\"\\nFeature selection complete. Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5902d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# 퍼플렉시티 버전인데 일단 주석 ##########\n",
    "# print(\"\\nStep 9: Feature selection and missing value handling...\")\n",
    "# exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "#                 'target_next_log_return', 'target_direction']\n",
    "# feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# cols_to_drop = []\n",
    "# for col in feature_cols:\n",
    "#     if train_df[col].isnull().sum() > len(train_df) * 0.5:\n",
    "#         cols_to_drop.append(col)\n",
    "\n",
    "# if cols_to_drop:\n",
    "#     print(f\"Dropping {len(cols_to_drop)} features with >50% missing in train\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in cols_to_drop]\n",
    "#     train_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# for col in feature_cols:\n",
    "#     train_df[col] = train_df[col].fillna(method='ffill').fillna(train_df[col].median())\n",
    "#     train_median = train_df[col].median()\n",
    "\n",
    "#     val_df[col] = val_df[col].fillna(method='ffill').fillna(train_median)\n",
    "#     test_df[col] = test_df[col].fillna(method='ffill').fillna(train_median)\n",
    "\n",
    "# print(\"\\nStep 10: Scaling with StandardScaler...\")\n",
    "# scaler = StandardScaler()\n",
    "# train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "# val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "# test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# print(f\"\\nFinal datasets:\")\n",
    "# print(f\"Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "# print(f\"Val: {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "# print(f\"Test: {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "# print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d29d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################    클로드 버전 #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0738b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 11: Feature Selection...\n",
      "Initial features: 208\n",
      "After variance filter: 208 features\n",
      "After correlation filter: 127 features\n",
      "\n",
      "Final selected features: 30\n",
      "\n",
      "Top 30 most important:\n",
      "  BTC_obv: 0.0551\n",
      "  returns_lag_3: 0.0467\n",
      "  returns: 0.0431\n",
      "  macd_signal_12_26_9: 0.0413\n",
      "  BTC_BTC_Volume: 0.0244\n",
      "  btc_dominance: 0.0237\n",
      "  volume_lag_3: 0.0222\n",
      "  lido_lido_eth_tvl: 0.0201\n",
      "  BTC_roc_20: 0.0170\n",
      "  BTC_returns: 0.0152\n",
      "  volume_sma_7: 0.0151\n",
      "  BTC_returns_lag_14: 0.0141\n",
      "  volume_lag_14: 0.0137\n",
      "  btc_eth_correlation: 0.0135\n",
      "  BTC_macd_12_26: 0.0131\n",
      "  BTC_mfi_14: 0.0119\n",
      "  dxy_DXY: 0.0116\n",
      "  returns_sma_60: 0.0111\n",
      "  vix_VIX: 0.0110\n",
      "  BTC_returns_ema_7: 0.0107\n",
      "  BTC_macd_hist_12_26_9: 0.0106\n",
      "  sentiment_mean: 0.0106\n",
      "  returns_ema_7: 0.0105\n",
      "  BTC_returns_lag_7: 0.0099\n",
      "  bnb_eth_ratio: 0.0098\n",
      "  BTC_returns_sma_14: 0.0097\n",
      "  funding_fundingRate_lag1: 0.0096\n",
      "  BTC_volatility_7: 0.0096\n",
      "  vwap: 0.0093\n",
      "  BTC_returns_lag_1: 0.0092\n",
      "\n",
      "Final dataset shapes:\n",
      "Train: (1411, 33)\n",
      "Val: (314, 33)\n",
      "Test: (322, 33)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStep 11: Feature Selection...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "                'target_next_log_return', 'target_direction']\n",
    "all_features = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Initial features: {len(all_features)}\")\n",
    "\n",
    "# Stage 1: Variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(train_df[all_features])\n",
    "features_after_variance = [feat for feat, selected in zip(all_features, selector.get_support()) if selected]\n",
    "print(f\"After variance filter: {len(features_after_variance)} features\")\n",
    "\n",
    "# Stage 2: Correlation filter\n",
    "corr_matrix = train_df[features_after_variance].corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]\n",
    "features_after_corr = [f for f in features_after_variance if f not in to_drop]\n",
    "print(f\"After correlation filter: {len(features_after_corr)} features\")\n",
    "\n",
    "# Stage 3: Tree-based importance\n",
    "X_train = train_df[features_after_corr].values\n",
    "y_train = train_df['target_next_log_return'].values\n",
    "mask = ~np.isnan(y_train)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'feature': features_after_corr,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "TOP_K = 30\n",
    "selected_features = importances.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nFinal selected features: {len(selected_features)}\")\n",
    "print(f\"\\nTop {TOP_K} most important:\")\n",
    "for idx, row in importances.head(TOP_K).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Apply selection to all datasets\n",
    "train_df = train_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "val_df = val_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "test_df = test_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "\n",
    "print(f\"\\nFinal dataset shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "\n",
    "targets = {\n",
    "    'log_return': 'target_next_log_return',\n",
    "    'direction': 'target_direction'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 퍼플렉시티 버전 ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9809b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Step 11: Advanced Feature Selection\n",
      "======================================================================\n",
      "\n",
      "Initial features: 208\n",
      "\n",
      "[Stage 1] Variance Threshold Filter...\n",
      "  Remaining: 208 features\n",
      "\n",
      "[Stage 2] Correlation Filter (threshold=0.95)...\n",
      "  Dropped 81 highly correlated features\n",
      "  Remaining: 127 features\n",
      "\n",
      "[Stage 3] Random Forest Feature Importance...\n",
      "\n",
      "[Stage 4] Top-K Selection (K=30)...\n",
      "  Cumulative importance at K=30: 52.82%\n",
      "  Features needed for 95% importance: 105\n",
      "\n",
      "======================================================================\n",
      "Selected 30 Features\n",
      "======================================================================\n",
      "\n",
      "Category Distribution:\n",
      "  BTC                 : 13 ( 43.3%)\n",
      "  Momentum            :  5 ( 16.7%)\n",
      "  Volume              :  5 ( 16.7%)\n",
      "  Market Structure    :  3 ( 10.0%)\n",
      "  Technical           :  1 (  3.3%)\n",
      "  DeFi/OnChain        :  1 (  3.3%)\n",
      "  Macro               :  1 (  3.3%)\n",
      "  Other               :  1 (  3.3%)\n",
      "\n",
      " Top 30 Most Important Features:\n",
      "Rank   Feature                             Importance   Category            \n",
      "---------------------------------------------------------------------------\n",
      "1      BTC_obv                             0.056564    BTC                 \n",
      "2      returns                             0.046740    Momentum            \n",
      "3      returns_lag_3                       0.043536    Momentum            \n",
      "4      macd_signal_12_26_9                 0.032343    Technical           \n",
      "5      btc_dominance                       0.023450    Market Structure    \n",
      "6      BTC_BTC_Volume                      0.021518    BTC                 \n",
      "7      BTC_returns                         0.018410    BTC                 \n",
      "8      volume_lag_3                        0.017045    Volume              \n",
      "9      lido_lido_eth_tvl                   0.016676    DeFi/OnChain        \n",
      "10     returns_sma_60                      0.015583    Momentum            \n",
      "11     returns_ema_7                       0.015508    Momentum            \n",
      "12     volume_sma_7                        0.014788    Volume              \n",
      "13     returns_lag_1                       0.014636    Momentum            \n",
      "14     volume_lag_14                       0.013902    Volume              \n",
      "15     BTC_returns_lag_14                  0.013847    BTC                 \n",
      "16     BTC_roc_20                          0.013339    BTC                 \n",
      "17     BTC_macd_12_26                      0.012726    BTC                 \n",
      "18     btc_eth_correlation                 0.012622    Market Structure    \n",
      "19     dxy_DXY                             0.012394    Macro               \n",
      "20     BTC_returns_lag_7                   0.011336    BTC                 \n",
      "21     volume_lag_7                        0.010723    Volume              \n",
      "22     BTC_returns_ema_7                   0.010668    BTC                 \n",
      "23     BTC_returns_lag_1                   0.010474    BTC                 \n",
      "24     bnb_eth_ratio                       0.010133    Market Structure    \n",
      "25     BTC_returns_lag_3                   0.010116    BTC                 \n",
      "26     BTC_momentum_10                     0.010009    BTC                 \n",
      "27     funding_fundingRate_lag1            0.009978    Other               \n",
      "28     BTC_cci_20                          0.009886    BTC                 \n",
      "29     volume_lag_1                        0.009616    Volume              \n",
      "30     BTC_mfi_14                          0.009606    BTC                 \n",
      "\n",
      "======================================================================\n",
      "Critical Feature Check\n",
      "======================================================================\n",
      "  ⚠ RSI: Not in top 30, best rank: 17\n",
      "  ⚠ ADX: Not in top 30, best rank: 34\n",
      "  ⚠ Bollinger Bands: Not in top 30, best rank: 22\n",
      "  ⚠ Stochastic: Not in top 30, best rank: 26\n",
      "\n",
      "======================================================================\n",
      "Applying Feature Selection to All Datasets\n",
      "======================================================================\n",
      "\n",
      "Final Dataset Shapes:\n",
      "  Train: (1411, 33)\n",
      "  Val:   (314, 33)\n",
      "  Test:  (322, 33)\n",
      "\n",
      "======================================================================\n",
      "Feature Selection Completed Successfully!\n",
      "======================================================================\n",
      "  TOP_K: 30\n",
      "  USE_RFECV: False\n",
      "  CUMULATIVE_THRESHOLD: 0.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 11: Advanced Feature Selection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TOP_K = 30  \n",
    "USE_RFECV = False \n",
    "CUMULATIVE_THRESHOLD = 0.95 \n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "                'target_next_log_return', 'target_direction']\n",
    "all_features = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nInitial features: {len(all_features)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 1: Variance Threshold\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 1] Variance Threshold Filter...\")\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(train_df[all_features])\n",
    "features_after_variance = [feat for feat, selected in zip(all_features, selector.get_support()) if selected]\n",
    "print(f\"  Remaining: {len(features_after_variance)} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 2: Correlation Filter (Removes redundant features)\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 2] Correlation Filter (threshold=0.95)...\")\n",
    "corr_matrix = train_df[features_after_variance].corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]\n",
    "features_after_corr = [f for f in features_after_variance if f not in to_drop]\n",
    "print(f\"  Dropped {len(to_drop)} highly correlated features\")\n",
    "print(f\"  Remaining: {len(features_after_corr)} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 3: Tree-based Feature Importance\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 3] Random Forest Feature Importance...\")\n",
    "X_train = train_df[features_after_corr].values\n",
    "y_train = train_df['target_next_log_return'].values\n",
    "mask = ~np.isnan(y_train)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,  # Increased for stability\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'feature': features_after_corr,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 4: Feature Selection Strategy\n",
    "# ============================================================================\n",
    "if USE_RFECV:\n",
    "    print(\"\\n[Stage 4] RFECV - Finding Optimal Feature Count...\")\n",
    "    print(\"  (This may take several minutes...)\")\n",
    "\n",
    "    rfecv = RFECV(\n",
    "        estimator=RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        step=1,\n",
    "        cv=TimeSeriesSplit(n_splits=3),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    rfecv.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "    selected_features = [features_after_corr[i] for i in range(len(features_after_corr)) if rfecv.support_[i]]\n",
    "    print(f\"  RFECV selected {len(selected_features)} optimal features\")\n",
    "    print(f\"  (Ignoring TOP_K={TOP_K} parameter)\")\n",
    "\n",
    "else:\n",
    "    # Method 1: Top K selection\n",
    "    print(f\"\\n[Stage 4] Top-K Selection (K={TOP_K})...\")\n",
    "    selected_features = importances.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "    # Calculate cumulative importance for reference\n",
    "    importances['cumulative'] = importances['importance'].cumsum()\n",
    "    importances['cumulative_pct'] = importances['cumulative'] / importances['importance'].sum()\n",
    "\n",
    "    cum_at_k = importances.iloc[TOP_K-1]['cumulative_pct']\n",
    "    print(f\"  Cumulative importance at K={TOP_K}: {cum_at_k:.2%}\")\n",
    "\n",
    "    # Show how many features needed for 95% cumulative importance\n",
    "    features_for_95 = len(importances[importances['cumulative_pct'] <= CUMULATIVE_THRESHOLD])\n",
    "    print(f\"  Features needed for {CUMULATIVE_THRESHOLD:.0%} importance: {features_for_95}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Display Top Features with Category Analysis\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Selected {len(selected_features)} Features\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Categorize features\n",
    "def categorize_feature(feat):\n",
    "    if feat.startswith('BTC_'):\n",
    "        return 'BTC'\n",
    "    elif any(x in feat for x in ['returns', 'log_returns', 'momentum', 'roc']):\n",
    "        return 'Momentum'\n",
    "    elif any(x in feat for x in ['volume', 'obv']):\n",
    "        return 'Volume'\n",
    "    elif any(x in feat for x in ['sma', 'ema', 'macd', 'rsi', 'bb_', 'atr', 'adx']):\n",
    "        return 'Technical'\n",
    "    elif any(x in feat for x in ['tvl', 'onchain', 'lido', 'aave', 'maker']):\n",
    "        return 'DeFi/OnChain'\n",
    "    elif any(x in feat for x in ['sentiment', 'news']):\n",
    "        return 'Sentiment'\n",
    "    elif any(x in feat for x in ['vix', 'sp500', 'gold', 'dxy']):\n",
    "        return 'Macro'\n",
    "    elif any(x in feat for x in ['correlation', 'ratio', 'dominance']):\n",
    "        return 'Market Structure'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Category distribution\n",
    "categories = {}\n",
    "for feat in selected_features:\n",
    "    cat = categorize_feature(feat)\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nCategory Distribution:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: -x[1]):\n",
    "    pct = count / len(selected_features) * 100\n",
    "    print(f\"  {cat:20s}: {count:2d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n Top {min(30, len(selected_features))} Most Important Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<35} {'Importance':<12} {'Category':<20}\")\n",
    "print(\"-\" * 75)\n",
    "for idx, row in importances.head(min(30, len(selected_features))).iterrows():\n",
    "    feat = row['feature']\n",
    "    if feat in selected_features:\n",
    "        rank = list(selected_features).index(feat) + 1\n",
    "        cat = categorize_feature(feat)\n",
    "        print(f\"{rank:<6} {feat:<35} {row['importance']:.6f}    {cat:<20}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check for Missing Critical Features\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Critical Feature Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "critical_features = {\n",
    "    'RSI': [f for f in features_after_corr if 'rsi' in f.lower()],\n",
    "    'ADX': [f for f in features_after_corr if 'adx' in f.lower()],\n",
    "    'Bollinger Bands': [f for f in features_after_corr if 'bb_' in f],\n",
    "    'Stochastic': [f for f in features_after_corr if 'stochastic' in f],\n",
    "}\n",
    "\n",
    "for indicator, features in critical_features.items():\n",
    "    if features:\n",
    "        selected_count = len([f for f in features if f in selected_features])\n",
    "        if selected_count > 0:\n",
    "            print(f\"  ✓ {indicator}: {selected_count}/{len(features)} selected\")\n",
    "        else:\n",
    "            # Find rank of best feature in this category\n",
    "            ranks = [importances[importances['feature']==f].index[0] for f in features if f in importances['feature'].values]\n",
    "            if ranks:\n",
    "                best_rank = min(ranks) + 1\n",
    "                print(f\"  ⚠ {indicator}: Not in top {len(selected_features)}, best rank: {best_rank}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {indicator}: Not computed\")\n",
    "\n",
    "# ============================================================================\n",
    "# Apply Selection to All Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Applying Feature Selection to All Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_df = train_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "val_df = val_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "test_df = test_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "\n",
    "print(f\"\\nFinal Dataset Shapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val:   {val_df.shape}\")\n",
    "print(f\"  Test:  {test_df.shape}\")\n",
    "\n",
    "\n",
    "importances[importances['feature'].isin(selected_features)].to_csv('feature_importance.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Feature Selection Completed Successfully!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"  TOP_K: {TOP_K}\")\n",
    "print(f\"  USE_RFECV: {USE_RFECV}\")\n",
    "print(f\"  CUMULATIVE_THRESHOLD: {CUMULATIVE_THRESHOLD}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "978aee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                        0\n",
       "BTC_obv                     0\n",
       "returns                     0\n",
       "returns_lag_3               0\n",
       "macd_signal_12_26_9         0\n",
       "btc_dominance               0\n",
       "BTC_BTC_Volume              0\n",
       "BTC_returns                 0\n",
       "volume_lag_3                0\n",
       "lido_lido_eth_tvl           0\n",
       "returns_sma_60              0\n",
       "returns_ema_7               0\n",
       "volume_sma_7                0\n",
       "returns_lag_1               0\n",
       "volume_lag_14               0\n",
       "BTC_returns_lag_14          0\n",
       "BTC_roc_20                  0\n",
       "BTC_macd_12_26              0\n",
       "btc_eth_correlation         0\n",
       "dxy_DXY                     0\n",
       "BTC_returns_lag_7           0\n",
       "volume_lag_7                0\n",
       "BTC_returns_ema_7           0\n",
       "BTC_returns_lag_1           0\n",
       "bnb_eth_ratio               0\n",
       "BTC_returns_lag_3           0\n",
       "BTC_momentum_10             0\n",
       "funding_fundingRate_lag1    0\n",
       "BTC_cci_20                  0\n",
       "volume_lag_1                0\n",
       "BTC_mfi_14                  0\n",
       "target_next_log_return      0\n",
       "target_direction            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d541cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
