{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac9fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 02:47:49.212656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-24 02:47:49.212704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-24 02:47:49.213954: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-24 02:47:49.221002: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-24 02:47:49.958718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 마지막 날짜: 2025-10-24 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-22 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-20 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "데이터 마지막 날짜: 2025-10-23 00:00:00\n",
      "\n",
      "설정된 end_date: 2025-10-20 00:00:00\n",
      "✓ 감성 지표 생성 완료: 25개 (date 제외)\n",
      "\n",
      "감성 지표 결측 처리:\n",
      "  sentiment_mean: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_std: 40개 → 0 (데이터 없음 = 중립)\n",
      "  news_count: 40개 → 0 (데이터 없음 = 중립)\n",
      "  positive_ratio: 40개 → 0 (데이터 없음 = 중립)\n",
      "  negative_ratio: 40개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_positive_count: 40개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_negative_count: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_sum: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_polarity: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_intensity: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_disagreement: 40개 → 0 (데이터 없음 = 중립)\n",
      "  bull_bear_ratio: 40개 → 0 (데이터 없음 = 중립)\n",
      "  weighted_sentiment: 40개 → 0 (데이터 없음 = 중립)\n",
      "  extremity_index: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma3: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_3: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma7: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_7: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma14: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_14: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_trend: 40개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_acceleration: 40개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_change: 40개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma7: 40개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma14: 40개 → 0 (데이터 없음 = 중립)\n",
      "\n",
      "외부 변수 FFill 처리:\n",
      "  3,123 → 673개 (FFill)\n",
      "\n",
      "Lookback 기간 제거:\n",
      "  1967 → 1967행\n",
      "\n",
      "초기 결측치 처리:\n",
      "  남은 결측: 673개 → 0\n",
      "\n",
      "✓ 최종 데이터: (1967, 96)\n",
      "  날짜: 2020-06-02 ~ 2025-10-20\n",
      "  결측: 0개\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 기본 라이브러리\n",
    "# ============================================================================\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# 데이터 전처리 및 Feature Engineering\n",
    "# ============================================================================\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE, \n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 시계열 분석\n",
    "# ============================================================================\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn ML 모델\n",
    "# ============================================================================\n",
    "# 선형 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 트리 기반 모델\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# 앙상블 모델\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Gradient Boosting 라이브러리\n",
    "# ============================================================================\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# ============================================================================\n",
    "# TabNet \n",
    "# ============================================================================\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"Warning: pytorch-tabnet not installed. TabNet models will be skipped.\")\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch (Optional)\n",
    "# ============================================================================\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"Warning: PyTorch not installed. Some models may not work.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn 평가 지표\n",
    "# ============================================================================\n",
    "# 분류 지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# 회귀 지표\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# TensorFlow/Keras 딥러닝\n",
    "# ============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    # 기본 레이어\n",
    "    Input, Dense, Flatten, Dropout, \n",
    "    \n",
    "    # RNN 레이어\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    \n",
    "    # CNN 레이어\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    \n",
    "    # 정규화 레이어\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    \n",
    "    # Attention 레이어\n",
    "    Attention, MultiHeadAttention,\n",
    "    \n",
    "    # 유틸리티 레이어\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed,\n",
    "    Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (LSTM, Bidirectional, GRU, Dense, Dropout, \n",
    "                                     BatchNormalization, Input, Conv1D, AveragePooling1D,\n",
    "                                     Concatenate, MultiHeadAttention, LayerNormalization,\n",
    "                                     GlobalAveragePooling1D, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, \n",
    "                              BaggingClassifier, GradientBoostingClassifier,\n",
    "                              StackingClassifier, VotingClassifier, AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 1. 날짜 파싱 및 CSV 로드 함수\n",
    "# ============================================================================ \n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    #print(df.shape)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    #print(df.shape)\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(filepath):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = standardize_date_column(df,filepath)\n",
    "    return df\n",
    "# ============================================================================ \n",
    "# 2. 데이터 로딩\n",
    "# ============================================================================ \n",
    "DATA_DIR = './macro_data'\n",
    "\n",
    "def load_from_macro_data(filename):\n",
    "    return load_and_standardize_data(os.path.join(DATA_DIR, filename))\n",
    "\n",
    "macro_df = load_from_macro_data('macro_crypto_data.csv')\n",
    "news_df = load_from_macro_data('news_data.csv')\n",
    "eth_onchain_df = load_from_macro_data('eth_onchain.csv')\n",
    "fear_greed_df = load_from_macro_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_from_macro_data('usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_from_macro_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_from_macro_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_from_macro_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_from_macro_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_from_macro_data('eth_funding_rate.csv')\n",
    "sp500_df = load_from_macro_data('SP500.csv')\n",
    "vix_df = load_from_macro_data('VIX.csv')\n",
    "gold_df = load_from_macro_data('GOLD.csv')\n",
    "dxy_df = load_from_macro_data('DXY.csv')\n",
    "\n",
    "# ============================================================================ \n",
    "# 3. 기준 날짜 설정 (Lido TVL 시작일 기준)\n",
    "# ============================================================================ \n",
    "train_start_date = pd.to_datetime('2020-12-19')\n",
    "lookback_start_date = train_start_date - timedelta(days=200)\n",
    "all_dataframes = [\n",
    "    macro_df, news_df, eth_onchain_df, fear_greed_df, \n",
    "    usdt_eth_mcap_df, aave_tvl_df, lido_tvl_df, makerdao_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "# 각 데이터프레임의 마지막 날짜 수집\n",
    "last_dates = []\n",
    "for df in all_dataframes:\n",
    "    if df is not None and not df.empty and 'date' in df.columns:\n",
    "        last_date = pd.to_datetime(df['date']).max()\n",
    "        last_dates.append(last_date)\n",
    "        print(f\"데이터 마지막 날짜: {last_date}\")\n",
    "\n",
    "# 가장 작은 마지막 날짜를 end_date로 설정\n",
    "end_date = min(last_dates)\n",
    "print(f\"\\n설정된 end_date: {end_date}\")\n",
    "\n",
    "# ============================================================================ \n",
    "# 4. 뉴스 감성 피처 생성 \n",
    "# ============================================================================ \n",
    "def create_sentiment_features(news_df):\n",
    "    \"\"\"\n",
    "    한국어 뉴스 감성 지표 생성\n",
    "    출처: \"Cryptocurrency Price Prediction Model Based on Sentiment Analysis\" (2024)\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        # ===== 기본 통계 =====\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        \n",
    "        # ===== 추가 지표 =====\n",
    "        # 1. 극단 감성 카운트\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        \n",
    "        # 2. 총 감성 점수\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    # ===== 파생 지표 계산 =====\n",
    "    \n",
    "    # 1. Sentiment Polarity \n",
    "    sentiment_agg['sentiment_polarity'] = (\n",
    "        sentiment_agg['positive_ratio'] - sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 2. Sentiment Intensity (감성 강도) \n",
    "    sentiment_agg['sentiment_intensity'] = (\n",
    "        sentiment_agg['positive_ratio'] + sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 3. Sentiment Disagreement \n",
    "    sentiment_agg['sentiment_disagreement'] = (\n",
    "        sentiment_agg['positive_ratio'] * sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 4. Bull/Bear Ratio \n",
    "    sentiment_agg['bull_bear_ratio'] = (\n",
    "        sentiment_agg['positive_ratio'] / (sentiment_agg['negative_ratio'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # 5. Weighted Sentiment \n",
    "    sentiment_agg['weighted_sentiment'] = (\n",
    "        sentiment_agg['sentiment_mean'] * np.log1p(sentiment_agg['news_count'])\n",
    "    )\n",
    "    \n",
    "    # 6. Extremity Index \n",
    "    sentiment_agg['extremity_index'] = (\n",
    "        (sentiment_agg['extreme_positive_count'] + sentiment_agg['extreme_negative_count']) / \n",
    "        (sentiment_agg['news_count'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # ===== 시계열 파생 지표 (이동 평균) =====\n",
    "    \n",
    "    for window in [3, 7, 14]:\n",
    "        # 감성 이동 평균\n",
    "        sentiment_agg[f'sentiment_ma{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # 감성 변동성 (이동 표준편차)\n",
    "        sentiment_agg[f'sentiment_volatility_{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # 7. Sentiment Trend \n",
    "    sentiment_agg['sentiment_trend'] = sentiment_agg['sentiment_mean'].diff()\n",
    "    \n",
    "    # 8. Sentiment Acceleration\n",
    "    sentiment_agg['sentiment_acceleration'] = sentiment_agg['sentiment_trend'].diff()\n",
    "    \n",
    "    # 9. News Volume Change\n",
    "    sentiment_agg['news_volume_change'] = sentiment_agg['news_count'].pct_change()\n",
    "    \n",
    "    # 10. News Volume MA \n",
    "    for window in [7, 14]:\n",
    "        sentiment_agg[f'news_volume_ma{window}'] = (\n",
    "            sentiment_agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ 감성 지표 생성 완료: {sentiment_agg.shape[1] - 1}개 (date 제외)\")\n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "sentiment_features = create_sentiment_features(news_df)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 5. 데이터 병합\n",
    "# ============================================================================ \n",
    "def add_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + col if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=lookback_start_date, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df, sentiment_features, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, eth_chain_tvl_df,\n",
    "    eth_funding_df, sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "# 1. 외부 데이터 Merge 후\n",
    "for df_to_merge in dataframes_to_merge:\n",
    "    df_merged = pd.merge(df_merged, df_to_merge, on='date', how='left')\n",
    "\n",
    "# 2. 감성 지표 결측 처리 (0)\n",
    "sentiment_cols = [col for col in df_merged.columns \n",
    "                 if any(x in col for x in ['sentiment', 'news', 'ext', 'bull_bear','positive','negative','extreme'])]\n",
    "\n",
    "print(f\"\\n감성 지표 결측 처리:\")\n",
    "for col in sentiment_cols:\n",
    "    missing_before = df_merged[col].isnull().sum()\n",
    "    if missing_before > 0:\n",
    "        df_merged[col] = df_merged[col].fillna(0)\n",
    "        print(f\"  {col}: {missing_before}개 → 0 (데이터 없음 = 중립)\")\n",
    "\n",
    "# 3. 외부 변수 FFill (bfill 절대 금지!)\n",
    "external_cols = [col for col in df_merged.columns \n",
    "                if any(x in col for x in ['eth_', 'fg_', 'usdt_', 'aave_', 'lido_', \n",
    "                                         'makerdao_', 'chain_', 'funding_',\n",
    "                                         'sp500_', 'vix_', 'gold_', 'dxy_'])]\n",
    "\n",
    "print(f\"\\n외부 변수 FFill 처리:\")\n",
    "missing_before = df_merged[external_cols].isnull().sum().sum()\n",
    "df_merged[external_cols] = df_merged[external_cols].fillna(method='ffill')\n",
    "missing_after = df_merged[external_cols].isnull().sum().sum()\n",
    "print(f\"  {missing_before:,} → {missing_after:,}개 (FFill)\")\n",
    "\n",
    "# 4. Lookback 기간 제거\n",
    "print(f\"\\nLookback 기간 제거:\")\n",
    "before = len(df_merged)\n",
    "df_merged = df_merged[df_merged['date'] >= lookback_start_date].reset_index(drop=True)\n",
    "print(f\"  {before} → {len(df_merged)}행\")\n",
    "\n",
    "remaining_missing = df_merged[external_cols].isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\n초기 결측치 처리:\")\n",
    "    print(f\"  남은 결측: {remaining_missing}개 → 0\")\n",
    "    df_merged[external_cols] = df_merged[external_cols].fillna(0)\n",
    "\n",
    "# 6. Lookback 기간 동안 모두 NaN인 컬럼 제거\n",
    "lookback_df = df_merged[df_merged['date'] < train_start_date]\n",
    "cols_to_drop = [col for col in lookback_df.columns \n",
    "               if lookback_df[col].isnull().all() and col != 'date']\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nLookback 기간 완전 결측 컬럼 제거:\")\n",
    "    print(f\"  {cols_to_drop}\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\n✓ 최종 데이터: {df_merged.shape}\")\n",
    "print(f\"  날짜: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"  결측: {df_merged.isnull().sum().sum()}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ffc3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    출처: \n",
    "    - \"CryptoPulse: Short-Term Cryptocurrency Forecasting\" (2024)\n",
    "    - \"Enhancing Price Prediction in Cryptocurrency Using Transformer\" (2024)\n",
    "    - \"Bitcoin Trend Prediction with Attention-Based Deep Learning\" (2024)\n",
    "    \"\"\"\n",
    "    #print(\"\\n=== 기술적 지표 생성 중 ===\")\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== [핵심] MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (필수)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        df_ta['RSI_30'] = ta.rsi(close, length=30)\n",
    "        df_ta['RSI_200'] = ta.rsi(close, length=200) \n",
    "        \n",
    "        # MACD (필수 - top feature importance)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic Oscillator (%K, %D - 논문에서 핵심 지표)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=30, d=3) \n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=200, d=3)  \n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (Rate of Change)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['ROC_20'] = ta.roc(close, length=20)\n",
    "        \n",
    "        # MOM (Momentum - 다양한 기간)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['MOM_30'] = ta.mom(close, length=30) \n",
    "        \n",
    "        # CCI (Commodity Channel Index)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_20'] = ta.cci(high, low, close, length=20)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_20'] > 100).astype(int)\n",
    "      \n",
    "        # TSI (True Strength Index)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "\n",
    "        \n",
    "        # =====  Ichimoku Cloud (암호화폐 트렌드 분석에 효과적) =====\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ ICHIMOKU 생성 실패\")\n",
    "\n",
    "        # ===== [핵심] OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (필수! - Golden/Death Cross)\n",
    "        df_ta['SMA_10'] = ta.sma(close, length=10)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['SMA_200'] = ta.sma(close, length=200)\n",
    "        \n",
    "        # EMA (필수!)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['EMA_50'] = ta.ema(close, length=50)\n",
    "        df_ta['EMA_200'] = ta.ema(close, length=200) \n",
    "        \n",
    "        # TEMA (Triple EMA - 논문에서 high importance)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['TEMA_30'] = ta.tema(close, length=30) \n",
    "        \n",
    "        # WMA (Weighted Moving Average)\n",
    "        df_ta['WMA_10'] = ta.wma(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)  \n",
    "        \n",
    "        # HMA (Hull Moving Average)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (Double EMA)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        \n",
    "        # VWMA (Volume Weighted)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== [핵심] VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands (필수 )\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        safe_add(df_ta, ta.bbands, close, length=50, std=2)  \n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_7'] = ta.atr(high, low, close, length=7)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['ATR_21'] = ta.atr(high, low, close, length=21) \n",
    "        \n",
    "        # NATR (Normalized ATR)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel \n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        # ===== [핵심] VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD (Accumulation/Distribution)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI (Money Flow Index)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF (Chaikin Money Flow - 논문에서 중요 지표)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Elder Force Index)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM (Ease of Movement)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP (Volume Weighted Average Price) \n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX \n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon \n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO (Detrended Price Oscillator)\n",
    "        try:\n",
    "            df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['PRICE_CHANGE_2'] = close.pct_change(periods=2)\n",
    "        df_ta['PRICE_CHANGE_5'] = close.pct_change(periods=5)\n",
    "        df_ta['PRICE_CHANGE_10'] = close.pct_change(periods=10) \n",
    "        \n",
    "        # 변동성 (Rolling Std)\n",
    "        df_ta['VOLATILITY_5'] = close.pct_change().rolling(window=5).std()\n",
    "        df_ta['VOLATILITY_10'] = close.pct_change().rolling(window=10).std()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['VOLATILITY_30'] = close.pct_change().rolling(window=30).std() \n",
    "        \n",
    "        # 모멘텀 (Price Ratio)\n",
    "        df_ta['MOMENTUM_5'] = close / close.shift(5) - 1\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['MOMENTUM_20'] = close / close.shift(20) - 1\n",
    "        df_ta['MOMENTUM_30'] = close / close.shift(30) - 1  \n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA10'] = close / df_ta['SMA_10'] - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_SMA50'] = close / df_ta['SMA_50'] - 1\n",
    "        df_ta['PRICE_VS_SMA200'] = close / df_ta['SMA_200'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1 \n",
    "        df_ta['PRICE_VS_EMA26'] = close / df_ta['EMA_26'] - 1  \n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_CROSS_SIGNAL'] = (df_ta['SMA_10'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_200']).astype(int) \n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)  \n",
    "        \n",
    "        # Range 지표\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치 \n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)  \n",
    "        \n",
    "        # Linear Regression Slope\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "            df_ta['SLOPE_10'] = ta.linreg(close, length=10, slope=True)\n",
    "            df_ta['LINREG_14'] = ta.linreg(close, length=14)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "            df_ta['SLOPE_10'] = close.rolling(window=10).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 10 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing/Decreasing 신호\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['DEC_1'] = (close < close.shift(1)).astype(int)\n",
    "        df_ta['INC_3'] = (close > close.shift(3)).astype(int)\n",
    "        df_ta['INC_5'] = (close > close.shift(5)).astype(int)  \n",
    "        \n",
    "        # BOP \n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 관련 파생\n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        else:\n",
    "            print(f\"    ⚠ Bollinger Bands 컬럼 미발견\")\n",
    "        \n",
    "        # RSI 파생 (Overbought/Oversold)\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile (상대적 거래량 강도)\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration (2차 미분)\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (시가-전일종가)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        # Fibonacci Pivots \n",
    "        high_20 = high.rolling(20).max()\n",
    "        low_20 = low.rolling(20).min()\n",
    "        diff = high_20 - low_20\n",
    "        \n",
    "        df_ta['FIB_0'] = high_20\n",
    "        df_ta['FIB_236'] = high_20 - 0.236 * diff\n",
    "        df_ta['FIB_382'] = high_20 - 0.382 * diff\n",
    "        df_ta['FIB_500'] = high_20 - 0.500 * diff\n",
    "        df_ta['FIB_618'] = high_20 - 0.618 * diff\n",
    "        df_ta['FIB_1'] = low_20\n",
    "        \n",
    "        #Directional Change Events \n",
    "        df_ta['DC_EVENT'] = 0\n",
    "        df_ta['DC_TYPE'] = 0\n",
    "        \n",
    "        threshold = 0.05\n",
    "        last_extreme = close.iloc[0]\n",
    "        last_type = 0\n",
    "        \n",
    "        for i in range(1, len(df_ta)):\n",
    "            price = close.iloc[i]\n",
    "            change = (price - last_extreme) / last_extreme\n",
    "            \n",
    "            if last_type <= 0 and change >= threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = 1\n",
    "                last_extreme = price\n",
    "                last_type = 1\n",
    "            elif last_type >= 0 and change <= -threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = -1\n",
    "                last_extreme = price\n",
    "                last_type = -1\n",
    "        \n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = (\n",
    "        df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "    )\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        if 'btc_log_return' not in df_transformed.columns:\n",
    "            df_transformed['btc_log_return'] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(1))\n",
    "        for period in [5, 10, 20, 30]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['btc_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['btc_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = (\n",
    "                    df_transformed[col_name].rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                    btc_volume.pct_change()\n",
    "                )\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (\n",
    "                eth_volume / (btc_volume + 1e-8)\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [\n",
    "        col for col in df_transformed.columns\n",
    "        if 'return' in col.lower() and 'next' not in col\n",
    "    ]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78da9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. Lag 적용\n",
    "# ============================================================================\n",
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = [\n",
    "        'sentiment_mean', 'sentiment_std', 'sentiment_sum',\n",
    "        'news_count', 'positive_ratio', 'negative_ratio',\n",
    "        'sentiment_polarity', 'sentiment_intensity', \n",
    "        'sentiment_disagreement', 'bull_bear_ratio',\n",
    "        'weighted_sentiment', 'extremity_index',\n",
    "        'extreme_positive_count', 'extreme_negative_count'\n",
    "    ]\n",
    "    \n",
    "    sentiment_ma_cols = [col for col in df.columns \n",
    "                         if 'sentiment' in col and ('_ma' in col or '_volatility_' in col)]\n",
    "    \n",
    "    no_lag_patterns = [\n",
    "        '_trend', '_acceleration', '_volume_change', \n",
    "        'news_volume_change', 'news_volume_ma'\n",
    "    ]\n",
    "    \n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', \n",
    "                                  'eth_large', 'eth_token', 'eth_contract',\n",
    "                                  'eth_avg_gas', 'eth_total_gas', \n",
    "                                  'eth_avg_block'])]\n",
    "    \n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col])\n",
    "    exclude_cols.extend([col for col in df.columns if '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            is_no_lag = any(pattern in col for pattern in no_lag_patterns)\n",
    "            if not is_no_lag:\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    \"\"\"\n",
    "    과거 가격을 피처로 추가 \n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    # 과거 종가 \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    # 과거 고가/저가\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "    \n",
    "    # 과거 거래량\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "    \n",
    "    # 과거 수익률\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    # 과거 가격 비율\n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    added = df_new.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 타겟 변수 생성\n",
    "# ============================================================================\n",
    "\n",
    "def create_targets(df):\n",
    "    \"\"\"타겟 변수 생성\"\"\"\n",
    "    df_target = df.copy()\n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "    df_target['next_direction'] = (next_close > next_open).astype(int)\n",
    "\n",
    "    df_target['next_open'] = next_open\n",
    "    df_target['next_close'] = next_close\n",
    "\n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393590d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_cyclic_features(df):\n",
    "    \"\"\"\n",
    "    시간 주기성 특징 추가 \n",
    "    \n",
    "    Reference:\n",
    "    - \"The Importance of Time-Based Cyclic Features\" (2025)\n",
    "    - \"Feature engineering for time-series data\" (Statsig, 2025)\n",
    "    \"\"\"\n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    # 기본 시간 특징\n",
    "    df_temporal['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df_temporal['day_of_month'] = df['date'].dt.day\n",
    "    df_temporal['month'] = df['date'].dt.month\n",
    "    df_temporal['quarter'] = df['date'].dt.quarter\n",
    "    df_temporal['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # 월말/월초 효과 \n",
    "    df_temporal['is_month_start'] = (df['date'].dt.is_month_start).astype(int)\n",
    "    df_temporal['is_month_end'] = (df['date'].dt.is_month_end).astype(int)\n",
    "    df_temporal['is_quarter_start'] = (df['date'].dt.is_quarter_start).astype(int)\n",
    "    df_temporal['is_quarter_end'] = (df['date'].dt.is_quarter_end).astype(int)\n",
    "    \n",
    "    # 주말 효과 \n",
    "    df_temporal['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical Encoding (Sine/Cosine for periodicity)\n",
    "    df_temporal['day_of_week_sin'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['day_of_week_cos'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['month_sin'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['month_cos'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['day_of_month_sin'] = np.sin(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    df_temporal['day_of_month_cos'] = np.cos(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    \n",
    "    added = df_temporal.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    \"\"\"\n",
    "    고차원 상호작용 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): Momentum × Volatility 상호작용 중요\n",
    "    - \"Causal Feature Engineering\" (2023): 특징 조합이 단일 특징보다 예측력 높음\n",
    "    \"\"\"\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    # 1. RSI × Volume\n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 2. Bollinger Band Position × Sentiment\n",
    "    if 'BB_POSITION' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['BB_Sentiment_Consensus'] = df['BB_POSITION'] * df['sentiment_polarity']\n",
    "    \n",
    "    # 3. VIX × ETH Volatility\n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    # 4. MACD × Volume\n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 5. BTC Return × ETH-BTC Correlation\n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    # 6. Sentiment × News Volume\n",
    "    if 'sentiment_polarity' in df.columns and 'news_count' in df.columns:\n",
    "        df_interact['Sentiment_Volume_Intensity'] = df['sentiment_polarity'] * np.log1p(df['news_count'])\n",
    "    \n",
    "    # 7. ATR × Volume Ratio\n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    # 8. RSI Overbought × High Volume\n",
    "    if 'RSI_OVERBOUGHT' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Overbought_High_Volume'] = df['RSI_OVERBOUGHT'] * (df['VOLUME_RATIO'] > 1.5).astype(int)\n",
    "    \n",
    "    # 9. Golden Cross × Positive Sentiment\n",
    "    if 'SMA_GOLDEN_CROSS' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['Golden_Sentiment_Align'] = df['SMA_GOLDEN_CROSS'] * (df['sentiment_polarity'] > 0).astype(int)\n",
    "    \n",
    "    # 10. Price Acceleration × Momentum\n",
    "    if 'PRICE_ACCELERATION' in df.columns and 'MOMENTUM_10' in df.columns:\n",
    "        df_interact['Acceleration_Momentum'] = df['PRICE_ACCELERATION'] * df['MOMENTUM_10']\n",
    "    \n",
    "    added = df_interact.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    \"\"\"\n",
    "    변동성 체제 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Intraday trading of cryptocurrencies\" (2023): 변동성 체제별 예측 정확도 차이 존재\n",
    "\n",
    "    \"\"\"\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        # 1. 고변동성 vs 저변동성 \n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        # 2. 변동성 급증 이벤트\n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        # 3. 변동성 백분위수\n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        # 4. 변동성 추세\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        \n",
    "        # 5. 변동성 체제 지속기간\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    added = df_regime.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    \"\"\"\n",
    "    정규화된 가격 Lag 특징 추가 (분류 모델용)\n",
    "    \n",
    "    Reference:\n",
    "    - \"Financial Forecasting with ML: Price vs Return\" (2021)\n",
    "    - 분류 문제에서 절대 가격보다 비율이 2-3배 더 예측력 높음\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        current_close = df['ETH_Close']\n",
    "    else:\n",
    "        return df_norm\n",
    "    \n",
    "    # 1. 가격 Lag를 현재 가격 대비 비율로 변환\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    # 2. High/Low Lag를 Close 대비 비율\n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    added = df_norm.shape[1] - df.shape[1]\n",
    "\n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_cumulative_streak_features(df):\n",
    "    \"\"\"\n",
    "    누적 및 연속 패턴 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Feature engineering for time-series\" (2025): 연속 패턴은 모멘텀 지속성 예측에 핵심\n",
    "    \"\"\"\n",
    "    df_cum = df.copy()\n",
    "    \n",
    "    if 'eth_log_return' in df.columns:\n",
    "        returns = df['eth_log_return']\n",
    "        \n",
    "        # 1. 연속 상승 일수\n",
    "        df_cum['consecutive_up_days'] = (returns > 0).astype(int).groupby(\n",
    "            (returns <= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 2. 연속 하락 일수\n",
    "        df_cum['consecutive_down_days'] = (returns < 0).astype(int).groupby(\n",
    "            (returns >= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 3. 최근 20일 내 최대 연속 상승\n",
    "        df_cum['max_consecutive_up_20d'] = df_cum['consecutive_up_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 4. 최근 20일 내 최대 연속 하락\n",
    "        df_cum['max_consecutive_down_20d'] = df_cum['consecutive_down_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 5. 누적 수익률 (20일)\n",
    "        df_cum['cumulative_return_20d'] = returns.rolling(20, min_periods=5).sum()\n",
    "        \n",
    "        # 6. 상승/하락 비율 (20일 내)\n",
    "        df_cum['up_down_ratio_20d'] = (\n",
    "            (returns > 0).rolling(20, min_periods=5).sum() / \n",
    "            ((returns < 0).rolling(20, min_periods=5).sum() + 1e-8)\n",
    "        )\n",
    "\n",
    "    added = df_cum.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_cum\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    \"\"\"\n",
    "\n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): 백분위수 특징이 상대적 위치 파악에 효과적\n",
    "    \"\"\"\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    # 1. 가격 백분위수 (250일)\n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 2. 거래량 백분위수 (90일)\n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 3. RSI 백분위수 (60일)\n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    added = df_pct.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff72206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=40):\n",
    "    \n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'return':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'price':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'direction_return':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Return (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        ret_features, ret_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + ret_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **ret_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'ret_stats': ret_stats,\n",
    "            'overlap': len(set(dir_features) & set(ret_features))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    elif target_type == 'direction_price':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Price (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        price_features, price_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + price_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **price_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'price_stats': price_stats,\n",
    "            'overlap': len(set(dir_features) & set(price_features))\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target_type: {target_type}\")\n",
    "    \n",
    "    print(\"Selected Features\")\n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=40, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "def split_tvt_method(df, train_start_date, test_start_date='2025-01-01', \n",
    "                     train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    test_start_date를 고정하고, 그 이전 데이터를 train/val로 분할\n",
    "    test_start_date 이후 데이터는 모두 test로 사용\n",
    "    \"\"\"\n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    \n",
    "    # 테스트 시작 날짜를 datetime으로 변환\n",
    "    if isinstance(test_start_date, str):\n",
    "        test_start_date = pd.to_datetime(test_start_date)\n",
    "    \n",
    "    # test_start_date 이전 데이터를 train/val로, 이후를 test로 분할\n",
    "    pre_test_df = df_period[df_period['date'] < test_start_date].copy()\n",
    "    test_df = df_period[df_period['date'] >= test_start_date].copy()\n",
    "    \n",
    "    # train/val 분할 (test 이전 데이터만 사용)\n",
    "    n_pre_test = len(pre_test_df)\n",
    "    train_end = int(n_pre_test * train_ratio / (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = pre_test_df.iloc[:train_end].copy()\n",
    "    val_df = pre_test_df.iloc[train_end:].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TVT Split (Fixed Test Start: {test_start_date.date()})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Train: {len(train_df):4d} ({train_df['date'].min().date()} ~ {train_df['date'].max().date()})\")\n",
    "    print(f\"  Val:   {len(val_df):4d} ({val_df['date'].min().date()} ~ {val_df['date'].max().date()})\")\n",
    "    print(f\"  Test:  {len(test_df):4d} ({test_df['date'].min().date()} ~ {test_df['date'].max().date()})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, \n",
    "                              final_test_start='2025-01-01',\n",
    "                              n_splits=6,\n",
    "                              initial_train_size=550,\n",
    "                              val_size=60,\n",
    "                              test_size=90,\n",
    "                              step=120,\n",
    "                              gap_size=15,\n",
    "                              lookback=30):\n",
    "\n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Walk-Forward Configuration (Production Mode)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total: {len(df_period)} days\")\n",
    "    print(f\"Pre-final: {len(pre_final_df)} days | Final holdout: {len(final_test_df)} days\")\n",
    "    print(f\"Gap: {gap_size} days | Step: {step} days\")\n",
    "    print(f\"Target: {n_splits} walk-forward + 1 final holdout\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        test_start_idx = initial_train_size + val_size + (gap_size * 2) + (fold_idx * step)\n",
    "        test_end_idx = test_start_idx + test_size\n",
    "        \n",
    "        if test_end_idx > len(pre_final_df):\n",
    "            break\n",
    "        \n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        train_end_idx = val_start_idx - gap_size\n",
    "        \n",
    "        if train_end_idx < initial_train_size:\n",
    "            continue\n",
    "        \n",
    "        train_fold = pre_final_df.iloc[:train_end_idx].copy()\n",
    "        val_fold = pre_final_df.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = pre_final_df.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        print(f\"Fold {fold_idx + 1} (walk_forward)\")\n",
    "        print(f\"  Train: {len(train_fold):4d}d  {train_fold['date'].min().date()} ~ {train_fold['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(val_fold):4d}d  {val_fold['date'].min().date()} ~ {val_fold['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(test_fold):4d}d  {test_fold['date'].min().date()} ~ {test_fold['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'fold_type': 'walk_forward'\n",
    "        })\n",
    "    \n",
    "    if len(final_test_df) > 0:\n",
    "        final_val_end_idx = len(pre_final_df)\n",
    "        final_val_start_idx = final_val_end_idx - val_size\n",
    "        final_train_end_idx = final_val_start_idx - gap_size\n",
    "        \n",
    "        final_train_data = pre_final_df.iloc[:final_train_end_idx].copy()\n",
    "        final_val_data = pre_final_df.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "        \n",
    "        print(f\"Fold {len(folds) + 1} (final_holdout)\")\n",
    "        print(f\"  Train: {len(final_train_data):4d}d  {final_train_data['date'].min().date()} ~ {final_train_data['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(final_val_data):4d}d  {final_val_data['date'].min().date()} ~ {final_val_data['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(final_test_df):4d}d  {final_test_df['date'].min().date()} ~ {final_test_df['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Created {len(folds)} folds total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "    \"\"\"\n",
    "    각 fold를 독립적으로 처리 (feature selection 포함)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    # Fold 정보 출력\n",
    "    if fold_idx is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Fold {fold_idx} ({fold_type})\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(),\n",
    "        train_start_date=train_df['date'].min(),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(),\n",
    "        train_start_date=val_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(),\n",
    "        train_start_date=test_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    \n",
    "    train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    print(f\"\\n[Feature Selection for Fold {fold_idx}]\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        target_type=target_type, \n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for this fold\")\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    print(f\"Scaling completed for Fold {fold_idx}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_df['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),  \n",
    "            'dates': test_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                     final_test_start='2025-01-01',\n",
    "                                     method='tvt', target_type='direction', **kwargs):\n",
    "    \"\"\"\n",
    "    전체 파이프라인 실행 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : DataFrame\n",
    "        원본 데이터\n",
    "    train_start_date : str\n",
    "        학습 데이터 시작 날짜\n",
    "    final_test_start : str, default='2025-01-01'\n",
    "        최종 고정 테스트 시작 날짜\n",
    "        - TVT: 이 날짜부터 마지막까지 테스트\n",
    "        - Walk-forward: 이 날짜 이전은 walk-forward folds, 이후는 final holdout\n",
    "    method : str, default='tvt'\n",
    "        'tvt' 또는 'walk_forward'\n",
    "    target_type : str, default='direction'\n",
    "        'direction', 'return', 'price', 'direction_return', 'direction_price'\n",
    "    **kwargs : dict\n",
    "        각 method에 필요한 추가 파라미터\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    df = create_targets(df)\n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_temporal_cyclic_features(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_cumulative_streak_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = remove_raw_prices_and_transform(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    df = df.iloc[:-1]  \n",
    "    \n",
    "    split_kwargs = {}\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        split_kwargs['test_start_date'] = final_test_start\n",
    "        if 'train_ratio' in kwargs:\n",
    "            split_kwargs['train_ratio'] = kwargs['train_ratio']\n",
    "        if 'val_ratio' in kwargs:\n",
    "            split_kwargs['val_ratio'] = kwargs['val_ratio']\n",
    "        splits = split_tvt_method(df, train_start_date, **split_kwargs)\n",
    "        \n",
    "    elif method == 'walk_forward':\n",
    "        split_kwargs['final_test_start'] = final_test_start\n",
    "        if 'n_splits' in kwargs:\n",
    "            split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "        if 'initial_train_size' in kwargs:\n",
    "            split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "        if 'test_size' in kwargs:\n",
    "            split_kwargs['test_size'] = kwargs['test_size']\n",
    "        if 'val_size' in kwargs:\n",
    "            split_kwargs['val_size'] = kwargs['val_size']\n",
    "        if 'step' in kwargs:\n",
    "            split_kwargs['step'] = kwargs['step']\n",
    "        if 'lookback' in kwargs:\n",
    "            split_kwargs['lookback'] = kwargs['lookback']\n",
    "        splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        result = process_single_split(\n",
    "            splits, \n",
    "            target_type=target_type,  \n",
    "            top_n=30,\n",
    "            fold_idx=1\n",
    "        )\n",
    "    else:\n",
    "        result = [\n",
    "            process_single_split(\n",
    "                fold, \n",
    "                target_type=target_type,  \n",
    "                top_n=30,\n",
    "                fold_idx=fold['fold_idx']\n",
    "            ) \n",
    "            for fold in splits\n",
    "        ]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736ad3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesAugmentation:\n",
    "    \"\"\"\n",
    "    시계열 데이터 증강을 위한 유틸리티 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def jittering(X, sigma=0.02):\n",
    "        \"\"\"\n",
    "        가우시안 노이즈 추가\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, sigma, X.shape)\n",
    "        return X + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaling(X, sigma=0.1):\n",
    "        \"\"\"\n",
    "        랜덤 스케일링 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], 1, X.shape[2]))\n",
    "        else:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], X.shape[1]))\n",
    "        return X * factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def magnitude_warping(X, sigma=0.2, num_knots=4):\n",
    "        \"\"\"\n",
    "        진폭 왜곡 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            seq_len = X.shape[1]\n",
    "            orig_steps = np.linspace(0, seq_len - 1, num_knots + 2)\n",
    "            random_warps = np.random.normal(1, sigma, size=(X.shape[0], num_knots + 2, X.shape[2]))\n",
    "            \n",
    "            warped_X = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    warper = np.interp(np.arange(seq_len), orig_steps, random_warps[i, :, j])\n",
    "                    warped_X[i, :, j] = X[i, :, j] * warper\n",
    "            return warped_X\n",
    "        else:\n",
    "            return X * np.random.normal(1, sigma, X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_augmentation(X, method='jittering', **kwargs):\n",
    "        \"\"\"\n",
    "        선택된 증강 기법 적용\n",
    "        \"\"\"\n",
    "        if method == 'jittering':\n",
    "            return TimeSeriesAugmentation.jittering(X, **kwargs)\n",
    "        elif method == 'scaling':\n",
    "            return TimeSeriesAugmentation.scaling(X, **kwargs)\n",
    "        elif method == 'magnitude_warping':\n",
    "            return TimeSeriesAugmentation.magnitude_warping(X, **kwargs)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "class DirectionModels:\n",
    "    \"\"\"\n",
    "    이더리움 가격 방향성 예측을 위한 머신러닝 모델 클래스\n",
    "    과적합 방지를 위한 정규화 기법 강화\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Random Forest 분류기 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [8, 12, 15],\n",
    "            'min_samples_split': [15, 20, 25],\n",
    "            'min_samples_leaf': [6, 8, 10],\n",
    "            'max_features': ['sqrt'],\n",
    "            'max_samples': [0.7, 0.8],\n",
    "            'max_leaf_nodes': [50, 100, 150]\n",
    "        }\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        LightGBM 분류기 with Optuna 하이퍼파라미터 최적화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 60),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 30, 50),\n",
    "                'max_bin': 128,\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            \n",
    "            model = LGBMClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[early_stopping(50, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        model = LGBMClassifier(**study.best_params, random_state=42, verbose=-1)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        XGBoost 분류기 with Optuna 하이퍼파라미터 최적화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 2.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 5, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 0.5),\n",
    "                'max_bin': 128,\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'logloss',\n",
    "                'tree_method': 'hist'\n",
    "            }\n",
    "            \n",
    "            model = XGBClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        model = XGBClassifier(**study.best_params, random_state=42, eval_metric='logloss')\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def svm(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Support Vector Machine 분류기\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'gamma': ['scale', 0.001, 0.01],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "        \n",
    "        model = SVC(random_state=42, probability=True, cache_size=1000)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        LSTM 네트워크 with 강화된 정규화 및 데이터 증강\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=32)\n",
    "            units2 = trial.suggest_int('units2', 16, 64, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.2, 0.4)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "            \n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "            \n",
    "            model = Sequential([\n",
    "                LSTM(\n",
    "                    units1,\n",
    "                    activation='tanh',\n",
    "                    return_sequences=True,\n",
    "                    input_shape=input_shape,\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LSTM(\n",
    "                    units2,\n",
    "                    activation='tanh',\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate,\n",
    "                    clipnorm=1.0\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_aug, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(\n",
    "                best_params['units1'],\n",
    "                activation='tanh',\n",
    "                return_sequences=True,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                best_params['units2'],\n",
    "                activation='tanh',\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        Bidirectional LSTM 네트워크 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=32)\n",
    "            units2 = trial.suggest_int('units2', 16, 64, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.2, 0.4)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "            \n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "            \n",
    "            model = Sequential([\n",
    "                Bidirectional(\n",
    "                    LSTM(\n",
    "                        units1,\n",
    "                        return_sequences=True,\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout\n",
    "                    ),\n",
    "                    input_shape=input_shape\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                Bidirectional(\n",
    "                    LSTM(\n",
    "                        units2,\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout\n",
    "                    )\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate,\n",
    "                    clipnorm=1.0\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_aug, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    best_params['units1'],\n",
    "                    return_sequences=True,\n",
    "                    kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                    recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                    dropout=best_params['dropout'],\n",
    "                    recurrent_dropout=best_params['recurrent_dropout']\n",
    "                ),\n",
    "                input_shape=input_shape\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    best_params['units2'],\n",
    "                    kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                    recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                    dropout=best_params['dropout'],\n",
    "                    recurrent_dropout=best_params['recurrent_dropout']\n",
    "                )\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        GRU 네트워크 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=32)\n",
    "            units2 = trial.suggest_int('units2', 16, 64, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.2, 0.4)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "            \n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "            \n",
    "            model = Sequential([\n",
    "                GRU(\n",
    "                    units1,\n",
    "                    activation='tanh',\n",
    "                    return_sequences=True,\n",
    "                    input_shape=input_shape,\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                GRU(\n",
    "                    units2,\n",
    "                    activation='tanh',\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate,\n",
    "                    clipnorm=1.0\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_aug, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            GRU(\n",
    "                best_params['units1'],\n",
    "                activation='tanh',\n",
    "                return_sequences=True,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            GRU(\n",
    "                best_params['units2'],\n",
    "                activation='tanh',\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacked_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        3-Layer Stacked LSTM 네트워크 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 48, 96, step=48)\n",
    "            units2 = trial.suggest_int('units2', 32, 64, step=32)\n",
    "            units3 = trial.suggest_int('units3', 16, 32, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.2, 0.4)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "            \n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "            \n",
    "            model = Sequential([\n",
    "                LSTM(\n",
    "                    units1,\n",
    "                    return_sequences=True,\n",
    "                    input_shape=input_shape,\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LSTM(\n",
    "                    units2,\n",
    "                    return_sequences=True,\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LSTM(\n",
    "                    units3,\n",
    "                    kernel_regularizer=l2(l2_reg),\n",
    "                    recurrent_regularizer=l2(l2_reg * 0.5),\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate,\n",
    "                    clipnorm=1.0\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_aug, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(\n",
    "                best_params['units1'],\n",
    "                return_sequences=True,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                best_params['units2'],\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                best_params['units3'],\n",
    "                kernel_regularizer=l2(best_params['l2_reg']),\n",
    "                recurrent_regularizer=l2(best_params['l2_reg'] * 0.5),\n",
    "                dropout=best_params['dropout'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tabnet(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        TabNet 분류기 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_d': trial.suggest_int('n_d', 16, 64, step=16),\n",
    "                'n_a': trial.suggest_int('n_a', 16, 64, step=16),\n",
    "                'n_steps': trial.suggest_int('n_steps', 3, 5),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 1.5),\n",
    "                'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-4, 1e-2, log=True),\n",
    "                'momentum': trial.suggest_float('momentum', 0.1, 0.4),\n",
    "                'clip_value': trial.suggest_float('clip_value', 0.5, 2.0),\n",
    "                'optimizer_params': dict(lr=trial.suggest_float('lr', 1e-3, 2e-2, log=True)),\n",
    "                'mask_type': 'entmax',\n",
    "                'n_independent': 2,\n",
    "                'n_shared': 2,\n",
    "                'scheduler_params': {\"step_size\": 50, \"gamma\": 0.9},\n",
    "                'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "                'verbose': 0,\n",
    "                'seed': 42\n",
    "            }\n",
    "            \n",
    "            model = TabNetClassifier(**param, optimizer_fn=torch.optim.Adam)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                max_epochs=80,\n",
    "                patience=10,\n",
    "                batch_size=512,\n",
    "                virtual_batch_size=256\n",
    "            )\n",
    "            \n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = TabNetClassifier(\n",
    "            n_d=best_params['n_d'],\n",
    "            n_a=best_params['n_a'],\n",
    "            n_steps=best_params['n_steps'],\n",
    "            gamma=best_params['gamma'],\n",
    "            lambda_sparse=best_params['lambda_sparse'],\n",
    "            momentum=best_params['momentum'],\n",
    "            clip_value=best_params['clip_value'],\n",
    "            optimizer_params=dict(lr=best_params['lr']),\n",
    "            mask_type='entmax',\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            verbose=0,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            max_epochs=80,\n",
    "            patience=10,\n",
    "            batch_size=512,\n",
    "            virtual_batch_size=256\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def vmd_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        VMD-Hybrid Transformer 모델 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.02))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(x)\n",
    "        low_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.02))(low_freq)\n",
    "        \n",
    "        mid_freq = x - low_freq\n",
    "        mid_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.02))(mid_freq)\n",
    "        \n",
    "        high_freq = x - low_freq - mid_freq\n",
    "        high_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.02))(high_freq)\n",
    "        \n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.2)(x, x)\n",
    "            attn = Dropout(0.2)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(96, activation='gelu', kernel_regularizer=l2(0.02))(x)\n",
    "            ff = Dropout(0.2)(ff)\n",
    "            ff = Dense(96, kernel_regularizer=l2(0.02))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.02))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Logistic Regression 분류기 with 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs', 'saga']\n",
    "        }\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def naive_bayes(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Gaussian Naive Bayes 분류기\n",
    "        \"\"\"\n",
    "        model = GaussianNB(var_smoothing=1e-8)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def knn(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        K-Nearest Neighbors 분류기\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_neighbors': [5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan'],\n",
    "            'leaf_size': [20, 30, 40]\n",
    "        }\n",
    "        \n",
    "        model = KNeighborsClassifier(n_jobs=-1)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        AdaBoost 분류기 with 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.5, 1.0, 1.5],\n",
    "            'algorithm': ['SAMME']\n",
    "        }\n",
    "        \n",
    "        model = AdaBoostClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        CatBoost 분류기 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'depth': trial.suggest_int('depth', 3, 6),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 10),\n",
    "                'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0.5, 2.0),\n",
    "                'random_seed': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "            \n",
    "            model = CatBoostClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_val, y_val),\n",
    "                early_stopping_rounds=30,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        model = CatBoostClassifier(**study.best_params, random_seed=42, verbose=False)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=30,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def decision_tree(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Decision Tree 분류기 with 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'max_depth': [8, 12, 15],\n",
    "            'min_samples_split': [15, 20, 25],\n",
    "            'min_samples_leaf': [6, 8, 10],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_leaf_nodes': [50, 100, 150]\n",
    "        }\n",
    "        \n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def extra_trees(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Extra Trees 분류기 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [8, 12, 15],\n",
    "            'min_samples_split': [15, 20, 25],\n",
    "            'min_samples_leaf': [6, 8, 10],\n",
    "            'max_features': ['sqrt'],\n",
    "            'max_samples': [0.7, 0.8],\n",
    "            'max_leaf_nodes': [50, 100, 150]\n",
    "        }\n",
    "        \n",
    "        model = ExtraTreesClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def bagging(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Bagging 분류기 with 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_samples': [0.7, 0.8, 0.9],\n",
    "            'max_features': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "        \n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            max_depth=12,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model = BaggingClassifier(\n",
    "            estimator=base_estimator,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Gradient Boosting 분류기 with 강화된 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'min_samples_split': [15, 20],\n",
    "            'min_samples_leaf': [6, 8],\n",
    "            'max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def mlp(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 192, step=64)\n",
    "            units2 = trial.suggest_int('units2', 32, 96, step=32)\n",
    "            units3 = trial.suggest_int('units3', 16, 64, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)\n",
    "            \n",
    "            input_dim = X_train.shape[1]\n",
    "            model = Sequential([\n",
    "                Dense(units1, activation='relu', input_dim=input_dim, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(units3, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate,\n",
    "                    clipnorm=1.0\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = Sequential([\n",
    "            Dense(best_params['units1'], activation='relu', input_dim=input_dim, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(best_params['units2'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(best_params['units3'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Stacking Ensemble with 강화된 정규화\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            rf_estimators = trial.suggest_int('rf_estimators', 50, 150)\n",
    "            rf_depth = trial.suggest_int('rf_depth', 5, 12)\n",
    "            xgb_estimators = trial.suggest_int('xgb_estimators', 50, 150)\n",
    "            xgb_depth = trial.suggest_int('xgb_depth', 3, 6)\n",
    "            lgbm_estimators = trial.suggest_int('lgbm_estimators', 50, 150)\n",
    "            lgbm_depth = trial.suggest_int('lgbm_depth', 3, 6)\n",
    "            \n",
    "            base_learners = [\n",
    "                ('rf', RandomForestClassifier(\n",
    "                    n_estimators=rf_estimators,\n",
    "                    max_depth=rf_depth,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=8,\n",
    "                    max_samples=0.8,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_estimators,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.5,\n",
    "                    reg_lambda=1.0,\n",
    "                    random_state=42\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=lgbm_estimators,\n",
    "                    max_depth=lgbm_depth,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.5,\n",
    "                    reg_lambda=0.5,\n",
    "                    random_state=42,\n",
    "                    verbose=-1\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            meta_learner = LogisticRegression(max_iter=1000, C=0.1, random_state=42)\n",
    "            model = StackingClassifier(\n",
    "                estimators=base_learners,\n",
    "                final_estimator=meta_learner,\n",
    "                cv=3,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=3,\n",
    "                n_warmup_steps=5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_learners = [\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=best_params['rf_estimators'],\n",
    "                max_depth=best_params['rf_depth'],\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=8,\n",
    "                max_samples=0.8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_estimators'],\n",
    "                max_depth=best_params['xgb_depth'],\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=best_params['lgbm_estimators'],\n",
    "                max_depth=best_params['lgbm_depth'],\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                reg_lambda=0.5,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        meta_learner = LogisticRegression(max_iter=1000, C=0.1, random_state=42)\n",
    "        model = StackingClassifier(\n",
    "            estimators=base_learners,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def voting_hard(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Hard Voting Ensemble with 강화된 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'rf__n_estimators': [50, 100],\n",
    "            'rf__max_depth': [8, 12],\n",
    "            'xgb__n_estimators': [50, 100],\n",
    "            'xgb__max_depth': [4, 6]\n",
    "        }\n",
    "        \n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=8,\n",
    "                max_samples=0.8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        model = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def voting_soft(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Soft Voting Ensemble with 강화된 정규화\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'rf__n_estimators': [50, 100],\n",
    "            'rf__max_depth': [8, 12],\n",
    "            'xgb__n_estimators': [50, 100],\n",
    "            'xgb__max_depth': [4, 6]\n",
    "        }\n",
    "        \n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=8,\n",
    "                max_samples=0.8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )),\n",
    "            ('lr', LogisticRegression(max_iter=1000, C=0.1, random_state=42))\n",
    "        ]\n",
    "        \n",
    "        model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def dtw_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        DTW-LSTM with 강화된 정규화\n",
    "        \"\"\"\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(\n",
    "                96,\n",
    "                return_sequences=True,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                32,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def emd_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        EMD-LSTM with 강화된 정규화\n",
    "        \"\"\"\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        inputs = Input(shape=input_shape)\n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        low_freq = LSTM(\n",
    "            48,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(low_freq)\n",
    "        \n",
    "        high_freq = inputs - AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        high_freq = LSTM(\n",
    "            48,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(high_freq)\n",
    "        \n",
    "        x = Concatenate()([low_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = LSTM(\n",
    "            64,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = LSTM(\n",
    "            32,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(16, activation='relu', kernel_regularizer=l2(0.02))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def hybrid_lstm_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        Hybrid LSTM-GRU with 강화된 정규화\n",
    "        \"\"\"\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(\n",
    "                96,\n",
    "                return_sequences=True,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            GRU(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            LSTM(\n",
    "                48,\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            GRU(\n",
    "                32,\n",
    "                kernel_regularizer=l2(0.02),\n",
    "                recurrent_regularizer=l2(0.01),\n",
    "                dropout=0.3,\n",
    "                recurrent_dropout=0.3\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def residual_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        Residual LSTM with 강화된 정규화\n",
    "        \"\"\"\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.02)\n",
    "        \n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(\n",
    "            96,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        lstm_out = LSTM(\n",
    "            96,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        x = Add()([x, lstm_out])\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = LSTM(\n",
    "            48,\n",
    "            kernel_regularizer=l2(0.02),\n",
    "            recurrent_regularizer=l2(0.01),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(16, activation='relu', kernel_regularizer=l2(0.02))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_aug, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=60,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e031051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"모델 평가 및 백테스팅 (Task별 전략 구현)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.predictions={}\n",
    "\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for i, p in enumerate(pred):\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "        \n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                     X_test, y_test, test_returns, test_dates, model_name,\n",
    "                                     is_deep_learning=False):\n",
    "        \n",
    "        train_pred = self._predict_model(model, X_train)\n",
    "        val_pred = self._predict_model(model, X_val)\n",
    "        test_pred = self._predict_model(model, X_test)\n",
    "        \n",
    "        # ===== 확률값 추출 =====\n",
    "        test_pred_proba = None\n",
    "        if is_deep_learning:\n",
    "            test_pred_proba = test_pred.copy()\n",
    "            # 멀티태스크: 분류 output만 선택\n",
    "            if isinstance(train_pred, list):\n",
    "                train_pred = train_pred[0]\n",
    "                val_pred = val_pred[0]\n",
    "                test_pred = test_pred[0]\n",
    "                test_pred_proba = test_pred_proba[0] if isinstance(test_pred_proba, list) else test_pred_proba\n",
    "            train_pred = (train_pred > 0.5).astype(int).ravel()\n",
    "            val_pred = (val_pred > 0.5).astype(int).ravel()\n",
    "            test_pred = (test_pred > 0.5).astype(int).ravel()\n",
    "        else:\n",
    "            # ML 모델 확률값 추출\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # 분류 지표\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        \n",
    "        test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "        test_rec = recall_score(y_test, test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, test_pred)\n",
    "        \n",
    "        # ===== 예측값 저장 =====\n",
    "        self._save_predictions(\n",
    "            model_name, test_pred, test_pred_proba,\n",
    "            y_test, test_returns, test_dates\n",
    "        )\n",
    "\n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_acc,\n",
    "            'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc,\n",
    "            'Test_Precision': test_prec,\n",
    "            'Test_Recall': test_rec,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        return self.results[-1]\n",
    "\n",
    "\n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba,\n",
    "                         actual_direction, actual_returns, dates):\n",
    "        \n",
    "        # 확률값 처리\n",
    "        if pred_proba is not None:\n",
    "            if pred_proba.ndim == 2 and pred_proba.shape[1] == 2:\n",
    "                # Binary classification: [P(down), P(up)]\n",
    "                pred_proba_up = pred_proba[:, 1]\n",
    "                pred_proba_down = pred_proba[:, 0]\n",
    "            else:\n",
    "                # Single output (sigmoid)\n",
    "                pred_proba_up = pred_proba.ravel()\n",
    "                pred_proba_down = 1 - pred_proba_up\n",
    "        else:\n",
    "            # 확률 미지원: 0.9/0.1 근사\n",
    "            pred_proba_up = np.where(pred_direction == 1, 0.9, 0.1)\n",
    "            pred_proba_down = 1 - pred_proba_up\n",
    "        \n",
    "        # 신뢰도 계산\n",
    "        confidence = np.abs(pred_proba_up - 0.5) * 2\n",
    "        max_proba = np.maximum(pred_proba_up, pred_proba_down)\n",
    "        \n",
    "        # DataFrame 생성\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': actual_direction,\n",
    "            'actual_return': actual_returns,\n",
    "            'pred_direction': pred_direction,\n",
    "            'pred_proba_up': pred_proba_up,\n",
    "            'pred_proba_down': pred_proba_down,\n",
    "            'confidence': confidence,\n",
    "            'max_proba': max_proba,\n",
    "            'correct': (pred_direction == actual_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        self.predictions[model_name] = predictions_df\n",
    "\n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_predictions_dict(self):\n",
    "        return self.predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'SVM', 'func': DirectionModels.svm, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    {'index': 6, 'name': 'NaiveBayes', 'func': DirectionModels.naive_bayes, 'needs_val': True},\n",
    "    {'index': 7, 'name': 'KNN', 'func': DirectionModels.knn, 'needs_val': True},\n",
    "    {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 10, 'name': 'DecisionTree', 'func': DirectionModels.decision_tree, 'needs_val': True},\n",
    "    {'index': 11, 'name': 'ExtraTrees', 'func': DirectionModels.extra_trees, 'needs_val': True},\n",
    "    {'index': 12, 'name': 'Bagging', 'func': DirectionModels.bagging, 'needs_val': True},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 14, 'name': 'TabNet', 'func': DirectionModels.tabnet, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "    {'index': 16, 'name': 'VotingHard', 'func': DirectionModels.voting_hard, 'needs_val': True},\n",
    "    {'index': 17, 'name': 'VotingSoft', 'func': DirectionModels.voting_soft, 'needs_val': True},\n",
    "    {'index': 18, 'name': 'MLP', 'func': DirectionModels.mlp, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# DL Models (8 models - 55% threshold)\n",
    "# ============================================================================\n",
    "\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True},\n",
    "    # {'index': 22, 'name': 'Stacked_LSTM', 'func': DirectionModels.stacked_lstm, 'needs_val': True},\n",
    "    # {'index': 23, 'name': 'CNN_LSTM', 'func': DirectionModels.cnn_lstm, 'needs_val': True},\n",
    "    # {'index': 24, 'name': 'CNN_GRU', 'func': DirectionModels.cnn_gru, 'needs_val': True},\n",
    "    # {'index': 25, 'name': 'CNN_BiLSTM', 'func': DirectionModels.cnn_bilstm, 'needs_val': True},\n",
    "    # {'index': 26, 'name': 'LSTM_Attention', 'func': DirectionModels.lstm_attention, 'needs_val': True},\n",
    "    # {'index': 27, 'name': 'Transformer', 'func': DirectionModels.transformer, 'needs_val': True},\n",
    "    # {'index': 28, 'name': 'TCN', 'func': DirectionModels.tcn, 'needs_val': True},\n",
    "    {'index': 29, 'name': 'DTW_LSTM', 'func': DirectionModels.dtw_lstm, 'needs_val': True},\n",
    "    # {'index': 30, 'name': 'Informer', 'func': DirectionModels.informer, 'needs_val': True},\n",
    "    # {'index': 31, 'name': 'NBEATS', 'func': DirectionModels.nbeats, 'needs_val': True},\n",
    "    # {'index': 32, 'name': 'TFT', 'func': DirectionModels.temporal_fusion_transformer, 'needs_val': True},\n",
    "    # {'index': 33, 'name': 'Performer', 'func': DirectionModels.performer, 'needs_val': True},\n",
    "    # {'index': 34, 'name': 'PatchTST', 'func': DirectionModels.patchtst, 'needs_val': True},\n",
    "    # {'index': 35, 'name': 'Autoformer', 'func': DirectionModels.autoformer, 'needs_val': True},\n",
    "    # {'index': 36, 'name': 'iTransformer', 'func': DirectionModels.itransformer, 'needs_val': True},\n",
    "    # {'index': 37, 'name': 'EtherVoyant', 'func': DirectionModels.ethervoyant, 'needs_val': True},\n",
    "    {'index': 38, 'name': 'VMD_Hybrid', 'func': DirectionModels.vmd_hybrid, 'needs_val': True},\n",
    "    # {'index': 39, 'name': 'SimpleRNN', 'func': DirectionModels.simple_rnn, 'needs_val': True},\n",
    "    {'index': 40, 'name': 'EMD_LSTM', 'func': DirectionModels.emd_lstm, 'needs_val': True},\n",
    "    {'index': 41, 'name': 'Hybrid_LSTM_GRU', 'func': DirectionModels.hybrid_lstm_gru, 'needs_val': True},\n",
    "    # {'index': 42, 'name': 'Parallel_CNN', 'func': DirectionModels.parallel_cnn, 'needs_val': True},\n",
    "    # {'index': 43, 'name': 'LSTM_XGBoost_Hybrid', 'func': DirectionModels.lstm_xgboost_hybrid, 'needs_val': True},\n",
    "    {'index': 44, 'name': 'Residual_LSTM', 'func': DirectionModels.residual_lstm, 'needs_val': True},\n",
    "    # {'index': 45, 'name': 'WaveNet', 'func': DirectionModels.wavenet, 'needs_val': True},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b27d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator, lookback=30,\n",
    "                    ml_models=None, dl_models=None, task='classification'):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{task.capitalize()} 모델 학습 시작 (총 {len(ml_models) + len(dl_models)}개 모델)\")\n",
    "    print(\"=\"*80)\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    # ML 모델\n",
    "    print(f\"\\n[Part 1/2] Machine Learning 모델 ({len(ml_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    ml_success_count = 0\n",
    "    for model_config in ml_models:\n",
    "        success = trainer.train_ml_model(\n",
    "            model_config, X_train, y_train, X_val, y_val,\n",
    "            X_test, y_test, test_returns, test_dates, task=task\n",
    "        )\n",
    "        if success:\n",
    "            ml_success_count += 1\n",
    "    print(f\"\\n✓ ML 모델 완료: {ml_success_count}/{len(ml_models)}개 성공\")\n",
    "\n",
    "    # DL 모델\n",
    "    print(f\"\\n[Part 2/2] Deep Learning/시계열 모델 ({len(dl_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\n시퀀스 데이터 생성 중 (lookback={lookback})...\")\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test, lookback)\n",
    "    test_returns_seq = test_returns[lookback:]\n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    print(f\"  ✓ Train shape: {X_train_seq.shape}\")\n",
    "    print(f\"  ✓ Val shape: {X_val_seq.shape}\")\n",
    "    print(f\"  ✓ Test shape: {X_test_seq.shape}\")\n",
    "    print(f\"  ✓ Input shape: {input_shape}\\n\")\n",
    "    dl_success_count = 0\n",
    "    for model_config in dl_models:\n",
    "        if model_config['name'] in ['TabNet', 'TabNet_Reg', 'Ensemble_Stacking', 'Ensemble_Voting']:\n",
    "            success = trainer.train_ml_model(\n",
    "                model_config, X_train, y_train, X_val, y_val,\n",
    "                X_test, y_test, test_returns, test_dates, task=task\n",
    "            )\n",
    "        else:\n",
    "            if 'outputs' in model_config and len(model_config['outputs']) > 1:\n",
    "                y_train_list = [y_train_seq[:, i] for i in range(y_train_seq.shape[1])]\n",
    "                y_val_list = [y_val_seq[:, i] for i in range(y_val_seq.shape[1])]\n",
    "                y_test_list = [y_test_seq[:, i] for i in range(y_test_seq.shape[1])]\n",
    "                success = trainer.train_dl_multitask_model(\n",
    "                    model_config, X_train_seq, y_train_list, X_val_seq, y_val_list,\n",
    "                    X_test_seq, y_test_list, test_returns_seq, test_dates_seq, input_shape\n",
    "                )\n",
    "            else:\n",
    "                success = trainer.train_dl_model(\n",
    "                    model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq, input_shape, task=task\n",
    "                )\n",
    "        if success:\n",
    "            dl_success_count += 1\n",
    "    print(f\"\\n✓ DL 모델 완료: {dl_success_count}/{len(dl_models)}개 성공\")\n",
    "    total_success = ml_success_count + dl_success_count\n",
    "    total_models = len(ml_models) + len(dl_models)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"전체 학습 완료: {total_success}/{total_models}개 모델 성공\")\n",
    "    print(\"=\"*80)\n",
    "    return total_success\n",
    "\n",
    "def train_models_for_fold(fold_idx, X_train, y_train, X_val, y_val,\n",
    "                          X_test, y_test, test_returns, test_dates,\n",
    "                          evaluator, all_fold_results, lookback=30,\n",
    "                          ml_models=None, dl_models=None, task='classification'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Fold {fold_idx + 1} - {task.capitalize()} 모델 학습\")\n",
    "    print(f\"{'='*80}\")\n",
    "    success_count = train_all_models(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        test_returns, test_dates, evaluator, lookback,\n",
    "        ml_models=ml_models, dl_models=dl_models, task=task\n",
    "    )\n",
    "    fold_summary = evaluator.get_summary_dataframe()\n",
    "    fold_summary['Fold'] = fold_idx + 1\n",
    "    all_fold_results.append(fold_summary)\n",
    "    print(f\"\\n✓ Fold {fold_idx + 1} 완료 ({success_count}개 모델)\")\n",
    "    return fold_summary\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"모델 학습 및 평가를 위한 통합 클래스 (분류/회귀 공통)\"\"\"\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            # DataFrame이면 .iloc, array면 직접 인덱싱\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val,\n",
    "                       X_test, y_test, test_returns, test_dates, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "        \n",
    "            # 평가\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name'],\n",
    "                    is_deep_learning=is_mlp  \n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name'],\n",
    "                    is_deep_learning=is_mlp  \n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "\n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq,\n",
    "                       X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "                       test_returns_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            model = model_config['func'](\n",
    "                X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape\n",
    "            )\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539ba23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: model_results/2025-10-24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {RESULT_DIR}\")\n",
    "\n",
    "target_cases = [\n",
    "    {'name': 'direction', 'target_type': 'direction', 'outputs': ['next_direction']}\n",
    "]\n",
    "\n",
    "split_methods = [\n",
    "    {'name': 'walk_forward', 'method': 'walk_forward'},\n",
    "    {'name': 'tvt', 'method': 'tvt'}\n",
    "]\n",
    "\n",
    "\n",
    "def save_walk_forward_results(all_fold_results, all_fold_predictions, target_name, task):\n",
    "    \"\"\"\n",
    "    Walk-Forward 결과 저장 (walk-forward vs final holdout 분리)\n",
    "    \n",
    "    Args:\n",
    "        all_fold_results: [(fold_df, fold_type), ...] 형태로 수정\n",
    "        all_fold_predictions: [(fold_pred_dict, fold_type), ...] 형태로 수정\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Detailed results (모든 fold 포함)\n",
    "    detailed_results = []\n",
    "    for fold_idx, (fold_df, fold_type) in enumerate(all_fold_results, start=1):\n",
    "        fold_df_copy = fold_df.copy()\n",
    "        fold_df_copy.insert(0, 'Fold', fold_idx)\n",
    "        fold_df_copy.insert(1, 'fold_type', fold_type) \n",
    "        detailed_results.append(fold_df_copy)\n",
    "    \n",
    "    detailed_df = pd.concat(detailed_results, ignore_index=True)\n",
    "    \n",
    "    if 'Test_Accuracy' in detailed_df.columns:\n",
    "        detailed_df = detailed_df.sort_values(\n",
    "            by=['Fold', 'Test_Accuracy'], \n",
    "            ascending=[True, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    detailed_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward___detailed.csv\")\n",
    "    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {detailed_path}\")\n",
    "    \n",
    "    # 2. Walk-forward vs Final holdout 분리\n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward'].copy()\n",
    "    final_data = detailed_df[detailed_df['fold_type'] == 'final_holdout'].copy()\n",
    "    \n",
    "    numeric_cols = detailed_df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'Fold']\n",
    "    \n",
    "    # 3. Walk-forward 평균 계산 (모델 선택용)\n",
    "    avg_results = []\n",
    "    for model in detailed_df['Model'].unique():\n",
    "        avg_row = {'Model': model}\n",
    "        \n",
    "        # Walk-forward 평균 및 표준편차\n",
    "        model_wf = wf_data[wf_data['Model'] == model]\n",
    "        if len(model_wf) > 0:\n",
    "            for col in numeric_cols:\n",
    "                if col in model_wf.columns:\n",
    "                    avg_row[f'WF_{col}_Mean'] = model_wf[col].mean()\n",
    "                    avg_row[f'WF_{col}_Std'] = model_wf[col].std()\n",
    "        \n",
    "        # Final holdout 성능 (보고용)\n",
    "        model_final = final_data[final_data['Model'] == model]\n",
    "        if len(model_final) > 0:\n",
    "            for col in numeric_cols:\n",
    "                if col in model_final.columns:\n",
    "                    avg_row[f'Final_{col}'] = model_final[col].iloc[0]\n",
    "        \n",
    "        avg_results.append(avg_row)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_results)\n",
    "    \n",
    "    # 4. Walk-forward 평균으로 정렬 (모델 선택 기준) \n",
    "    if 'WF_Test_Accuracy_Mean' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='WF_Test_Accuracy_Mean', ascending=False).reset_index(drop=True)\n",
    "    elif 'WF_Test_RMSE_Mean' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='WF_Test_RMSE_Mean', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    avg_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward___avg.csv\")\n",
    "    avg_df.to_csv(avg_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {avg_path}\")\n",
    "    \n",
    "    # 5. 성능 요약 출력\n",
    "    if len(avg_df) > 0:\n",
    "        best_model = avg_df.iloc[0]['Model']\n",
    "        print(f\"\\n성능 요약 (Best Model: {best_model}):\")\n",
    "        if 'WF_Test_Accuracy_Mean' in avg_df.columns:\n",
    "            wf_score = avg_df.iloc[0]['WF_Test_Accuracy_Mean']\n",
    "            final_score = avg_df.iloc[0].get('Final_Test_Accuracy', 'N/A')\n",
    "            print(f\"  WF 평균 (2022-2024): {wf_score:.2f}%\")\n",
    "            print(f\"  Final Holdout (2025): {final_score}%\")\n",
    "    \n",
    "    # 6. 예측값 저장 (fold_type 구분) \n",
    "    if all_fold_predictions:\n",
    "        pred_dir = os.path.join(RESULT_DIR, \"predictions\", f\"{target_name}_walk_forward\")\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        all_models = set()\n",
    "        for fold_pred, _ in all_fold_predictions:  \n",
    "            all_models.update(fold_pred.keys())\n",
    "        \n",
    "        for model_name in all_models:\n",
    "            combined_predictions = []\n",
    "            \n",
    "            for fold_idx, (fold_pred, fold_type) in enumerate(all_fold_predictions, start=1):\n",
    "                if model_name in fold_pred:\n",
    "                    fold_df = fold_pred[model_name].copy()\n",
    "                    fold_df.insert(0, 'fold', fold_idx)\n",
    "                    fold_df.insert(1, 'fold_type', fold_type)  \n",
    "                    combined_predictions.append(fold_df)\n",
    "            \n",
    "            if combined_predictions:\n",
    "                combined_df = pd.concat(combined_predictions, ignore_index=True)\n",
    "                pred_filename = f\"{model_name}_all_folds.csv\"\n",
    "                pred_path = os.path.join(pred_dir, pred_filename)\n",
    "                combined_df.to_csv(pred_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Saved {len(all_models)} combined prediction files to {pred_dir}\")\n",
    "    \n",
    "    return detailed_df, avg_df\n",
    "\n",
    "\n",
    "def save_summary_csv(summary_df, predictions_dict, target_name, split_name, task):\n",
    "    \"\"\"\n",
    "    모델 평가 지표 + 예측값 저장\n",
    "    \n",
    "    Args:\n",
    "        predictions_dict: {model_name: predictions_df} 딕셔너리 추가\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 평가 지표 저장\n",
    "    if task == 'classification':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', \n",
    "                       'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_AUC_ROC']\n",
    "        \n",
    "    elif task == 'regression':\n",
    "        metric_cols = ['Model', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Train_MAE', 'Val_MAE', 'Test_MAE', 'Test_R2', 'Test_MAPE', 'Direction_Accuracy']\n",
    "                         \n",
    "    elif task == 'multitask':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', 'Test_Precision', \n",
    "                       'Test_Recall', 'Test_F1', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Test_MAE', 'Test_R2', 'Direction_Accuracy']\n",
    "    \n",
    "    # 기존 지표 저장\n",
    "    if task == 'classification':\n",
    "        available_cols = [col for col in metric_cols if col in summary_df.columns]\n",
    "    else:\n",
    "        # backtest_cols가 정의되어 있다면 추가\n",
    "        backtest_cols = []  # 필요시 정의\n",
    "        available_cols = [col for col in metric_cols + backtest_cols if col in summary_df.columns]\n",
    "    \n",
    "    save_df = summary_df[available_cols]\n",
    "    \n",
    "    if 'Test_Accuracy' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_Accuracy', ascending=False).reset_index(drop=True)\n",
    "    elif 'Test_RMSE' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_RMSE', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    filename = f\"{target_name}_{split_name}__metrics.csv\"\n",
    "    file_path = os.path.join(RESULT_DIR, filename)\n",
    "    save_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved metrics: {file_path}\")\n",
    "    \n",
    "    # 2. 예측값 저장\n",
    "    if predictions_dict:\n",
    "        pred_dir = os.path.join(RESULT_DIR, \"predictions\", f\"{target_name}_{split_name}\")\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name, pred_df in predictions_dict.items():\n",
    "            pred_filename = f\"{model_name}.csv\"\n",
    "            pred_path = os.path.join(pred_dir, pred_filename)\n",
    "            pred_df.to_csv(pred_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Saved {len(predictions_dict)} prediction files to {pred_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3263ec9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Experiment: direction x walk_forward\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Walk-Forward Configuration (Production Mode)\n",
      "================================================================================\n",
      "Total: 1766 days\n",
      "Pre-final: 1474 days | Final holdout: 292 days\n",
      "Gap: 15 days | Step: 120 days\n",
      "Target: 6 walk-forward + 1 final holdout\n",
      "================================================================================\n",
      "\n",
      "Fold 1 (walk_forward)\n",
      "  Train:  550d  2020-12-19 ~ 2022-06-21\n",
      "  Val:     60d  2022-07-07 ~ 2022-09-04\n",
      "  Test:    90d  2022-09-20 ~ 2022-12-18\n",
      "\n",
      "Fold 2 (walk_forward)\n",
      "  Train:  670d  2020-12-19 ~ 2022-10-19\n",
      "  Val:     60d  2022-11-04 ~ 2023-01-02\n",
      "  Test:    90d  2023-01-18 ~ 2023-04-17\n",
      "\n",
      "Fold 3 (walk_forward)\n",
      "  Train:  790d  2020-12-19 ~ 2023-02-16\n",
      "  Val:     60d  2023-03-04 ~ 2023-05-02\n",
      "  Test:    90d  2023-05-18 ~ 2023-08-15\n",
      "\n",
      "Fold 4 (walk_forward)\n",
      "  Train:  910d  2020-12-19 ~ 2023-06-16\n",
      "  Val:     60d  2023-07-02 ~ 2023-08-30\n",
      "  Test:    90d  2023-09-15 ~ 2023-12-13\n",
      "\n",
      "Fold 5 (walk_forward)\n",
      "  Train: 1030d  2020-12-19 ~ 2023-10-14\n",
      "  Val:     60d  2023-10-30 ~ 2023-12-28\n",
      "  Test:    90d  2024-01-13 ~ 2024-04-11\n",
      "\n",
      "Fold 6 (walk_forward)\n",
      "  Train: 1150d  2020-12-19 ~ 2024-02-11\n",
      "  Val:     60d  2024-02-27 ~ 2024-04-26\n",
      "  Test:    90d  2024-05-12 ~ 2024-08-09\n",
      "\n",
      "Fold 7 (final_holdout)\n",
      "  Train: 1399d  2020-12-19 ~ 2024-10-17\n",
      "  Val:     60d  2024-11-02 ~ 2024-12-31\n",
      "  Test:   292d  2025-01-01 ~ 2025-10-19\n",
      "\n",
      "================================================================================\n",
      "Created 7 folds total\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 1 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 1]\n",
      "Training data shape: (550, 355)\n",
      "Selected Features\n",
      "DPO_20, VOLUME_STRENGTH, sentiment_ma3_lag1, btc_return_lag10, VOLUME_CHANGE, day_of_month, return_lag2, HIGH_LOW_RANGE, high_lag3_ratio, volume_lag3, SLOPE_5, BOP, BB_WIDTH, GAP, btc_return_lag5, eth_btc_corr_3d, btc_dominance, Acceleration_Momentum, bnb_return, sol_return, sol_volume_change, doge_volume_change, sentiment_volatility_7_lag1, eth_total_gas_used_lag1, is_month_end, low_lag5_ratio, HIGH_CLOSE_RANGE, eth_btc_corr_7d, PRICE_VS_SMA10, eth_btc_volume_ratio_ma30, PRICE_VS_SMA200, is_quarter_start, sentiment_intensity_lag2, AD, VIX_ETH_Vol_Cross_lag1, LINREG_14, sp500_SP500_lag1, CCI_SIGNAL, eth_btc_corr_lowvol, btc_intraday_range\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 2]\n",
      "Training data shape: (670, 355)\n",
      "Selected Features\n",
      "DPO_20, sol_return, btc_return_lag2, HIGH_CLOSE_RANGE, btc_intraday_range, VOLUME_CHANGE_5, low_lag2_ratio, VOLUME_STRENGTH, btc_return_lag10, volume_lag3, eth_large_eth_transfers_lag1, HIGH_LOW_RANGE, bnb_volume_change, news_volume_ma14, GAP, eth_btc_corr_3d, eth_btc_beta_30d, vol_trend, bnb_return, xrp_volume_ratio_20d, sol_volume_change, dot_volume_ratio_20d, eth_active_addresses_lag1, eth_total_gas_used_lag1, PRICE_VS_SMA10, is_month_end, eth_btc_corr_7d, is_quarter_start, HLC3, RSI_200, ADX_14, news_count_lag2, low_lag5_ratio, sentiment_volatility_3_lag1, ada_volume_ratio_20d, day_of_month, AD, btc_return_10d, low_lag1, eth_intraday_range\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 3]\n",
      "Training data shape: (790, 355)\n",
      "Selected Features\n",
      "DPO_20, low_lag2_ratio, SLOPE_5, btc_return_lag5, eth_btc_corr_7d, VOLUME_CHANGE_5, bnb_volume_ratio_20d, high_lag1_ratio, sentiment_acceleration, GAP, btc_return_lag1, eth_btc_corr_3d, Acceleration_Momentum, bnb_return, bnb_volume_change, xrp_volume_change, sol_return, ada_volume_change, doge_volume_change, avax_return, dot_volume_change, eth_active_addresses_lag1, eth_large_eth_transfers_lag1, PRICE_VS_SMA10, low_lag1, HLC3, is_month_end, news_count_lag2, Liquidity_Risk, AD, is_quarter_start, ADX_14, btc_intraday_range, close_lag14, INC_1, VOLUME_CHANGE, VOLUME_STRENGTH, btc_return_20d, eth_btc_volcorr_30d, sentiment_volatility_3_lag1\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 4]\n",
      "Training data shape: (910, 355)\n",
      "Selected Features\n",
      "DPO_20, bnb_volume_ratio_20d, eth_large_eth_transfers_lag1, ada_volume_change, eth_intraday_range, news_count_lag2, btc_return_lag5, sentiment_volatility_3_lag1, return_lag5, close_lag2_logret, low_lag1_ratio, sentiment_volatility_14_lag1, news_volume_change, GAP, btc_return_lag1, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, bnb_volume_change, xrp_volume_change, avax_volume_ratio_20d, sentiment_std_lag1, eth_active_addresses_lag1, HIGH_LOW_RANGE, HIGH_CLOSE_RANGE, sp500_SP500_lag1, PRICE_VS_SMA10, VTXM_14, news_volume_ma14, sentiment_sum_lag2, low_lag2_ratio, HLC3, eth_btc_corr_7d, RSI_30, is_month_end, low_lag1, eth_btc_spread, is_quarter_start, volume_lag3, day_of_month\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 4\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 5 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 5]\n",
      "Training data shape: (1030, 355)\n",
      "Selected Features\n",
      "DPO_20, eth_btc_corr_3d, bnb_volume_ratio_20d, VOLUME_CHANGE, HIGH_CLOSE_RANGE, high_lag5_ratio, dot_volume_ratio_20d, news_volume_ma14, sp500_SP500_lag1, btc_return_lag5, low_lag1_ratio, volume_lag5, eth_btc_corr_7d, eth_intraday_range, eth_large_eth_transfers_lag1, doge_volume_ratio_20d, BTC_Weighted_Impact, news_volume_change, GAP, btc_return_lag1, btc_return_lag2, eth_btc_volcorr_sq_7d, vol_trend, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, bnb_volume_change, sol_return, VTXM_14, sentiment_sum_lag2, news_count_lag2, HLC3, INC_1, MACDH_12_26_9, PRICE_VS_SMA20, btc_intraday_range, PRICE_VS_SMA10, AD, HIGH_LOW_RANGE, eth_btc_volcorr_30d\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 6 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 6]\n",
      "Training data shape: (1150, 355)\n",
      "Selected Features\n",
      "DPO_20, VOLUME_CHANGE, eth_btc_corr_3d, xrp_volume_change, bnb_volume_ratio_20d, GAP, btc_return_lag5, HIGH_CLOSE_RANGE, eth_btc_corr_7d, eth_btc_volcorr_30d, BTC_Weighted_Impact, news_volume_ma14, HIGH_LOW_RANGE, news_volume_change, return_lag1, btc_return_lag1, eth_btc_corr_lowvol, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, xrp_return, sol_return, DISTANCE_FROM_LOW, PRICE_VS_SMA10, CCI_14, day_of_month, vol_percentile_90d, sp500_SP500_lag1, vol_trend, high_lag5_ratio, eth_btc_spread, RV_RATIO, PRICE_VS_SMA20, RSI_30, VTXM_14, news_count_lag2, btc_volatility_30d, STOCHF_3, RV_5, eth_intraday_range\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 6\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 7 (final_holdout)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 7]\n",
      "Training data shape: (1399, 355)\n",
      "Selected Features\n",
      "DPO_20, eth_btc_corr_3d, eth_btc_volcorr_sq_7d, VOLUME_CHANGE, btc_return_lag5, GAP, sentiment_intensity_lag2, eth_btc_volcorr_7d, HIGH_LOW_RANGE, eth_btc_spread, sentiment_trend, VOLUME_CHANGE_5, btc_return_lag1, btc_return_lag2, btc_return_lag3, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, xrp_return, sol_return, ada_volume_change, doge_volume_change, avax_volume_change, eth_btc_volume_ratio, DISTANCE_FROM_LOW, DEMA_10, BTC_Weighted_Impact, STOCHF_3, price_percentile_250d, eth_btc_volcorr_30d, PRICE_VS_SMA20, day_of_month, HIGH_CLOSE_RANGE, high_lag5_ratio, btc_return_20d, VIX_ETH_Vol_Cross_lag1, is_month_end, RV_RATIO, low_lag3, eth_btc_corr_7d\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 7\n",
      "============================================================\n",
      "\n",
      "\n",
      "  Processing Fold 1/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.6596\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.55357\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.60045\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.51116\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.64844\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.67076\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.6875\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.59821\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.58371\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.74442\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.53571\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.57031\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.63058\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.55357\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.52009\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.6875\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 02:58:24.441927: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (520, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "  [20] BiLSTM...\n",
      "17/17 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "  [21] GRU...\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "  [29] DTW_LSTM...\n",
      "17/17 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "  [40] EMD_LSTM...\n",
      "17/17 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "17/17 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [44] Residual_LSTM...\n",
      "17/17 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 1 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 2/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.57701\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.56027\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.61161\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.48326\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.55357\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.54018\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.51562\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.59933\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.49219\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.58147\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.52344\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.5067\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.60268\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.59152\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.50558\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.59933\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (640, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "20/20 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "  [20] BiLSTM...\n",
      "20/20 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "  [21] GRU...\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "  [29] DTW_LSTM...\n",
      "20/20 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "20/20 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [40] EMD_LSTM...\n",
      "20/20 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 1s 621ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "20/20 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "  [44] Residual_LSTM...\n",
      "20/20 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 2 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 3/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.71205\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.58929\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.58259\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.46652\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.73438\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.61942\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.57031\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.64621\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.59821\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.70201\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.66518\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.69196\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.48772\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.6183\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.47768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.73438\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (760, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "  [20] BiLSTM...\n",
      "24/24 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "  [21] GRU...\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "  [29] DTW_LSTM...\n",
      "24/24 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "24/24 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "  [40] EMD_LSTM...\n",
      "24/24 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "24/24 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "  [44] Residual_LSTM...\n",
      "24/24 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 3 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 4/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.53236\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.55556\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.60684\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.56288\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.61905\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.45788\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.59463\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.56777\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.55556\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.58608\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.63858\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.63248\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.64713\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_auc = 0.55922\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.57143\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.59463\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (880, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "28/28 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "  [20] BiLSTM...\n",
      "28/28 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "  [21] GRU...\n",
      "28/28 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "  [29] DTW_LSTM...\n",
      "28/28 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "28/28 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "  [40] EMD_LSTM...\n",
      "28/28 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "28/28 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "  [44] Residual_LSTM...\n",
      "28/28 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 4 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 5/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_auc = 0.74057\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.544\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.56914\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.61943\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_auc = 0.69829\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.56\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.64571\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.77829\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 56 and best_val_0_auc = 0.71429\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.63429\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.57943\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.70057\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.70286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.52457\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.56457\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.77829\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "33/33 [==============================] - 0s 992us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1000, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "  [20] BiLSTM...\n",
      "32/32 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "  [21] GRU...\n",
      "32/32 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "  [29] DTW_LSTM...\n",
      "32/32 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "32/32 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "  [40] EMD_LSTM...\n",
      "32/32 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "32/32 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "  [44] Residual_LSTM...\n",
      "32/32 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 5 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 6/7 (walk_forward)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.72\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.61556\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.63889\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.68444\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.67333\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.49222\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.63222\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.60333\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.52222\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.71111\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.59222\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.63889\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.64444\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.65778\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.67444\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.68444\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1120, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (60, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "35/35 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "  [20] BiLSTM...\n",
      "35/35 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "  [21] GRU...\n",
      "35/35 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "  [29] DTW_LSTM...\n",
      "35/35 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "35/35 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "  [40] EMD_LSTM...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 1s 574ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 1s 615ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "  [44] Residual_LSTM...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 6 (walk_forward) completed\n",
      "\n",
      "  Processing Fold 7/7 (final_holdout)\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.66185\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.47164\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.5762\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.55617\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.70857\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.58732\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.65851\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.69188\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.73971\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.56396\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.67964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.52503\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.59844\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.59177\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.63293\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.73971\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "44/44 [==============================] - 0s 976us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 968us/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1369, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (262, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "43/43 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "  [20] BiLSTM...\n",
      "43/43 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "  [21] GRU...\n",
      "43/43 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "  [29] DTW_LSTM...\n",
      "43/43 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "43/43 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "9/9 [==============================] - 0s 9ms/step\n",
      "  [40] EMD_LSTM...\n",
      "43/43 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "43/43 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "9/9 [==============================] - 0s 14ms/step\n",
      "  [44] Residual_LSTM...\n",
      "43/43 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "  Fold 7 (final_holdout) completed\n",
      "\n",
      "  Aggregating 7 folds...\n",
      "Saved: model_results/2025-10-24/direction_walk_forward___detailed.csv\n",
      "Saved: model_results/2025-10-24/direction_walk_forward___avg.csv\n",
      "\n",
      "성능 요약 (Best Model: Bagging):\n",
      "  WF 평균 (2022-2024): 0.66%\n",
      "  Final Holdout (2025): 0.6301369863013698%\n",
      "Saved 26 combined prediction files to model_results/2025-10-24/predictions/direction_walk_forward\n",
      "\n",
      "================================================================================\n",
      "Experiment: direction x tvt\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TVT Split (Fixed Test Start: 2025-01-01)\n",
      "================================================================================\n",
      "  Train: 1213 (2020-12-19 ~ 2024-04-14)\n",
      "  Val:    261 (2024-04-15 ~ 2024-12-31)\n",
      "  Test:   292 (2025-01-01 ~ 2025-10-19)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 1 (unknown)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 1]\n",
      "Training data shape: (1213, 355)\n",
      "Selected Features\n",
      "DPO_20, btc_return_lag5, GAP, eth_btc_corr_3d, bnb_return, eth_btc_corr_7d, sp500_SP500_lag1, BTC_Weighted_Impact, eth_btc_spread, bnb_volume_ratio_20d, VOLUME_CHANGE_5, RV_5, eth_btc_volcorr_sq_7d, btc_dominance, eth_btc_corr_lowvol, BB_Sentiment_Consensus, Acceleration_Momentum, xrp_volume_change, sol_return, ada_volume_change, dot_volume_ratio_20d, sentiment_std_lag1, eth_token_transfers_lag1, DISTANCE_FROM_LOW, PRICE_VS_SMA10, HIGH_CLOSE_RANGE, CCI_14, vol_percentile_90d, is_month_end, day_of_month, low_lag3, negative_ratio_lag1, DEMA_10, return_lag3, INC_1, Golden_Sentiment_Align, btc_return_20d, AD, VOLUME_RATIO, eth_total_gas_used_lag1\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 26개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.64808\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.48437\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.5741\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.60736\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.60959\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.59037\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.63262\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_auc = 0.61934\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.5681\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.57657\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.63568\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.63915\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.63392\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.62587\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.62328\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.63915\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "38/38 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "\n",
      "✓ ML 모델 완료: 18/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (8개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1183, 30, 40)\n",
      "  ✓ Val shape: (231, 30, 40)\n",
      "  ✓ Test shape: (262, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "37/37 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "  [20] BiLSTM...\n",
      "37/37 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "  [21] GRU...\n",
      "37/37 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "  [29] DTW_LSTM...\n",
      "37/37 [==============================] - 1s 10ms/step\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "37/37 [==============================] - 1s 10ms/step\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 9ms/step\n",
      "  [40] EMD_LSTM...\n",
      "37/37 [==============================] - 1s 11ms/step\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "37/37 [==============================] - 1s 12ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "  [44] Residual_LSTM...\n",
      "37/37 [==============================] - 1s 12ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "\n",
      "✓ DL 모델 완료: 8/8개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 26/26개 모델 성공\n",
      "================================================================================\n",
      "Saved metrics: model_results/2025-10-24/direction_tvt__metrics.csv\n",
      "Saved 26 prediction files to model_results/2025-10-24/predictions/direction_tvt\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for target_case in target_cases:\n",
    "    for split_method in split_methods:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Experiment: {target_case['name']} x {split_method['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = build_complete_pipeline_corrected(\n",
    "            df_merged, train_start_date,\n",
    "            method=split_method['method'],\n",
    "            target_type=target_case['target_type'],\n",
    "            test_start_date='2025-01-01'  \n",
    "        )\n",
    "        \n",
    "        if split_method['method'] == 'tvt':\n",
    "            # TVT 방식\n",
    "            X_train = result['train']['X_robust']\n",
    "            X_val = result['val']['X_robust']\n",
    "            X_test = result['test']['X_robust']\n",
    "            test_returns = result['test']['y']['next_log_return'].values  \n",
    "            test_dates = result['test']['dates'].values \n",
    "            \n",
    "            if len(target_case['outputs']) == 1:\n",
    "                y_train = result['train']['y'][target_case['outputs'][0]].values\n",
    "                y_val = result['val']['y'][target_case['outputs'][0]].values\n",
    "                y_test = result['test']['y'][target_case['outputs'][0]].values\n",
    "                ml_models = ML_MODELS_CLASSIFICATION\n",
    "                dl_models = DL_MODELS_CLASSIFICATION\n",
    "                task = 'classification'\n",
    "            \n",
    "            evaluator = ModelEvaluator()\n",
    "            train_all_models(\n",
    "                X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                test_returns, test_dates, evaluator,\n",
    "                ml_models=ml_models, dl_models=dl_models, task=task\n",
    "            )\n",
    "            \n",
    "\n",
    "            summary_df = evaluator.get_summary_dataframe()\n",
    "            predictions_dict = evaluator.get_predictions_dict()  \n",
    "            \n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = summary_df\n",
    "\n",
    "            save_summary_csv(\n",
    "                summary_df, predictions_dict,  \n",
    "                target_case['name'], split_method['name'], task\n",
    "            )\n",
    "        else:\n",
    "            fold_results = []\n",
    "            fold_predictions = []\n",
    "\n",
    "            for fold_idx, fold in enumerate(result, start=1):\n",
    "\n",
    "                fold_type = fold['stats']['fold_type']  \n",
    "                \n",
    "                print(f\"\\n  Processing Fold {fold_idx}/{len(result)} ({fold_type})\")\n",
    "\n",
    "                X_train = fold['train']['X_robust']\n",
    "                X_val = fold['val']['X_robust']\n",
    "                X_test = fold['test']['X_robust']\n",
    "                test_returns = fold['test']['y']['next_log_return'].values  \n",
    "                test_dates = fold['test']['dates'].values  \n",
    "\n",
    "                if len(target_case['outputs']) == 1:\n",
    "                    y_train = fold['train']['y'][target_case['outputs'][0]].values  \n",
    "                    y_val = fold['val']['y'][target_case['outputs'][0]].values\n",
    "                    y_test = fold['test']['y'][target_case['outputs'][0]].values\n",
    "                    ml_models = ML_MODELS_CLASSIFICATION\n",
    "                    dl_models = DL_MODELS_CLASSIFICATION\n",
    "                    task = 'classification'\n",
    "\n",
    "                evaluator = ModelEvaluator()\n",
    "                train_all_models(\n",
    "                    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator,\n",
    "                    ml_models=ml_models, dl_models=dl_models, task=task\n",
    "                )\n",
    "\n",
    "                fold_summary = evaluator.get_summary_dataframe()\n",
    "                fold_pred_dict = evaluator.get_predictions_dict()  \n",
    "\n",
    "                fold_results.append((fold_summary, fold_type))\n",
    "                fold_predictions.append((fold_pred_dict, fold_type))\n",
    "\n",
    "                print(f\"  Fold {fold_idx} ({fold_type}) completed\")\n",
    "\n",
    "            print(f\"\\n  Aggregating {len(fold_results)} folds...\")\n",
    "            detailed_df, avg_df = save_walk_forward_results(\n",
    "                fold_results, fold_predictions,\n",
    "                target_case['name'], task\n",
    "            )\n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = avg_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd61dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BTC_Open</th>\n",
       "      <th>BTC_High</th>\n",
       "      <th>BTC_Low</th>\n",
       "      <th>BTC_Close</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>ETH_Volume</th>\n",
       "      <th>BNB_Open</th>\n",
       "      <th>BNB_High</th>\n",
       "      <th>BNB_Low</th>\n",
       "      <th>BNB_Close</th>\n",
       "      <th>BNB_Volume</th>\n",
       "      <th>XRP_Open</th>\n",
       "      <th>XRP_High</th>\n",
       "      <th>XRP_Low</th>\n",
       "      <th>XRP_Close</th>\n",
       "      <th>XRP_Volume</th>\n",
       "      <th>SOL_Open</th>\n",
       "      <th>SOL_High</th>\n",
       "      <th>SOL_Low</th>\n",
       "      <th>SOL_Close</th>\n",
       "      <th>SOL_Volume</th>\n",
       "      <th>ADA_Open</th>\n",
       "      <th>ADA_High</th>\n",
       "      <th>ADA_Low</th>\n",
       "      <th>ADA_Close</th>\n",
       "      <th>ADA_Volume</th>\n",
       "      <th>DOGE_Open</th>\n",
       "      <th>DOGE_High</th>\n",
       "      <th>DOGE_Low</th>\n",
       "      <th>DOGE_Close</th>\n",
       "      <th>DOGE_Volume</th>\n",
       "      <th>AVAX_Open</th>\n",
       "      <th>AVAX_High</th>\n",
       "      <th>AVAX_Low</th>\n",
       "      <th>AVAX_Close</th>\n",
       "      <th>AVAX_Volume</th>\n",
       "      <th>DOT_Open</th>\n",
       "      <th>DOT_High</th>\n",
       "      <th>DOT_Low</th>\n",
       "      <th>DOT_Close</th>\n",
       "      <th>DOT_Volume</th>\n",
       "      <th>sentiment_mean</th>\n",
       "      <th>sentiment_std</th>\n",
       "      <th>news_count</th>\n",
       "      <th>positive_ratio</th>\n",
       "      <th>negative_ratio</th>\n",
       "      <th>extreme_positive_count</th>\n",
       "      <th>extreme_negative_count</th>\n",
       "      <th>sentiment_sum</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>sentiment_disagreement</th>\n",
       "      <th>bull_bear_ratio</th>\n",
       "      <th>weighted_sentiment</th>\n",
       "      <th>extremity_index</th>\n",
       "      <th>sentiment_ma3</th>\n",
       "      <th>sentiment_volatility_3</th>\n",
       "      <th>sentiment_ma7</th>\n",
       "      <th>sentiment_volatility_7</th>\n",
       "      <th>sentiment_ma14</th>\n",
       "      <th>sentiment_volatility_14</th>\n",
       "      <th>sentiment_trend</th>\n",
       "      <th>sentiment_acceleration</th>\n",
       "      <th>news_volume_change</th>\n",
       "      <th>news_volume_ma7</th>\n",
       "      <th>news_volume_ma14</th>\n",
       "      <th>eth_tx_count</th>\n",
       "      <th>eth_active_addresses</th>\n",
       "      <th>eth_new_addresses</th>\n",
       "      <th>eth_large_eth_transfers</th>\n",
       "      <th>eth_token_transfers</th>\n",
       "      <th>eth_contract_events</th>\n",
       "      <th>eth_avg_gas_price</th>\n",
       "      <th>eth_total_gas_used</th>\n",
       "      <th>eth_avg_block_size</th>\n",
       "      <th>eth_avg_block_difficulty</th>\n",
       "      <th>fg_fear_greed</th>\n",
       "      <th>usdt_totalCirculating</th>\n",
       "      <th>usdt_totalCirculatingUSD</th>\n",
       "      <th>usdt_totalMintedUSD</th>\n",
       "      <th>usdt_totalUnreleased</th>\n",
       "      <th>usdt_totalBridgedToUSD</th>\n",
       "      <th>aave_aave_eth_tvl</th>\n",
       "      <th>lido_lido_eth_tvl</th>\n",
       "      <th>makerdao_makerdao_eth_tvl</th>\n",
       "      <th>chain_eth_chain_tvl</th>\n",
       "      <th>funding_fundingRate</th>\n",
       "      <th>sp500_SP500</th>\n",
       "      <th>vix_VIX</th>\n",
       "      <th>gold_GOLD</th>\n",
       "      <th>dxy_DXY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>2025-10-11</td>\n",
       "      <td>113236.429688</td>\n",
       "      <td>113429.726562</td>\n",
       "      <td>109760.562500</td>\n",
       "      <td>110807.882812</td>\n",
       "      <td>1.102369e+11</td>\n",
       "      <td>3840.960449</td>\n",
       "      <td>3882.241455</td>\n",
       "      <td>3652.790039</td>\n",
       "      <td>3750.611572</td>\n",
       "      <td>6.247548e+10</td>\n",
       "      <td>1106.898560</td>\n",
       "      <td>1180.525879</td>\n",
       "      <td>1080.718384</td>\n",
       "      <td>1137.199341</td>\n",
       "      <td>8.101404e+09</td>\n",
       "      <td>2.358754</td>\n",
       "      <td>2.501994</td>\n",
       "      <td>2.322168</td>\n",
       "      <td>2.385982</td>\n",
       "      <td>1.174965e+10</td>\n",
       "      <td>188.638901</td>\n",
       "      <td>190.879135</td>\n",
       "      <td>173.754791</td>\n",
       "      <td>178.054169</td>\n",
       "      <td>1.285001e+10</td>\n",
       "      <td>0.634835</td>\n",
       "      <td>0.681960</td>\n",
       "      <td>0.614932</td>\n",
       "      <td>0.632180</td>\n",
       "      <td>2.703055e+09</td>\n",
       "      <td>0.193155</td>\n",
       "      <td>0.200234</td>\n",
       "      <td>0.179012</td>\n",
       "      <td>0.185443</td>\n",
       "      <td>6.948750e+09</td>\n",
       "      <td>20.734438</td>\n",
       "      <td>23.346888</td>\n",
       "      <td>20.566242</td>\n",
       "      <td>21.472492</td>\n",
       "      <td>1.589011e+09</td>\n",
       "      <td>2.962416</td>\n",
       "      <td>3.314361</td>\n",
       "      <td>2.900451</td>\n",
       "      <td>3.000315</td>\n",
       "      <td>864188141.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.990430</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.369678</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.446719</td>\n",
       "      <td>0.347336</td>\n",
       "      <td>0.493185</td>\n",
       "      <td>0.294966</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>-1.866667</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>12.785714</td>\n",
       "      <td>1647316</td>\n",
       "      <td>477151</td>\n",
       "      <td>126597</td>\n",
       "      <td>457</td>\n",
       "      <td>2346993</td>\n",
       "      <td>4407685</td>\n",
       "      <td>4.097661e+09</td>\n",
       "      <td>161594088388</td>\n",
       "      <td>127926.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.961207e+10</td>\n",
       "      <td>7.973872e+10</td>\n",
       "      <td>9.641883e+10</td>\n",
       "      <td>1.495715e+09</td>\n",
       "      <td>312401.0</td>\n",
       "      <td>29753089779</td>\n",
       "      <td>3.289364e+10</td>\n",
       "      <td>5596531595</td>\n",
       "      <td>175751448682</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>6552.509766</td>\n",
       "      <td>21.660000</td>\n",
       "      <td>3975.899902</td>\n",
       "      <td>98.980003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>2025-10-12</td>\n",
       "      <td>110811.515625</td>\n",
       "      <td>115805.062500</td>\n",
       "      <td>109715.539062</td>\n",
       "      <td>115169.765625</td>\n",
       "      <td>9.371041e+10</td>\n",
       "      <td>3750.946045</td>\n",
       "      <td>4195.397461</td>\n",
       "      <td>3701.478271</td>\n",
       "      <td>4164.427734</td>\n",
       "      <td>6.121617e+10</td>\n",
       "      <td>1137.199341</td>\n",
       "      <td>1319.771973</td>\n",
       "      <td>1110.197388</td>\n",
       "      <td>1303.117310</td>\n",
       "      <td>1.020104e+10</td>\n",
       "      <td>2.385956</td>\n",
       "      <td>2.581764</td>\n",
       "      <td>2.320973</td>\n",
       "      <td>2.534958</td>\n",
       "      <td>9.877639e+09</td>\n",
       "      <td>178.056198</td>\n",
       "      <td>198.873978</td>\n",
       "      <td>173.521393</td>\n",
       "      <td>197.508545</td>\n",
       "      <td>1.179704e+10</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.708216</td>\n",
       "      <td>0.621524</td>\n",
       "      <td>0.701505</td>\n",
       "      <td>1.600069e+09</td>\n",
       "      <td>0.185441</td>\n",
       "      <td>0.214049</td>\n",
       "      <td>0.181756</td>\n",
       "      <td>0.207795</td>\n",
       "      <td>5.183181e+09</td>\n",
       "      <td>21.472364</td>\n",
       "      <td>23.327850</td>\n",
       "      <td>20.714027</td>\n",
       "      <td>22.583330</td>\n",
       "      <td>1.271749e+09</td>\n",
       "      <td>3.000277</td>\n",
       "      <td>3.304046</td>\n",
       "      <td>2.923780</td>\n",
       "      <td>3.246990</td>\n",
       "      <td>427368745.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.924196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.676593</td>\n",
       "      <td>0.323469</td>\n",
       "      <td>0.450771</td>\n",
       "      <td>0.433662</td>\n",
       "      <td>0.368422</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>1480041</td>\n",
       "      <td>453698</td>\n",
       "      <td>129918</td>\n",
       "      <td>252</td>\n",
       "      <td>2701386</td>\n",
       "      <td>4730346</td>\n",
       "      <td>1.671546e+09</td>\n",
       "      <td>162883506170</td>\n",
       "      <td>122616.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.001645e+10</td>\n",
       "      <td>8.010518e+10</td>\n",
       "      <td>9.711055e+10</td>\n",
       "      <td>7.584217e+08</td>\n",
       "      <td>312250.0</td>\n",
       "      <td>30241372474</td>\n",
       "      <td>3.185586e+10</td>\n",
       "      <td>5567724995</td>\n",
       "      <td>171886426278</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>6552.509766</td>\n",
       "      <td>21.660000</td>\n",
       "      <td>3975.899902</td>\n",
       "      <td>98.980003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>2025-10-13</td>\n",
       "      <td>115161.679688</td>\n",
       "      <td>116020.484375</td>\n",
       "      <td>113821.187500</td>\n",
       "      <td>115271.078125</td>\n",
       "      <td>7.158203e+10</td>\n",
       "      <td>4164.049316</td>\n",
       "      <td>4292.845703</td>\n",
       "      <td>4061.224609</td>\n",
       "      <td>4245.467773</td>\n",
       "      <td>5.025378e+10</td>\n",
       "      <td>1303.147339</td>\n",
       "      <td>1370.546021</td>\n",
       "      <td>1257.110229</td>\n",
       "      <td>1293.473267</td>\n",
       "      <td>1.182039e+10</td>\n",
       "      <td>2.535090</td>\n",
       "      <td>2.645213</td>\n",
       "      <td>2.521791</td>\n",
       "      <td>2.606500</td>\n",
       "      <td>8.579699e+09</td>\n",
       "      <td>197.512192</td>\n",
       "      <td>209.249191</td>\n",
       "      <td>191.944168</td>\n",
       "      <td>208.367294</td>\n",
       "      <td>1.146945e+10</td>\n",
       "      <td>0.701531</td>\n",
       "      <td>0.735054</td>\n",
       "      <td>0.698125</td>\n",
       "      <td>0.729258</td>\n",
       "      <td>1.714657e+09</td>\n",
       "      <td>0.207795</td>\n",
       "      <td>0.218270</td>\n",
       "      <td>0.204746</td>\n",
       "      <td>0.213991</td>\n",
       "      <td>4.745995e+09</td>\n",
       "      <td>22.583363</td>\n",
       "      <td>24.057014</td>\n",
       "      <td>21.945959</td>\n",
       "      <td>23.838850</td>\n",
       "      <td>1.570963e+09</td>\n",
       "      <td>3.247090</td>\n",
       "      <td>3.441121</td>\n",
       "      <td>3.205383</td>\n",
       "      <td>3.377107</td>\n",
       "      <td>477219506.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.967906</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.304452</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.260342</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>0.420805</td>\n",
       "      <td>0.432868</td>\n",
       "      <td>0.369181</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>13.357143</td>\n",
       "      <td>1580074</td>\n",
       "      <td>486049</td>\n",
       "      <td>142490</td>\n",
       "      <td>298</td>\n",
       "      <td>2367316</td>\n",
       "      <td>4372228</td>\n",
       "      <td>1.494701e+09</td>\n",
       "      <td>162391660577</td>\n",
       "      <td>122354.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.020210e+10</td>\n",
       "      <td>8.028963e+10</td>\n",
       "      <td>9.733910e+10</td>\n",
       "      <td>5.284217e+08</td>\n",
       "      <td>312245.0</td>\n",
       "      <td>32405097849</td>\n",
       "      <td>3.535026e+10</td>\n",
       "      <td>6110212606</td>\n",
       "      <td>186667365937</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>6654.720215</td>\n",
       "      <td>19.030001</td>\n",
       "      <td>4108.600098</td>\n",
       "      <td>99.269997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>2025-10-14</td>\n",
       "      <td>115264.882812</td>\n",
       "      <td>115502.882812</td>\n",
       "      <td>110029.484375</td>\n",
       "      <td>113118.664062</td>\n",
       "      <td>9.221292e+10</td>\n",
       "      <td>4245.372559</td>\n",
       "      <td>4265.105469</td>\n",
       "      <td>3895.973633</td>\n",
       "      <td>4125.412109</td>\n",
       "      <td>6.709415e+10</td>\n",
       "      <td>1293.473267</td>\n",
       "      <td>1316.407104</td>\n",
       "      <td>1147.750000</td>\n",
       "      <td>1211.051147</td>\n",
       "      <td>1.010659e+10</td>\n",
       "      <td>2.606494</td>\n",
       "      <td>2.620473</td>\n",
       "      <td>2.407625</td>\n",
       "      <td>2.505660</td>\n",
       "      <td>8.358304e+09</td>\n",
       "      <td>208.363556</td>\n",
       "      <td>211.105042</td>\n",
       "      <td>191.671021</td>\n",
       "      <td>202.460342</td>\n",
       "      <td>1.360020e+10</td>\n",
       "      <td>0.729262</td>\n",
       "      <td>0.732587</td>\n",
       "      <td>0.665723</td>\n",
       "      <td>0.699692</td>\n",
       "      <td>1.861960e+09</td>\n",
       "      <td>0.213991</td>\n",
       "      <td>0.215544</td>\n",
       "      <td>0.195104</td>\n",
       "      <td>0.204532</td>\n",
       "      <td>4.841726e+09</td>\n",
       "      <td>23.838711</td>\n",
       "      <td>23.889275</td>\n",
       "      <td>21.988726</td>\n",
       "      <td>22.770382</td>\n",
       "      <td>1.229012e+09</td>\n",
       "      <td>3.377107</td>\n",
       "      <td>3.393776</td>\n",
       "      <td>3.088582</td>\n",
       "      <td>3.250740</td>\n",
       "      <td>435483489.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.782718</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.136106</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.934468</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.125121</td>\n",
       "      <td>0.471517</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.444033</td>\n",
       "      <td>0.436664</td>\n",
       "      <td>0.370809</td>\n",
       "      <td>0.508696</td>\n",
       "      <td>0.075362</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>1571829</td>\n",
       "      <td>482561</td>\n",
       "      <td>129811</td>\n",
       "      <td>546</td>\n",
       "      <td>2405692</td>\n",
       "      <td>4336474</td>\n",
       "      <td>1.662172e+09</td>\n",
       "      <td>162625955934</td>\n",
       "      <td>122305.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.008058e+10</td>\n",
       "      <td>8.013571e+10</td>\n",
       "      <td>9.768328e+10</td>\n",
       "      <td>1.145325e+09</td>\n",
       "      <td>312119.0</td>\n",
       "      <td>32891489038</td>\n",
       "      <td>3.604987e+10</td>\n",
       "      <td>6087962406</td>\n",
       "      <td>190022043707</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>6644.310059</td>\n",
       "      <td>20.809999</td>\n",
       "      <td>4138.700195</td>\n",
       "      <td>99.050003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>113113.968750</td>\n",
       "      <td>113622.382812</td>\n",
       "      <td>110235.835938</td>\n",
       "      <td>110783.164062</td>\n",
       "      <td>7.257413e+10</td>\n",
       "      <td>4125.361328</td>\n",
       "      <td>4213.855957</td>\n",
       "      <td>3935.161377</td>\n",
       "      <td>3987.459473</td>\n",
       "      <td>5.046289e+10</td>\n",
       "      <td>1211.051147</td>\n",
       "      <td>1221.707520</td>\n",
       "      <td>1151.835205</td>\n",
       "      <td>1162.691772</td>\n",
       "      <td>5.853930e+09</td>\n",
       "      <td>2.505667</td>\n",
       "      <td>2.525715</td>\n",
       "      <td>2.389588</td>\n",
       "      <td>2.412666</td>\n",
       "      <td>5.540248e+09</td>\n",
       "      <td>202.461868</td>\n",
       "      <td>208.331161</td>\n",
       "      <td>192.700058</td>\n",
       "      <td>194.021866</td>\n",
       "      <td>9.867609e+09</td>\n",
       "      <td>0.699693</td>\n",
       "      <td>0.713327</td>\n",
       "      <td>0.661867</td>\n",
       "      <td>0.668802</td>\n",
       "      <td>1.209548e+09</td>\n",
       "      <td>0.204532</td>\n",
       "      <td>0.207973</td>\n",
       "      <td>0.195098</td>\n",
       "      <td>0.196327</td>\n",
       "      <td>2.877365e+09</td>\n",
       "      <td>22.770382</td>\n",
       "      <td>23.234428</td>\n",
       "      <td>21.639189</td>\n",
       "      <td>21.916134</td>\n",
       "      <td>7.284511e+08</td>\n",
       "      <td>3.250765</td>\n",
       "      <td>3.336066</td>\n",
       "      <td>3.110547</td>\n",
       "      <td>3.138686</td>\n",
       "      <td>307547964.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.236232</td>\n",
       "      <td>0.326415</td>\n",
       "      <td>0.215528</td>\n",
       "      <td>0.444266</td>\n",
       "      <td>0.412854</td>\n",
       "      <td>0.388246</td>\n",
       "      <td>-0.608696</td>\n",
       "      <td>-1.117391</td>\n",
       "      <td>-0.565217</td>\n",
       "      <td>12.428571</td>\n",
       "      <td>13.571429</td>\n",
       "      <td>1634250</td>\n",
       "      <td>553593</td>\n",
       "      <td>158808</td>\n",
       "      <td>396</td>\n",
       "      <td>2338271</td>\n",
       "      <td>4286551</td>\n",
       "      <td>1.455372e+09</td>\n",
       "      <td>162655224073</td>\n",
       "      <td>121957.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.078072e+10</td>\n",
       "      <td>8.084477e+10</td>\n",
       "      <td>9.842875e+10</td>\n",
       "      <td>4.106251e+08</td>\n",
       "      <td>312152.0</td>\n",
       "      <td>32277871173</td>\n",
       "      <td>3.474401e+10</td>\n",
       "      <td>6052013458</td>\n",
       "      <td>185877171143</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>6671.060059</td>\n",
       "      <td>20.639999</td>\n",
       "      <td>4176.899902</td>\n",
       "      <td>98.790001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>2025-10-16</td>\n",
       "      <td>110782.171875</td>\n",
       "      <td>111990.812500</td>\n",
       "      <td>107537.031250</td>\n",
       "      <td>108186.039062</td>\n",
       "      <td>8.730642e+10</td>\n",
       "      <td>3987.148926</td>\n",
       "      <td>4079.648193</td>\n",
       "      <td>3829.654053</td>\n",
       "      <td>3894.754395</td>\n",
       "      <td>4.914739e+10</td>\n",
       "      <td>1162.693726</td>\n",
       "      <td>1193.773926</td>\n",
       "      <td>1125.333252</td>\n",
       "      <td>1145.825317</td>\n",
       "      <td>5.605348e+09</td>\n",
       "      <td>2.412672</td>\n",
       "      <td>2.462859</td>\n",
       "      <td>2.287637</td>\n",
       "      <td>2.329068</td>\n",
       "      <td>7.020912e+09</td>\n",
       "      <td>194.021866</td>\n",
       "      <td>197.955750</td>\n",
       "      <td>182.251343</td>\n",
       "      <td>184.688950</td>\n",
       "      <td>1.004289e+10</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.683554</td>\n",
       "      <td>0.635841</td>\n",
       "      <td>0.644944</td>\n",
       "      <td>1.359176e+09</td>\n",
       "      <td>0.196328</td>\n",
       "      <td>0.201235</td>\n",
       "      <td>0.185209</td>\n",
       "      <td>0.188367</td>\n",
       "      <td>3.498982e+09</td>\n",
       "      <td>21.915653</td>\n",
       "      <td>22.346117</td>\n",
       "      <td>20.674866</td>\n",
       "      <td>20.959126</td>\n",
       "      <td>8.083594e+08</td>\n",
       "      <td>3.138686</td>\n",
       "      <td>3.200600</td>\n",
       "      <td>2.980015</td>\n",
       "      <td>3.015078</td>\n",
       "      <td>372522780.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.786796</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.188252</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>0.341182</td>\n",
       "      <td>0.297161</td>\n",
       "      <td>0.450518</td>\n",
       "      <td>0.398725</td>\n",
       "      <td>0.377736</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.180124</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>13.142857</td>\n",
       "      <td>13.142857</td>\n",
       "      <td>1569376</td>\n",
       "      <td>502395</td>\n",
       "      <td>144816</td>\n",
       "      <td>337</td>\n",
       "      <td>2045266</td>\n",
       "      <td>4099252</td>\n",
       "      <td>1.663318e+09</td>\n",
       "      <td>162119681022</td>\n",
       "      <td>125317.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.155061e+10</td>\n",
       "      <td>8.159707e+10</td>\n",
       "      <td>9.918112e+10</td>\n",
       "      <td>6.367670e+08</td>\n",
       "      <td>312082.0</td>\n",
       "      <td>31598594443</td>\n",
       "      <td>3.382954e+10</td>\n",
       "      <td>5872771981</td>\n",
       "      <td>181145394259</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6629.069824</td>\n",
       "      <td>25.309999</td>\n",
       "      <td>4280.200195</td>\n",
       "      <td>98.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>108179.132812</td>\n",
       "      <td>109235.804688</td>\n",
       "      <td>103598.429688</td>\n",
       "      <td>106467.789062</td>\n",
       "      <td>9.970305e+10</td>\n",
       "      <td>3894.377686</td>\n",
       "      <td>3950.566895</td>\n",
       "      <td>3678.620361</td>\n",
       "      <td>3832.558838</td>\n",
       "      <td>5.740427e+10</td>\n",
       "      <td>1145.817993</td>\n",
       "      <td>1154.891235</td>\n",
       "      <td>1024.083252</td>\n",
       "      <td>1071.963013</td>\n",
       "      <td>6.656428e+09</td>\n",
       "      <td>2.328990</td>\n",
       "      <td>2.380460</td>\n",
       "      <td>2.197175</td>\n",
       "      <td>2.294239</td>\n",
       "      <td>7.915225e+09</td>\n",
       "      <td>184.686203</td>\n",
       "      <td>187.428574</td>\n",
       "      <td>174.744171</td>\n",
       "      <td>182.034790</td>\n",
       "      <td>1.040174e+10</td>\n",
       "      <td>0.644944</td>\n",
       "      <td>0.653942</td>\n",
       "      <td>0.595169</td>\n",
       "      <td>0.624843</td>\n",
       "      <td>1.684168e+09</td>\n",
       "      <td>0.188366</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.184842</td>\n",
       "      <td>3.317945e+09</td>\n",
       "      <td>20.960007</td>\n",
       "      <td>21.224400</td>\n",
       "      <td>19.092056</td>\n",
       "      <td>19.956615</td>\n",
       "      <td>9.257953e+08</td>\n",
       "      <td>3.015115</td>\n",
       "      <td>3.054085</td>\n",
       "      <td>2.784597</td>\n",
       "      <td>2.891856</td>\n",
       "      <td>397906017.0</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>1.029857</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.427492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134921</td>\n",
       "      <td>0.387103</td>\n",
       "      <td>0.130494</td>\n",
       "      <td>0.352256</td>\n",
       "      <td>0.336225</td>\n",
       "      <td>0.394581</td>\n",
       "      <td>-0.738095</td>\n",
       "      <td>-1.309524</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14.571429</td>\n",
       "      <td>12.285714</td>\n",
       "      <td>1649015</td>\n",
       "      <td>521677</td>\n",
       "      <td>152311</td>\n",
       "      <td>436</td>\n",
       "      <td>2200707</td>\n",
       "      <td>4014625</td>\n",
       "      <td>2.959400e+09</td>\n",
       "      <td>162588760223</td>\n",
       "      <td>124824.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.154415e+10</td>\n",
       "      <td>8.160802e+10</td>\n",
       "      <td>9.932227e+10</td>\n",
       "      <td>5.168670e+08</td>\n",
       "      <td>328145.0</td>\n",
       "      <td>31073951308</td>\n",
       "      <td>3.330488e+10</td>\n",
       "      <td>5940121789</td>\n",
       "      <td>177704176644</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>6664.009766</td>\n",
       "      <td>20.780001</td>\n",
       "      <td>4189.899902</td>\n",
       "      <td>98.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2025-10-18</td>\n",
       "      <td>106483.734375</td>\n",
       "      <td>107490.984375</td>\n",
       "      <td>106387.453125</td>\n",
       "      <td>107198.265625</td>\n",
       "      <td>3.777991e+10</td>\n",
       "      <td>3833.009521</td>\n",
       "      <td>3927.245605</td>\n",
       "      <td>3822.266357</td>\n",
       "      <td>3890.346191</td>\n",
       "      <td>2.381568e+10</td>\n",
       "      <td>1071.965576</td>\n",
       "      <td>1129.192505</td>\n",
       "      <td>1071.473877</td>\n",
       "      <td>1093.152466</td>\n",
       "      <td>3.767783e+09</td>\n",
       "      <td>2.294411</td>\n",
       "      <td>2.391555</td>\n",
       "      <td>2.294411</td>\n",
       "      <td>2.360510</td>\n",
       "      <td>2.903250e+09</td>\n",
       "      <td>182.036163</td>\n",
       "      <td>188.267883</td>\n",
       "      <td>182.023987</td>\n",
       "      <td>187.660294</td>\n",
       "      <td>4.108903e+09</td>\n",
       "      <td>0.624848</td>\n",
       "      <td>0.640819</td>\n",
       "      <td>0.624350</td>\n",
       "      <td>0.634461</td>\n",
       "      <td>5.493960e+08</td>\n",
       "      <td>0.184850</td>\n",
       "      <td>0.190223</td>\n",
       "      <td>0.184532</td>\n",
       "      <td>0.189599</td>\n",
       "      <td>1.238849e+09</td>\n",
       "      <td>19.956371</td>\n",
       "      <td>20.389362</td>\n",
       "      <td>19.857420</td>\n",
       "      <td>20.206861</td>\n",
       "      <td>3.456040e+08</td>\n",
       "      <td>2.891856</td>\n",
       "      <td>2.964360</td>\n",
       "      <td>2.891856</td>\n",
       "      <td>2.932491</td>\n",
       "      <td>146148632.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.462098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.481518</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>0.393388</td>\n",
       "      <td>0.255273</td>\n",
       "      <td>0.408138</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>11.071429</td>\n",
       "      <td>1357463</td>\n",
       "      <td>450565</td>\n",
       "      <td>142652</td>\n",
       "      <td>82</td>\n",
       "      <td>2109711</td>\n",
       "      <td>4199593</td>\n",
       "      <td>9.334411e+08</td>\n",
       "      <td>162919330366</td>\n",
       "      <td>110939.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.142839e+10</td>\n",
       "      <td>8.147652e+10</td>\n",
       "      <td>9.935264e+10</td>\n",
       "      <td>4.674531e+08</td>\n",
       "      <td>328082.0</td>\n",
       "      <td>30484787546</td>\n",
       "      <td>3.261156e+10</td>\n",
       "      <td>5994395509</td>\n",
       "      <td>174687064832</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>6664.009766</td>\n",
       "      <td>20.780001</td>\n",
       "      <td>4189.899902</td>\n",
       "      <td>98.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2025-10-19</td>\n",
       "      <td>107204.312500</td>\n",
       "      <td>109488.992188</td>\n",
       "      <td>106157.789062</td>\n",
       "      <td>108666.710938</td>\n",
       "      <td>4.765701e+10</td>\n",
       "      <td>3890.583496</td>\n",
       "      <td>4029.355469</td>\n",
       "      <td>3843.772949</td>\n",
       "      <td>3984.649658</td>\n",
       "      <td>3.287066e+10</td>\n",
       "      <td>1093.152466</td>\n",
       "      <td>1132.347290</td>\n",
       "      <td>1069.307495</td>\n",
       "      <td>1109.680054</td>\n",
       "      <td>3.690437e+09</td>\n",
       "      <td>2.360510</td>\n",
       "      <td>2.417567</td>\n",
       "      <td>2.321040</td>\n",
       "      <td>2.390632</td>\n",
       "      <td>3.399617e+09</td>\n",
       "      <td>187.660294</td>\n",
       "      <td>192.200607</td>\n",
       "      <td>183.458252</td>\n",
       "      <td>187.798203</td>\n",
       "      <td>4.863227e+09</td>\n",
       "      <td>0.634461</td>\n",
       "      <td>0.664487</td>\n",
       "      <td>0.623411</td>\n",
       "      <td>0.652551</td>\n",
       "      <td>8.740383e+08</td>\n",
       "      <td>0.189599</td>\n",
       "      <td>0.198535</td>\n",
       "      <td>0.186295</td>\n",
       "      <td>0.195323</td>\n",
       "      <td>1.982259e+09</td>\n",
       "      <td>20.206861</td>\n",
       "      <td>20.832535</td>\n",
       "      <td>19.774445</td>\n",
       "      <td>20.594934</td>\n",
       "      <td>4.488137e+08</td>\n",
       "      <td>2.932491</td>\n",
       "      <td>3.056875</td>\n",
       "      <td>2.888869</td>\n",
       "      <td>3.007835</td>\n",
       "      <td>205544790.0</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.792961</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-1.068729</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>0.127294</td>\n",
       "      <td>0.051923</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.187696</td>\n",
       "      <td>0.436588</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.428571</td>\n",
       "      <td>10.714286</td>\n",
       "      <td>1276777</td>\n",
       "      <td>394132</td>\n",
       "      <td>120858</td>\n",
       "      <td>121</td>\n",
       "      <td>1774094</td>\n",
       "      <td>3897490</td>\n",
       "      <td>9.952229e+08</td>\n",
       "      <td>163227107169</td>\n",
       "      <td>106516.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.179682e+10</td>\n",
       "      <td>8.184542e+10</td>\n",
       "      <td>9.969915e+10</td>\n",
       "      <td>1.121453e+09</td>\n",
       "      <td>328083.0</td>\n",
       "      <td>30834078452</td>\n",
       "      <td>3.301850e+10</td>\n",
       "      <td>6063156836</td>\n",
       "      <td>176509406573</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>6664.009766</td>\n",
       "      <td>20.780001</td>\n",
       "      <td>4189.899902</td>\n",
       "      <td>98.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2025-10-20</td>\n",
       "      <td>108667.445312</td>\n",
       "      <td>111711.031250</td>\n",
       "      <td>107485.015625</td>\n",
       "      <td>110588.929688</td>\n",
       "      <td>6.350779e+10</td>\n",
       "      <td>3984.696289</td>\n",
       "      <td>4084.159668</td>\n",
       "      <td>3911.726318</td>\n",
       "      <td>3980.760254</td>\n",
       "      <td>4.022461e+10</td>\n",
       "      <td>1109.682129</td>\n",
       "      <td>1142.744507</td>\n",
       "      <td>1090.006836</td>\n",
       "      <td>1101.061646</td>\n",
       "      <td>4.209253e+09</td>\n",
       "      <td>2.390652</td>\n",
       "      <td>2.543082</td>\n",
       "      <td>2.358992</td>\n",
       "      <td>2.493419</td>\n",
       "      <td>4.680467e+09</td>\n",
       "      <td>187.798309</td>\n",
       "      <td>194.341156</td>\n",
       "      <td>184.046631</td>\n",
       "      <td>189.750671</td>\n",
       "      <td>6.103946e+09</td>\n",
       "      <td>0.652550</td>\n",
       "      <td>0.675499</td>\n",
       "      <td>0.640680</td>\n",
       "      <td>0.663251</td>\n",
       "      <td>1.048326e+09</td>\n",
       "      <td>0.195324</td>\n",
       "      <td>0.202447</td>\n",
       "      <td>0.191941</td>\n",
       "      <td>0.200114</td>\n",
       "      <td>2.148033e+09</td>\n",
       "      <td>20.595200</td>\n",
       "      <td>21.161469</td>\n",
       "      <td>20.196623</td>\n",
       "      <td>20.356581</td>\n",
       "      <td>5.884645e+08</td>\n",
       "      <td>3.007848</td>\n",
       "      <td>3.126680</td>\n",
       "      <td>2.952706</td>\n",
       "      <td>3.087536</td>\n",
       "      <td>240039585.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>765134</td>\n",
       "      <td>249721</td>\n",
       "      <td>61761</td>\n",
       "      <td>160</td>\n",
       "      <td>1276677</td>\n",
       "      <td>2275181</td>\n",
       "      <td>9.960484e+08</td>\n",
       "      <td>90302399470</td>\n",
       "      <td>103976.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.203794e+10</td>\n",
       "      <td>8.207787e+10</td>\n",
       "      <td>9.974847e+10</td>\n",
       "      <td>1.058851e+09</td>\n",
       "      <td>328048.0</td>\n",
       "      <td>30723035561</td>\n",
       "      <td>3.383053e+10</td>\n",
       "      <td>6268715245</td>\n",
       "      <td>179308643850</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>6735.129883</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>4336.399902</td>\n",
       "      <td>98.589996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       BTC_Open       BTC_High        BTC_Low      BTC_Close  \\\n",
       "1957 2025-10-11  113236.429688  113429.726562  109760.562500  110807.882812   \n",
       "1958 2025-10-12  110811.515625  115805.062500  109715.539062  115169.765625   \n",
       "1959 2025-10-13  115161.679688  116020.484375  113821.187500  115271.078125   \n",
       "1960 2025-10-14  115264.882812  115502.882812  110029.484375  113118.664062   \n",
       "1961 2025-10-15  113113.968750  113622.382812  110235.835938  110783.164062   \n",
       "1962 2025-10-16  110782.171875  111990.812500  107537.031250  108186.039062   \n",
       "1963 2025-10-17  108179.132812  109235.804688  103598.429688  106467.789062   \n",
       "1964 2025-10-18  106483.734375  107490.984375  106387.453125  107198.265625   \n",
       "1965 2025-10-19  107204.312500  109488.992188  106157.789062  108666.710938   \n",
       "1966 2025-10-20  108667.445312  111711.031250  107485.015625  110588.929688   \n",
       "\n",
       "        BTC_Volume     ETH_Open     ETH_High      ETH_Low    ETH_Close  \\\n",
       "1957  1.102369e+11  3840.960449  3882.241455  3652.790039  3750.611572   \n",
       "1958  9.371041e+10  3750.946045  4195.397461  3701.478271  4164.427734   \n",
       "1959  7.158203e+10  4164.049316  4292.845703  4061.224609  4245.467773   \n",
       "1960  9.221292e+10  4245.372559  4265.105469  3895.973633  4125.412109   \n",
       "1961  7.257413e+10  4125.361328  4213.855957  3935.161377  3987.459473   \n",
       "1962  8.730642e+10  3987.148926  4079.648193  3829.654053  3894.754395   \n",
       "1963  9.970305e+10  3894.377686  3950.566895  3678.620361  3832.558838   \n",
       "1964  3.777991e+10  3833.009521  3927.245605  3822.266357  3890.346191   \n",
       "1965  4.765701e+10  3890.583496  4029.355469  3843.772949  3984.649658   \n",
       "1966  6.350779e+10  3984.696289  4084.159668  3911.726318  3980.760254   \n",
       "\n",
       "        ETH_Volume     BNB_Open     BNB_High      BNB_Low    BNB_Close  \\\n",
       "1957  6.247548e+10  1106.898560  1180.525879  1080.718384  1137.199341   \n",
       "1958  6.121617e+10  1137.199341  1319.771973  1110.197388  1303.117310   \n",
       "1959  5.025378e+10  1303.147339  1370.546021  1257.110229  1293.473267   \n",
       "1960  6.709415e+10  1293.473267  1316.407104  1147.750000  1211.051147   \n",
       "1961  5.046289e+10  1211.051147  1221.707520  1151.835205  1162.691772   \n",
       "1962  4.914739e+10  1162.693726  1193.773926  1125.333252  1145.825317   \n",
       "1963  5.740427e+10  1145.817993  1154.891235  1024.083252  1071.963013   \n",
       "1964  2.381568e+10  1071.965576  1129.192505  1071.473877  1093.152466   \n",
       "1965  3.287066e+10  1093.152466  1132.347290  1069.307495  1109.680054   \n",
       "1966  4.022461e+10  1109.682129  1142.744507  1090.006836  1101.061646   \n",
       "\n",
       "        BNB_Volume  XRP_Open  XRP_High   XRP_Low  XRP_Close    XRP_Volume  \\\n",
       "1957  8.101404e+09  2.358754  2.501994  2.322168   2.385982  1.174965e+10   \n",
       "1958  1.020104e+10  2.385956  2.581764  2.320973   2.534958  9.877639e+09   \n",
       "1959  1.182039e+10  2.535090  2.645213  2.521791   2.606500  8.579699e+09   \n",
       "1960  1.010659e+10  2.606494  2.620473  2.407625   2.505660  8.358304e+09   \n",
       "1961  5.853930e+09  2.505667  2.525715  2.389588   2.412666  5.540248e+09   \n",
       "1962  5.605348e+09  2.412672  2.462859  2.287637   2.329068  7.020912e+09   \n",
       "1963  6.656428e+09  2.328990  2.380460  2.197175   2.294239  7.915225e+09   \n",
       "1964  3.767783e+09  2.294411  2.391555  2.294411   2.360510  2.903250e+09   \n",
       "1965  3.690437e+09  2.360510  2.417567  2.321040   2.390632  3.399617e+09   \n",
       "1966  4.209253e+09  2.390652  2.543082  2.358992   2.493419  4.680467e+09   \n",
       "\n",
       "        SOL_Open    SOL_High     SOL_Low   SOL_Close    SOL_Volume  ADA_Open  \\\n",
       "1957  188.638901  190.879135  173.754791  178.054169  1.285001e+10  0.634835   \n",
       "1958  178.056198  198.873978  173.521393  197.508545  1.179704e+10  0.632143   \n",
       "1959  197.512192  209.249191  191.944168  208.367294  1.146945e+10  0.701531   \n",
       "1960  208.363556  211.105042  191.671021  202.460342  1.360020e+10  0.729262   \n",
       "1961  202.461868  208.331161  192.700058  194.021866  9.867609e+09  0.699693   \n",
       "1962  194.021866  197.955750  182.251343  184.688950  1.004289e+10  0.668800   \n",
       "1963  184.686203  187.428574  174.744171  182.034790  1.040174e+10  0.644944   \n",
       "1964  182.036163  188.267883  182.023987  187.660294  4.108903e+09  0.624848   \n",
       "1965  187.660294  192.200607  183.458252  187.798203  4.863227e+09  0.634461   \n",
       "1966  187.798309  194.341156  184.046631  189.750671  6.103946e+09  0.652550   \n",
       "\n",
       "      ADA_High   ADA_Low  ADA_Close    ADA_Volume  DOGE_Open  DOGE_High  \\\n",
       "1957  0.681960  0.614932   0.632180  2.703055e+09   0.193155   0.200234   \n",
       "1958  0.708216  0.621524   0.701505  1.600069e+09   0.185441   0.214049   \n",
       "1959  0.735054  0.698125   0.729258  1.714657e+09   0.207795   0.218270   \n",
       "1960  0.732587  0.665723   0.699692  1.861960e+09   0.213991   0.215544   \n",
       "1961  0.713327  0.661867   0.668802  1.209548e+09   0.204532   0.207973   \n",
       "1962  0.683554  0.635841   0.644944  1.359176e+09   0.196328   0.201235   \n",
       "1963  0.653942  0.595169   0.624843  1.684168e+09   0.188366   0.190797   \n",
       "1964  0.640819  0.624350   0.634461  5.493960e+08   0.184850   0.190223   \n",
       "1965  0.664487  0.623411   0.652551  8.740383e+08   0.189599   0.198535   \n",
       "1966  0.675499  0.640680   0.663251  1.048326e+09   0.195324   0.202447   \n",
       "\n",
       "      DOGE_Low  DOGE_Close   DOGE_Volume  AVAX_Open  AVAX_High   AVAX_Low  \\\n",
       "1957  0.179012    0.185443  6.948750e+09  20.734438  23.346888  20.566242   \n",
       "1958  0.181756    0.207795  5.183181e+09  21.472364  23.327850  20.714027   \n",
       "1959  0.204746    0.213991  4.745995e+09  22.583363  24.057014  21.945959   \n",
       "1960  0.195104    0.204532  4.841726e+09  23.838711  23.889275  21.988726   \n",
       "1961  0.195098    0.196327  2.877365e+09  22.770382  23.234428  21.639189   \n",
       "1962  0.185209    0.188367  3.498982e+09  21.915653  22.346117  20.674866   \n",
       "1963  0.176211    0.184842  3.317945e+09  20.960007  21.224400  19.092056   \n",
       "1964  0.184532    0.189599  1.238849e+09  19.956371  20.389362  19.857420   \n",
       "1965  0.186295    0.195323  1.982259e+09  20.206861  20.832535  19.774445   \n",
       "1966  0.191941    0.200114  2.148033e+09  20.595200  21.161469  20.196623   \n",
       "\n",
       "      AVAX_Close   AVAX_Volume  DOT_Open  DOT_High   DOT_Low  DOT_Close  \\\n",
       "1957   21.472492  1.589011e+09  2.962416  3.314361  2.900451   3.000315   \n",
       "1958   22.583330  1.271749e+09  3.000277  3.304046  2.923780   3.246990   \n",
       "1959   23.838850  1.570963e+09  3.247090  3.441121  3.205383   3.377107   \n",
       "1960   22.770382  1.229012e+09  3.377107  3.393776  3.088582   3.250740   \n",
       "1961   21.916134  7.284511e+08  3.250765  3.336066  3.110547   3.138686   \n",
       "1962   20.959126  8.083594e+08  3.138686  3.200600  2.980015   3.015078   \n",
       "1963   19.956615  9.257953e+08  3.015115  3.054085  2.784597   2.891856   \n",
       "1964   20.206861  3.456040e+08  2.891856  2.964360  2.891856   2.932491   \n",
       "1965   20.594934  4.488137e+08  2.932491  3.056875  2.888869   3.007835   \n",
       "1966   20.356581  5.884645e+08  3.007848  3.126680  2.952706   3.087536   \n",
       "\n",
       "       DOT_Volume  sentiment_mean  sentiment_std  news_count  positive_ratio  \\\n",
       "1957  864188141.0        0.133333       0.990430        15.0        0.533333   \n",
       "1958  427368745.0       -0.333333       0.975900        15.0        0.333333   \n",
       "1959  477219506.0        0.100000       0.967906        20.0        0.500000   \n",
       "1960  435483489.0        0.608696       0.782718        23.0        0.782609   \n",
       "1961  307547964.0        0.000000       0.816497        10.0        0.300000   \n",
       "1962  372522780.0        0.571429       0.786796         7.0        0.714286   \n",
       "1963  397906017.0       -0.166667       1.029857        12.0        0.416667   \n",
       "1964  146148632.0       -0.333333       1.154701         3.0        0.333333   \n",
       "1965  205544790.0       -0.416667       0.792961        12.0        0.166667   \n",
       "1966  240039585.0        0.000000       0.000000         0.0        0.000000   \n",
       "\n",
       "      negative_ratio  extreme_positive_count  extreme_negative_count  \\\n",
       "1957        0.400000                     8.0                     6.0   \n",
       "1958        0.666667                     5.0                    10.0   \n",
       "1959        0.400000                    10.0                     8.0   \n",
       "1960        0.173913                    18.0                     4.0   \n",
       "1961        0.300000                     3.0                     3.0   \n",
       "1962        0.142857                     5.0                     1.0   \n",
       "1963        0.583333                     5.0                     7.0   \n",
       "1964        0.666667                     1.0                     2.0   \n",
       "1965        0.583333                     2.0                     7.0   \n",
       "1966        0.000000                     0.0                     0.0   \n",
       "\n",
       "      sentiment_sum  sentiment_polarity  sentiment_intensity  \\\n",
       "1957            2.0            0.133333             0.933333   \n",
       "1958           -5.0           -0.333333             1.000000   \n",
       "1959            2.0            0.100000             0.900000   \n",
       "1960           14.0            0.608696             0.956522   \n",
       "1961            0.0            0.000000             0.600000   \n",
       "1962            4.0            0.571429             0.857143   \n",
       "1963           -2.0           -0.166667             1.000000   \n",
       "1964           -1.0           -0.333333             1.000000   \n",
       "1965           -5.0           -0.416667             0.750000   \n",
       "1966            0.0            0.000000             0.000000   \n",
       "\n",
       "      sentiment_disagreement  bull_bear_ratio  weighted_sentiment  \\\n",
       "1957                0.213333         1.333333            0.369678   \n",
       "1958                0.222222         0.500000           -0.924196   \n",
       "1959                0.200000         1.250000            0.304452   \n",
       "1960                0.136106         4.500000            1.934468   \n",
       "1961                0.090000         1.000000            0.000000   \n",
       "1962                0.102041         5.000000            1.188252   \n",
       "1963                0.243056         0.714286           -0.427492   \n",
       "1964                0.222222         0.500000           -0.462098   \n",
       "1965                0.097222         0.285714           -1.068729   \n",
       "1966                0.000000         0.000000            0.000000   \n",
       "\n",
       "      extremity_index  sentiment_ma3  sentiment_volatility_3  sentiment_ma7  \\\n",
       "1957         0.933333       0.377778                0.542969       0.446719   \n",
       "1958         1.000000       0.266667                0.676593       0.323469   \n",
       "1959         0.900000      -0.033333                0.260342       0.235714   \n",
       "1960         0.956522       0.125121                0.471517       0.286957   \n",
       "1961         0.600000       0.236232                0.326415       0.215528   \n",
       "1962         0.857143       0.393375                0.341182       0.297161   \n",
       "1963         1.000000       0.134921                0.387103       0.130494   \n",
       "1964         1.000000       0.023810                0.481518       0.063827   \n",
       "1965         0.750000      -0.305556                0.127294       0.051923   \n",
       "1966         0.000000       0.000000                0.000000       0.000000   \n",
       "\n",
       "      sentiment_volatility_7  sentiment_ma14  sentiment_volatility_14  \\\n",
       "1957                0.347336        0.493185                 0.294966   \n",
       "1958                0.450771        0.433662                 0.368422   \n",
       "1959                0.420805        0.432868                 0.369181   \n",
       "1960                0.444033        0.436664                 0.370809   \n",
       "1961                0.444266        0.412854                 0.388246   \n",
       "1962                0.450518        0.398725                 0.377736   \n",
       "1963                0.352256        0.336225                 0.394581   \n",
       "1964                0.393388        0.255273                 0.408138   \n",
       "1965                0.408385        0.187696                 0.436588   \n",
       "1966                0.000000        0.000000                 0.000000   \n",
       "\n",
       "      sentiment_trend  sentiment_acceleration  news_volume_change  \\\n",
       "1957        -0.866667               -1.866667            6.500000   \n",
       "1958        -0.466667                0.400000            0.000000   \n",
       "1959         0.433333                0.900000            0.333333   \n",
       "1960         0.508696                0.075362            0.150000   \n",
       "1961        -0.608696               -1.117391           -0.565217   \n",
       "1962         0.571429                1.180124           -0.300000   \n",
       "1963        -0.738095               -1.309524            0.714286   \n",
       "1964        -0.166667                0.571429           -0.750000   \n",
       "1965        -0.083333                0.083333            3.000000   \n",
       "1966         0.000000                0.000000            0.000000   \n",
       "\n",
       "      news_volume_ma7  news_volume_ma14  eth_tx_count  eth_active_addresses  \\\n",
       "1957         9.285714         12.785714       1647316                477151   \n",
       "1958         9.000000         12.571429       1480041                453698   \n",
       "1959         8.857143         13.357143       1580074                486049   \n",
       "1960        11.571429         13.714286       1571829                482561   \n",
       "1961        12.428571         13.571429       1634250                553593   \n",
       "1962        13.142857         13.142857       1569376                502395   \n",
       "1963        14.571429         12.285714       1649015                521677   \n",
       "1964        12.857143         11.071429       1357463                450565   \n",
       "1965        12.428571         10.714286       1276777                394132   \n",
       "1966         0.000000          0.000000        765134                249721   \n",
       "\n",
       "      eth_new_addresses  eth_large_eth_transfers  eth_token_transfers  \\\n",
       "1957             126597                      457              2346993   \n",
       "1958             129918                      252              2701386   \n",
       "1959             142490                      298              2367316   \n",
       "1960             129811                      546              2405692   \n",
       "1961             158808                      396              2338271   \n",
       "1962             144816                      337              2045266   \n",
       "1963             152311                      436              2200707   \n",
       "1964             142652                       82              2109711   \n",
       "1965             120858                      121              1774094   \n",
       "1966              61761                      160              1276677   \n",
       "\n",
       "      eth_contract_events  eth_avg_gas_price  eth_total_gas_used  \\\n",
       "1957              4407685       4.097661e+09        161594088388   \n",
       "1958              4730346       1.671546e+09        162883506170   \n",
       "1959              4372228       1.494701e+09        162391660577   \n",
       "1960              4336474       1.662172e+09        162625955934   \n",
       "1961              4286551       1.455372e+09        162655224073   \n",
       "1962              4099252       1.663318e+09        162119681022   \n",
       "1963              4014625       2.959400e+09        162588760223   \n",
       "1964              4199593       9.334411e+08        162919330366   \n",
       "1965              3897490       9.952229e+08        163227107169   \n",
       "1966              2275181       9.960484e+08         90302399470   \n",
       "\n",
       "      eth_avg_block_size  eth_avg_block_difficulty  fg_fear_greed  \\\n",
       "1957           127926.75                       0.0           27.0   \n",
       "1958           122616.43                       0.0           24.0   \n",
       "1959           122354.04                       0.0           38.0   \n",
       "1960           122305.69                       0.0           38.0   \n",
       "1961           121957.18                       0.0           34.0   \n",
       "1962           125317.89                       0.0           28.0   \n",
       "1963           124824.62                       0.0           22.0   \n",
       "1964           110939.81                       0.0           23.0   \n",
       "1965           106516.90                       0.0           29.0   \n",
       "1966           103976.92                       0.0           29.0   \n",
       "\n",
       "      usdt_totalCirculating  usdt_totalCirculatingUSD  usdt_totalMintedUSD  \\\n",
       "1957           7.961207e+10              7.973872e+10         9.641883e+10   \n",
       "1958           8.001645e+10              8.010518e+10         9.711055e+10   \n",
       "1959           8.020210e+10              8.028963e+10         9.733910e+10   \n",
       "1960           8.008058e+10              8.013571e+10         9.768328e+10   \n",
       "1961           8.078072e+10              8.084477e+10         9.842875e+10   \n",
       "1962           8.155061e+10              8.159707e+10         9.918112e+10   \n",
       "1963           8.154415e+10              8.160802e+10         9.932227e+10   \n",
       "1964           8.142839e+10              8.147652e+10         9.935264e+10   \n",
       "1965           8.179682e+10              8.184542e+10         9.969915e+10   \n",
       "1966           8.203794e+10              8.207787e+10         9.974847e+10   \n",
       "\n",
       "      usdt_totalUnreleased  usdt_totalBridgedToUSD  aave_aave_eth_tvl  \\\n",
       "1957          1.495715e+09                312401.0        29753089779   \n",
       "1958          7.584217e+08                312250.0        30241372474   \n",
       "1959          5.284217e+08                312245.0        32405097849   \n",
       "1960          1.145325e+09                312119.0        32891489038   \n",
       "1961          4.106251e+08                312152.0        32277871173   \n",
       "1962          6.367670e+08                312082.0        31598594443   \n",
       "1963          5.168670e+08                328145.0        31073951308   \n",
       "1964          4.674531e+08                328082.0        30484787546   \n",
       "1965          1.121453e+09                328083.0        30834078452   \n",
       "1966          1.058851e+09                328048.0        30723035561   \n",
       "\n",
       "      lido_lido_eth_tvl  makerdao_makerdao_eth_tvl  chain_eth_chain_tvl  \\\n",
       "1957       3.289364e+10                 5596531595         175751448682   \n",
       "1958       3.185586e+10                 5567724995         171886426278   \n",
       "1959       3.535026e+10                 6110212606         186667365937   \n",
       "1960       3.604987e+10                 6087962406         190022043707   \n",
       "1961       3.474401e+10                 6052013458         185877171143   \n",
       "1962       3.382954e+10                 5872771981         181145394259   \n",
       "1963       3.330488e+10                 5940121789         177704176644   \n",
       "1964       3.261156e+10                 5994395509         174687064832   \n",
       "1965       3.301850e+10                 6063156836         176509406573   \n",
       "1966       3.383053e+10                 6268715245         179308643850   \n",
       "\n",
       "      funding_fundingRate  sp500_SP500    vix_VIX    gold_GOLD    dxy_DXY  \n",
       "1957            -0.000011  6552.509766  21.660000  3975.899902  98.980003  \n",
       "1958            -0.000036  6552.509766  21.660000  3975.899902  98.980003  \n",
       "1959             0.000026  6654.720215  19.030001  4108.600098  99.269997  \n",
       "1960             0.000029  6644.310059  20.809999  4138.700195  99.050003  \n",
       "1961             0.000027  6671.060059  20.639999  4176.899902  98.790001  \n",
       "1962             0.000003  6629.069824  25.309999  4280.200195  98.389999  \n",
       "1963             0.000040  6664.009766  20.780001  4189.899902  98.430000  \n",
       "1964             0.000030  6664.009766  20.780001  4189.899902  98.430000  \n",
       "1965             0.000040  6664.009766  20.780001  4189.899902  98.430000  \n",
       "1966             0.000046  6735.129883  18.230000  4336.399902  98.589996  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
