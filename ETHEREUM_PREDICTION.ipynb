{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05847aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 22:38:17.751834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-29 22:38:17.751873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-29 22:38:17.753481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-29 22:38:17.761154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 22:38:18.606750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING\n",
      "================================================================================\n",
      "(26212, 3) news_data.csv\n",
      "(3732, 11) eth_onchain.csv\n",
      "(3224, 46) macro_crypto_data.csv\n",
      "(2218, 2) SP500.csv\n",
      "(2218, 2) VIX.csv\n",
      "(2219, 2) GOLD.csv\n",
      "(2220, 2) DXY.csv\n",
      "(2824, 2) fear_greed.csv\n",
      "(2164, 2) eth_funding_rate.csv\n",
      "(2892, 6) usdt_eth_mcap.csv\n",
      "(1990, 2) aave_eth_tvl.csv\n",
      "(1776, 2) lido_eth_tvl.csv\n",
      "(2492, 2) makerdao_eth_tvl.csv\n",
      "(2549, 2) uniswap_eth_tvl.csv\n",
      "(2084, 2) curve-dex_eth_tvl.csv\n",
      "(2955, 2) eth_chain_tvl.csv\n",
      "(1582, 5) layer2_tvl.csv\n",
      "Loaded 10 files\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT FEATURES\n",
      "================================================================================\n",
      "Generated 23 features\n",
      "\n",
      "================================================================================\n",
      "DATA MERGING\n",
      "================================================================================\n",
      "Merged shape: (2325, 100)\n",
      "Missing before fill: 20,652\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUE HANDLING\n",
      "================================================================================\n",
      "Missing after fill: 0\n",
      "Shape: (2325, 100)\n",
      "Period: 2019-06-15 ~ 2025-10-24\n",
      "Missing: 0\n",
      "\n",
      "Feature groups:\n",
      "  Crypto prices: 45\n",
      "  On-chain: 10\n",
      "  DeFi TVL: 6\n",
      "  Layer 2: 4\n",
      "  Sentiment: 22\n",
      "  Macro: 4\n",
      "  Fear & Greed: 1\n",
      "  Funding Rate: 1\n",
      "  Stablecoin: 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ethereum Price Prediction - Data Loading & Preprocessing\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# 기본 라이브러리 및 유틸리티\n",
    "# ============================================================================\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "# 날짜/시간\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 데이터 처리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# ML/DL 라이브러리 및 도구\n",
    "# ============================================================================\n",
    "\n",
    "# 하이퍼파라미터 최적화\n",
    "import optuna\n",
    "\n",
    "# Scikit-learn: 데이터 전처리\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE,\n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# Scikit-learn: 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Scikit-learn: 앙상블 모델\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, AdaBoostRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    HistGradientBoostingClassifier, # 추가된 항목\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "\n",
    "# Scikit-learn: 평가 지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, # 분류 지표\n",
    "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error # 회귀 지표\n",
    ")\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm.callback import early_stopping \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# TensorFlow/Keras 딥러닝\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    # 기본 레이어\n",
    "    Input, Dense, Flatten, Dropout, Activation,\n",
    "    # RNN 레이어\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    # CNN 레이어\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    # 정규화 레이어\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    # Attention 레이어\n",
    "    Attention, MultiHeadAttention,\n",
    "    # 유틸리티 레이어\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 시계열 분석 (Statsmodels)\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "# PyTorch \n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, AdaBoostClassifier, BaggingClassifier,\n",
    "    GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier,\n",
    "    VotingClassifier, HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# ============================================================================\n",
    "# 환경 설정 및 경고 무시\n",
    "# ============================================================================\n",
    "\n",
    "# GPU 메모리 증가 허용 설정\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DATA_DIR_MAIN = './macro_data'\n",
    "DATA_DIR_NEW = './macro_data/macro_data'\n",
    "\n",
    "TRAIN_START_DATE = pd.to_datetime('2020-01-01')\n",
    "LOOKBACK_DAYS = 200\n",
    "LOOKBACK_START_DATE = TRAIN_START_DATE - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "\n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    print(df.shape,file_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"[Warning] {filename} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    return standardize_date_column(df, filename)\n",
    "\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [f\"{prefix}_{col}\" if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sentiment_features(news_df):\n",
    "    if news_df.empty:\n",
    "        return pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index().fillna(0)\n",
    "    \n",
    "    agg['sentiment_polarity'] = agg['positive_ratio'] - agg['negative_ratio']\n",
    "    agg['sentiment_intensity'] = agg['positive_ratio'] + agg['negative_ratio']\n",
    "    agg['sentiment_disagreement'] = agg['positive_ratio'] * agg['negative_ratio']\n",
    "    agg['bull_bear_ratio'] = agg['positive_ratio'] / (agg['negative_ratio'] + 1e-10)\n",
    "    agg['weighted_sentiment'] = agg['sentiment_mean'] * np.log1p(agg['news_count'])\n",
    "    agg['extremity_index'] = (agg['extreme_positive_count'] + agg['extreme_negative_count']) / (agg['news_count'] + 1e-10)\n",
    "    \n",
    "    for window in [3,7]:\n",
    "        agg[f'sentiment_ma{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        agg[f'sentiment_volatility_{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    agg['sentiment_trend'] = agg['sentiment_mean'].diff()\n",
    "    agg['sentiment_acceleration'] = agg['sentiment_trend'].diff()\n",
    "    agg['news_volume_change'] = agg['news_count'].pct_change()\n",
    "    \n",
    "    for window in [7, 14]:\n",
    "        agg[f'news_volume_ma{window}'] = agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return agg.fillna(0)\n",
    "\n",
    "\n",
    "def smart_fill_missing(df_merged):\n",
    "    REFERENCE_START_DATE = pd.to_datetime('2020-01-01')\n",
    "    \n",
    "    for col in df_merged.columns:\n",
    "        if col == 'date':\n",
    "            continue\n",
    "        \n",
    "        if df_merged[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        non_null_idx = df_merged[col].first_valid_index()\n",
    "        \n",
    "        if non_null_idx is None:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "            continue\n",
    "        \n",
    "        first_date = df_merged.loc[non_null_idx, 'date']\n",
    "        \n",
    "        before_mask = df_merged['date'] < first_date\n",
    "        after_mask = df_merged['date'] >= first_date\n",
    "        \n",
    "        df_merged.loc[before_mask, col] = df_merged.loc[before_mask, col].fillna(0)\n",
    "        df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(method='ffill')\n",
    "        \n",
    "        remaining = df_merged.loc[after_mask, col].isnull().sum()\n",
    "        if remaining > 0:\n",
    "            df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(0)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "news_df = load_csv(DATA_DIR_MAIN, 'news_data.csv')\n",
    "eth_onchain_df = load_csv(DATA_DIR_MAIN, 'eth_onchain.csv')\n",
    "macro_df = load_csv(DATA_DIR_NEW, 'macro_crypto_data.csv')\n",
    "sp500_df = load_csv(DATA_DIR_NEW, 'SP500.csv')\n",
    "vix_df = load_csv(DATA_DIR_NEW, 'VIX.csv')\n",
    "gold_df = load_csv(DATA_DIR_NEW, 'GOLD.csv')\n",
    "dxy_df = load_csv(DATA_DIR_NEW, 'DXY.csv')\n",
    "fear_greed_df = load_csv(DATA_DIR_NEW, 'fear_greed.csv')\n",
    "eth_funding_df = load_csv(DATA_DIR_NEW, 'eth_funding_rate.csv')\n",
    "usdt_eth_mcap_df = load_csv(DATA_DIR_NEW, 'usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_csv(DATA_DIR_NEW, 'aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_csv(DATA_DIR_NEW, 'lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_csv(DATA_DIR_NEW, 'makerdao_eth_tvl.csv')\n",
    "uniswap_tvl_df = load_csv(DATA_DIR_NEW, 'uniswap_eth_tvl.csv')\n",
    "curve_tvl_df = load_csv(DATA_DIR_NEW, 'curve-dex_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_csv(DATA_DIR_NEW, 'eth_chain_tvl.csv')\n",
    "layer2_tvl_df = load_csv(DATA_DIR_NEW, 'layer2_tvl.csv')\n",
    "\n",
    "print(f\"Loaded {len([df for df in [fear_greed_df, eth_funding_df, usdt_eth_mcap_df, aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df, eth_chain_tvl_df, layer2_tvl_df] if not df.empty])} files\")\n",
    "\n",
    "all_dataframes = [\n",
    "    macro_df, news_df, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df, \n",
    "    sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "last_dates = [\n",
    "    pd.to_datetime(df['date']).max() \n",
    "    for df in all_dataframes \n",
    "    if not df.empty and 'date' in df.columns\n",
    "]\n",
    "\n",
    "end_date = min(last_dates) if last_dates else pd.Timestamp.today()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "sentiment_features = create_sentiment_features(news_df)\n",
    "print(f\"Generated {sentiment_features.shape[1]-1} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA MERGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "uniswap_tvl_df = add_prefix(uniswap_tvl_df, 'uniswap')\n",
    "curve_tvl_df = add_prefix(curve_tvl_df, 'curve')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "layer2_tvl_df = add_prefix(layer2_tvl_df, 'l2')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=LOOKBACK_START_DATE, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df, sentiment_features, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df,\n",
    "    sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "for df in dataframes_to_merge:\n",
    "    if not df.empty:\n",
    "        df_merged = pd.merge(df_merged, df, on='date', how='left')\n",
    "\n",
    "print(f\"Merged shape: {df_merged.shape}\")\n",
    "print(f\"Missing before fill: {df_merged.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUE HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_merged = smart_fill_missing(df_merged)\n",
    "\n",
    "missing_after = df_merged.isnull().sum().sum()\n",
    "print(f\"Missing after fill: {missing_after:,}\")\n",
    "\n",
    "if missing_after > 0:\n",
    "    df_merged = df_merged.fillna(0)\n",
    "    print(f\"Remaining filled with 0\")\n",
    "\n",
    "lookback_df = df_merged[df_merged['date'] < TRAIN_START_DATE]\n",
    "cols_to_drop = [\n",
    "    col for col in lookback_df.columns \n",
    "    if lookback_df[col].isnull().all() and col != 'date'\n",
    "]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping {len(cols_to_drop)} fully missing columns\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Shape: {df_merged.shape}\")\n",
    "print(f\"Period: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"Missing: {df_merged.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nFeature groups:\")\n",
    "print(f\"  Crypto prices: {len([c for c in df_merged.columns if any(x in c for x in ['BTC_', 'ETH_', 'BNB_', 'XRP_', 'SOL_', 'ADA_', 'DOGE_', 'AVAX_', 'DOT_'])])}\")\n",
    "print(f\"  On-chain: {len([c for c in df_merged.columns if c.startswith('eth_')])}\")\n",
    "print(f\"  DeFi TVL: {len([c for c in df_merged.columns if any(x in c for x in ['aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_'])])}\")\n",
    "print(f\"  Layer 2: {len([c for c in df_merged.columns if c.startswith('l2_')])}\")\n",
    "print(f\"  Sentiment: {len([c for c in df_merged.columns if any(x in c for x in ['sentiment', 'news', 'bull_bear', 'positive', 'negative', 'extreme'])])}\")\n",
    "print(f\"  Macro: {len([c for c in df_merged.columns if any(x in c for x in ['sp500_', 'vix_', 'gold_', 'dxy_'])])}\")\n",
    "print(f\"  Fear & Greed: {len([c for c in df_merged.columns if c.startswith('fg_')])}\")\n",
    "print(f\"  Funding Rate: {len([c for c in df_merged.columns if c.startswith('funding_')])}\")\n",
    "print(f\"  Stablecoin: {len([c for c in df_merged.columns if c.startswith('usdt_')])}\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311eef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>eth_tx_count</th>\n",
       "      <th>eth_active_addresses</th>\n",
       "      <th>eth_new_addresses</th>\n",
       "      <th>eth_large_eth_transfers</th>\n",
       "      <th>eth_token_transfers</th>\n",
       "      <th>eth_contract_events</th>\n",
       "      <th>eth_avg_gas_price</th>\n",
       "      <th>eth_total_gas_used</th>\n",
       "      <th>eth_avg_block_size</th>\n",
       "      <th>eth_avg_block_difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-07</td>\n",
       "      <td>2050</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.046842e+11</td>\n",
       "      <td>49353826</td>\n",
       "      <td>632.63</td>\n",
       "      <td>1.470839e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>2881</td>\n",
       "      <td>605</td>\n",
       "      <td>430</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.227136e+11</td>\n",
       "      <td>376006093</td>\n",
       "      <td>667.59</td>\n",
       "      <td>1.586124e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>1329</td>\n",
       "      <td>462</td>\n",
       "      <td>252</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.754671e+11</td>\n",
       "      <td>38863003</td>\n",
       "      <td>618.30</td>\n",
       "      <td>1.709480e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>2037</td>\n",
       "      <td>821</td>\n",
       "      <td>632</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>4.216549e+11</td>\n",
       "      <td>74070061</td>\n",
       "      <td>631.19</td>\n",
       "      <td>1.837696e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>4963</td>\n",
       "      <td>2132</td>\n",
       "      <td>1881</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>7.783882e+10</td>\n",
       "      <td>163481740</td>\n",
       "      <td>692.01</td>\n",
       "      <td>2.036391e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2036</td>\n",
       "      <td>581</td>\n",
       "      <td>259</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>4.449024e+11</td>\n",
       "      <td>70102332</td>\n",
       "      <td>653.43</td>\n",
       "      <td>2.207080e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-08-13</td>\n",
       "      <td>2842</td>\n",
       "      <td>872</td>\n",
       "      <td>452</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2.686835e+11</td>\n",
       "      <td>88234087</td>\n",
       "      <td>665.72</td>\n",
       "      <td>2.336980e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>3174</td>\n",
       "      <td>1115</td>\n",
       "      <td>562</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1.934555e+11</td>\n",
       "      <td>78746522</td>\n",
       "      <td>659.75</td>\n",
       "      <td>2.671253e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>2284</td>\n",
       "      <td>857</td>\n",
       "      <td>294</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1.443689e+11</td>\n",
       "      <td>59565914</td>\n",
       "      <td>638.34</td>\n",
       "      <td>3.378028e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-08-16</td>\n",
       "      <td>2440</td>\n",
       "      <td>765</td>\n",
       "      <td>202</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1.209401e+11</td>\n",
       "      <td>58241191</td>\n",
       "      <td>640.65</td>\n",
       "      <td>3.631632e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  eth_tx_count  eth_active_addresses  eth_new_addresses  \\\n",
       "0 2015-08-07          2050                   784                784   \n",
       "1 2015-08-08          2881                   605                430   \n",
       "2 2015-08-09          1329                   462                252   \n",
       "3 2015-08-10          2037                   821                632   \n",
       "4 2015-08-11          4963                  2132               1881   \n",
       "5 2015-08-12          2036                   581                259   \n",
       "6 2015-08-13          2842                   872                452   \n",
       "7 2015-08-14          3174                  1115                562   \n",
       "8 2015-08-15          2284                   857                294   \n",
       "9 2015-08-16          2440                   765                202   \n",
       "\n",
       "   eth_large_eth_transfers  eth_token_transfers  eth_contract_events  \\\n",
       "0                      283                    0                    0   \n",
       "1                      186                    0                    6   \n",
       "2                      124                    0                   11   \n",
       "3                      115                    0                   22   \n",
       "4                      150                    0                   42   \n",
       "5                      118                    0                  111   \n",
       "6                      176                    0                  136   \n",
       "7                      265                    0                   81   \n",
       "8                      179                    0                   61   \n",
       "9                      165                    0                   59   \n",
       "\n",
       "   eth_avg_gas_price  eth_total_gas_used  eth_avg_block_size  \\\n",
       "0       6.046842e+11            49353826              632.63   \n",
       "1       3.227136e+11           376006093              667.59   \n",
       "2       4.754671e+11            38863003              618.30   \n",
       "3       4.216549e+11            74070061              631.19   \n",
       "4       7.783882e+10           163481740              692.01   \n",
       "5       4.449024e+11            70102332              653.43   \n",
       "6       2.686835e+11            88234087              665.72   \n",
       "7       1.934555e+11            78746522              659.75   \n",
       "8       1.443689e+11            59565914              638.34   \n",
       "9       1.209401e+11            58241191              640.65   \n",
       "\n",
       "   eth_avg_block_difficulty  \n",
       "0              1.470839e+12  \n",
       "1              1.586124e+12  \n",
       "2              1.709480e+12  \n",
       "3              1.837696e+12  \n",
       "4              2.036391e+12  \n",
       "5              2.207080e+12  \n",
       "6              2.336980e+12  \n",
       "7              2.671253e+12  \n",
       "8              3.378028e+12  \n",
       "9              3.631632e+12  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_onchain_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8ecc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (14만 - 모든 fold 선택)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        \n",
    "        # MACD (필수 - 자주 선택됨)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic (14만 - 나머지는 중복)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (10만 - 20과 거의 동일)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        \n",
    "        # MOM (10만 유지)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        \n",
    "        # CCI (14, 50만 - 극단값 비교용)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_14'] > 100).astype(int)\n",
    "      \n",
    "        # TSI\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "        \n",
    "        # Ichimoku (유지 - 복합 지표로 유용)\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (20, 50만 - Golden Cross용)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        \n",
    "        # EMA (12, 26만 - MACD 구성 요소)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        \n",
    "        # TEMA (10만 - 30과 중복)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        \n",
    "        # WMA (20만 - 10과 중복)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)\n",
    "        \n",
    "        # HMA (유지 - 독특한 smoothing)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (유지)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        # VWMA (유지 - 거래량 가중)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합 (유지 - 다른 정보)\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands \n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        \n",
    "        # NATR\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Supertrend\n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        # ===== VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Fold에서 선택됨)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX (필수)\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex \n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO \n",
    "        df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        \n",
    "        # 변동성 \n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        \n",
    "        # 모멘텀 \n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1\n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)\n",
    "        \n",
    "        # Range 지표 \n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)\n",
    "        \n",
    "        # Linear Regression Slope \n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing \n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        \n",
    "        # BOP\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 파생 \n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        \n",
    "        # RSI 파생\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (Fold에서 선택됨)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        # Distance from High/Low \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 5]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        for period in [5, 20]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        \n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['eth_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        \n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['eth_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = df_transformed[col_name].rolling(30, min_periods=10).std().fillna(0)\n",
    "            \n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(btc_volume.pct_change())\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (eth_volume / (btc_volume + 1e-8)).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [col for col in df_transformed.columns if 'return' in col.lower() and 'next' not in col]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35380b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = ['sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio', 'negative_ratio']\n",
    "    sentiment_ma_cols = [col for col in df.columns if 'sentiment' in col and ('_ma7' in col or '_volatility_7' in col)]\n",
    "    no_lag_patterns = ['_trend', '_acceleration', '_volume_change', 'news_volume_change', 'news_volume_ma']\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', \n",
    "                                  'eth_contract', 'eth_avg_gas', 'eth_total_gas', 'eth_avg_block'])]\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col or '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            if not any(pattern in col for pattern in no_lag_patterns):\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def create_targets(df):\n",
    "    df_target = df.copy()\n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "    df_target['next_direction'] = (next_close > next_open).astype(int)\n",
    "    df_target['next_open'] = next_open\n",
    "    df_target['next_close'] = next_close\n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3b62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df):\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' not in df.columns:\n",
    "        return df_norm\n",
    "    \n",
    "    current_close = df['ETH_Close']\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb17c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=30):\n",
    "    \n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'return':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'price':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'direction_return':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Return (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        ret_features, ret_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + ret_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **ret_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'ret_stats': ret_stats,\n",
    "            'overlap': len(set(dir_features) & set(ret_features))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    elif target_type == 'direction_price':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Price (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        price_features, price_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + price_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **price_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'price_stats': price_stats,\n",
    "            'overlap': len(set(dir_features) & set(price_features))\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target_type: {target_type}\")\n",
    "    \n",
    "    print(\"Selected Features\")\n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=30, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "def split_tvt_method(df, train_start_date, test_start_date='2025-01-01', \n",
    "                     train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    test_start_date를 고정하고, 그 이전 데이터를 train/val로 분할\n",
    "    test_start_date 이후 데이터는 모두 test로 사용\n",
    "    \"\"\"\n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    \n",
    "    # 테스트 시작 날짜를 datetime으로 변환\n",
    "    if isinstance(test_start_date, str):\n",
    "        test_start_date = pd.to_datetime(test_start_date)\n",
    "    \n",
    "    # test_start_date 이전 데이터를 train/val로, 이후를 test로 분할\n",
    "    pre_test_df = df_period[df_period['date'] < test_start_date].copy()\n",
    "    test_df = df_period[df_period['date'] >= test_start_date].copy()\n",
    "    \n",
    "    # train/val 분할 (test 이전 데이터만 사용)\n",
    "    n_pre_test = len(pre_test_df)\n",
    "    train_end = int(n_pre_test * train_ratio / (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = pre_test_df.iloc[:train_end].copy()\n",
    "    val_df = pre_test_df.iloc[train_end:].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TVT Split (Fixed Test Start: {test_start_date.date()})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Train: {len(train_df):4d} ({train_df['date'].min().date()} ~ {train_df['date'].max().date()})\")\n",
    "    print(f\"  Val:   {len(val_df):4d} ({val_df['date'].min().date()} ~ {val_df['date'].max().date()})\")\n",
    "    print(f\"  Test:  {len(test_df):4d} ({test_df['date'].min().date()} ~ {test_df['date'].max().date()})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, \n",
    "                              final_test_start='2025-01-01',\n",
    "                              initial_train_size=800,    \n",
    "                              val_size=150,             \n",
    "                              test_size=150,           \n",
    "                              step=150,                  \n",
    "                              gap_size=7):\n",
    "    \"\"\"\n",
    "    Reverse Rolling Walk-Forward Validation\n",
    "    - 마지막 날짜부터 시작해서 과거로 rolling\n",
    "    - Train 크기는 고정 (initial_train_size)\n",
    "    - Final holdout은 2025-01-01부터 고정\n",
    "    \"\"\"\n",
    "    \n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    total_days = len(df_period)\n",
    "    min_required_days = initial_train_size + val_size + (gap_size * 2) + test_size\n",
    "    n_splits = (total_days - min_required_days) // step + 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Reverse Rolling Walk-Forward Configuration \")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total: {len(df_period)} days\")\n",
    "    print(f\"Rolling train size: {initial_train_size} days (FIXED)\")\n",
    "    print(f\"Val: {val_size} days | Test: {test_size} days\")\n",
    "    print(f\"Gap: {gap_size} days | Step: {step} days (BACKWARD)\")\n",
    "    print(f\"Target: {n_splits} walk-forward + 1 final holdout\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    # 역방향 rolling\n",
    "    for fold_idx in range(n_splits):\n",
    "        test_end_idx = total_days - (fold_idx * step)\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        \n",
    "        if test_start_idx < 0:\n",
    "            break\n",
    "        \n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        \n",
    "        train_end_idx = val_start_idx - gap_size\n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0:\n",
    "            break\n",
    "        \n",
    "        train_fold = df_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'fold_type': 'walk_forward_rolling_reverse'\n",
    "        })\n",
    "    \n",
    "    # 시간순으로 정렬\n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "        \n",
    "        print(f\"Fold {fold['fold_idx']} (walk_forward_rolling)\")\n",
    "        print(f\"  Train: {len(fold['train']):4d}d  {fold['train']['date'].min().date()} ~ {fold['train']['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(fold['val']):4d}d  {fold['val']['date'].min().date()} ~ {fold['val']['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(fold['test']):4d}d  {fold['test']['date'].min().date()} ~ {fold['test']['date'].max().date()}\\n\")\n",
    "    \n",
    "    # Final holdout\n",
    "    if len(final_test_df) > 0:\n",
    "        pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "        \n",
    "        final_val_end_idx = len(pre_final_df)\n",
    "        final_val_start_idx = final_val_end_idx - val_size\n",
    "        final_train_end_idx = final_val_start_idx - gap_size\n",
    "        final_train_start_idx = final_train_end_idx - initial_train_size\n",
    "        \n",
    "        if final_train_start_idx < 0:\n",
    "            final_train_start_idx = 0\n",
    "        \n",
    "        final_train_data = pre_final_df.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "        final_val_data = pre_final_df.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "        \n",
    "        print(f\"Fold {len(folds) + 1} (final_holdout)\")\n",
    "        print(f\"  Train: {len(final_train_data):4d}d  {final_train_data['date'].min().date()} ~ {final_train_data['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(final_val_data):4d}d  {final_val_data['date'].min().date()} ~ {final_val_data['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(final_test_df):4d}d  {final_test_df['date'].min().date()} ~ {final_test_df['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Created {len(folds)} folds total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "    \"\"\"\n",
    "    각 fold를 독립적으로 처리 (feature selection 포함)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    if fold_idx is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Fold {fold_idx} ({fold_type})\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(),\n",
    "        train_start_date=train_df['date'].min(),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(),\n",
    "        train_start_date=val_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(),\n",
    "        train_start_date=test_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    \n",
    "    train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    print(f\"\\n[Feature Selection for Fold {fold_idx}]\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        target_type=target_type, \n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for this fold\")\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    print(f\"Scaling completed for Fold {fold_idx}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_df['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),  \n",
    "            'dates': test_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'scaler': robust_scaler, \n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                     final_test_start='2025-01-01',\n",
    "                                     method='tvt', target_type='direction', **kwargs):\n",
    "    \"\"\"\n",
    "    전체 파이프라인 실행 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : DataFrame\n",
    "        원본 데이터\n",
    "    train_start_date : str\n",
    "        학습 데이터 시작 날짜\n",
    "    final_test_start : str, default='2025-01-01'\n",
    "        최종 고정 테스트 시작 날짜\n",
    "        - TVT: 이 날짜부터 마지막까지 테스트\n",
    "        - Walk-forward: 이 날짜 이전은 walk-forward folds, 이후는 final holdout\n",
    "    method : str, default='tvt'\n",
    "        'tvt' 또는 'walk_forward'\n",
    "    target_type : str, default='direction'\n",
    "        'direction', 'return', 'price', 'direction_return', 'direction_price'\n",
    "    **kwargs : dict\n",
    "        각 method에 필요한 추가 파라미터\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "\n",
    "    df = create_targets(df)\n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = remove_raw_prices_and_transform(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    df = df.iloc[:-1]  \n",
    "    \n",
    "    split_kwargs = {}\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        split_kwargs['test_start_date'] = final_test_start\n",
    "        if 'train_ratio' in kwargs:\n",
    "            split_kwargs['train_ratio'] = kwargs['train_ratio']\n",
    "        if 'val_ratio' in kwargs:\n",
    "            split_kwargs['val_ratio'] = kwargs['val_ratio']\n",
    "        splits = split_tvt_method(df, train_start_date, **split_kwargs)\n",
    "        \n",
    "    elif method == 'walk_forward':\n",
    "        split_kwargs['final_test_start'] = final_test_start\n",
    "        if 'n_splits' in kwargs:\n",
    "            split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "        if 'initial_train_size' in kwargs:\n",
    "            split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "        if 'test_size' in kwargs:\n",
    "            split_kwargs['test_size'] = kwargs['test_size']\n",
    "        if 'val_size' in kwargs:\n",
    "            split_kwargs['val_size'] = kwargs['val_size']\n",
    "        if 'step' in kwargs:\n",
    "            split_kwargs['step'] = kwargs['step']\n",
    "        splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        result = process_single_split(\n",
    "            splits, \n",
    "            target_type=target_type,  \n",
    "            top_n=30,\n",
    "            fold_idx=1\n",
    "        )\n",
    "    else:\n",
    "        result = [\n",
    "            process_single_split(\n",
    "                fold, \n",
    "                target_type=target_type,  \n",
    "                top_n=30,\n",
    "                fold_idx=fold['fold_idx']\n",
    "            ) \n",
    "            for fold in splits\n",
    "        ]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33110e9",
   "metadata": {},
   "source": [
    "## dropout 설정때매 오류난다 캐서 그거 변경한 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9db4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TimeSeriesAugmentation:\n",
    "    \"\"\"\n",
    "    시계열 데이터 증강을 위한 유틸리티 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def jittering(X, sigma=0.02):\n",
    "        \"\"\"\n",
    "        가우시안 노이즈 추가\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, sigma, X.shape)\n",
    "        return X + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaling(X, sigma=0.1):\n",
    "        \"\"\"\n",
    "        랜덤 스케일링 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], 1, X.shape[2]))\n",
    "        else:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], X.shape[1]))\n",
    "        return X * factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def magnitude_warping(X, sigma=0.2, num_knots=4):\n",
    "        \"\"\"\n",
    "        진폭 왜곡 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            seq_len = X.shape[1]\n",
    "            orig_steps = np.linspace(0, seq_len - 1, num_knots + 2)\n",
    "            random_warps = np.random.normal(1, sigma, size=(X.shape[0], num_knots + 2, X.shape[2]))\n",
    "            \n",
    "            warped_X = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    warper = np.interp(np.arange(seq_len), orig_steps, random_warps[i, :, j])\n",
    "                    warped_X[i, :, j] = X[i, :, j] * warper\n",
    "            return warped_X\n",
    "        else:\n",
    "            return X * np.random.normal(1, sigma, X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_augmentation(X, method='jittering', **kwargs):\n",
    "        \"\"\"\n",
    "        선택된 증강 기법 적용\n",
    "        \"\"\"\n",
    "        if method == 'jittering':\n",
    "            return TimeSeriesAugmentation.jittering(X, **kwargs)\n",
    "        elif method == 'scaling':\n",
    "            return TimeSeriesAugmentation.scaling(X, **kwargs)\n",
    "        elif method == 'magnitude_warping':\n",
    "            return TimeSeriesAugmentation.magnitude_warping(X, **kwargs)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 70),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 35),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_samples': trial.suggest_float('max_samples', 0.6, 0.8),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 40, 100),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.01),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.01),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'bootstrap': True\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False, n_jobs=1)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1, bootstrap=True)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Random Forest] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 15, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 20.0, log=True),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 50, 100),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0, log=True),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.01, 1.0, log=True),\n",
    "                'path_smooth': trial.suggest_float('path_smooth', 0.0, 1.0),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.8),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.8),\n",
    "                'bagging_freq': 1,\n",
    "                'random_state': 42,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='binary_logloss',\n",
    "                callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "        best_params['verbose'] = -1\n",
    "        best_params['force_col_wise'] = True\n",
    "        best_params['bagging_freq'] = 1\n",
    "\n",
    "        final_model = LGBMClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[LightGBM] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return final_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.8),\n",
    "                'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 2.0, 20.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 10, 30),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 2.0, log=True),\n",
    "                'max_delta_step': trial.suggest_float('max_delta_step', 0, 3),\n",
    "                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.8, 1.5),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'tree_method': 'hist',\n",
    "                'eval_metric': 'logloss'\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "        best_params['n_jobs'] = -1\n",
    "        best_params['tree_method'] = 'hist'\n",
    "        best_params['eval_metric'] = 'logloss'\n",
    "\n",
    "        final_model = XGBClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[XGBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def histgradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 25, 70),\n",
    "                'l2_regularization': trial.suggest_float('l2_regularization', 1.0, 20.0, log=True),\n",
    "                'max_bins': trial.suggest_int('max_bins', 128, 255),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 40),\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 20,\n",
    "                'validation_fraction': 0.1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = HistGradientBoostingClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        \n",
    "        best_model = HistGradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[HistGradientBoosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def svm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'C': trial.suggest_float('C', 0.01, 1.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0001, 0.01, log=True),\n",
    "                'kernel': 'rbf',\n",
    "                'probability': True,\n",
    "                'random_state': 42,\n",
    "                'cache_size': 2000,\n",
    "                'max_iter': 2000\n",
    "            }\n",
    "            \n",
    "            model = SVC(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_model = SVC(**study.best_params, random_state=42, probability=True, cache_size=2000)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[SVM] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'C': trial.suggest_float('C', 0.01, 5.0, log=True),\n",
    "                'penalty': 'l2',\n",
    "                'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "                'max_iter': 3000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = LogisticRegression(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_model = LogisticRegression(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Logistic Regression] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def naive_bayes(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            var_smoothing = trial.suggest_float('var_smoothing', 1e-11, 1e-5, log=True)\n",
    "            \n",
    "            model = GaussianNB(var_smoothing=var_smoothing)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        model = GaussianNB(var_smoothing=study.best_params['var_smoothing'])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = model.score(X_train, y_train)\n",
    "        val_acc = model.score(X_val, y_val)\n",
    "        print(f\"[Naive Bayes] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def knn(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_neighbors': trial.suggest_int('n_neighbors', 15, 35),\n",
    "                'weights': 'distance',\n",
    "                'metric': 'manhattan',\n",
    "                'leaf_size': trial.suggest_int('leaf_size', 30, 80),\n",
    "                'p': 1,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = KNeighborsClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_model = KNeighborsClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[KNN] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 30, 100),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5),\n",
    "                'algorithm': 'SAMME',\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            base_max_depth = trial.suggest_int('base_max_depth', 1, 3)\n",
    "            base_min_samples_split = trial.suggest_int('base_min_samples_split', 30, 60)\n",
    "            base_min_samples_leaf = trial.suggest_int('base_min_samples_leaf', 15, 30)\n",
    "            \n",
    "            base_estimator = DecisionTreeClassifier(\n",
    "                max_depth=base_max_depth,\n",
    "                min_samples_split=base_min_samples_split,\n",
    "                min_samples_leaf=base_min_samples_leaf,\n",
    "                max_features='sqrt',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            model = AdaBoostClassifier(estimator=base_estimator, **param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            max_depth=best_params['base_max_depth'],\n",
    "            min_samples_split=best_params['base_min_samples_split'],\n",
    "            min_samples_leaf=best_params['base_min_samples_leaf'],\n",
    "            max_features='sqrt',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model = AdaBoostClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            algorithm='SAMME',\n",
    "            random_state=42\n",
    "        )\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[AdaBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'depth': trial.suggest_int('depth', 2, 4),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 8.0, 20.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'rsm': trial.suggest_float('rsm', 0.4, 0.7),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 40, 80),\n",
    "                'random_seed': 42,\n",
    "                'verbose': False,\n",
    "                'early_stopping_rounds': 20\n",
    "            }\n",
    "            \n",
    "            model = CatBoostClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_val, y_val),\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=15)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        model = CatBoostClassifier(**study.best_params, random_seed=42, verbose=False)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        train_acc = model.score(X_train, y_train)\n",
    "        val_acc = model.score(X_val, y_val)\n",
    "        print(f\"[CatBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def decision_tree(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 30, 70),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 15, 35),\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 40, 120),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.02),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.02),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = DecisionTreeClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        \n",
    "        best_model = DecisionTreeClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Decision Tree] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def extra_trees(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 150, step=25)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 20, 60)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 10, 30)\n",
    "            max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "            max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 30, 100)\n",
    "            min_impurity_decrease = trial.suggest_float('min_impurity_decrease', 0.0, 0.02)\n",
    "            ccp_alpha = trial.suggest_float('ccp_alpha', 0.0, 0.02)\n",
    "            bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "            if bootstrap:\n",
    "                max_samples = trial.suggest_float('max_samples', 0.5, 0.8)\n",
    "            else:\n",
    "                max_samples = None\n",
    "\n",
    "            model = ExtraTreesClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                min_impurity_decrease=min_impurity_decrease,\n",
    "                ccp_alpha=ccp_alpha,\n",
    "                bootstrap=bootstrap,\n",
    "                max_samples=max_samples,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        bootstrap = best_params.pop('bootstrap')\n",
    "        max_samples = best_params.pop('max_samples', None)\n",
    "\n",
    "        model = ExtraTreesClassifier(\n",
    "            **best_params,\n",
    "            bootstrap=bootstrap,\n",
    "            max_samples=max_samples if bootstrap else None,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        train_pred = model.predict(X_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[ExtraTrees] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bagging(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            base_max_depth = trial.suggest_int('base_max_depth', 6, 12)\n",
    "            base_min_samples_split = trial.suggest_int('base_min_samples_split', 25, 50)\n",
    "            base_min_samples_leaf = trial.suggest_int('base_min_samples_leaf', 10, 25)\n",
    "            \n",
    "            base_estimator = DecisionTreeClassifier(\n",
    "                max_depth=base_max_depth,\n",
    "                min_samples_split=base_min_samples_split,\n",
    "                min_samples_leaf=base_min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'max_samples': trial.suggest_float('max_samples', 0.6, 0.8),\n",
    "                'max_features': trial.suggest_float('max_features', 0.6, 0.8),\n",
    "                'bootstrap': True,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = BaggingClassifier(estimator=base_estimator, **param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            max_depth=best_params['base_max_depth'],\n",
    "            min_samples_split=best_params['base_min_samples_split'],\n",
    "            min_samples_leaf=best_params['base_min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model = BaggingClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_samples=best_params['max_samples'],\n",
    "            max_features=best_params['max_features'],\n",
    "            bootstrap=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Bagging] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 80),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 40),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 80),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.02),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.02),\n",
    "                'validation_fraction': 0.15,\n",
    "                'n_iter_no_change': 15,\n",
    "                'tol': 0.001,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=12),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_model = GradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Gradient Boosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def mlp(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 160, step=32)\n",
    "            units2 = trial.suggest_int('units2', 32, 80, step=16)\n",
    "            units3 = trial.suggest_int('units3', 16, 48, step=8)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.7)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "            \n",
    "            input_dim = X_train.shape[1]\n",
    "            model = Sequential([\n",
    "                Dense(units1, activation='relu', input_dim=input_dim, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout * 0.8),\n",
    "                Dense(units3, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.6),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=60,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=8)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = Sequential([\n",
    "            Dense(best_params['units1'], activation='relu', input_dim=input_dim, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(best_params['units2'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout'] * 0.8),\n",
    "            Dense(best_params['units3'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.6),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=8, min_lr=1e-7, mode='min', verbose=0)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=120,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[MLP] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            xgb_estimators = trial.suggest_int('xgb_estimators', 100, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_depth', 3, 5)\n",
    "            xgb_lr = trial.suggest_float('xgb_lr', 0.01, 0.05, log=True)\n",
    "            \n",
    "            lgbm_estimators = trial.suggest_int('lgbm_estimators', 100, 200)\n",
    "            lgbm_depth = trial.suggest_int('lgbm_depth', 3, 5)\n",
    "            lgbm_lr = trial.suggest_float('lgbm_lr', 0.01, 0.05, log=True)\n",
    "            \n",
    "            meta_C = trial.suggest_float('meta_C', 0.1, 2.0, log=True)\n",
    "            \n",
    "            base_learners = [\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_estimators,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=xgb_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=3.0,\n",
    "                    min_child_weight=10,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=lgbm_estimators,\n",
    "                    max_depth=lgbm_depth,\n",
    "                    learning_rate=lgbm_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=2.0,\n",
    "                    min_child_samples=60,\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    force_col_wise=True\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            meta_learner = LogisticRegression(max_iter=3000, C=meta_C, random_state=42, penalty='l2')\n",
    "            \n",
    "            model = StackingClassifier(\n",
    "                estimators=base_learners,\n",
    "                final_estimator=meta_learner,\n",
    "                cv=7,\n",
    "                n_jobs=-1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=3)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_learners = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_estimators'],\n",
    "                max_depth=best_params['xgb_depth'],\n",
    "                learning_rate=best_params['xgb_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=3.0,\n",
    "                min_child_weight=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=best_params['lgbm_estimators'],\n",
    "                max_depth=best_params['lgbm_depth'],\n",
    "                learning_rate=best_params['lgbm_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=60,\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "                force_col_wise=True\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        meta_learner = LogisticRegression(max_iter=3000, C=best_params['meta_C'], random_state=42, penalty='l2')\n",
    "        \n",
    "        best_model = StackingClassifier(\n",
    "            estimators=base_learners,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=7,\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Stacking] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def voting_hard(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            xgb_n_est = trial.suggest_int('xgb_n_estimators', 100, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_max_depth', 3, 5)\n",
    "            lgbm_n_est = trial.suggest_int('lgbm_n_estimators', 100, 200)\n",
    "            \n",
    "            estimators = [\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_n_est,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=0.03,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=3.0,\n",
    "                    min_child_weight=10,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=lgbm_n_est,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.03,\n",
    "                    subsample=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=2.0,\n",
    "                    min_child_samples=60,\n",
    "                    random_state=42,\n",
    "                    verbose=-1\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            model = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_n_estimators'],\n",
    "                max_depth=best_params['xgb_max_depth'],\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=3.0,\n",
    "                min_child_weight=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=best_params['lgbm_n_estimators'],\n",
    "                max_depth=4,\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=60,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        best_model = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Voting Hard] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def voting_soft(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            xgb_n_est = trial.suggest_int('xgb_n_estimators', 100, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_max_depth', 3, 5)\n",
    "            \n",
    "            estimators = [\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_n_est,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=0.03,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=3.0,\n",
    "                    min_child_weight=10,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=120,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.03,\n",
    "                    subsample=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=2.0,\n",
    "                    min_child_samples=60,\n",
    "                    random_state=42,\n",
    "                    verbose=-1\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_n_estimators'],\n",
    "                max_depth=best_params['xgb_max_depth'],\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=3.0,\n",
    "                min_child_weight=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=120,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=60,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        best_model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Voting Soft] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 48, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 24, 64, step=16)\n",
    "            units2 = trial.suggest_int('units2', 12, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0), input_shape=input_shape),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                Bidirectional(LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0)),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0), input_shape=input_shape),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            Bidirectional(LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0)),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[BiLSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 56, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                GRU(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                GRU(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            GRU(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[GRU] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def stacked_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 72, step=16)\n",
    "            units2 = trial.suggest_int('units2', 24, 48, step=12)\n",
    "            units3 = trial.suggest_int('units3', 12, 32, step=8)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.00005, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units2, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units3, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units3'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-8, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[Stacked LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def vmd_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            n_filters = trial.suggest_int('n_filters', 16, 32, step=8)\n",
    "            num_heads = trial.suggest_int('num_heads', 2, 4)\n",
    "            key_dim = trial.suggest_int('key_dim', 16, 32, step=8)\n",
    "            ff_dim = trial.suggest_int('ff_dim', 48, 96, step=24)\n",
    "            dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = Conv1D(n_filters, 1, padding='same', kernel_regularizer=l2(l2_reg))(inputs)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "            low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(x)\n",
    "            low_freq = Conv1D(n_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(low_freq)\n",
    "\n",
    "            mid_freq = x - low_freq\n",
    "            mid_freq = Conv1D(n_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(mid_freq)\n",
    "\n",
    "            high_freq = x - low_freq - mid_freq\n",
    "            high_freq = Conv1D(n_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(high_freq)\n",
    "\n",
    "            x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "\n",
    "            attn = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(x, x)\n",
    "            attn = Dropout(dropout_rate)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(ff_dim, activation='gelu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            ff = Dropout(dropout_rate)(ff)\n",
    "            ff = Dense(n_filters * 3, kernel_regularizer=l2(l2_reg))(ff)\n",
    "            ff = Dropout(dropout_rate)(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = Dense(24, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(best_params['n_filters'], 1, padding='same', kernel_regularizer=l2(best_params['l2_reg']))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(x)\n",
    "        low_freq = Conv1D(best_params['n_filters'], 3, activation='relu', padding='same', kernel_regularizer=l2(best_params['l2_reg']))(low_freq)\n",
    "\n",
    "        mid_freq = x - low_freq\n",
    "        mid_freq = Conv1D(best_params['n_filters'], 3, activation='relu', padding='same', kernel_regularizer=l2(best_params['l2_reg']))(mid_freq)\n",
    "\n",
    "        high_freq = x - low_freq - mid_freq\n",
    "        high_freq = Conv1D(best_params['n_filters'], 3, activation='relu', padding='same', kernel_regularizer=l2(best_params['l2_reg']))(high_freq)\n",
    "\n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout_rate'])(x)\n",
    "\n",
    "        attn = MultiHeadAttention(num_heads=best_params['num_heads'], key_dim=best_params['key_dim'], dropout=best_params['dropout_rate'], kernel_regularizer=l2(best_params['l2_reg']))(x, x)\n",
    "        attn = Dropout(best_params['dropout_rate'])(attn)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "        ff = Dense(best_params['ff_dim'], activation='gelu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        ff = Dropout(best_params['dropout_rate'])(ff)\n",
    "        ff = Dense(best_params['n_filters'] * 3, kernel_regularizer=l2(best_params['l2_reg']))(ff)\n",
    "        ff = Dropout(best_params['dropout_rate'])(ff)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(24, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout_rate'])(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[VMD-Hybrid] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dtw_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 48, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 32, 56, step=12)\n",
    "            units3 = trial.suggest_int('units3', 16, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units2, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units3, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units3'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[DTW-LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def emd_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            freq_units = trial.suggest_int('freq_units', 24, 48, step=12)\n",
    "            units1 = trial.suggest_int('units1', 32, 64, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "            low_freq = LSTM(freq_units, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(low_freq)\n",
    "\n",
    "            high_freq = inputs - AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "            high_freq = LSTM(freq_units, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(high_freq)\n",
    "\n",
    "            x = Concatenate()([low_freq, high_freq])\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "            x = LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "            x = LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "            x = Dense(12, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2))\n",
    "        study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        low_freq = LSTM(best_params['freq_units'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(low_freq)\n",
    "\n",
    "        high_freq = inputs - AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        high_freq = LSTM(best_params['freq_units'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(high_freq)\n",
    "\n",
    "        x = Concatenate()([low_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "\n",
    "        x = LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "\n",
    "        x = Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[EMD-LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def hybrid_lstm_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 48, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 32, 56, step=12)\n",
    "            units3 = trial.suggest_int('units3', 16, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), recurrent_dropout=0.0),\n",
    "                Dropout(dropout),\n",
    "                BatchNormalization(),\n",
    "                GRU(units2, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units3, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), recurrent_dropout=0.0),\n",
    "            Dropout(best_params['dropout']),\n",
    "            BatchNormalization(),\n",
    "            GRU(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units3'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[Hybrid LSTM-GRU] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def residual_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 48, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            x = LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(inputs)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "            lstm_out = LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(x)\n",
    "            lstm_out = BatchNormalization()(lstm_out)\n",
    "            x = Add()([x, lstm_out])\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "            x = LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "            x = Dense(12, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2))\n",
    "        study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        x = LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        lstm_out = LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(x)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        x = Add()([x, lstm_out])\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "\n",
    "        x = LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "\n",
    "        x = Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[Residual LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tcn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            n_filters = trial.suggest_int('n_filters', 16, 48, step=16)\n",
    "            kernel_size = trial.suggest_int('kernel_size', 3, 5)\n",
    "            n_stacks = trial.suggest_int('n_stacks', 1, 2)\n",
    "            dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "\n",
    "            for stack in range(n_stacks):\n",
    "                dilation_rates = [2**i for i in range(4)]\n",
    "\n",
    "                for dilation_rate in dilation_rates:\n",
    "                    residual = x\n",
    "\n",
    "                    conv1 = Conv1D(filters=n_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal', activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "                    conv1 = BatchNormalization()(conv1)\n",
    "                    conv1 = Dropout(dropout)(conv1)\n",
    "\n",
    "                    conv2 = Conv1D(filters=n_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal', activation='relu', kernel_regularizer=l2(l2_reg))(conv1)\n",
    "                    conv2 = BatchNormalization()(conv2)\n",
    "                    conv2 = Dropout(dropout)(conv2)\n",
    "\n",
    "                    if residual.shape[-1] != n_filters:\n",
    "                        residual = Conv1D(n_filters, 1, padding='same')(residual)\n",
    "\n",
    "                    x = Add()([residual, conv2])\n",
    "\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = Dense(24, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "\n",
    "        for stack in range(best_params['n_stacks']):\n",
    "            dilation_rates = [2**i for i in range(4)]\n",
    "\n",
    "            for dilation_rate in dilation_rates:\n",
    "                residual = x\n",
    "\n",
    "                conv1 = Conv1D(filters=best_params['n_filters'], kernel_size=best_params['kernel_size'], dilation_rate=dilation_rate, padding='causal', activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "                conv1 = BatchNormalization()(conv1)\n",
    "                conv1 = Dropout(best_params['dropout'])(conv1)\n",
    "\n",
    "                conv2 = Conv1D(filters=best_params['n_filters'], kernel_size=best_params['kernel_size'], dilation_rate=dilation_rate, padding='causal', activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(conv1)\n",
    "                conv2 = BatchNormalization()(conv2)\n",
    "                conv2 = Dropout(best_params['dropout'])(conv2)\n",
    "\n",
    "                if residual.shape[-1] != best_params['n_filters']:\n",
    "                    residual = Conv1D(best_params['n_filters'], 1, padding='same')(residual)\n",
    "\n",
    "                x = Add()([residual, conv2])\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(24, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[TCN] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def cnn_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            cnn_filters = trial.suggest_int('cnn_filters', 16, 48, step=16)\n",
    "            cnn_kernel = trial.suggest_int('cnn_kernel', 3, 5)\n",
    "            lstm_units1 = trial.suggest_int('lstm_units1', 32, 64, step=16)\n",
    "            lstm_units2 = trial.suggest_int('lstm_units2', 16, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                Conv1D(filters=cnn_filters, kernel_size=cnn_kernel, activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Conv1D(filters=cnn_filters, kernel_size=cnn_kernel, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                Dropout(dropout),\n",
    "                LSTM(lstm_units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(lstm_units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv1D(filters=best_params['cnn_filters'], kernel_size=best_params['cnn_kernel'], activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Conv1D(filters=best_params['cnn_filters'], kernel_size=best_params['cnn_kernel'], activation='relu', padding='same', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(best_params['dropout']),\n",
    "            LSTM(best_params['lstm_units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['lstm_units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[CNN-LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm_attention(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            lstm_units1 = trial.suggest_int('lstm_units1', 48, 80, step=16)\n",
    "            lstm_units2 = trial.suggest_int('lstm_units2', 32, 56, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.001, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            lstm_out = LSTM(lstm_units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)(inputs)\n",
    "            lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "            attention = Dense(1, activation='tanh', kernel_regularizer=l2(l2_reg))(lstm_out)\n",
    "            attention = Flatten()(attention)\n",
    "            attention = Activation('softmax')(attention)\n",
    "            attention = RepeatVector(lstm_units1)(attention)\n",
    "            attention = Permute([2, 1])(attention)\n",
    "\n",
    "            attended = Multiply()([lstm_out, attention])\n",
    "            attended = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n",
    "            attended = BatchNormalization()(attended)\n",
    "            attended = Dropout(dropout)(attended)\n",
    "\n",
    "            x = Dense(lstm_units2, activation='relu', kernel_regularizer=l2(l2_reg))(attended)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "            x = Dense(16, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        lstm_out = LSTM(best_params['lstm_units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "        attention = Dense(1, activation='tanh', kernel_regularizer=l2(best_params['l2_reg']))(lstm_out)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(best_params['lstm_units1'])(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        attended = Multiply()([lstm_out, attention])\n",
    "        attended = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n",
    "        attended = BatchNormalization()(attended)\n",
    "        attended = Dropout(best_params['dropout'])(attended)\n",
    "\n",
    "        x = Dense(best_params['lstm_units2'], activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(attended)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "\n",
    "        x = Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = Dropout(best_params['dropout'])(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[LSTM-Attention] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fa38d",
   "metadata": {},
   "source": [
    "## dropout 설정때매 오류난다 캐서 그거 변경 하기 전 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models (19개)\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'SVM', 'func': DirectionModels.svm, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    {'index': 6, 'name': 'NaiveBayes', 'func': DirectionModels.naive_bayes, 'needs_val': True},\n",
    "    {'index': 7, 'name': 'KNN', 'func': DirectionModels.knn, 'needs_val': True},\n",
    "    {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 10, 'name': 'DecisionTree', 'func': DirectionModels.decision_tree, 'needs_val': True},\n",
    "    {'index': 11, 'name': 'ExtraTrees', 'func': DirectionModels.extra_trees, 'needs_val': True},\n",
    "    {'index': 12, 'name': 'Bagging', 'func': DirectionModels.bagging, 'needs_val': True},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 14, 'name': 'HistGradientBoosting', 'func': DirectionModels.histgradient_boosting, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "    {'index': 16, 'name': 'VotingHard', 'func': DirectionModels.voting_hard, 'needs_val': True},\n",
    "    {'index': 17, 'name': 'VotingSoft', 'func': DirectionModels.voting_soft, 'needs_val': True},\n",
    "    {'index': 18, 'name': 'MLP', 'func': DirectionModels.mlp, 'needs_val': True},\n",
    "    #{'index': 30, 'name': 'TabNet', 'func': DirectionModels.tabnet, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# DL Models (11개)\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True},\n",
    "    {'index': 22, 'name': 'TCN', 'func': DirectionModels.tcn, 'needs_val': True},\n",
    "    {'index': 23, 'name': 'CNN_LSTM', 'func': DirectionModels.cnn_lstm, 'needs_val': True},\n",
    "    {'index': 24, 'name': 'LSTM_Attention', 'func': DirectionModels.lstm_attention, 'needs_val': True},\n",
    "    {'index': 25, 'name': 'DTW_LSTM', 'func': DirectionModels.dtw_lstm, 'needs_val': True},\n",
    "    {'index': 26, 'name': 'VMD_Hybrid', 'func': DirectionModels.vmd_hybrid, 'needs_val': True},\n",
    "    {'index': 27, 'name': 'EMD_LSTM', 'func': DirectionModels.emd_lstm, 'needs_val': True},\n",
    "    {'index': 28, 'name': 'Hybrid_LSTM_GRU', 'func': DirectionModels.hybrid_lstm_gru, 'needs_val': True},\n",
    "    {'index': 29, 'name': 'Residual_LSTM', 'func': DirectionModels.residual_lstm, 'needs_val': True},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f2ff8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled for 1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "timestamp='2025-10-26'\n",
    "RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.predictions = {}\n",
    "        self.models = {} if save_models else None\n",
    "        self.save_models = save_models\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for p in pred:\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "    \n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                     X_test, y_test, test_returns, test_dates, model_name,\n",
    "                                     is_deep_learning=False):\n",
    "        \n",
    "        train_pred = self._predict_model(model, X_train)\n",
    "        val_pred = self._predict_model(model, X_val)\n",
    "        test_pred = self._predict_model(model, X_test)\n",
    "        \n",
    "        test_pred_proba = None\n",
    "        if is_deep_learning:\n",
    "            test_pred_proba = test_pred.copy()\n",
    "            if isinstance(train_pred, list):\n",
    "                train_pred = train_pred[0]\n",
    "                val_pred = val_pred[0]\n",
    "                test_pred = test_pred[0]\n",
    "                test_pred_proba = test_pred_proba[0] if isinstance(test_pred_proba, list) else test_pred_proba\n",
    "            train_pred = (train_pred > 0.5).astype(int).ravel()\n",
    "            val_pred = (val_pred > 0.5).astype(int).ravel()\n",
    "            test_pred = (test_pred > 0.5).astype(int).ravel()\n",
    "        else:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "        test_rec = recall_score(y_test, test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, test_pred)\n",
    "        \n",
    "        self._save_predictions(model_name, test_pred, test_pred_proba, y_test, test_returns, test_dates)\n",
    "        \n",
    "        if self.save_models and self.models is not None:\n",
    "            self.models[model_name] = model\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_acc,\n",
    "            'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc,\n",
    "            'Test_Precision': test_prec,\n",
    "            'Test_Recall': test_rec,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        del train_pred, val_pred, test_pred, test_pred_proba\n",
    "        gc.collect()\n",
    "        \n",
    "        return self.results[-1]\n",
    "    \n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba, actual_direction, actual_returns, dates):\n",
    "        if pred_proba is not None:\n",
    "            if pred_proba.ndim == 2 and pred_proba.shape[1] == 2:\n",
    "                pred_proba_up = pred_proba[:, 1]\n",
    "                pred_proba_down = pred_proba[:, 0]\n",
    "            else:\n",
    "                pred_proba_up = pred_proba.ravel()\n",
    "                pred_proba_down = 1 - pred_proba_up\n",
    "        else:\n",
    "            pred_proba_up = np.where(pred_direction == 1, 0.9, 0.1)\n",
    "            pred_proba_down = 1 - pred_proba_up\n",
    "        \n",
    "        max_proba = np.maximum(pred_proba_up, pred_proba_down)\n",
    "        confidence = np.abs(pred_proba_up - 0.5) * 2\n",
    "        \n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': actual_direction,\n",
    "            'actual_return': actual_returns,\n",
    "            'pred_direction': pred_direction,\n",
    "            'pred_proba_up': pred_proba_up,\n",
    "            'pred_proba_down': pred_proba_down,\n",
    "            'max_proba': max_proba,\n",
    "            'confidence': confidence,\n",
    "            'correct': (pred_direction == actual_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        self.predictions[model_name] = predictions_df\n",
    "    \n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_predictions_dict(self):\n",
    "        return self.predictions\n",
    "    \n",
    "    def get_models_dict(self):\n",
    "        return self.models if self.models is not None else {}\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        X_arr = np.array(Xs)\n",
    "        y_arr = np.array(ys)\n",
    "        del Xs, ys\n",
    "        gc.collect()\n",
    "        return X_arr, y_arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        keras.backend.clear_session()\n",
    "        try:\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                test_returns, test_dates, model_config['name'], is_deep_learning=is_mlp\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                if is_mlp:\n",
    "                    self.clear_memory()\n",
    "                else:\n",
    "                    gc.collect()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            if model_config.get('name') == 'MLP':\n",
    "                self.clear_memory()\n",
    "            else:\n",
    "                gc.collect()\n",
    "    \n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, test_returns_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            self.clear_memory()\n",
    "            \n",
    "            model = model_config['func'](X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape)\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                model_config['name'], is_deep_learning=True\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                self.clear_memory()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates, evaluator, lookback=30, ml_models=None, dl_models=None):\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "    \n",
    "    ml_success = 0\n",
    "    for model_config in ml_models:\n",
    "        if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates):\n",
    "            ml_success += 1\n",
    "        gc.collect()\n",
    "    \n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test, lookback)\n",
    "    test_returns_seq = test_returns[lookback:]\n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    dl_success = 0\n",
    "    for model_config in dl_models:\n",
    "        trainer.clear_memory()\n",
    "        \n",
    "        if model_config['name'] in ['TabNet', 'StackingEnsemble', 'VotingHard', 'VotingSoft']:\n",
    "            if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates):\n",
    "                dl_success += 1\n",
    "        else:\n",
    "            if trainer.train_dl_model(model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, test_returns_seq, test_dates_seq, input_shape):\n",
    "                dl_success += 1\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    del X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, test_returns_seq, test_dates_seq\n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    return ml_success + dl_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0eb3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Reverse Rolling Walk-Forward Configuration \n",
      "================================================================================\n",
      "Total: 2121 days\n",
      "Rolling train size: 800 days (FIXED)\n",
      "Val: 150 days | Test: 150 days\n",
      "Gap: 7 days | Step: 150 days (BACKWARD)\n",
      "Target: 7 walk-forward + 1 final holdout\n",
      "================================================================================\n",
      "\n",
      "Fold 1 (walk_forward_rolling)\n",
      "  Train:  800d  2020-04-17 ~ 2022-06-25\n",
      "  Val:    150d  2022-07-03 ~ 2022-11-29\n",
      "  Test:   150d  2022-12-07 ~ 2023-05-05\n",
      "\n",
      "Fold 2 (walk_forward_rolling)\n",
      "  Train:  800d  2020-09-14 ~ 2022-11-22\n",
      "  Val:    150d  2022-11-30 ~ 2023-04-28\n",
      "  Test:   150d  2023-05-06 ~ 2023-10-02\n",
      "\n",
      "Fold 3 (walk_forward_rolling)\n",
      "  Train:  800d  2021-02-11 ~ 2023-04-21\n",
      "  Val:    150d  2023-04-29 ~ 2023-09-25\n",
      "  Test:   150d  2023-10-03 ~ 2024-02-29\n",
      "\n",
      "Fold 4 (walk_forward_rolling)\n",
      "  Train:  800d  2021-07-11 ~ 2023-09-18\n",
      "  Val:    150d  2023-09-26 ~ 2024-02-22\n",
      "  Test:   150d  2024-03-01 ~ 2024-07-28\n",
      "\n",
      "Fold 5 (walk_forward_rolling)\n",
      "  Train:  800d  2021-12-08 ~ 2024-02-15\n",
      "  Val:    150d  2024-02-23 ~ 2024-07-21\n",
      "  Test:   150d  2024-07-29 ~ 2024-12-25\n",
      "\n",
      "Fold 6 (walk_forward_rolling)\n",
      "  Train:  800d  2022-05-07 ~ 2024-07-14\n",
      "  Val:    150d  2024-07-22 ~ 2024-12-18\n",
      "  Test:   150d  2024-12-26 ~ 2025-05-24\n",
      "\n",
      "Fold 7 (walk_forward_rolling)\n",
      "  Train:  800d  2022-10-04 ~ 2024-12-11\n",
      "  Val:    150d  2024-12-19 ~ 2025-05-17\n",
      "  Test:   150d  2025-05-25 ~ 2025-10-21\n",
      "\n",
      "Fold 8 (final_holdout)\n",
      "  Train:  800d  2022-05-20 ~ 2024-07-27\n",
      "  Val:    150d  2024-08-04 ~ 2024-12-31\n",
      "  Test:   294d  2025-01-01 ~ 2025-10-21\n",
      "\n",
      "================================================================================\n",
      "Created 8 folds total\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 1]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, VOLUME_CHANGE, ADX_14, low_lag5_ratio, eth_avg_gas_price_lag1, sentiment_acceleration, return_lag1, CCI_14, GAP, btc_eth_strength_ratio, eth_btc_corr_lowvol, bnb_return, doge_volume_change, dot_volume_change, sentiment_volatility_7_lag1, eth_large_eth_transfers_lag1, eth_btc_corr_7d, l2_base_tvl_lag1, RSI_OVERBOUGHT, negative_ratio_lag2, BTC_Weighted_Impact, MACDS_12_26_9, close_lag1, VOLUME_CHANGE_5, RV_5, sentiment_std_lag1, EOM_14_100000000, VTXP_14, volume_lag1, sentiment_trend\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 2]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, btc_intraday_range, low_lag5_ratio, VOLUME_CHANGE_5, close_lag2_logret, bnb_volume_change, vol_trend, return_lag1, return_lag2, VOLUME_CHANGE, GAP, eth_btc_corr_3d, btc_dominance, low_lag3_ratio, dot_volume_ratio_20d, sentiment_volatility_7_lag1, eth_large_eth_transfers_lag1, ADX_14, HLC3, low_lag1, low_lag2_ratio, eth_btc_corr_7d, eth_total_gas_used_lag1, positive_ratio_lag2, CCI_SIGNAL, AD, eth_avg_gas_price_lag1, TRUERANGE, vol_regime_duration, btc_return\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 3]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, eth_btc_corr_7d, VOLUME_CHANGE, bnb_volume_change, low_lag2_ratio, btc_intraday_range, eth_btc_spread, bull_bear_ratio, return_lag1, VOLUME_CHANGE_5, BOP, eth_btc_corr_3d, eth_btc_spread_ma7, low_lag1_ratio, bnb_return, sol_return, doge_volume_change, dot_volume_ratio_20d, HIGH_CLOSE_RANGE, eth_avg_block_size_lag1, low_lag1, btc_return_20d, PRICE_VS_SMA20, sp500_SP500_lag1, HIGH_LOW_RANGE, eth_intraday_range, news_volume_ma14, btc_return_lag5, high_lag5, bnb_volume_ratio_20d\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 4]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, sol_return, sentiment_ma3, bnb_volume_ratio_20d, btc_return_lag5, eth_large_eth_transfers_lag1, news_volume_ma14, btc_return_5d, CMF_20, btc_return_lag1, btc_dominance, bnb_return, xrp_volume_change, doge_volume_ratio_20d, dot_volume_ratio_20d, positive_ratio_lag1, eth_new_addresses_lag1, HIGH_CLOSE_RANGE, volume_percentile_90d, HL2, btc_return_20d, PRICE_VS_SMA20, high_lag5_ratio, RSI_percentile_60d, price_percentile_250d, vol_regime_duration, btc_intraday_range, doge_return, high_lag5, vol_spike\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 4\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 5 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 5]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, dot_volume_ratio_20d, bnb_return, bnb_volume_change, btc_return_lag5, eth_large_eth_transfers_lag1, VOLUME_CHANGE_5, GAP, eth_btc_corr_3d, eth_btc_volcorr_sq_7d, ada_volume_change, sentiment_mean_lag2, sentiment_std_lag1, sentiment_volatility_7_lag1, eth_token_transfers_lag1, HIGH_CLOSE_RANGE, VOLUME_RATIO, high_lag2_ratio, high_lag5_ratio, PRICE_VS_SMA20, btc_return_20d, vol_regime_high, DISTANCE_FROM_LOW, eth_active_addresses_lag1, VIX_ETH_Vol_Cross_lag1, l2_arbitrum_tvl_lag1, low_lag1, INC_1, eth_btc_volume_ratio, close_lag14\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 6 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 6]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, GAP, sentiment_polarity, eth_btc_volume_ratio, btc_return_lag5, bnb_volume_change, eth_btc_corr_3d, eth_btc_spread, BOP, eth_btc_corr_7d, bnb_return, volume_lag5, eth_btc_volcorr_14d, btc_dominance, xrp_return, sentiment_std_lag1, sentiment_volatility_7_lag1, sentiment_std_lag2, avax_volume_ratio_20d, price_percentile_250d, l2_base_tvl_lag1, DISTANCE_FROM_LOW, ada_volume_ratio_20d, high_lag7, RSI_percentile_60d, l2_arbitrum_tvl_lag1, close_lag14_ratio, close_lag14, close_lag30, HIGH_LOW_RANGE\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 6\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 7 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 7]\n",
      "Training data shape: (800, 282)\n",
      "Selected Features\n",
      "DPO_20, dot_volume_change, sentiment_intensity, BB_POSITION, low_lag1_ratio, BTC_Weighted_Impact, GAP, INTRADAY_POSITION, return_lag5, STOCH_3, eth_btc_corr_3d, btc_dominance, eth_btc_spread, close_lag2_logret, bnb_return, xrp_return, sentiment_volatility_7_lag1, eth_token_transfers_lag1, doge_volatility_30d, DEMA_10, close_lag1_logret, return_lag1, doge_volume_ratio_20d, price_percentile_250d, eth_btc_volume_ratio, eth_avg_block_difficulty_lag1, RSI_14, extremity_index, bull_bear_ratio, high_lag2_ratio\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 7\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 8 (final_holdout)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 8]\n",
      "Training data shape: (800, 282)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features\n",
      "DPO_20, btc_return_lag5, eth_btc_volume_ratio, eth_btc_corr_3d, sol_return, bnb_volume_change, GAP, ada_volume_ratio_20d, eth_btc_corr_7d, eth_intraday_range, sentiment_polarity, volume_lag2, INTRADAY_POSITION, btc_dominance, bnb_return, xrp_return, sentiment_volatility_7_lag1, btc_return_20d, price_percentile_250d, avax_volume_ratio_20d, l2_arbitrum_tvl_lag1, high_lag7, SMA_GOLDEN_CROSS, close_lag14, close_lag30, close_lag14_ratio, close_lag2, ATR_14, doge_volatility_30d, doge_return\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 8\n",
      "============================================================\n",
      "\n",
      "Saved raw data CSVs to model_results/2025-10-26/raw_data/direction/walk_forward\n",
      "\n",
      "=== Fold 1 (walk_forward_rolling_reverse) ===\n",
      "Fold 1 already completed. Loading results...\n",
      "Loaded results for Fold 1\n",
      "\n",
      "=== Fold 2 (walk_forward_rolling_reverse) ===\n",
      "Fold 2 already completed. Loading results...\n",
      "Loaded results for Fold 2\n",
      "\n",
      "=== Fold 3 (walk_forward_rolling_reverse) ===\n",
      "Fold 3 already completed. Loading results...\n",
      "Loaded results for Fold 3\n",
      "\n",
      "=== Fold 4 (walk_forward_rolling_reverse) ===\n",
      "Fold 4 already completed. Loading results...\n",
      "Loaded results for Fold 4\n",
      "\n",
      "=== Fold 5 (walk_forward_rolling_reverse) ===\n",
      "Fold 5 already completed. Loading results...\n",
      "Loaded results for Fold 5\n",
      "\n",
      "=== Fold 6 (walk_forward_rolling_reverse) ===\n",
      "[Random Forest] Train Acc: 0.6725 | Val Acc: 0.6933 | Gap: -0.0208\n",
      "[LightGBM] Train Acc: 0.6837 | Val Acc: 0.6933 | Gap: -0.0096\n",
      "[XGBoost] Train Acc: 0.6913 | Val Acc: 0.6933 | Gap: -0.0021\n",
      "[SVM] Train Acc: 0.6150 | Val Acc: 0.6733 | Gap: -0.0583\n",
      "[Logistic Regression] Train Acc: 0.6562 | Val Acc: 0.6867 | Gap: -0.0304\n",
      "[Naive Bayes] Train Acc: 0.5375 | Val Acc: 0.5133 | Gap: 0.0242\n",
      "[KNN] Train Acc: 0.6262 | Val Acc: 0.5800 | Gap: 0.0463\n",
      "[AdaBoost] Train Acc: 0.6987 | Val Acc: 0.7067 | Gap: -0.0079\n",
      "[CatBoost] Train Acc: 0.6663 | Val Acc: 0.6867 | Gap: -0.0204\n",
      "[Decision Tree] Train Acc: 0.6488 | Val Acc: 0.6667 | Gap: -0.0179\n",
      "[ExtraTrees] Train Acc: 0.6488 | Val Acc: 0.6667 | Gap: -0.0179\n",
      "[Bagging] Train Acc: 0.7412 | Val Acc: 0.6933 | Gap: 0.0479\n",
      "[Gradient Boosting] Train Acc: 0.6725 | Val Acc: 0.6867 | Gap: -0.0142\n",
      "[HistGradientBoosting] Train Acc: 0.6813 | Val Acc: 0.6933 | Gap: -0.0121\n",
      "[Stacking] Train Acc: 0.7087 | Val Acc: 0.6733 | Gap: 0.0354\n",
      "[Voting Hard] Train Acc: 0.7837 | Val Acc: 0.6800 | Gap: 0.1037\n",
      "[Voting Soft] Train Acc: 0.7550 | Val Acc: 0.6733 | Gap: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 09:00:44.600329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 45865 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1f:00.0, compute capability: 8.6\n",
      "2025-10-27 09:00:50.224427: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe8f9d44050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-27 09:00:50.224463: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-10-27 09:00:50.231620: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-27 09:00:50.680231: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761523250.771710 3825656 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Train Acc: 0.7563 | Val Acc: 0.6267 | Gap: 0.1296\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[LSTM] Train Loss: 1.2416 | Train Acc: 0.6260 | Val Loss: 1.2551 | Val Acc: 0.5833\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Train Loss: 0.6710 | Train Acc: 0.6506 | Val Loss: 0.6831 | Val Acc: 0.6083\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[GRU] Train Loss: 0.6829 | Train Acc: 0.6208 | Val Loss: 0.6871 | Val Acc: 0.6250\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[TCN] Train Loss: 0.7135 | Train Acc: 0.6247 | Val Loss: 0.7459 | Val Acc: 0.5500\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[CNN-LSTM] Train Loss: 0.6933 | Train Acc: 0.5026 | Val Loss: 0.6943 | Val Acc: 0.4750\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[LSTM-Attention] Train Loss: 0.7163 | Train Acc: 0.5234 | Val Loss: 0.7174 | Val Acc: 0.5250\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[DTW-LSTM] Train Loss: 0.6980 | Train Acc: 0.6039 | Val Loss: 0.7063 | Val Acc: 0.5750\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[VMD-Hybrid] Train Loss: 0.7987 | Train Acc: 0.5727 | Val Loss: 0.8270 | Val Acc: 0.5167\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[EMD-LSTM] Train Loss: 0.8113 | Train Acc: 0.6078 | Val Loss: 0.8234 | Val Acc: 0.5500\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[Hybrid LSTM-GRU] Train Loss: 2.5160 | Train Acc: 0.5740 | Val Loss: 2.5421 | Val Acc: 0.5250\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[Residual LSTM] Train Loss: 3.5049 | Train Acc: 0.5558 | Val Loss: 3.5121 | Val Acc: 0.5333\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Saved fold 6 (walk_forward_rolling_reverse) results to model_results/2025-10-26/fold_results/direction/fold_6_walk_forward_rolling_reverse\n",
      "\n",
      "=== Fold 7 (walk_forward_rolling_reverse) ===\n",
      "[Random Forest] Train Acc: 0.6925 | Val Acc: 0.6000 | Gap: 0.0925\n",
      "[LightGBM] Train Acc: 0.6587 | Val Acc: 0.6000 | Gap: 0.0587\n",
      "[XGBoost] Train Acc: 0.6700 | Val Acc: 0.6200 | Gap: 0.0500\n",
      "[SVM] Train Acc: 0.6125 | Val Acc: 0.6200 | Gap: -0.0075\n",
      "[Logistic Regression] Train Acc: 0.5012 | Val Acc: 0.5333 | Gap: -0.0321\n",
      "[Naive Bayes] Train Acc: 0.5025 | Val Acc: 0.5533 | Gap: -0.0508\n",
      "[KNN] Train Acc: 0.6150 | Val Acc: 0.5933 | Gap: 0.0217\n",
      "[AdaBoost] Train Acc: 0.7600 | Val Acc: 0.6267 | Gap: 0.1333\n",
      "[CatBoost] Train Acc: 0.6913 | Val Acc: 0.6133 | Gap: 0.0779\n",
      "[Decision Tree] Train Acc: 0.6575 | Val Acc: 0.5933 | Gap: 0.0642\n",
      "[ExtraTrees] Train Acc: 0.5763 | Val Acc: 0.5933 | Gap: -0.0171\n",
      "[Bagging] Train Acc: 0.8325 | Val Acc: 0.6267 | Gap: 0.2058\n",
      "[Gradient Boosting] Train Acc: 0.6650 | Val Acc: 0.5933 | Gap: 0.0717\n",
      "[HistGradientBoosting] Train Acc: 0.7388 | Val Acc: 0.6067 | Gap: 0.1321\n",
      "[Stacking] Train Acc: 0.7800 | Val Acc: 0.6133 | Gap: 0.1667\n",
      "[Voting Hard] Train Acc: 0.7800 | Val Acc: 0.6333 | Gap: 0.1467\n",
      "[Voting Soft] Train Acc: 0.7788 | Val Acc: 0.6133 | Gap: 0.1654\n",
      "[MLP] Train Acc: 0.7200 | Val Acc: 0.6267 | Gap: 0.0933\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[LSTM] Train Loss: 0.6897 | Train Acc: 0.5974 | Val Loss: 0.7053 | Val Acc: 0.5750\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Train Loss: 0.9962 | Train Acc: 0.6130 | Val Loss: 1.0405 | Val Acc: 0.4917\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[GRU] Train Loss: 0.6768 | Train Acc: 0.6208 | Val Loss: 0.7123 | Val Acc: 0.5667\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[TCN] Train Loss: 0.9580 | Train Acc: 0.5234 | Val Loss: 1.0087 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[CNN-LSTM] Train Loss: 2.8358 | Train Acc: 0.5325 | Val Loss: 2.8526 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[LSTM-Attention] Train Loss: 0.7377 | Train Acc: 0.5156 | Val Loss: 0.7377 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[DTW-LSTM] Train Loss: 0.7231 | Train Acc: 0.5091 | Val Loss: 0.7384 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[VMD-Hybrid] Train Loss: 2.6979 | Train Acc: 0.5286 | Val Loss: 3.1670 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[EMD-LSTM] Train Loss: 2.9868 | Train Acc: 0.5364 | Val Loss: 2.9999 | Val Acc: 0.4750\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "[Hybrid LSTM-GRU] Train Loss: 0.6944 | Train Acc: 0.4909 | Val Loss: 0.6921 | Val Acc: 0.5417\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[Residual LSTM] Train Loss: 3.5378 | Train Acc: 0.5766 | Val Loss: 3.5623 | Val Acc: 0.4917\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Saved fold 7 (walk_forward_rolling_reverse) results to model_results/2025-10-26/fold_results/direction/fold_7_walk_forward_rolling_reverse\n",
      "\n",
      "=== Fold 8 (final_holdout) ===\n",
      "[Random Forest] Train Acc: 0.6800 | Val Acc: 0.6667 | Gap: 0.0133\n",
      "[LightGBM] Train Acc: 0.6625 | Val Acc: 0.6733 | Gap: -0.0108\n",
      "[XGBoost] Train Acc: 0.6900 | Val Acc: 0.6733 | Gap: 0.0167\n",
      "[SVM] Train Acc: 0.6737 | Val Acc: 0.6400 | Gap: 0.0337\n",
      "[Logistic Regression] Train Acc: 0.6300 | Val Acc: 0.5800 | Gap: 0.0500\n",
      "[Naive Bayes] Train Acc: 0.5375 | Val Acc: 0.4667 | Gap: 0.0708\n",
      "[KNN] Train Acc: 0.6212 | Val Acc: 0.5467 | Gap: 0.0746\n",
      "[AdaBoost] Train Acc: 0.6750 | Val Acc: 0.6933 | Gap: -0.0183\n",
      "[CatBoost] Train Acc: 0.6562 | Val Acc: 0.6733 | Gap: -0.0171\n",
      "[Decision Tree] Train Acc: 0.6438 | Val Acc: 0.6533 | Gap: -0.0096\n",
      "[ExtraTrees] Train Acc: 0.6350 | Val Acc: 0.6533 | Gap: -0.0183\n",
      "[Bagging] Train Acc: 0.7625 | Val Acc: 0.6600 | Gap: 0.1025\n",
      "[Gradient Boosting] Train Acc: 0.6475 | Val Acc: 0.6600 | Gap: -0.0125\n",
      "[HistGradientBoosting] Train Acc: 0.7512 | Val Acc: 0.6400 | Gap: 0.1112\n",
      "[Stacking] Train Acc: 0.7462 | Val Acc: 0.6533 | Gap: 0.0929\n",
      "[Voting Hard] Train Acc: 0.7762 | Val Acc: 0.6467 | Gap: 0.1296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting Soft] Train Acc: 0.7700 | Val Acc: 0.6400 | Gap: 0.1300\n",
      "[MLP] Train Acc: 0.7225 | Val Acc: 0.6467 | Gap: 0.0758\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "[LSTM] Train Loss: 0.8487 | Train Acc: 0.6545 | Val Loss: 0.8751 | Val Acc: 0.6083\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[BiLSTM] Train Loss: 1.5827 | Train Acc: 0.5727 | Val Loss: 1.5885 | Val Acc: 0.5500\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "[GRU] Train Loss: 0.7609 | Train Acc: 0.6494 | Val Loss: 0.7847 | Val Acc: 0.6250\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[TCN] Train Loss: 0.8289 | Train Acc: 0.6286 | Val Loss: 0.8537 | Val Acc: 0.6083\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 25ms/step\n",
      "[CNN-LSTM] Train Loss: 0.6934 | Train Acc: 0.4987 | Val Loss: 0.6935 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "[LSTM-Attention] Train Loss: 0.7251 | Train Acc: 0.5091 | Val Loss: 0.7274 | Val Acc: 0.4583\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[DTW-LSTM] Train Loss: 2.8743 | Train Acc: 0.5662 | Val Loss: 2.8953 | Val Acc: 0.5583\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[VMD-Hybrid] Train Loss: 1.8442 | Train Acc: 0.5662 | Val Loss: 1.8879 | Val Acc: 0.4917\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "[EMD-LSTM] Train Loss: 7.0173 | Train Acc: 0.5519 | Val Loss: 7.0349 | Val Acc: 0.4750\n",
      "25/25 [==============================] - 1s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "[Hybrid LSTM-GRU] Train Loss: 0.7200 | Train Acc: 0.6117 | Val Loss: 0.7297 | Val Acc: 0.5583\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[Residual LSTM] Train Loss: 3.0339 | Train Acc: 0.6013 | Val Loss: 3.0593 | Val Acc: 0.5250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def save_raw_data_once(result, target_name, split_method):\n",
    "    raw_dir = os.path.join(RESULT_DIR, \"raw_data\", target_name, split_method)\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    \n",
    "    if split_method == 'tvt':\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            df = pd.DataFrame(result[split]['X_raw'], columns=result['stats']['selected_features'])\n",
    "            df['date'] = result[split]['dates']\n",
    "            for col in result[split]['y'].columns:\n",
    "                df[col] = result[split]['y'][col].values\n",
    "            df.to_csv(os.path.join(raw_dir, f\"{split}_raw.csv\"), index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        del result['train']['X_raw'], result['val']['X_raw'], result['test']['X_raw']\n",
    "    else:\n",
    "        for fold_idx, fold in enumerate(result, start=1):\n",
    "            fold_type = fold['stats']['fold_type']\n",
    "            fold_dir = os.path.join(raw_dir, f\"fold_{fold_idx}_{fold_type}\")\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "            \n",
    "            for split in ['train', 'val', 'test']:\n",
    "                df = pd.DataFrame(fold[split]['X_raw'], columns=fold['stats']['selected_features'])\n",
    "                df['date'] = fold[split]['dates']\n",
    "                for col in fold[split]['y'].columns:\n",
    "                    df[col] = fold[split]['y'][col].values\n",
    "                df.to_csv(os.path.join(fold_dir, f\"{split}_raw.csv\"), index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            del fold['train']['X_raw'], fold['val']['X_raw'], fold['test']['X_raw']\n",
    "    \n",
    "    print(f\"Saved raw data CSVs to {raw_dir}\")\n",
    "    gc.collect()\n",
    "\n",
    "def check_fold_completed(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    return os.path.exists(summary_path)\n",
    "\n",
    "def save_fold_results(fold_idx, fold_type, evaluator, target_name):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    models = evaluator.get_models_dict()\n",
    "    for model_name, model_obj in models.items():\n",
    "        is_dl = model_name in ['LSTM', 'BiLSTM', 'GRU', 'TCN', 'CNN_LSTM', \n",
    "                               'LSTM_Attention', 'DTW_LSTM', 'VMD_Hybrid', \n",
    "                               'EMD_LSTM', 'Hybrid_LSTM_GRU', 'Residual_LSTM', \n",
    "                               'MLP', 'Stacked_LSTM']\n",
    "        \n",
    "        if is_dl:\n",
    "            model_path = os.path.join(fold_dir, f\"{model_name}.h5\")\n",
    "            model_obj.save(model_path)\n",
    "        else:\n",
    "            model_path = os.path.join(fold_dir, f\"{model_name}.pkl\")\n",
    "            joblib.dump(model_obj, model_path, compress=3)\n",
    "    \n",
    "    fold_predictions = evaluator.get_predictions_dict()\n",
    "    for model_name, pred_df in fold_predictions.items():\n",
    "        pred_path = os.path.join(fold_dir, f\"{model_name}_predictions.csv\")\n",
    "        pred_df.to_csv(pred_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    fold_summary = evaluator.get_summary_dataframe()\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    fold_summary.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Saved fold {fold_idx} ({fold_type}) results to {fold_dir}\")\n",
    "    \n",
    "    return fold_summary, fold_predictions\n",
    "\n",
    "def load_fold_results(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    \n",
    "    if not os.path.exists(summary_path):\n",
    "        return None, None\n",
    "    \n",
    "    fold_summary = pd.read_csv(summary_path)\n",
    "    \n",
    "    fold_predictions = {}\n",
    "    for file in os.listdir(fold_dir):\n",
    "        if file.endswith('_predictions.csv'):\n",
    "            model_name = file.replace('_predictions.csv', '')\n",
    "            fold_predictions[model_name] = pd.read_csv(os.path.join(fold_dir, file))\n",
    "    \n",
    "    return fold_summary, fold_predictions\n",
    "\n",
    "def save_walk_forward_summary(all_fold_results, target_name):\n",
    "    detailed_results = []\n",
    "    for fold_idx, (fold_df, fold_type) in enumerate(all_fold_results, start=1):\n",
    "        fold_df_copy = fold_df.copy()\n",
    "        fold_df_copy.insert(0, 'Fold', fold_idx)\n",
    "        fold_df_copy.insert(1, 'fold_type', fold_type)\n",
    "        detailed_results.append(fold_df_copy)\n",
    "    \n",
    "    detailed_df = pd.concat(detailed_results, ignore_index=True)\n",
    "    detailed_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_all_folds_detailed.csv\"), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward'].copy()\n",
    "    \n",
    "    numeric_cols = [col for col in wf_data.select_dtypes(include=[np.number]).columns if col != 'Fold']\n",
    "    \n",
    "    avg_results = []\n",
    "    for model in wf_data['Model'].unique():\n",
    "        avg_row = {'Model': model}\n",
    "        model_wf = wf_data[wf_data['Model'] == model]\n",
    "        for col in numeric_cols:\n",
    "            if col in model_wf.columns:\n",
    "                avg_row[f'{col}_Mean'] = model_wf[col].mean()\n",
    "                avg_row[f'{col}_Std'] = model_wf[col].std()\n",
    "        avg_results.append(avg_row)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_results)\n",
    "    if 'Test_Accuracy_Mean' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='Test_Accuracy_Mean', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    avg_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_walk_forward_average.csv\"), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Saved walk-forward summary to {RESULT_DIR}\")\n",
    "\n",
    "def save_tvt_results(summary_df, predictions_dict, target_name):\n",
    "    summary_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_tvt_summary.csv\"), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    if predictions_dict:\n",
    "        pred_dir = os.path.join(RESULT_DIR, \"predictions\", f\"{target_name}_tvt\")\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name, pred_df in predictions_dict.items():\n",
    "            pred_df.to_csv(os.path.join(pred_dir, f\"{model_name}.csv\"), index=False, encoding='utf-8-sig')\n",
    "\n",
    "target_cases = [{'name': 'direction', 'target_type': 'direction', 'outputs': ['next_direction']}]\n",
    "split_methods = [{'name': 'walk_forward', 'method': 'walk_forward'}]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for target_case in target_cases:\n",
    "    for split_method in split_methods:\n",
    "        \n",
    "        result = build_complete_pipeline_corrected(\n",
    "            df_merged, TRAIN_START_DATE,\n",
    "            method=split_method['method'],\n",
    "            target_type=target_case['target_type'],\n",
    "            test_start_date='2025-01-01'\n",
    "        )\n",
    "        \n",
    "        save_raw_data_once(result, target_case['name'], split_method['method'])\n",
    "        \n",
    "        if split_method['method'] == 'tvt':\n",
    "            X_train = result['train']['X_robust']\n",
    "            X_val = result['val']['X_robust']\n",
    "            X_test = result['test']['X_robust']\n",
    "            test_returns = result['test']['y']['next_log_return'].values\n",
    "            test_dates = result['test']['dates'].values\n",
    "            y_train = result['train']['y'][target_case['outputs'][0]].values\n",
    "            y_val = result['val']['y'][target_case['outputs'][0]].values\n",
    "            y_test = result['test']['y'][target_case['outputs'][0]].values\n",
    "            \n",
    "            evaluator = ModelEvaluator(save_models=False)\n",
    "            train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates, evaluator, ml_models=ML_MODELS_CLASSIFICATION, dl_models=DL_MODELS_CLASSIFICATION)\n",
    "            \n",
    "            summary_df = evaluator.get_summary_dataframe()\n",
    "            predictions_dict = evaluator.get_predictions_dict()\n",
    "            \n",
    "            save_tvt_results(summary_df, predictions_dict, target_case['name'])\n",
    "            all_results[f\"{target_case['name']}_tvt\"] = summary_df\n",
    "            \n",
    "            del evaluator\n",
    "            keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "        \n",
    "        else:\n",
    "            fold_results = []\n",
    "            \n",
    "            for fold_idx, fold in enumerate(result, start=1):\n",
    "                fold_type = fold['stats']['fold_type']\n",
    "                print(f\"\\n=== Fold {fold_idx} ({fold_type}) ===\")\n",
    "                \n",
    "                if check_fold_completed(target_case['name'], fold_idx, fold_type):\n",
    "                    print(f\"Fold {fold_idx} already completed. Loading results...\")\n",
    "                    fold_summary, fold_pred_dict = load_fold_results(target_case['name'], fold_idx, fold_type)\n",
    "                    if fold_summary is not None:\n",
    "                        fold_results.append((fold_summary, fold_type))\n",
    "                        print(f\"Loaded results for Fold {fold_idx}\")\n",
    "                        continue\n",
    "                \n",
    "                X_train = fold['train']['X_robust']\n",
    "                X_val = fold['val']['X_robust']\n",
    "                X_test = fold['test']['X_robust']\n",
    "                test_returns = fold['test']['y']['next_log_return'].values\n",
    "                test_dates = fold['test']['dates'].values\n",
    "                y_train = fold['train']['y'][target_case['outputs'][0]].values\n",
    "                y_val = fold['val']['y'][target_case['outputs'][0]].values\n",
    "                y_test = fold['test']['y'][target_case['outputs'][0]].values\n",
    "                \n",
    "                evaluator = ModelEvaluator(save_models=True)\n",
    "                \n",
    "                try:\n",
    "                    train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, test_returns, test_dates, evaluator, ml_models=ML_MODELS_CLASSIFICATION, dl_models=DL_MODELS_CLASSIFICATION)\n",
    "                    \n",
    "                    fold_summary, fold_pred_dict = save_fold_results(fold_idx, fold_type, evaluator, target_case['name'])\n",
    "                    fold_results.append((fold_summary, fold_type))\n",
    "                    \n",
    "                    del evaluator\n",
    "                except Exception as e:\n",
    "                    print(f\"Fold {fold_idx} failed: {e}\")\n",
    "                    if 'evaluator' in locals():\n",
    "                        del evaluator\n",
    "                finally:\n",
    "                    keras.backend.clear_session()\n",
    "                    try:\n",
    "                        tf.compat.v1.reset_default_graph()\n",
    "                    except:\n",
    "                        pass\n",
    "                    gc.collect()\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            save_walk_forward_summary(fold_results, target_case['name'])\n",
    "            \n",
    "            keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"\\nComplete! Results saved to: {RESULT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2f7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795231ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
