{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac9fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 01:24:09.805784: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-22 01:24:09.805835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-22 01:24:09.807025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-22 01:24:09.814033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-22 01:24:10.527073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 감성 지표 생성 완료: 25개 (date 제외)\n",
      "\n",
      "감성 지표 결측 처리:\n",
      "  sentiment_mean: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_std: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  positive_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  negative_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_positive_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_negative_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_sum: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_polarity: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_intensity: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_disagreement: 39개 → 0 (데이터 없음 = 중립)\n",
      "  bull_bear_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  weighted_sentiment: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extremity_index: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma3: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_3: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma14: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_14: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_trend: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_acceleration: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_change: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma14: 39개 → 0 (데이터 없음 = 중립)\n",
      "\n",
      "외부 변수 FFill 처리:\n",
      "  3,107 → 673개 (FFill)\n",
      "\n",
      "Lookback 기간 제거:\n",
      "  1953 → 1953행\n",
      "\n",
      "초기 결측치 처리:\n",
      "  남은 결측: 673개 → 0\n",
      "\n",
      "✓ 최종 데이터: (1953, 96)\n",
      "  날짜: 2020-06-02 ~ 2025-10-06\n",
      "  결측: 945개\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 기본 라이브러리\n",
    "# ============================================================================\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# 데이터 전처리 및 Feature Engineering\n",
    "# ============================================================================\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE, \n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 시계열 분석\n",
    "# ============================================================================\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn ML 모델\n",
    "# ============================================================================\n",
    "# 선형 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 트리 기반 모델\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# 앙상블 모델\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Gradient Boosting 라이브러리\n",
    "# ============================================================================\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# ============================================================================\n",
    "# TabNet \n",
    "# ============================================================================\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"Warning: pytorch-tabnet not installed. TabNet models will be skipped.\")\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch (Optional)\n",
    "# ============================================================================\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"Warning: PyTorch not installed. Some models may not work.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn 평가 지표\n",
    "# ============================================================================\n",
    "# 분류 지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# 회귀 지표\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# TensorFlow/Keras 딥러닝\n",
    "# ============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    # 기본 레이어\n",
    "    Input, Dense, Flatten, Dropout, \n",
    "    \n",
    "    # RNN 레이어\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    \n",
    "    # CNN 레이어\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    \n",
    "    # 정규화 레이어\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    \n",
    "    # Attention 레이어\n",
    "    Attention, MultiHeadAttention,\n",
    "    \n",
    "    # 유틸리티 레이어\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed,\n",
    "    Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 1. 날짜 파싱 및 CSV 로드 함수\n",
    "# ============================================================================ \n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    #print(df.shape)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    #print(df.shape)\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(filepath):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = standardize_date_column(df,filepath)\n",
    "    return df\n",
    "# ============================================================================ \n",
    "# 2. 데이터 로딩\n",
    "# ============================================================================ \n",
    "DATA_DIR = './macro_data'\n",
    "\n",
    "def load_from_macro_data(filename):\n",
    "    return load_and_standardize_data(os.path.join(DATA_DIR, filename))\n",
    "\n",
    "macro_df = load_from_macro_data('macro_crypto_data.csv')\n",
    "news_df = load_from_macro_data('news_data.csv')\n",
    "eth_onchain_df = load_from_macro_data('eth_onchain.csv')\n",
    "fear_greed_df = load_from_macro_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_from_macro_data('usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_from_macro_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_from_macro_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_from_macro_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_from_macro_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_from_macro_data('eth_funding_rate.csv')\n",
    "sp500_df = load_from_macro_data('SP500.csv')\n",
    "vix_df = load_from_macro_data('VIX.csv')\n",
    "gold_df = load_from_macro_data('GOLD.csv')\n",
    "dxy_df = load_from_macro_data('DXY.csv')\n",
    "\n",
    "# ============================================================================ \n",
    "# 3. 기준 날짜 설정 (Lido TVL 시작일 기준)\n",
    "# ============================================================================ \n",
    "train_start_date = pd.to_datetime('2020-12-19')\n",
    "lookback_start_date = train_start_date - timedelta(days=200)\n",
    "end_date= pd.to_datetime('2025-10-06')\n",
    "\n",
    "# ============================================================================ \n",
    "# 4. 뉴스 감성 피처 생성 \n",
    "# ============================================================================ \n",
    "def create_sentiment_features(news_df):\n",
    "    \"\"\"\n",
    "    한국어 뉴스 감성 지표 생성\n",
    "    출처: \"Cryptocurrency Price Prediction Model Based on Sentiment Analysis\" (2024)\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        # ===== 기본 통계 =====\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        \n",
    "        # ===== 추가 지표 =====\n",
    "        # 1. 극단 감성 카운트\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        \n",
    "        # 2. 총 감성 점수\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    # ===== 파생 지표 계산 =====\n",
    "    \n",
    "    # 1. Sentiment Polarity \n",
    "    sentiment_agg['sentiment_polarity'] = (\n",
    "        sentiment_agg['positive_ratio'] - sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 2. Sentiment Intensity (감성 강도) \n",
    "    sentiment_agg['sentiment_intensity'] = (\n",
    "        sentiment_agg['positive_ratio'] + sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 3. Sentiment Disagreement \n",
    "    sentiment_agg['sentiment_disagreement'] = (\n",
    "        sentiment_agg['positive_ratio'] * sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 4. Bull/Bear Ratio \n",
    "    sentiment_agg['bull_bear_ratio'] = (\n",
    "        sentiment_agg['positive_ratio'] / (sentiment_agg['negative_ratio'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # 5. Weighted Sentiment \n",
    "    sentiment_agg['weighted_sentiment'] = (\n",
    "        sentiment_agg['sentiment_mean'] * np.log1p(sentiment_agg['news_count'])\n",
    "    )\n",
    "    \n",
    "    # 6. Extremity Index \n",
    "    sentiment_agg['extremity_index'] = (\n",
    "        (sentiment_agg['extreme_positive_count'] + sentiment_agg['extreme_negative_count']) / \n",
    "        (sentiment_agg['news_count'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # ===== 시계열 파생 지표 (이동 평균) =====\n",
    "    \n",
    "    for window in [3, 7, 14]:\n",
    "        # 감성 이동 평균\n",
    "        sentiment_agg[f'sentiment_ma{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # 감성 변동성 (이동 표준편차)\n",
    "        sentiment_agg[f'sentiment_volatility_{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # 7. Sentiment Trend \n",
    "    sentiment_agg['sentiment_trend'] = sentiment_agg['sentiment_mean'].diff()\n",
    "    \n",
    "    # 8. Sentiment Acceleration\n",
    "    sentiment_agg['sentiment_acceleration'] = sentiment_agg['sentiment_trend'].diff()\n",
    "    \n",
    "    # 9. News Volume Change\n",
    "    sentiment_agg['news_volume_change'] = sentiment_agg['news_count'].pct_change()\n",
    "    \n",
    "    # 10. News Volume MA \n",
    "    for window in [7, 14]:\n",
    "        sentiment_agg[f'news_volume_ma{window}'] = (\n",
    "            sentiment_agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ 감성 지표 생성 완료: {sentiment_agg.shape[1] - 1}개 (date 제외)\")\n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "sentiment_features = create_sentiment_features(news_df)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 5. 데이터 병합\n",
    "# ============================================================================ \n",
    "def add_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + col if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=lookback_start_date, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df, sentiment_features, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, eth_chain_tvl_df,\n",
    "    eth_funding_df, sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "# 1. 외부 데이터 Merge 후\n",
    "for df_to_merge in dataframes_to_merge:\n",
    "    df_merged = pd.merge(df_merged, df_to_merge, on='date', how='left')\n",
    "\n",
    "# 2. 감성 지표 결측 처리 (0)\n",
    "sentiment_cols = [col for col in df_merged.columns \n",
    "                 if any(x in col for x in ['sentiment', 'news', 'ext', 'bull_bear','positive','negative','extreme'])]\n",
    "\n",
    "print(f\"\\n감성 지표 결측 처리:\")\n",
    "for col in sentiment_cols:\n",
    "    missing_before = df_merged[col].isnull().sum()\n",
    "    if missing_before > 0:\n",
    "        df_merged[col] = df_merged[col].fillna(0)\n",
    "        print(f\"  {col}: {missing_before}개 → 0 (데이터 없음 = 중립)\")\n",
    "\n",
    "# 3. 외부 변수 FFill (bfill 절대 금지!)\n",
    "external_cols = [col for col in df_merged.columns \n",
    "                if any(x in col for x in ['eth_', 'fg_', 'usdt_', 'aave_', 'lido_', \n",
    "                                         'makerdao_', 'chain_', 'funding_',\n",
    "                                         'sp500_', 'vix_', 'gold_', 'dxy_'])]\n",
    "\n",
    "print(f\"\\n외부 변수 FFill 처리:\")\n",
    "missing_before = df_merged[external_cols].isnull().sum().sum()\n",
    "df_merged[external_cols] = df_merged[external_cols].fillna(method='ffill')\n",
    "missing_after = df_merged[external_cols].isnull().sum().sum()\n",
    "print(f\"  {missing_before:,} → {missing_after:,}개 (FFill)\")\n",
    "\n",
    "# 4. Lookback 기간 제거\n",
    "print(f\"\\nLookback 기간 제거:\")\n",
    "before = len(df_merged)\n",
    "df_merged = df_merged[df_merged['date'] >= lookback_start_date].reset_index(drop=True)\n",
    "print(f\"  {before} → {len(df_merged)}행\")\n",
    "\n",
    "remaining_missing = df_merged[external_cols].isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\n초기 결측치 처리:\")\n",
    "    print(f\"  남은 결측: {remaining_missing}개 → 0\")\n",
    "    df_merged[external_cols] = df_merged[external_cols].fillna(0)\n",
    "\n",
    "# 6. Lookback 기간 동안 모두 NaN인 컬럼 제거\n",
    "lookback_df = df_merged[df_merged['date'] < train_start_date]\n",
    "cols_to_drop = [col for col in lookback_df.columns \n",
    "               if lookback_df[col].isnull().all() and col != 'date']\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nLookback 기간 완전 결측 컬럼 제거:\")\n",
    "    print(f\"  {cols_to_drop}\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\n✓ 최종 데이터: {df_merged.shape}\")\n",
    "print(f\"  날짜: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"  결측: {df_merged.isnull().sum().sum()}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ffc3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    출처: \n",
    "    - \"CryptoPulse: Short-Term Cryptocurrency Forecasting\" (2024)\n",
    "    - \"Enhancing Price Prediction in Cryptocurrency Using Transformer\" (2024)\n",
    "    - \"Bitcoin Trend Prediction with Attention-Based Deep Learning\" (2024)\n",
    "    \"\"\"\n",
    "    #print(\"\\n=== 기술적 지표 생성 중 ===\")\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== [핵심] MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (필수)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        df_ta['RSI_30'] = ta.rsi(close, length=30)\n",
    "        df_ta['RSI_200'] = ta.rsi(close, length=200)  # 장기 RSI 추가\n",
    "        \n",
    "        # MACD (필수 - top feature importance)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic Oscillator (%K, %D - 논문에서 핵심 지표)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=30, d=3)  # 30일 추가\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=200, d=3)  # 200일 추가\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (Rate of Change)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['ROC_20'] = ta.roc(close, length=20)\n",
    "        \n",
    "        # MOM (Momentum - 다양한 기간)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['MOM_30'] = ta.mom(close, length=30) \n",
    "        \n",
    "        # CCI (Commodity Channel Index)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_20'] = ta.cci(high, low, close, length=20)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_20'] > 100).astype(int)\n",
    "      \n",
    "        # TSI (True Strength Index)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "\n",
    "        \n",
    "        # =====  Ichimoku Cloud (암호화폐 트렌드 분석에 효과적) =====\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ ICHIMOKU 생성 실패\")\n",
    "\n",
    "        # ===== [핵심] OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (필수! - Golden/Death Cross)\n",
    "        df_ta['SMA_10'] = ta.sma(close, length=10)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['SMA_200'] = ta.sma(close, length=200)\n",
    "        \n",
    "        # EMA (필수!)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['EMA_50'] = ta.ema(close, length=50)\n",
    "        df_ta['EMA_200'] = ta.ema(close, length=200) \n",
    "        \n",
    "        # TEMA (Triple EMA - 논문에서 high importance)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['TEMA_30'] = ta.tema(close, length=30) \n",
    "        \n",
    "        # WMA (Weighted Moving Average)\n",
    "        df_ta['WMA_10'] = ta.wma(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)  \n",
    "        \n",
    "        # HMA (Hull Moving Average)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (Double EMA)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        \n",
    "        # VWMA (Volume Weighted)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== [핵심] VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands (필수 )\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        safe_add(df_ta, ta.bbands, close, length=50, std=2)  \n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_7'] = ta.atr(high, low, close, length=7)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['ATR_21'] = ta.atr(high, low, close, length=21) \n",
    "        \n",
    "        # NATR (Normalized ATR)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel \n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        # ===== [핵심] VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD (Accumulation/Distribution)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI (Money Flow Index)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF (Chaikin Money Flow - 논문에서 중요 지표)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Elder Force Index)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM (Ease of Movement)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP (Volume Weighted Average Price) \n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX \n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon \n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO (Detrended Price Oscillator)\n",
    "        try:\n",
    "            df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['PRICE_CHANGE_2'] = close.pct_change(periods=2)\n",
    "        df_ta['PRICE_CHANGE_5'] = close.pct_change(periods=5)\n",
    "        df_ta['PRICE_CHANGE_10'] = close.pct_change(periods=10) \n",
    "        \n",
    "        # 변동성 (Rolling Std)\n",
    "        df_ta['VOLATILITY_5'] = close.pct_change().rolling(window=5).std()\n",
    "        df_ta['VOLATILITY_10'] = close.pct_change().rolling(window=10).std()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['VOLATILITY_30'] = close.pct_change().rolling(window=30).std() \n",
    "        \n",
    "        # 모멘텀 (Price Ratio)\n",
    "        df_ta['MOMENTUM_5'] = close / close.shift(5) - 1\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['MOMENTUM_20'] = close / close.shift(20) - 1\n",
    "        df_ta['MOMENTUM_30'] = close / close.shift(30) - 1  \n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA10'] = close / df_ta['SMA_10'] - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_SMA50'] = close / df_ta['SMA_50'] - 1\n",
    "        df_ta['PRICE_VS_SMA200'] = close / df_ta['SMA_200'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1 \n",
    "        df_ta['PRICE_VS_EMA26'] = close / df_ta['EMA_26'] - 1  \n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_CROSS_SIGNAL'] = (df_ta['SMA_10'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_200']).astype(int) \n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)  \n",
    "        \n",
    "        # Range 지표\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치 \n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)  \n",
    "        \n",
    "        # Linear Regression Slope\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "            df_ta['SLOPE_10'] = ta.linreg(close, length=10, slope=True)\n",
    "            df_ta['LINREG_14'] = ta.linreg(close, length=14)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "            df_ta['SLOPE_10'] = close.rolling(window=10).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 10 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing/Decreasing 신호\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['DEC_1'] = (close < close.shift(1)).astype(int)\n",
    "        df_ta['INC_3'] = (close > close.shift(3)).astype(int)\n",
    "        df_ta['INC_5'] = (close > close.shift(5)).astype(int)  \n",
    "        \n",
    "        # BOP \n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 관련 파생\n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        else:\n",
    "            print(f\"    ⚠ Bollinger Bands 컬럼 미발견\")\n",
    "        \n",
    "        # RSI 파생 (Overbought/Oversold)\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile (상대적 거래량 강도)\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration (2차 미분)\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (시가-전일종가)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        # Fibonacci Pivots \n",
    "        high_20 = high.rolling(20).max()\n",
    "        low_20 = low.rolling(20).min()\n",
    "        diff = high_20 - low_20\n",
    "        \n",
    "        df_ta['FIB_0'] = high_20\n",
    "        df_ta['FIB_236'] = high_20 - 0.236 * diff\n",
    "        df_ta['FIB_382'] = high_20 - 0.382 * diff\n",
    "        df_ta['FIB_500'] = high_20 - 0.500 * diff\n",
    "        df_ta['FIB_618'] = high_20 - 0.618 * diff\n",
    "        df_ta['FIB_1'] = low_20\n",
    "        \n",
    "        #Directional Change Events \n",
    "        df_ta['DC_EVENT'] = 0\n",
    "        df_ta['DC_TYPE'] = 0\n",
    "        \n",
    "        threshold = 0.05\n",
    "        last_extreme = close.iloc[0]\n",
    "        last_type = 0\n",
    "        \n",
    "        for i in range(1, len(df_ta)):\n",
    "            price = close.iloc[i]\n",
    "            change = (price - last_extreme) / last_extreme\n",
    "            \n",
    "            if last_type <= 0 and change >= threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = 1\n",
    "                last_extreme = price\n",
    "                last_type = 1\n",
    "            elif last_type >= 0 and change <= -threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = -1\n",
    "                last_extreme = price\n",
    "                last_type = -1\n",
    "        \n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = (\n",
    "        df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "    )\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        if 'btc_log_return' not in df_transformed.columns:\n",
    "            df_transformed['btc_log_return'] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(1))\n",
    "        for period in [5, 10, 20, 30]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['btc_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['btc_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = (\n",
    "                    df_transformed[col_name].rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                    btc_volume.pct_change()\n",
    "                )\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (\n",
    "                eth_volume / (btc_volume + 1e-8)\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [\n",
    "        col for col in df_transformed.columns\n",
    "        if 'return' in col.lower() and 'next' not in col\n",
    "    ]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78da9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. Lag 적용\n",
    "# ============================================================================\n",
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    \"\"\"\n",
    "    Lag 피처 적용 (원본 유지 + lag 추가)\n",
    "    \n",
    "    핵심 원칙:\n",
    "    1. 원본(lag0) 피처는 그대로 유지\n",
    "    2. lag1, lag2 피처를 추가로 생성\n",
    "    3. 이동평균/차분은 lag 불필요 (이미 과거 참조)\n",
    "    4. 이벤트는 lag 없음 (당일 반영)\n",
    "    \n",
    "    출처: \"Seeing Beyond Noise\" (2024), scikit-learn\n",
    "    \"\"\"\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    # ===== Lag 적용 대상: 원본 감성 지표만 =====\n",
    "    raw_sentiment_cols = [\n",
    "        'sentiment_mean', 'sentiment_std', 'sentiment_sum',\n",
    "        'news_count', 'positive_ratio', 'negative_ratio',\n",
    "        'sentiment_polarity', 'sentiment_intensity', \n",
    "        'sentiment_disagreement', 'bull_bear_ratio',\n",
    "        'weighted_sentiment', 'extremity_index',\n",
    "        'extreme_positive_count', 'extreme_negative_count'\n",
    "    ]\n",
    "    \n",
    "    # ===== Lag 제외: 이동평균, 차분 (이미 과거 참조) =====\n",
    "    no_lag_patterns = [\n",
    "        '_ma', '_volatility_', '_trend', '_acceleration', \n",
    "        '_volume_change', '_volume_ma'\n",
    "    ]\n",
    "    \n",
    "    # ===== 온체인 데이터 =====\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', \n",
    "                                  'eth_large', 'eth_token', 'eth_contract',\n",
    "                                  'eth_avg_gas', 'eth_total_gas', \n",
    "                                  'eth_avg_block'])]\n",
    "    \n",
    "    # ===== 기타 외부 변수 =====\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    # ===== 제외 컬럼 =====\n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open','date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col])\n",
    "    exclude_cols.extend([col for col in df.columns if '_lag' in col])\n",
    "    \n",
    "    lag_count = 0\n",
    "    \n",
    "    # ===== 1. 원본 감성 지표에만 lag 적용 =====\n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            is_derived = any(pattern in col for pattern in no_lag_patterns)\n",
    "            \n",
    "            if not is_derived:\n",
    "                for lag in range(1, news_lag):\n",
    "                    new_col = f\"{col}_lag{lag}\"\n",
    "                    df_lagged[new_col] = df[col].shift(lag)\n",
    "                    lag_count += 1\n",
    "    \n",
    "    # ===== 2. 온체인 lag =====\n",
    "    onchain_lag_count = 0\n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            onchain_lag_count += 1\n",
    "    \n",
    "    # ===== 3. 기타 외부 변수 lag  =====\n",
    "    other_lag_count = 0\n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            other_lag_count += 1\n",
    "    \n",
    "    total_lag = lag_count + onchain_lag_count + other_lag_count\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    \"\"\"\n",
    "    과거 가격을 피처로 추가 \n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    # 과거 종가 \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    # 과거 고가/저가\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "    \n",
    "    # 과거 거래량\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "    \n",
    "    # 과거 수익률\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    # 과거 가격 비율\n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    added = df_new.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 타겟 변수 생성\n",
    "# ============================================================================\n",
    "\n",
    "def create_targets(df):\n",
    "    \"\"\"타겟 변수 생성\"\"\"\n",
    "    df_target = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "\n",
    "    # 내일 종가\n",
    "    next_close = close.shift(-1)\n",
    "    \n",
    "    # 오늘 → 내일 로그 수익률\n",
    "    df_target['next_log_return'] = np.log(next_close / close)\n",
    "    \n",
    "    # 오늘 → 내일 방향성\n",
    "    df_target['next_direction'] = (next_close > close).astype(int)\n",
    "    \n",
    "    # 내일 실제 종가\n",
    "    df_target['next_close'] = next_close   \n",
    "    \n",
    "    return df_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393590d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_cyclic_features(df):\n",
    "    \"\"\"\n",
    "    시간 주기성 특징 추가 \n",
    "    \n",
    "    Reference:\n",
    "    - \"The Importance of Time-Based Cyclic Features\" (2025)\n",
    "    - \"Feature engineering for time-series data\" (Statsig, 2025)\n",
    "    \"\"\"\n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    # 기본 시간 특징\n",
    "    df_temporal['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df_temporal['day_of_month'] = df['date'].dt.day\n",
    "    df_temporal['month'] = df['date'].dt.month\n",
    "    df_temporal['quarter'] = df['date'].dt.quarter\n",
    "    df_temporal['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # 월말/월초 효과 \n",
    "    df_temporal['is_month_start'] = (df['date'].dt.is_month_start).astype(int)\n",
    "    df_temporal['is_month_end'] = (df['date'].dt.is_month_end).astype(int)\n",
    "    df_temporal['is_quarter_start'] = (df['date'].dt.is_quarter_start).astype(int)\n",
    "    df_temporal['is_quarter_end'] = (df['date'].dt.is_quarter_end).astype(int)\n",
    "    \n",
    "    # 주말 효과 \n",
    "    df_temporal['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical Encoding (Sine/Cosine for periodicity)\n",
    "    df_temporal['day_of_week_sin'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['day_of_week_cos'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['month_sin'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['month_cos'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['day_of_month_sin'] = np.sin(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    df_temporal['day_of_month_cos'] = np.cos(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    \n",
    "    added = df_temporal.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    \"\"\"\n",
    "    고차원 상호작용 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): Momentum × Volatility 상호작용 중요\n",
    "    - \"Causal Feature Engineering\" (2023): 특징 조합이 단일 특징보다 예측력 높음\n",
    "    \"\"\"\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    # 1. RSI × Volume\n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 2. Bollinger Band Position × Sentiment\n",
    "    if 'BB_POSITION' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['BB_Sentiment_Consensus'] = df['BB_POSITION'] * df['sentiment_polarity']\n",
    "    \n",
    "    # 3. VIX × ETH Volatility\n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    # 4. MACD × Volume\n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 5. BTC Return × ETH-BTC Correlation\n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    # 6. Sentiment × News Volume\n",
    "    if 'sentiment_polarity' in df.columns and 'news_count' in df.columns:\n",
    "        df_interact['Sentiment_Volume_Intensity'] = df['sentiment_polarity'] * np.log1p(df['news_count'])\n",
    "    \n",
    "    # 7. ATR × Volume Ratio\n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    # 8. RSI Overbought × High Volume\n",
    "    if 'RSI_OVERBOUGHT' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Overbought_High_Volume'] = df['RSI_OVERBOUGHT'] * (df['VOLUME_RATIO'] > 1.5).astype(int)\n",
    "    \n",
    "    # 9. Golden Cross × Positive Sentiment\n",
    "    if 'SMA_GOLDEN_CROSS' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['Golden_Sentiment_Align'] = df['SMA_GOLDEN_CROSS'] * (df['sentiment_polarity'] > 0).astype(int)\n",
    "    \n",
    "    # 10. Price Acceleration × Momentum\n",
    "    if 'PRICE_ACCELERATION' in df.columns and 'MOMENTUM_10' in df.columns:\n",
    "        df_interact['Acceleration_Momentum'] = df['PRICE_ACCELERATION'] * df['MOMENTUM_10']\n",
    "    \n",
    "    added = df_interact.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    \"\"\"\n",
    "    변동성 체제 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Intraday trading of cryptocurrencies\" (2023): 변동성 체제별 예측 정확도 차이 존재\n",
    "\n",
    "    \"\"\"\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        # 1. 고변동성 vs 저변동성 \n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        # 2. 변동성 급증 이벤트\n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        # 3. 변동성 백분위수\n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        # 4. 변동성 추세\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        \n",
    "        # 5. 변동성 체제 지속기간\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    added = df_regime.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    \"\"\"\n",
    "    정규화된 가격 Lag 특징 추가 (분류 모델용)\n",
    "    \n",
    "    Reference:\n",
    "    - \"Financial Forecasting with ML: Price vs Return\" (2021)\n",
    "    - 분류 문제에서 절대 가격보다 비율이 2-3배 더 예측력 높음\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        current_close = df['ETH_Close']\n",
    "    else:\n",
    "        return df_norm\n",
    "    \n",
    "    # 1. 가격 Lag를 현재 가격 대비 비율로 변환\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    # 2. High/Low Lag를 Close 대비 비율\n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    added = df_norm.shape[1] - df.shape[1]\n",
    "\n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_cumulative_streak_features(df):\n",
    "    \"\"\"\n",
    "    누적 및 연속 패턴 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Feature engineering for time-series\" (2025): 연속 패턴은 모멘텀 지속성 예측에 핵심\n",
    "    \"\"\"\n",
    "    df_cum = df.copy()\n",
    "    \n",
    "    if 'eth_log_return' in df.columns:\n",
    "        returns = df['eth_log_return']\n",
    "        \n",
    "        # 1. 연속 상승 일수\n",
    "        df_cum['consecutive_up_days'] = (returns > 0).astype(int).groupby(\n",
    "            (returns <= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 2. 연속 하락 일수\n",
    "        df_cum['consecutive_down_days'] = (returns < 0).astype(int).groupby(\n",
    "            (returns >= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 3. 최근 20일 내 최대 연속 상승\n",
    "        df_cum['max_consecutive_up_20d'] = df_cum['consecutive_up_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 4. 최근 20일 내 최대 연속 하락\n",
    "        df_cum['max_consecutive_down_20d'] = df_cum['consecutive_down_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 5. 누적 수익률 (20일)\n",
    "        df_cum['cumulative_return_20d'] = returns.rolling(20, min_periods=5).sum()\n",
    "        \n",
    "        # 6. 상승/하락 비율 (20일 내)\n",
    "        df_cum['up_down_ratio_20d'] = (\n",
    "            (returns > 0).rolling(20, min_periods=5).sum() / \n",
    "            ((returns < 0).rolling(20, min_periods=5).sum() + 1e-8)\n",
    "        )\n",
    "\n",
    "    added = df_cum.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_cum\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    \"\"\"\n",
    "\n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): 백분위수 특징이 상대적 위치 파악에 효과적\n",
    "    \"\"\"\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    # 1. 가격 백분위수 (250일)\n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 2. 거래량 백분위수 (90일)\n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 3. RSI 백분위수 (60일)\n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    added = df_pct.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff72206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=40):\n",
    "    \n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'return':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'price':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'direction_return':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Return (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        ret_features, ret_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + ret_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **ret_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'ret_stats': ret_stats,\n",
    "            'overlap': len(set(dir_features) & set(ret_features))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    elif target_type == 'direction_price':\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Price (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        price_features, price_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + price_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **price_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'price_stats': price_stats,\n",
    "            'overlap': len(set(dir_features) & set(price_features))\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target_type: {target_type}\")\n",
    "    \n",
    "    print(\"Selected Features\")\n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=40, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "def split_tvt_method(df, train_start_date, test_start_date='2025-01-01', \n",
    "                     train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    test_start_date를 고정하고, 그 이전 데이터를 train/val로 분할\n",
    "    test_start_date 이후 데이터는 모두 test로 사용\n",
    "    \"\"\"\n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    \n",
    "    # 테스트 시작 날짜를 datetime으로 변환\n",
    "    if isinstance(test_start_date, str):\n",
    "        test_start_date = pd.to_datetime(test_start_date)\n",
    "    \n",
    "    # test_start_date 이전 데이터를 train/val로, 이후를 test로 분할\n",
    "    pre_test_df = df_period[df_period['date'] < test_start_date].copy()\n",
    "    test_df = df_period[df_period['date'] >= test_start_date].copy()\n",
    "    \n",
    "    # train/val 분할 (test 이전 데이터만 사용)\n",
    "    n_pre_test = len(pre_test_df)\n",
    "    train_end = int(n_pre_test * train_ratio / (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = pre_test_df.iloc[:train_end].copy()\n",
    "    val_df = pre_test_df.iloc[train_end:].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TVT Split (Fixed Test Start: {test_start_date.date()})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Train: {len(train_df):4d} ({train_df['date'].min().date()} ~ {train_df['date'].max().date()})\")\n",
    "    print(f\"  Val:   {len(val_df):4d} ({val_df['date'].min().date()} ~ {val_df['date'].max().date()})\")\n",
    "    print(f\"  Test:  {len(test_df):4d} ({test_df['date'].min().date()} ~ {test_df['date'].max().date()})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, \n",
    "                              final_test_start='2025-01-01',\n",
    "                              n_splits=4,\n",
    "                              initial_train_size=600,\n",
    "                              val_size=60,\n",
    "                              test_size=60,\n",
    "                              step=240,\n",
    "                              lookback=30):\n",
    "    \"\"\"\n",
    "    과거 기간에 대한 walk-forward folds + 최종 고정 테스트 기간\n",
    "    \n",
    "    최적 설정 (기본값):\n",
    "    - n_splits=4: 약세장, 회복기, 강세전환, 통합기 모두 평가\n",
    "    - step=240 (8개월): fold 간 충분한 독립성 확보\n",
    "    - 총 5회 학습 (4 walk-forward + 1 final holdout)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        'date' 컬럼을 포함한 시계열 데이터\n",
    "    train_start_date : str\n",
    "        학습 시작 날짜 (예: '2020-12-19')\n",
    "    final_test_start : str\n",
    "        고정 holdout 테스트 시작 날짜 (기본: '2025-01-01')\n",
    "    n_splits : int\n",
    "        생성할 walk-forward fold 수 (기본: 4)\n",
    "    initial_train_size : int\n",
    "        첫 fold의 최소 학습 데이터 일수 (기본: 600)\n",
    "    val_size : int\n",
    "        검증 기간 일수 (기본: 60)\n",
    "    test_size : int\n",
    "        테스트 기간 일수 (기본: 60)\n",
    "    step : int\n",
    "        fold 간 이동 간격 일수 (기본: 240)\n",
    "    lookback : int\n",
    "        특성 생성용 lookback 기간 (기본: 30)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        각 fold의 train/val/test 데이터프레임과 메타정보\n",
    "    \"\"\"\n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Walk-Forward Configuration\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total: {len(df_period)} days\")\n",
    "    print(f\"Pre-final: {len(pre_final_df)} days | Final holdout: {len(final_test_df)} days\")\n",
    "    print(f\"Target: {n_splits} walk-forward + 1 final holdout = {n_splits + 1} folds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    if n_splits is None:\n",
    "        available_data = len(pre_final_df) - initial_train_size - val_size - test_size\n",
    "        n_splits = max(1, (available_data // step) + 1)\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        test_start_idx = initial_train_size + val_size + (fold_idx * step)\n",
    "        test_end_idx = test_start_idx + test_size\n",
    "        \n",
    "        if test_end_idx > len(pre_final_df):\n",
    "            break\n",
    "        \n",
    "        val_end_idx = test_start_idx\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        train_end_idx = val_start_idx\n",
    "        \n",
    "        if train_end_idx < initial_train_size:\n",
    "            continue\n",
    "        \n",
    "        train_fold = pre_final_df.iloc[:train_end_idx].copy()\n",
    "        val_fold = pre_final_df.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = pre_final_df.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        assert train_fold['date'].max() < val_fold['date'].min(), \"Train/Val overlap!\"\n",
    "        assert val_fold['date'].max() <= test_fold['date'].min(), \"Val/Test overlap!\"\n",
    "        \n",
    "        # ========== 출력 추가 ==========\n",
    "        print(f\"Fold {fold_idx + 1} (walk_forward)\")\n",
    "        print(f\"  Train: {len(train_fold):4d}d  {train_fold['date'].min().date()} ~ {train_fold['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(val_fold):4d}d  {val_fold['date'].min().date()} ~ {val_fold['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(test_fold):4d}d  {test_fold['date'].min().date()} ~ {test_fold['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'fold_type': 'walk_forward'\n",
    "        })\n",
    "    \n",
    "    if len(final_test_df) > 0:\n",
    "        final_train_end = len(pre_final_df)\n",
    "        final_val_start = final_train_end - val_size\n",
    "        final_train_data = pre_final_df.iloc[:final_val_start].copy()\n",
    "        final_val_data = pre_final_df.iloc[final_val_start:final_train_end].copy()\n",
    "        \n",
    "        # ========== 출력 추가 ==========\n",
    "        print(f\"Fold {len(folds) + 1} (final_holdout)\")\n",
    "        print(f\"  Train: {len(final_train_data):4d}d  {final_train_data['date'].min().date()} ~ {final_train_data['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(final_val_data):4d}d  {final_val_data['date'].min().date()} ~ {final_val_data['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(final_test_df):4d}d  {final_test_df['date'].min().date()} ~ {final_test_df['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Created {len(folds)} folds total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "    \"\"\"\n",
    "    각 fold를 독립적으로 처리 (feature selection 포함)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    # Fold 정보 출력\n",
    "    if fold_idx is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Fold {fold_idx} ({fold_type})\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(),\n",
    "        train_start_date=train_df['date'].min(),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(),\n",
    "        train_start_date=val_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(),\n",
    "        train_start_date=test_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close']\n",
    "    \n",
    "    train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    print(f\"\\n[Feature Selection for Fold {fold_idx}]\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        target_type=target_type, \n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for this fold\")\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    print(f\"Scaling completed for Fold {fold_idx}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_df['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),  \n",
    "            'dates': test_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                     final_test_start='2025-01-01',\n",
    "                                     method='tvt', target_type='direction', **kwargs):\n",
    "    \"\"\"\n",
    "    전체 파이프라인 실행 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : DataFrame\n",
    "        원본 데이터\n",
    "    train_start_date : str\n",
    "        학습 데이터 시작 날짜\n",
    "    final_test_start : str, default='2025-01-01'\n",
    "        최종 고정 테스트 시작 날짜\n",
    "        - TVT: 이 날짜부터 마지막까지 테스트\n",
    "        - Walk-forward: 이 날짜 이전은 walk-forward folds, 이후는 final holdout\n",
    "    method : str, default='tvt'\n",
    "        'tvt' 또는 'walk_forward'\n",
    "    target_type : str, default='direction'\n",
    "        'direction', 'return', 'price', 'direction_return', 'direction_price'\n",
    "    **kwargs : dict\n",
    "        각 method에 필요한 추가 파라미터\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    df = create_targets(df)\n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_temporal_cyclic_features(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_cumulative_streak_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = remove_raw_prices_and_transform(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    df = df.iloc[:-1]  \n",
    "    \n",
    "    split_kwargs = {}\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        split_kwargs['test_start_date'] = final_test_start\n",
    "        if 'train_ratio' in kwargs:\n",
    "            split_kwargs['train_ratio'] = kwargs['train_ratio']\n",
    "        if 'val_ratio' in kwargs:\n",
    "            split_kwargs['val_ratio'] = kwargs['val_ratio']\n",
    "        splits = split_tvt_method(df, train_start_date, **split_kwargs)\n",
    "        \n",
    "    elif method == 'walk_forward':\n",
    "        split_kwargs['final_test_start'] = final_test_start\n",
    "        if 'n_splits' in kwargs:\n",
    "            split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "        if 'initial_train_size' in kwargs:\n",
    "            split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "        if 'test_size' in kwargs:\n",
    "            split_kwargs['test_size'] = kwargs['test_size']\n",
    "        if 'val_size' in kwargs:\n",
    "            split_kwargs['val_size'] = kwargs['val_size']\n",
    "        if 'step' in kwargs:\n",
    "            split_kwargs['step'] = kwargs['step']\n",
    "        if 'lookback' in kwargs:\n",
    "            split_kwargs['lookback'] = kwargs['lookback']\n",
    "        splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    if method == 'tvt':\n",
    "        result = process_single_split(\n",
    "            splits, \n",
    "            target_type=target_type,  \n",
    "            top_n=40,\n",
    "            fold_idx=1\n",
    "        )\n",
    "    else:\n",
    "        result = [\n",
    "            process_single_split(\n",
    "                fold, \n",
    "                target_type=target_type,  \n",
    "                top_n=40,\n",
    "                fold_idx=fold['fold_idx']\n",
    "            ) \n",
    "            for fold in splits\n",
    "        ]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff3b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_features_multi_target(X_train, y_train, target_type='direction', top_n=40):\n",
    "#     \"\"\"\n",
    "#     Multi-Target Feature Selection\n",
    "    \n",
    "#     5가지 케이스별 최적화된 feature selection:\n",
    "#     1. direction (분류)\n",
    "#     2. return (회귀)  \n",
    "#     3. price (회귀)\n",
    "#     4. direction_return (혼합)\n",
    "#     5. direction_price (혼합)\n",
    "    \n",
    "#     Reference:\n",
    "#     - \"Multi-target HSIC-Lasso\" (2024)\n",
    "#     - \"Feature selection for multi-target regression\" (2021)\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     if target_type == 'direction':\n",
    "#         # 순수 분류\n",
    "#         selected, stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_direction'], \n",
    "#             task='class', \n",
    "#             top_n=top_n\n",
    "#         )\n",
    "        \n",
    "#     elif target_type == 'return':\n",
    "#         # 순수 회귀 (수익률)\n",
    "#         selected, stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_log_return'], \n",
    "#             task='reg', \n",
    "#             top_n=top_n\n",
    "#         )\n",
    "        \n",
    "#     elif target_type == 'price':\n",
    "#         # 순수 회귀 (가격)\n",
    "#         selected, stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_close'], \n",
    "#             task='reg', \n",
    "#             top_n=top_n\n",
    "#         )\n",
    "        \n",
    "#     elif target_type == 'direction_return':\n",
    "#         # 혼합: 분류 + 회귀 (방향 + 수익률)\n",
    "#         print(\"\\n[Hybrid] Direction (50%) + Return (50%)\")\n",
    "        \n",
    "#         # 각각 절반씩 선택\n",
    "#         dir_features, dir_stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_direction'], \n",
    "#             task='class', \n",
    "#             top_n=top_n // 2,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         ret_features, ret_stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_log_return'], \n",
    "#             task='reg', \n",
    "#             top_n=top_n // 2,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         # 합집합으로 결합 (중복 제거)\n",
    "#         selected = list(dict.fromkeys(dir_features + ret_features))\n",
    "        \n",
    "#         # 부족하면 MI 스코어 높은 순으로 추가\n",
    "#         if len(selected) < top_n:\n",
    "#             all_mi_scores = {**dir_stats['mi_scores'], **ret_stats['mi_scores']}\n",
    "#             sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "#             for feat, _ in sorted_features:\n",
    "#                 if feat not in selected:\n",
    "#                     selected.append(feat)\n",
    "#                     if len(selected) >= top_n:\n",
    "#                         break\n",
    "        \n",
    "#         # 너무 많으면 자르기\n",
    "#         selected = selected[:top_n]\n",
    "        \n",
    "#         stats = {\n",
    "#             'dir_stats': dir_stats,\n",
    "#             'ret_stats': ret_stats,\n",
    "#             'overlap': len(set(dir_features) & set(ret_features))\n",
    "#         }\n",
    "        \n",
    "        \n",
    "#     elif target_type == 'direction_price':\n",
    "#         # 혼합: 분류 + 회귀 (방향 + 가격)\n",
    "#         print(\"\\n[Hybrid] Direction (50%) + Price (50%)\")\n",
    "        \n",
    "#         dir_features, dir_stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_direction'], \n",
    "#             task='class', \n",
    "#             top_n=top_n // 2,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         price_features, price_stats = select_features_verified(\n",
    "#             X_train, \n",
    "#             y_train['next_close'], \n",
    "#             task='reg', \n",
    "#             top_n=top_n // 2,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         selected = list(dict.fromkeys(dir_features + price_features))\n",
    "        \n",
    "#         if len(selected) < top_n:\n",
    "#             all_mi_scores = {**dir_stats['mi_scores'], **price_stats['mi_scores']}\n",
    "#             sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "#             for feat, _ in sorted_features:\n",
    "#                 if feat not in selected:\n",
    "#                     selected.append(feat)\n",
    "#                     if len(selected) >= top_n:\n",
    "#                         break\n",
    "        \n",
    "#         selected = selected[:top_n]\n",
    "        \n",
    "#         stats = {\n",
    "#             'dir_stats': dir_stats,\n",
    "#             'price_stats': price_stats,\n",
    "#             'overlap': len(set(dir_features) & set(price_features))\n",
    "#         }\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown target_type: {target_type}\")\n",
    "    \n",
    "#     print(\"선택된 지표들\")\n",
    "#     print(\", \".join(selected))\n",
    "#     return selected, stats\n",
    "\n",
    "\n",
    "# def select_features_verified(X_train, y_train, task='class', top_n=40, verbose=True):\n",
    "#     \"\"\"\n",
    "#     검증된 Feature Selection 방법 (2025 연구 기반)\n",
    "    \n",
    "#     핵심 원칙:\n",
    "#     1. 하이퍼파라미터 튜닝 없이 기본 파라미터 사용\n",
    "#     2. MI + RFE + RF Importance 앙상블\n",
    "#     3. 빠른 실행 속도\n",
    "    \n",
    "#     Reference:\n",
    "#     - \"Optimizing Forecast Accuracy in Cryptocurrency Markets\" (2025)\n",
    "#     - \"Feature Selection After Split\" (Reddit, 2022)\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     if task == 'class':\n",
    "#         mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "#     else:\n",
    "#         mi_scores = mutual_info_regression(X_train, y_train, random_state=42)\n",
    "    \n",
    "#     mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "#     mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    \n",
    "#     # 기본 파라미터만 사용 \n",
    "#     if task == 'class':\n",
    "#         estimator = LGBMClassifier(\n",
    "#             n_estimators=100,\n",
    "#             learning_rate=0.05,\n",
    "#             max_depth=5,\n",
    "#             random_state=42,\n",
    "#             verbose=-1\n",
    "#         )\n",
    "#     else:\n",
    "#         estimator = LGBMRegressor(\n",
    "#             n_estimators=100,\n",
    "#             learning_rate=0.05,\n",
    "#             max_depth=5,\n",
    "#             random_state=42,\n",
    "#             verbose=-1\n",
    "#         )\n",
    "    \n",
    "#     rfe = RFE(\n",
    "#         estimator=estimator,\n",
    "#         n_features_to_select=top_n,\n",
    "#         step=0.1,  # 10%씩 제거\n",
    "#         verbose=0\n",
    "#     )\n",
    "    \n",
    "#     rfe.fit(X_train, y_train)\n",
    "#     rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    \n",
    "#     if task == 'class':\n",
    "#         rf_model = RandomForestClassifier(\n",
    "#             n_estimators=100,\n",
    "#             max_depth=10,\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "#     else:\n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             n_estimators=100,\n",
    "#             max_depth=10,\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "    \n",
    "#     rf_model.fit(X_train, y_train)\n",
    "#     rf_importances = rf_model.feature_importances_\n",
    "#     rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "#     rf_features = X_train.columns[rf_idx].tolist()\n",
    "#     all_features = mi_features + rfe_features + rf_features\n",
    "#     feature_votes = Counter(all_features)\n",
    "#     selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "#     if len(selected_features) < top_n:\n",
    "#         remaining = top_n - len(selected_features)\n",
    "#         for feat in mi_features:\n",
    "#             if feat not in selected_features:\n",
    "#                 selected_features.append(feat)\n",
    "#                 remaining -= 1\n",
    "#                 if remaining == 0:\n",
    "#                     break\n",
    "    \n",
    "#     return selected_features, {\n",
    "#         'mi_features': mi_features,\n",
    "#         'rfe_features': rfe_features,\n",
    "#         'rf_features': rf_features,\n",
    "#         'feature_votes': feature_votes,\n",
    "#         'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "#         'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "#     }\n",
    "\n",
    "# # ============================================================================\n",
    "# # 전체 파이프라인 \n",
    "# # ============================================================================\n",
    "\n",
    "# def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "#                                      method='tvt', target_type='direction', **kwargs):\n",
    "#     \"\"\"\n",
    "\n",
    "#     1. Feature Engineering (전체 데이터)\n",
    "#     2. Target 생성 (전체 데이터)  \n",
    "#     3. Train/Val/Test Split\n",
    "#     4. Missing Value Handling \n",
    "#     5. Feature Selection \n",
    "#     6. Scaling (Train에서만 Fit)\n",
    "    \n",
    "#     Reference:\n",
    "#     - \"Feature Selection After Split\" (Stack Overflow, 2019)\n",
    "#     - \"Scaling After Feature Selection\" (Reddit, 2023)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df = df_raw.copy()\n",
    "    \n",
    "#     # Target 생성 \n",
    "#     df = create_targets(df)\n",
    "    \n",
    "#     # 과거 가격 Lag\n",
    "#     df = add_price_lag_features_first(df)\n",
    "    \n",
    "#     # 기술적 지표\n",
    "#     df = calculate_technical_indicators(df)\n",
    "    \n",
    "#     # 시간 주기성\n",
    "#     df = add_temporal_cyclic_features(df)\n",
    "    \n",
    "#     # BTC-ETH 교차 특징\n",
    "#     df = add_enhanced_cross_crypto_features(df)\n",
    "    \n",
    "#     # 변동성 체제\n",
    "#     df = add_volatility_regime_features(df)\n",
    "    \n",
    "#     # 상호작용 특징\n",
    "#     df = add_interaction_features(df)\n",
    "    \n",
    "#     # 누적/연속 특징\n",
    "#     df = add_cumulative_streak_features(df)\n",
    "    \n",
    "#     # 백분위수 특징\n",
    "#     df = add_percentile_features(df)\n",
    "    \n",
    "#     # 정규화 가격 Lag\n",
    "#     df = add_normalized_price_lags(df)\n",
    "    \n",
    "#     # Raw 가격 제거\n",
    "#     df = remove_raw_prices_and_transform(df)\n",
    "    \n",
    "#     # Lag 적용 (감성, 온체인)\n",
    "#     df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "\n",
    "#     # 1. 원본 VIX 확인\n",
    "#     if 'vix_VIX' in df.columns:\n",
    "#         vix_missing = df['vix_VIX'].isnull().sum()\n",
    "\n",
    "#     # 2. VOLATILITY_20 확인\n",
    "#     if 'VOLATILITY_20' in df.columns:\n",
    "#         vol_missing = df['VOLATILITY_20'].isnull().sum()\n",
    "\n",
    "#     # 3. 상호작용 특징 확인\n",
    "#     if 'VIX_ETH_Vol_Cross' in df.columns:\n",
    "#         cross_missing = df['VIX_ETH_Vol_Cross'].isnull().sum()\n",
    "\n",
    "\n",
    "#     # 4. Lag 적용 후 확인\n",
    "#     if 'VIX_ETH_Vol_Cross_lag1' in df.columns:\n",
    "#         cross_lag_missing = df['VIX_ETH_Vol_Cross_lag1'].isnull().sum()\n",
    "\n",
    "\n",
    "#     pd.set_option('display.max_columns', None)\n",
    "#     column_list = df.columns.tolist()\n",
    "#     df = df.iloc[:-1]  \n",
    "#     split_kwargs = {}\n",
    "#     if method == 'tvt':\n",
    "#         if 'train_ratio' in kwargs:\n",
    "#             split_kwargs['train_ratio'] = kwargs['train_ratio']\n",
    "#         if 'val_ratio' in kwargs:\n",
    "#             split_kwargs['val_ratio'] = kwargs['val_ratio']\n",
    "#         splits = split_tvt_method(df, train_start_date, **split_kwargs)\n",
    "#     elif method == 'walk_forward':\n",
    "#         if 'n_splits' in kwargs:\n",
    "#             split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "#         if 'initial_train_size' in kwargs:\n",
    "#             split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "#         if 'test_size' in kwargs:\n",
    "#             split_kwargs['test_size'] = kwargs['test_size']\n",
    "#         splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "#     # ===================================================================\n",
    "#     # PHASE 3: 각 Split에 대해 Missing/Selection/Scaling 수행\n",
    "#     # ===================================================================\n",
    "\n",
    "    \n",
    "#     if method == 'tvt':\n",
    "#             result = process_single_split(\n",
    "#         splits, \n",
    "#         target_type=target_type,  \n",
    "#         top_n=40                 \n",
    "#         )\n",
    "#     else:\n",
    "#             result = [\n",
    "#         process_single_split(\n",
    "#             fold, \n",
    "#             target_type=target_type,  \n",
    "#             top_n=40,\n",
    "#             fold_idx=i+1\n",
    "#         ) \n",
    "#         for i, fold in enumerate(splits)\n",
    "#         ]\n",
    "#     return result\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # Split 함수들 \n",
    "# # ============================================================================\n",
    "\n",
    "# def split_tvt_method(df, train_start_date, train_ratio=0.7, val_ratio=0.15):\n",
    "#     \"\"\"TVT 분할 (결측치 처리 X, 단순 분할만)\"\"\"\n",
    "    \n",
    "#     df_period = df[df['date'] >= train_start_date].copy()\n",
    "    \n",
    "#     n = len(df_period)\n",
    "#     train_end = int(n * train_ratio)\n",
    "#     val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "#     train_df = df_period.iloc[:train_end].copy()\n",
    "#     val_df = df_period.iloc[train_end:val_end].copy()\n",
    "#     test_df = df_period.iloc[val_end:].copy()\n",
    "    \n",
    "#     print(f\"  Train: {len(train_df)} ({train_df['date'].min().date()} ~ {train_df['date'].max().date()})\")\n",
    "#     print(f\"  Val:   {len(val_df)} ({val_df['date'].min().date()} ~ {val_df['date'].max().date()})\")\n",
    "#     print(f\"  Test:  {len(test_df)} ({test_df['date'].min().date()} ~ {test_df['date'].max().date()})\")\n",
    "    \n",
    "#     return {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "\n",
    "# def split_walk_forward_method(df, train_start_date, \n",
    "#                               n_splits=None,\n",
    "#                               initial_train_size=600, \n",
    "#                               val_size=60,      \n",
    "#                               test_size=60,\n",
    "#                               lookback=30):     \n",
    "#     \"\"\"\n",
    "#     Walk-Forward 분할 (Anchored/Expanding Window)\n",
    "    \n",
    "#     설정:\n",
    "#     - Initial Train: 600일\n",
    "#     - Val: 60일\n",
    "#     - Test: 60일\n",
    "#     - Step: 60일\n",
    "#     - n_splits: None이면 데이터 최대 활용하여 자동 계산\n",
    "    \n",
    "#     Reference:\n",
    "#     - \"Optimizing Forecast Accuracy in Cryptocurrency\" (2025)\n",
    "#     - Anchored Window: Train이 매 Fold마다 확장\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df_period = df[df['date'] >= train_start_date].copy()\n",
    "#     df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "#     step = 60\n",
    "    \n",
    "#     if n_splits is None:\n",
    "#         total_data = len(df_period)\n",
    "#         min_required = initial_train_size + val_size + test_size\n",
    "#         remaining = total_data - min_required\n",
    "#         n_splits = (remaining // step) + 1\n",
    "#         print(f\"Auto-calculated n_splits: {n_splits} (from {total_data} days)\")\n",
    "    \n",
    "#     folds = []\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Walk-Forward Configuration\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     print(f\"Total data: {len(df_period)} days\")\n",
    "#     print(f\"Train={initial_train_size}d, Val={val_size}d, Test={test_size}d, Step={step}d\")\n",
    "#     print(f\"Lookback={lookback}d, Val sequences: {val_size - lookback}\")\n",
    "#     print(f\"Target folds: {n_splits}\")\n",
    "#     print(f\"{'='*80}\\n\")\n",
    "    \n",
    "#     for fold_idx in range(n_splits):\n",
    "#         train_end_idx = initial_train_size + (fold_idx * step)\n",
    "#         val_start_idx = train_end_idx\n",
    "#         val_end_idx = val_start_idx + val_size\n",
    "#         test_start_idx = val_end_idx\n",
    "#         test_end_idx = test_start_idx + test_size\n",
    "        \n",
    "#         if test_end_idx > len(df_period):\n",
    "#             print(f\"Insufficient data: Fold {fold_idx+1} stopped (need {test_end_idx}, have {len(df_period)})\")\n",
    "#             break\n",
    "        \n",
    "#         train_fold = df_period.iloc[:train_end_idx].copy()\n",
    "#         val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "#         test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "#         assert train_fold['date'].max() < val_fold['date'].min(), \"Train/Val overlap detected!\"\n",
    "#         assert val_fold['date'].max() < test_fold['date'].min(), \"Val/Test overlap detected!\"\n",
    "        \n",
    "#         print(f\"Fold {fold_idx + 1:2d}:\")\n",
    "#         print(f\"  Train: {len(train_fold):4d}d  ({train_fold['date'].min().date()} ~ {train_fold['date'].max().date()})\")\n",
    "#         print(f\"  Val:   {len(val_fold):4d}d  ({val_fold['date'].min().date()} ~ {val_fold['date'].max().date()})\")\n",
    "#         print(f\"  Test:  {len(test_fold):4d}d  ({test_fold['date'].min().date()} ~ {test_fold['date'].max().date()})\")\n",
    "        \n",
    "#         folds.append({\n",
    "#             'train': train_fold,\n",
    "#             'val': val_fold,\n",
    "#             'test': test_fold,\n",
    "#             'fold_idx': fold_idx + 1\n",
    "#         })\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Summary: {len(folds)} folds generated\")\n",
    "#     print(f\"Total test days: {len(folds) * test_size}\")\n",
    "#     print(f\"Test coverage: {folds[0]['test']['date'].min().date()} ~ {folds[-1]['test']['date'].max().date()}\")\n",
    "#     print(f\"Data utilization: {(test_end_idx/len(df_period)*100):.1f}%\")\n",
    "#     print(f\"{'='*80}\\n\")\n",
    "    \n",
    "#     return folds\n",
    "\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # 핵심: 각 Split 처리 \n",
    "# # ============================================================================\n",
    "\n",
    "# def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "#     \"\"\"\n",
    "#     개선된 전처리 파이프라인\n",
    "    \n",
    "#     변경사항:\n",
    "#     1. GridSearchCV 제거 (feature selection 단계에서)\n",
    "#     2. 검증된 MI+RFE+RF 앙상블 사용\n",
    "#     3. Multi-target 지원\n",
    "#     \"\"\"\n",
    "    \n",
    "#     train_df = split_data['train']\n",
    "#     val_df = split_data['val']\n",
    "#     test_df = split_data['test']\n",
    "    \n",
    "#     # ===== 1. 결측치 처리 =====\n",
    "    \n",
    "#     train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "#         train_df.copy(),\n",
    "#         train_start_date=train_df['date'].min(),\n",
    "#         is_train=True\n",
    "#     )\n",
    "    \n",
    "#     val_processed = handle_missing_values_paper_based(\n",
    "#         val_df.copy(),\n",
    "#         train_start_date=val_df['date'].min(),\n",
    "#         is_train=False,\n",
    "#         train_stats=missing_stats\n",
    "#     )\n",
    "    \n",
    "#     test_processed = handle_missing_values_paper_based(\n",
    "#         test_df.copy(),\n",
    "#         train_start_date=test_df['date'].min(),\n",
    "#         is_train=False,\n",
    "#         train_stats=missing_stats\n",
    "#     )\n",
    "    \n",
    "#     target_cols = ['next_log_return', 'next_direction', 'next_close']\n",
    "    \n",
    "#     train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "#     val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "#     test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     feature_cols = [col for col in train_processed.columns \n",
    "#                    if col not in target_cols + ['date']]\n",
    "    \n",
    "#     X_train = train_processed[feature_cols]\n",
    "#     y_train = train_processed[target_cols]\n",
    "    \n",
    "#     X_val = val_processed[feature_cols]\n",
    "#     y_val = val_processed[target_cols]\n",
    "    \n",
    "#     X_test = test_processed[feature_cols]\n",
    "#     y_test = test_processed[target_cols]\n",
    "\n",
    "    \n",
    "#     selected_features, selection_stats = select_features_multi_target(\n",
    "#         X_train, \n",
    "#         y_train, \n",
    "#         target_type=target_type, \n",
    "#         top_n=top_n\n",
    "#     )\n",
    "    \n",
    "#     X_train_sel = X_train[selected_features]\n",
    "#     X_val_sel = X_val[selected_features]\n",
    "#     X_test_sel = X_test[selected_features]\n",
    "    \n",
    "#     robust_scaler = RobustScaler()\n",
    "#     standard_scaler = StandardScaler()\n",
    "    \n",
    "#     X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "#     X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "#     X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "#     X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "#     X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "#     X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "#     # ===== 6. 결과 패키징 =====\n",
    "#     result = {\n",
    "#         'train': {\n",
    "#             'X_robust': X_train_robust,\n",
    "#             'X_standard': X_train_standard,\n",
    "#             'X_raw': X_train_sel,\n",
    "#             'y': y_train.reset_index(drop=True), \n",
    "#             'dates': train_df['date'].reset_index(drop=True) \n",
    "#         },\n",
    "#         'val': {\n",
    "#             'X_robust': X_val_robust,\n",
    "#             'X_standard': X_val_standard,\n",
    "#             'X_raw': X_val_sel,\n",
    "#             'y': y_val.reset_index(drop=True), \n",
    "#             'dates': val_df['date'].reset_index(drop=True)  \n",
    "#         },\n",
    "#         'test': {\n",
    "#             'X_robust': X_test_robust,\n",
    "#             'X_standard': X_test_standard,\n",
    "#             'X_raw': X_test_sel,\n",
    "#             'y': y_test.reset_index(drop=True),  \n",
    "#             'dates': test_df['date'].reset_index(drop=True)  \n",
    "#         },\n",
    "#         'stats': {\n",
    "#             'robust_scaler': robust_scaler,\n",
    "#             'standard_scaler': standard_scaler,\n",
    "#             'selected_features': selected_features,\n",
    "#             'selection_stats': selection_stats,\n",
    "#             'target_type': target_type,\n",
    "#             'target_cols': target_cols\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdc0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 15, 20],\n",
    "            'min_samples_split': [10, 15, 20],\n",
    "            'min_samples_leaf': [4, 6, 8],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model = LGBMClassifier(**param)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                     callbacks=[early_stopping(50, verbose=False)])\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        model = LGBMClassifier(**study.best_params, random_state=42, verbose=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                 callbacks=[early_stopping(50, verbose=False)])\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 2.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 0.5),\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'logloss'\n",
    "            }\n",
    "            model = XGBClassifier(**param)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        model = XGBClassifier(**study.best_params, random_state=42, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def svm(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "        model = SVC(random_state=42, probability=True)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 256, step=32)\n",
    "            units2 = trial.suggest_int('units2', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', return_sequences=True, \n",
    "                     input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                LSTM(units2, activation='tanh', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', return_sequences=True, \n",
    "                 input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            LSTM(best_params['units2'], activation='tanh', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 256, step=32)\n",
    "            units2 = trial.suggest_int('units2', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(units1, return_sequences=True, kernel_regularizer=l2(l2_reg)), \n",
    "                             input_shape=input_shape),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Bidirectional(LSTM(units2, kernel_regularizer=l2(l2_reg))),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(best_params['units1'], return_sequences=True, kernel_regularizer=l2(best_params['l2_reg'])), \n",
    "                         input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Bidirectional(LSTM(best_params['units2'], kernel_regularizer=l2(best_params['l2_reg']))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 256, step=32)\n",
    "            units2 = trial.suggest_int('units2', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                GRU(units1, activation='tanh', return_sequences=True, \n",
    "                    input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                GRU(units2, activation='tanh', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            GRU(best_params['units1'], activation='tanh', return_sequences=True, \n",
    "                input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            GRU(best_params['units2'], activation='tanh', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacked_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 256, step=32)\n",
    "            units2 = trial.suggest_int('units2', 48, 128, step=16)\n",
    "            units3 = trial.suggest_int('units3', 32, 96, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                LSTM(units1, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                LSTM(units2, return_sequences=True, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                LSTM(units3, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            LSTM(best_params['units2'], return_sequences=True, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            LSTM(best_params['units3'], kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), \n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            conv_filters1 = trial.suggest_int('conv_filters1', 32, 128, step=32)\n",
    "            conv_filters2 = trial.suggest_int('conv_filters2', 16, 64, step=16)\n",
    "            lstm_units1 = trial.suggest_int('lstm_units1', 64, 192, step=32)\n",
    "            lstm_units2 = trial.suggest_int('lstm_units2', 32, 96, step=32)\n",
    "            dropout_conv = trial.suggest_float('dropout_conv', 0.1, 0.3)\n",
    "            dropout_lstm = trial.suggest_float('dropout_lstm', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "            \n",
    "            model = Sequential([\n",
    "                Conv1D(conv_filters1, 3, activation='relu', padding='same', \n",
    "                       input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                MaxPooling1D(2),\n",
    "                Dropout(dropout_conv),\n",
    "                Conv1D(conv_filters2, 3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout_conv),\n",
    "                LSTM(lstm_units1, return_sequences=True, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout_lstm),\n",
    "                LSTM(lstm_units2, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout_lstm),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout_lstm * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            Conv1D(best_params['conv_filters1'], 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(best_params['dropout_conv']),\n",
    "            Conv1D(best_params['conv_filters2'], 3, activation='relu', padding='same', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout_conv']),\n",
    "            LSTM(best_params['lstm_units1'], return_sequences=True, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout_lstm']),\n",
    "            LSTM(best_params['lstm_units2'], kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout_lstm']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout_lstm'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), \n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            GRU(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm_attention(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "        lstm_out = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "        attention = Attention()([lstm_out, lstm_out])\n",
    "        combined = Add()([lstm_out, attention])\n",
    "        pooled = GlobalAveragePooling1D()(combined)\n",
    "        dense = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(pooled)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        attn_output = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "        ff = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        ff = Dropout(0.1)(ff)\n",
    "        ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tcn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for dilation_rate in [1, 2, 4, 8]:\n",
    "            conv = Conv1D(64, 3, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "            x = Add()([x, conv]) if x.shape[-1] == 64 else conv\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def dtw_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, \n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tabnet(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_d': trial.suggest_int('n_d', 32, 128, step=32),\n",
    "                'n_a': trial.suggest_int('n_a', 32, 128, step=32),\n",
    "                'n_steps': trial.suggest_int('n_steps', 3, 7),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "                'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-5, 1e-3, log=True),\n",
    "                'momentum': trial.suggest_float('momentum', 0.1, 0.5),\n",
    "                'optimizer_params': dict(lr=trial.suggest_float('lr', 1e-3, 5e-2, log=True)),\n",
    "                'mask_type': 'entmax',\n",
    "                'n_independent': 2,\n",
    "                'n_shared': 2,\n",
    "                'scheduler_params': {\"step_size\": 50, \"gamma\": 0.9},\n",
    "                'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "                'verbose': 0,\n",
    "                'seed': 42\n",
    "            }\n",
    "            model = TabNetClassifier(**param, optimizer_fn=torch.optim.Adam)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                     max_epochs=100, patience=20, batch_size=256, virtual_batch_size=128)\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        model = TabNetClassifier(\n",
    "            n_d=best_params['n_d'], n_a=best_params['n_a'], n_steps=best_params['n_steps'],\n",
    "            gamma=best_params['gamma'], lambda_sparse=best_params['lambda_sparse'],\n",
    "            momentum=best_params['momentum'], optimizer_params=dict(lr=best_params['lr']),\n",
    "            mask_type='entmax', n_independent=2, n_shared=2,\n",
    "            scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            optimizer_fn=torch.optim.Adam, verbose=0, seed=42\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                 max_epochs=100, patience=20, batch_size=256, virtual_batch_size=128)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def informer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            num_heads = trial.suggest_int('num_heads', 2, 8, step=2)\n",
    "            key_dim = trial.suggest_int('key_dim', 16, 64, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "            for _ in range(2):\n",
    "                attn = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout)(x, x)\n",
    "                attn = Dropout(dropout)(attn)\n",
    "                x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "                x = Conv1D(input_shape[1], 1, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "                x = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout + 0.2)(x)\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=best_params['num_heads'], key_dim=best_params['key_dim'], \n",
    "                                      dropout=best_params['dropout'])(x, x)\n",
    "            attn = Dropout(best_params['dropout'])(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            x = Conv1D(input_shape[1], 1, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "            x = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(best_params['l2_reg']))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(best_params['dropout'] + 0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def nbeats(X_train, y_train, X_val, y_val, input_shape):\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=input_shape),\n",
    "            Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def temporal_fusion_transformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Flatten()(inputs)\n",
    "        var_weights = Dense(input_shape[0] * input_shape[1], activation='softmax',\n",
    "                           kernel_regularizer=l2(0.01))(x)\n",
    "        var_weights = Reshape(input_shape)(var_weights)\n",
    "        selected = Multiply()([inputs, var_weights])\n",
    "\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(selected)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def performer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Dense(128, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def patchtst(X_train, y_train, X_val, y_val, input_shape, patch_len=16, stride=8):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        patches = []\n",
    "        for i in range(0, input_shape[0] - patch_len + 1, stride):\n",
    "            patch = Lambda(lambda z: z[:, i:i+patch_len, :])(x)\n",
    "            patch = Flatten()(patch)\n",
    "            patch = Dense(128, kernel_regularizer=l2(0.01))(patch)\n",
    "            patches.append(patch)\n",
    "\n",
    "        if len(patches) > 1:\n",
    "            x = tf.stack(patches, axis=1)\n",
    "        else:\n",
    "            x = tf.expand_dims(patches[0], axis=1)\n",
    "\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def autoformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        trend = tf.keras.layers.AveragePooling1D(pool_size=25, strides=1, padding='same')(x)\n",
    "        seasonal = tf.subtract(x, trend)\n",
    "\n",
    "        x = seasonal\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        seasonal_out = GlobalAveragePooling1D()(x)\n",
    "        trend_out = GlobalAveragePooling1D()(trend)\n",
    "        combined = Concatenate()([seasonal_out, trend_out])\n",
    "\n",
    "        combined = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "        combined = BatchNormalization()(combined)\n",
    "        combined = Dropout(0.3)(combined)\n",
    "        outputs = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def itransformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=16, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(64, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def ethervoyant(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "\n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "\n",
    "        x = Concatenate()([conv1, conv2])\n",
    "        x = MaxPooling1D(2)(x)\n",
    "\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        x = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def vmd_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(x)\n",
    "        low_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(low_freq)\n",
    "\n",
    "        mid_freq = x - low_freq\n",
    "        mid_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(mid_freq)\n",
    "\n",
    "        high_freq = x - low_freq - mid_freq\n",
    "        high_freq = Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(high_freq)\n",
    "\n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(96, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def naive_bayes(X_train, y_train, X_val, y_val):\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def knn(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "        }\n",
    "        model = KNeighborsClassifier(n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.1, 0.5, 1.0, 1.5]\n",
    "        }\n",
    "        model = AdaBoostClassifier(algorithm='SAMME', random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "                'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "                'random_seed': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "            model = CatBoostClassifier(**param)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "                     early_stopping_rounds=50, verbose=False)\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        model = CatBoostClassifier(**study.best_params, random_seed=42, verbose=False)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "                 early_stopping_rounds=50, verbose=False)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def decision_tree(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'max_depth': [10, 15, 20, None],\n",
    "            'min_samples_split': [10, 15, 20],\n",
    "            'min_samples_leaf': [4, 6, 8],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def extra_trees(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 15, 20],\n",
    "            'min_samples_split': [10, 15, 20],\n",
    "            'min_samples_leaf': [4, 6, 8],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        model = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def bagging(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_samples': [0.6, 0.8, 1.0],\n",
    "            'max_features': [0.6, 0.8, 1.0]\n",
    "        }\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        model = BaggingClassifier(estimator=base_estimator, random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [5, 7, 9],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'min_samples_split': [10, 15, 20]\n",
    "        }\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def simple_rnn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 64, 192, step=32)\n",
    "            units2 = trial.suggest_int('units2', 32, 96, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "\n",
    "            model = Sequential([\n",
    "                SimpleRNN(units1, activation='tanh', return_sequences=True,\n",
    "                         input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                SimpleRNN(units2, activation='tanh', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        model = Sequential([\n",
    "            SimpleRNN(best_params['units1'], activation='tanh', return_sequences=True,\n",
    "                     input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            SimpleRNN(best_params['units2'], activation='tanh', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def mlp(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 128, 512, step=128)\n",
    "            units2 = trial.suggest_int('units2', 64, 256, step=64)\n",
    "            units3 = trial.suggest_int('units3', 32, 128, step=32)\n",
    "            dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.001, 0.1, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "\n",
    "            input_dim = X_train.shape[1]\n",
    "            model = Sequential([\n",
    "                Dense(units1, activation='relu', input_dim=input_dim, kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(units3, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(dropout),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout * 0.7),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "            _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=15, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = Sequential([\n",
    "            Dense(best_params['units1'], activation='relu', input_dim=input_dim, kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(best_params['units2'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(best_params['units3'], activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout'] * 0.7),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def emd_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        low_freq = tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        low_freq = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(low_freq)\n",
    "\n",
    "        high_freq = inputs - tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        high_freq = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(high_freq)\n",
    "\n",
    "        x = Concatenate()([low_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def hybrid_lstm_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(32, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def parallel_cnn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = MaxPooling1D(2)(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "\n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = MaxPooling1D(2)(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "\n",
    "        conv3 = Conv1D(64, 7, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = MaxPooling1D(2)(conv3)\n",
    "        conv3 = Dropout(0.2)(conv3)\n",
    "\n",
    "        x = Concatenate()([conv1, conv2, conv3])\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        import optuna\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            rf_estimators = trial.suggest_int('rf_estimators', 50, 200)\n",
    "            rf_depth = trial.suggest_int('rf_depth', 5, 15)\n",
    "            xgb_estimators = trial.suggest_int('xgb_estimators', 50, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_depth', 3, 10)\n",
    "            lgbm_estimators = trial.suggest_int('lgbm_estimators', 50, 200)\n",
    "            lgbm_depth = trial.suggest_int('lgbm_depth', 3, 10)\n",
    "\n",
    "            base_learners = [\n",
    "                ('rf', RandomForestClassifier(n_estimators=rf_estimators, max_depth=rf_depth, \n",
    "                                             random_state=42, n_jobs=-1)),\n",
    "                ('xgb', XGBClassifier(n_estimators=xgb_estimators, max_depth=xgb_depth, \n",
    "                                     learning_rate=0.1, random_state=42)),\n",
    "                ('lgbm', LGBMClassifier(n_estimators=lgbm_estimators, max_depth=lgbm_depth, \n",
    "                                       learning_rate=0.1, random_state=42, verbose=-1))\n",
    "            ]\n",
    "            meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, \n",
    "                                       cv=3, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_val)\n",
    "            accuracy = (preds == y_val).sum() / len(y_val)\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        base_learners = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=best_params['rf_estimators'], \n",
    "                                         max_depth=best_params['rf_depth'], random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(n_estimators=best_params['xgb_estimators'], \n",
    "                                 max_depth=best_params['xgb_depth'], learning_rate=0.1, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=best_params['lgbm_estimators'], \n",
    "                                   max_depth=best_params['lgbm_depth'], learning_rate=0.1, \n",
    "                                   random_state=42, verbose=-1))\n",
    "        ]\n",
    "        meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=5, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def voting_hard(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'rf__n_estimators': [50, 100, 150],\n",
    "            'rf__max_depth': [10, 15, 20],\n",
    "            'xgb__n_estimators': [50, 100, 150],\n",
    "            'xgb__max_depth': [5, 7, 9]\n",
    "        }\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(random_state=42, verbose=-1))\n",
    "        ]\n",
    "        model = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def voting_soft(X_train, y_train, X_val, y_val):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {\n",
    "            'rf__n_estimators': [50, 100, 150],\n",
    "            'rf__max_depth': [10, 15, 20],\n",
    "            'xgb__n_estimators': [50, 100, 150],\n",
    "            'xgb__max_depth': [5, 7, 9]\n",
    "        }\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(random_state=42, verbose=-1)),\n",
    "            ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]\n",
    "        model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm_xgboost_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        lstm_features = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(lstm_features)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def residual_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        x = Add()([x, lstm_out])\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def wavenet(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        skip_connections = []\n",
    "\n",
    "        for dilation_rate in [1, 2, 4, 8, 16, 32]:\n",
    "            conv = Conv1D(64, 2, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "\n",
    "            skip = Conv1D(64, 1, kernel_regularizer=l2(0.01))(conv)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "            res = Conv1D(64, 1, kernel_regularizer=l2(0.01))(conv)\n",
    "            if x.shape[-1] != 64:\n",
    "                x = Conv1D(64, 1, kernel_regularizer=l2(0.01))(x)\n",
    "            x = Add()([x, res])\n",
    "\n",
    "        x = Add()(skip_connections)\n",
    "        x = Activation('relu')(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e031051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"모델 평가 및 백테스팅 (Task별 전략 구현)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.predictions={}\n",
    "\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for i, p in enumerate(pred):\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "        \n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                     X_test, y_test, test_returns, test_dates, model_name,\n",
    "                                     is_deep_learning=False):\n",
    "        \"\"\"분류 모델 평가 - Binary Signal 전략\"\"\"\n",
    "        \n",
    "        # 예측\n",
    "        train_pred = self._predict_model(model, X_train)\n",
    "        val_pred = self._predict_model(model, X_val)\n",
    "        test_pred = self._predict_model(model, X_test)\n",
    "        \n",
    "        # ===== 확률값 추출 =====\n",
    "        test_pred_proba = None\n",
    "        if is_deep_learning:\n",
    "            test_pred_proba = test_pred.copy()\n",
    "            # 멀티태스크: 분류 output만 선택\n",
    "            if isinstance(train_pred, list):\n",
    "                train_pred = train_pred[0]\n",
    "                val_pred = val_pred[0]\n",
    "                test_pred = test_pred[0]\n",
    "                test_pred_proba = test_pred_proba[0] if isinstance(test_pred_proba, list) else test_pred_proba\n",
    "            train_pred = (train_pred > 0.5).astype(int).ravel()\n",
    "            val_pred = (val_pred > 0.5).astype(int).ravel()\n",
    "            test_pred = (test_pred > 0.5).astype(int).ravel()\n",
    "        else:\n",
    "            # ML 모델 확률값 추출\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # 분류 지표\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        \n",
    "        test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "        test_rec = recall_score(y_test, test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, test_pred)\n",
    "        \n",
    "        # ===== 예측값 저장 =====\n",
    "        self._save_predictions(\n",
    "            model_name, test_pred, test_pred_proba,\n",
    "            y_test, test_returns, test_dates\n",
    "        )\n",
    "\n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_acc,\n",
    "            'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc,\n",
    "            'Test_Precision': test_prec,\n",
    "            'Test_Recall': test_rec,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        return self.results[-1]\n",
    "\n",
    "    # ===== 추가: 예측값 저장 메서드 =====\n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba,\n",
    "                         actual_direction, actual_returns, dates):\n",
    "        \"\"\"예측값 저장 (백테스팅용)\"\"\"\n",
    "        \n",
    "        # 확률값 처리\n",
    "        if pred_proba is not None:\n",
    "            if pred_proba.ndim == 2 and pred_proba.shape[1] == 2:\n",
    "                # Binary classification: [P(down), P(up)]\n",
    "                pred_proba_up = pred_proba[:, 1]\n",
    "                pred_proba_down = pred_proba[:, 0]\n",
    "            else:\n",
    "                # Single output (sigmoid)\n",
    "                pred_proba_up = pred_proba.ravel()\n",
    "                pred_proba_down = 1 - pred_proba_up\n",
    "        else:\n",
    "            # 확률 미지원: 0.9/0.1 근사\n",
    "            pred_proba_up = np.where(pred_direction == 1, 0.9, 0.1)\n",
    "            pred_proba_down = 1 - pred_proba_up\n",
    "        \n",
    "        # 신뢰도 계산\n",
    "        confidence = np.abs(pred_proba_up - 0.5) * 2\n",
    "        max_proba = np.maximum(pred_proba_up, pred_proba_down)\n",
    "        \n",
    "        # DataFrame 생성\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': actual_direction,\n",
    "            'actual_return': actual_returns,\n",
    "            'pred_direction': pred_direction,\n",
    "            'pred_proba_up': pred_proba_up,\n",
    "            'pred_proba_down': pred_proba_down,\n",
    "            'confidence': confidence,\n",
    "            'max_proba': max_proba,\n",
    "            'correct': (pred_direction == actual_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        # 딕셔너리에 저장\n",
    "        self.predictions[model_name] = predictions_df\n",
    "\n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    # ===== 예측값 반환 메서드 =====\n",
    "    def get_predictions_dict(self):\n",
    "        \"\"\"저장된 예측값 딕셔너리 반환\"\"\"\n",
    "        return self.predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # 모델 설정 정의 (Classification)\n",
    "# # ============================================================================\n",
    "\n",
    "# ML_MODELS_CLASSIFICATION = [\n",
    "#     {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "#     {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "#     {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "#     {'index': 4, 'name': 'SVM', 'func': DirectionModels.svm, 'needs_val': True},\n",
    "#     {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "#     {'index': 6, 'name': 'NaiveBayes', 'func': DirectionModels.naive_bayes, 'needs_val': True},\n",
    "#     {'index': 7, 'name': 'KNN', 'func': DirectionModels.knn, 'needs_val': True},\n",
    "#     {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "#     {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "#     {'index': 10, 'name': 'DecisionTree', 'func': DirectionModels.decision_tree, 'needs_val': True},\n",
    "#     {'index': 11, 'name': 'ExtraTrees', 'func': DirectionModels.extra_trees, 'needs_val': True},\n",
    "#     {'index': 12, 'name': 'Bagging', 'func': DirectionModels.bagging, 'needs_val': True},\n",
    "#     {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "#     {'index': 14, 'name': 'TabNet', 'func': DirectionModels.tabnet, 'needs_val': True},\n",
    "#     {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "#     {'index': 16, 'name': 'VotingHard', 'func': DirectionModels.voting_hard, 'needs_val': True},\n",
    "#     {'index': 17, 'name': 'VotingSoft', 'func': DirectionModels.voting_soft, 'needs_val': True},\n",
    "#     {'index': 18, 'name': 'MLP', 'func': DirectionModels.mlp, 'needs_val': True},\n",
    "# ]\n",
    "\n",
    "# DL_MODELS_CLASSIFICATION = [\n",
    "#     {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "#     {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "#     {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True},\n",
    "#     {'index': 22, 'name': 'Stacked_LSTM', 'func': DirectionModels.stacked_lstm, 'needs_val': True},\n",
    "#     {'index': 23, 'name': 'CNN_LSTM', 'func': DirectionModels.cnn_lstm, 'needs_val': True},\n",
    "#     {'index': 24, 'name': 'CNN_GRU', 'func': DirectionModels.cnn_gru, 'needs_val': True},\n",
    "#     {'index': 25, 'name': 'CNN_BiLSTM', 'func': DirectionModels.cnn_bilstm, 'needs_val': True},\n",
    "#     {'index': 26, 'name': 'LSTM_Attention', 'func': DirectionModels.lstm_attention, 'needs_val': True},\n",
    "#     {'index': 27, 'name': 'Transformer', 'func': DirectionModels.transformer, 'needs_val': True},\n",
    "#     {'index': 28, 'name': 'TCN', 'func': DirectionModels.tcn, 'needs_val': True},\n",
    "#     {'index': 29, 'name': 'DTW_LSTM', 'func': DirectionModels.dtw_lstm, 'needs_val': True},\n",
    "#     {'index': 30, 'name': 'Informer', 'func': DirectionModels.informer, 'needs_val': True},\n",
    "#     {'index': 31, 'name': 'NBEATS', 'func': DirectionModels.nbeats, 'needs_val': True},\n",
    "#     {'index': 32, 'name': 'TFT', 'func': DirectionModels.temporal_fusion_transformer, 'needs_val': True},\n",
    "#     {'index': 33, 'name': 'Performer', 'func': DirectionModels.performer, 'needs_val': True},\n",
    "#     {'index': 34, 'name': 'PatchTST', 'func': DirectionModels.patchtst, 'needs_val': True},\n",
    "#     {'index': 35, 'name': 'Autoformer', 'func': DirectionModels.autoformer, 'needs_val': True},\n",
    "#     {'index': 36, 'name': 'iTransformer', 'func': DirectionModels.itransformer, 'needs_val': True},\n",
    "#     {'index': 37, 'name': 'EtherVoyant', 'func': DirectionModels.ethervoyant, 'needs_val': True},\n",
    "#     {'index': 38, 'name': 'VMD_Hybrid', 'func': DirectionModels.vmd_hybrid, 'needs_val': True},\n",
    "#     {'index': 39, 'name': 'SimpleRNN', 'func': DirectionModels.simple_rnn, 'needs_val': True},\n",
    "#     {'index': 40, 'name': 'EMD_LSTM', 'func': DirectionModels.emd_lstm, 'needs_val': True},\n",
    "#     {'index': 41, 'name': 'Hybrid_LSTM_GRU', 'func': DirectionModels.hybrid_lstm_gru, 'needs_val': True},\n",
    "#     {'index': 42, 'name': 'Parallel_CNN', 'func': DirectionModels.parallel_cnn, 'needs_val': True},\n",
    "#     {'index': 43, 'name': 'LSTM_XGBoost_Hybrid', 'func': DirectionModels.lstm_xgboost_hybrid, 'needs_val': True},\n",
    "#     {'index': 44, 'name': 'Residual_LSTM', 'func': DirectionModels.residual_lstm, 'needs_val': True},\n",
    "#     {'index': 45, 'name': 'WaveNet', 'func': DirectionModels.wavenet, 'needs_val': True},\n",
    "# ]\n",
    "# ============================================================================\n",
    "# ML Models (15 models - 52.84% threshold)\n",
    "# ============================================================================\n",
    "\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'SVM', 'func': DirectionModels.svm, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    {'index': 6, 'name': 'NaiveBayes', 'func': DirectionModels.naive_bayes, 'needs_val': True},\n",
    "    {'index': 7, 'name': 'KNN', 'func': DirectionModels.knn, 'needs_val': True},\n",
    "    {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 10, 'name': 'DecisionTree', 'func': DirectionModels.decision_tree, 'needs_val': True},\n",
    "    {'index': 11, 'name': 'ExtraTrees', 'func': DirectionModels.extra_trees, 'needs_val': True},\n",
    "    {'index': 12, 'name': 'Bagging', 'func': DirectionModels.bagging, 'needs_val': True},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 14, 'name': 'TabNet', 'func': DirectionModels.tabnet, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "    {'index': 16, 'name': 'VotingHard', 'func': DirectionModels.voting_hard, 'needs_val': True},\n",
    "    {'index': 17, 'name': 'VotingSoft', 'func': DirectionModels.voting_soft, 'needs_val': True},\n",
    "    {'index': 18, 'name': 'MLP', 'func': DirectionModels.mlp, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# DL Models (8 models - 55% threshold)\n",
    "# ============================================================================\n",
    "\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True},\n",
    "    # {'index': 22, 'name': 'Stacked_LSTM', 'func': DirectionModels.stacked_lstm, 'needs_val': True},\n",
    "    # {'index': 23, 'name': 'CNN_LSTM', 'func': DirectionModels.cnn_lstm, 'needs_val': True},\n",
    "    # {'index': 24, 'name': 'CNN_GRU', 'func': DirectionModels.cnn_gru, 'needs_val': True},\n",
    "    # {'index': 25, 'name': 'CNN_BiLSTM', 'func': DirectionModels.cnn_bilstm, 'needs_val': True},\n",
    "    # {'index': 26, 'name': 'LSTM_Attention', 'func': DirectionModels.lstm_attention, 'needs_val': True},\n",
    "    # {'index': 27, 'name': 'Transformer', 'func': DirectionModels.transformer, 'needs_val': True},\n",
    "    # {'index': 28, 'name': 'TCN', 'func': DirectionModels.tcn, 'needs_val': True},\n",
    "    {'index': 29, 'name': 'DTW_LSTM', 'func': DirectionModels.dtw_lstm, 'needs_val': True},\n",
    "    # {'index': 30, 'name': 'Informer', 'func': DirectionModels.informer, 'needs_val': True},\n",
    "    # {'index': 31, 'name': 'NBEATS', 'func': DirectionModels.nbeats, 'needs_val': True},\n",
    "    # {'index': 32, 'name': 'TFT', 'func': DirectionModels.temporal_fusion_transformer, 'needs_val': True},\n",
    "    # {'index': 33, 'name': 'Performer', 'func': DirectionModels.performer, 'needs_val': True},\n",
    "    # {'index': 34, 'name': 'PatchTST', 'func': DirectionModels.patchtst, 'needs_val': True},\n",
    "    # {'index': 35, 'name': 'Autoformer', 'func': DirectionModels.autoformer, 'needs_val': True},\n",
    "    # {'index': 36, 'name': 'iTransformer', 'func': DirectionModels.itransformer, 'needs_val': True},\n",
    "    # {'index': 37, 'name': 'EtherVoyant', 'func': DirectionModels.ethervoyant, 'needs_val': True},\n",
    "    {'index': 38, 'name': 'VMD_Hybrid', 'func': DirectionModels.vmd_hybrid, 'needs_val': True},\n",
    "    # {'index': 39, 'name': 'SimpleRNN', 'func': DirectionModels.simple_rnn, 'needs_val': True},\n",
    "    {'index': 40, 'name': 'EMD_LSTM', 'func': DirectionModels.emd_lstm, 'needs_val': True},\n",
    "    {'index': 41, 'name': 'Hybrid_LSTM_GRU', 'func': DirectionModels.hybrid_lstm_gru, 'needs_val': True},\n",
    "    # {'index': 42, 'name': 'Parallel_CNN', 'func': DirectionModels.parallel_cnn, 'needs_val': True},\n",
    "    # {'index': 43, 'name': 'LSTM_XGBoost_Hybrid', 'func': DirectionModels.lstm_xgboost_hybrid, 'needs_val': True},\n",
    "    {'index': 44, 'name': 'Residual_LSTM', 'func': DirectionModels.residual_lstm, 'needs_val': True},\n",
    "    # {'index': 45, 'name': 'WaveNet', 'func': DirectionModels.wavenet, 'needs_val': True},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b27d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator, lookback=30,\n",
    "                    ml_models=None, dl_models=None, task='classification'):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{task.capitalize()} 모델 학습 시작 (총 {len(ml_models) + len(dl_models)}개 모델)\")\n",
    "    print(\"=\"*80)\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    # ML 모델\n",
    "    print(f\"\\n[Part 1/2] Machine Learning 모델 ({len(ml_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    ml_success_count = 0\n",
    "    for model_config in ml_models:\n",
    "        success = trainer.train_ml_model(\n",
    "            model_config, X_train, y_train, X_val, y_val,\n",
    "            X_test, y_test, test_returns, test_dates, task=task\n",
    "        )\n",
    "        if success:\n",
    "            ml_success_count += 1\n",
    "    print(f\"\\n✓ ML 모델 완료: {ml_success_count}/{len(ml_models)}개 성공\")\n",
    "\n",
    "    # DL 모델\n",
    "    print(f\"\\n[Part 2/2] Deep Learning/시계열 모델 ({len(dl_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\n시퀀스 데이터 생성 중 (lookback={lookback})...\")\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test, lookback)\n",
    "    test_returns_seq = test_returns[lookback:]\n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    print(f\"  ✓ Train shape: {X_train_seq.shape}\")\n",
    "    print(f\"  ✓ Val shape: {X_val_seq.shape}\")\n",
    "    print(f\"  ✓ Test shape: {X_test_seq.shape}\")\n",
    "    print(f\"  ✓ Input shape: {input_shape}\\n\")\n",
    "    dl_success_count = 0\n",
    "    for model_config in dl_models:\n",
    "        if model_config['name'] in ['TabNet', 'TabNet_Reg', 'Ensemble_Stacking', 'Ensemble_Voting']:\n",
    "            success = trainer.train_ml_model(\n",
    "                model_config, X_train, y_train, X_val, y_val,\n",
    "                X_test, y_test, test_returns, test_dates, task=task\n",
    "            )\n",
    "        else:\n",
    "            if 'outputs' in model_config and len(model_config['outputs']) > 1:\n",
    "                y_train_list = [y_train_seq[:, i] for i in range(y_train_seq.shape[1])]\n",
    "                y_val_list = [y_val_seq[:, i] for i in range(y_val_seq.shape[1])]\n",
    "                y_test_list = [y_test_seq[:, i] for i in range(y_test_seq.shape[1])]\n",
    "                success = trainer.train_dl_multitask_model(\n",
    "                    model_config, X_train_seq, y_train_list, X_val_seq, y_val_list,\n",
    "                    X_test_seq, y_test_list, test_returns_seq, test_dates_seq, input_shape\n",
    "                )\n",
    "            else:\n",
    "                success = trainer.train_dl_model(\n",
    "                    model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq, input_shape, task=task\n",
    "                )\n",
    "        if success:\n",
    "            dl_success_count += 1\n",
    "    print(f\"\\n✓ DL 모델 완료: {dl_success_count}/{len(dl_models)}개 성공\")\n",
    "    total_success = ml_success_count + dl_success_count\n",
    "    total_models = len(ml_models) + len(dl_models)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"전체 학습 완료: {total_success}/{total_models}개 모델 성공\")\n",
    "    print(\"=\"*80)\n",
    "    return total_success\n",
    "\n",
    "def train_models_for_fold(fold_idx, X_train, y_train, X_val, y_val,\n",
    "                          X_test, y_test, test_returns, test_dates,\n",
    "                          evaluator, all_fold_results, lookback=30,\n",
    "                          ml_models=None, dl_models=None, task='classification'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Fold {fold_idx + 1} - {task.capitalize()} 모델 학습\")\n",
    "    print(f\"{'='*80}\")\n",
    "    success_count = train_all_models(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        test_returns, test_dates, evaluator, lookback,\n",
    "        ml_models=ml_models, dl_models=dl_models, task=task\n",
    "    )\n",
    "    fold_summary = evaluator.get_summary_dataframe()\n",
    "    fold_summary['Fold'] = fold_idx + 1\n",
    "    all_fold_results.append(fold_summary)\n",
    "    print(f\"\\n✓ Fold {fold_idx + 1} 완료 ({success_count}개 모델)\")\n",
    "    return fold_summary\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"모델 학습 및 평가를 위한 통합 클래스 (분류/회귀 공통)\"\"\"\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            # DataFrame이면 .iloc, array면 직접 인덱싱\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val,\n",
    "                       X_test, y_test, test_returns, test_dates, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "        \n",
    "            # 평가\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name'],\n",
    "                    is_deep_learning=is_mlp  # 여기만 수정\n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name'],\n",
    "                    is_deep_learning=is_mlp  # 여기만 수정\n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "\n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq,\n",
    "                       X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "                       test_returns_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            model = model_config['func'](\n",
    "                X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape\n",
    "            )\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539ba23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cases = [\n",
    "    {'name': 'direction', 'target_type': 'direction', 'outputs': ['next_direction']}\n",
    "]\n",
    "\n",
    "split_methods = [\n",
    "    {'name': 'walk_forward', 'method': 'walk_forward'},\n",
    "    {'name': 'tvt', 'method': 'tvt'}\n",
    "]\n",
    "\n",
    "\n",
    "RESULT_DIR = \"model_results\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "def save_walk_forward_results(all_fold_results, all_fold_predictions, target_name, task):\n",
    "    \"\"\"\n",
    "    Walk-Forward 결과 저장 (walk-forward vs final holdout 분리)\n",
    "    \n",
    "    Args:\n",
    "        all_fold_results: [(fold_df, fold_type), ...] 형태로 수정\n",
    "        all_fold_predictions: [(fold_pred_dict, fold_type), ...] 형태로 수정\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Detailed results (모든 fold 포함)\n",
    "    detailed_results = []\n",
    "    for fold_idx, (fold_df, fold_type) in enumerate(all_fold_results, start=1):\n",
    "        fold_df_copy = fold_df.copy()\n",
    "        fold_df_copy.insert(0, 'Fold', fold_idx)\n",
    "        fold_df_copy.insert(1, 'fold_type', fold_type)  # ← 추가\n",
    "        detailed_results.append(fold_df_copy)\n",
    "    \n",
    "    detailed_df = pd.concat(detailed_results, ignore_index=True)\n",
    "    \n",
    "    if 'Test_Accuracy' in detailed_df.columns:\n",
    "        detailed_df = detailed_df.sort_values(\n",
    "            by=['Fold', 'Test_Accuracy'], \n",
    "            ascending=[True, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    detailed_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward___detailed.csv\")\n",
    "    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {detailed_path}\")\n",
    "    \n",
    "    # 2. Walk-forward vs Final holdout 분리 ← 핵심!\n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward'].copy()\n",
    "    final_data = detailed_df[detailed_df['fold_type'] == 'final_holdout'].copy()\n",
    "    \n",
    "    numeric_cols = detailed_df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'Fold']\n",
    "    \n",
    "    # 3. Walk-forward 평균 계산 (모델 선택용) ← 핵심!\n",
    "    avg_results = []\n",
    "    for model in detailed_df['Model'].unique():\n",
    "        avg_row = {'Model': model}\n",
    "        \n",
    "        # Walk-forward 평균 및 표준편차\n",
    "        model_wf = wf_data[wf_data['Model'] == model]\n",
    "        if len(model_wf) > 0:\n",
    "            for col in numeric_cols:\n",
    "                if col in model_wf.columns:\n",
    "                    avg_row[f'WF_{col}_Mean'] = model_wf[col].mean()\n",
    "                    avg_row[f'WF_{col}_Std'] = model_wf[col].std()\n",
    "        \n",
    "        # Final holdout 성능 (보고용) ← 핵심!\n",
    "        model_final = final_data[final_data['Model'] == model]\n",
    "        if len(model_final) > 0:\n",
    "            for col in numeric_cols:\n",
    "                if col in model_final.columns:\n",
    "                    avg_row[f'Final_{col}'] = model_final[col].iloc[0]\n",
    "        \n",
    "        avg_results.append(avg_row)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_results)\n",
    "    \n",
    "    # 4. Walk-forward 평균으로 정렬 (모델 선택 기준) ← 핵심!\n",
    "    if 'WF_Test_Accuracy_Mean' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='WF_Test_Accuracy_Mean', ascending=False).reset_index(drop=True)\n",
    "    elif 'WF_Test_RMSE_Mean' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='WF_Test_RMSE_Mean', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    avg_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward___avg.csv\")\n",
    "    avg_df.to_csv(avg_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {avg_path}\")\n",
    "    \n",
    "    # 5. 성능 요약 출력 ← 추가\n",
    "    if len(avg_df) > 0:\n",
    "        best_model = avg_df.iloc[0]['Model']\n",
    "        print(f\"\\n성능 요약 (Best Model: {best_model}):\")\n",
    "        if 'WF_Test_Accuracy_Mean' in avg_df.columns:\n",
    "            wf_score = avg_df.iloc[0]['WF_Test_Accuracy_Mean']\n",
    "            final_score = avg_df.iloc[0].get('Final_Test_Accuracy', 'N/A')\n",
    "            print(f\"  WF 평균 (2022-2024): {wf_score:.2f}%\")\n",
    "            print(f\"  Final Holdout (2025): {final_score}%\")\n",
    "    \n",
    "    # 6. 예측값 저장 (fold_type 구분) ← 수정\n",
    "    if all_fold_predictions:\n",
    "        pred_dir = os.path.join(RESULT_DIR, \"predictions\", f\"{target_name}_walk_forward\")\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        all_models = set()\n",
    "        for fold_pred, _ in all_fold_predictions:  # ← 튜플 언팩\n",
    "            all_models.update(fold_pred.keys())\n",
    "        \n",
    "        for model_name in all_models:\n",
    "            combined_predictions = []\n",
    "            \n",
    "            for fold_idx, (fold_pred, fold_type) in enumerate(all_fold_predictions, start=1):  # ← 튜플 언팩\n",
    "                if model_name in fold_pred:\n",
    "                    fold_df = fold_pred[model_name].copy()\n",
    "                    fold_df.insert(0, 'fold', fold_idx)\n",
    "                    fold_df.insert(1, 'fold_type', fold_type)  # ← 추가\n",
    "                    combined_predictions.append(fold_df)\n",
    "            \n",
    "            if combined_predictions:\n",
    "                combined_df = pd.concat(combined_predictions, ignore_index=True)\n",
    "                pred_filename = f\"{model_name}_all_folds.csv\"\n",
    "                pred_path = os.path.join(pred_dir, pred_filename)\n",
    "                combined_df.to_csv(pred_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Saved {len(all_models)} combined prediction files to {pred_dir}\")\n",
    "    \n",
    "    return detailed_df, avg_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_summary_csv(summary_df, predictions_dict, target_name, split_name, task):\n",
    "    \"\"\"\n",
    "    모델 평가 지표 + 예측값 저장\n",
    "    \n",
    "    Args:\n",
    "        predictions_dict: {model_name: predictions_df} 딕셔너리 추가\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 평가 지표 저장 (기존 코드)\n",
    "    if task == 'classification':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', \n",
    "                       'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_AUC_ROC']\n",
    "        \n",
    "    elif task == 'regression':\n",
    "        metric_cols = ['Model', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Train_MAE', 'Val_MAE', 'Test_MAE', 'Test_R2', 'Test_MAPE', 'Direction_Accuracy']\n",
    "        backtest_cols = ['Model', 'Directional_Return(%)', 'Directional_Sharpe',\n",
    "                         'Total_Return(%)', 'Sharpe', 'Sortino', 'Calmar',\n",
    "                         'Max_Drawdown(%)', 'Win_Rate(%)', 'Total_Trades', 'Profit_Factor',\n",
    "                         'VolScaled_Return(%)', 'VolScaled_Sharpe']\n",
    "                         \n",
    "    elif task == 'multitask':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', 'Test_Precision', \n",
    "                       'Test_Recall', 'Test_F1', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Test_MAE', 'Test_R2', 'Direction_Accuracy']\n",
    "        backtest_cols = ['Model', 'Total_Return(%)', 'Sharpe', 'Sortino', 'Calmar',\n",
    "                         'Max_Drawdown(%)', 'Win_Rate(%)', 'Total_Trades', 'Profit_Factor']\n",
    "        if 'Directional_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['Directional_Return(%)', 'Directional_Sharpe']\n",
    "        if 'VolScaled_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['VolScaled_Return(%)', 'VolScaled_Sharpe']\n",
    "        if 'Confident_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['Confident_Return(%)', 'Confident_Sharpe', 'Confident_Trades']\n",
    "    \n",
    "    # 기존 지표 저장\n",
    "    if task == 'classification':\n",
    "        available_cols = [col for col in metric_cols if col in summary_df.columns]\n",
    "    else:\n",
    "        available_cols = [col for col in metric_cols + backtest_cols if col in summary_df.columns]\n",
    "    \n",
    "    save_df = summary_df[available_cols]\n",
    "    \n",
    "    if 'Test_Accuracy' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_Accuracy', ascending=False).reset_index(drop=True)\n",
    "    elif 'Test_RMSE' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_RMSE', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    filename = f\"{target_name}_{split_name}__metrics.csv\"\n",
    "    file_path = os.path.join(RESULT_DIR, filename)\n",
    "    save_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved metrics: {file_path}\")\n",
    "    \n",
    "    # ===== 예측값 저장 =====\n",
    "    if predictions_dict:\n",
    "        pred_dir = os.path.join(RESULT_DIR, \"predictions\", f\"{target_name}_{split_name}\")\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name, pred_df in predictions_dict.items():\n",
    "            pred_filename = f\"{model_name}.csv\"\n",
    "            pred_path = os.path.join(pred_dir, pred_filename)\n",
    "            pred_df.to_csv(pred_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Saved {len(predictions_dict)} prediction files to {pred_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3263ec9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Experiment: direction x walk_forward\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Walk-Forward Configuration\n",
      "================================================================================\n",
      "Total: 1752 days\n",
      "Pre-final: 1474 days | Final holdout: 278 days\n",
      "Target: 4 walk-forward + 1 final holdout = 5 folds\n",
      "================================================================================\n",
      "\n",
      "Fold 1 (walk_forward)\n",
      "  Train:  600d  2020-12-19 ~ 2022-08-10\n",
      "  Val:     60d  2022-08-11 ~ 2022-10-09\n",
      "  Test:    60d  2022-10-10 ~ 2022-12-08\n",
      "\n",
      "Fold 2 (walk_forward)\n",
      "  Train:  840d  2020-12-19 ~ 2023-04-07\n",
      "  Val:     60d  2023-04-08 ~ 2023-06-06\n",
      "  Test:    60d  2023-06-07 ~ 2023-08-05\n",
      "\n",
      "Fold 3 (walk_forward)\n",
      "  Train: 1080d  2020-12-19 ~ 2023-12-03\n",
      "  Val:     60d  2023-12-04 ~ 2024-02-01\n",
      "  Test:    60d  2024-02-02 ~ 2024-04-01\n",
      "\n",
      "Fold 4 (walk_forward)\n",
      "  Train: 1320d  2020-12-19 ~ 2024-07-30\n",
      "  Val:     60d  2024-07-31 ~ 2024-09-28\n",
      "  Test:    60d  2024-09-29 ~ 2024-11-27\n",
      "\n",
      "Fold 5 (final_holdout)\n",
      "  Train: 1414d  2020-12-19 ~ 2024-11-01\n",
      "  Val:     60d  2024-11-02 ~ 2024-12-31\n",
      "  Test:   278d  2025-01-01 ~ 2025-10-05\n",
      "\n",
      "================================================================================\n",
      "Created 5 folds total\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 1 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 1]\n",
      "Training data shape: (600, 381)\n",
      "Selected Features\n",
      "DPO_20, VOLUME_CHANGE, HIGH_CLOSE_RANGE, btc_return_lag10, VOLUME_STRENGTH, extremity_index_lag1, AD, eth_btc_volume_ratio_ma30, high_lag3_ratio, MOM_30, GAP, btc_return_lag5, BB_Sentiment_Consensus, low_lag2_ratio, btc_volatility_7d, bnb_return, xrp_volume_ratio_20d, sol_return, sol_volume_change, ada_volume_change, dot_volume_ratio_20d, eth_large_eth_transfers_lag1, eth_total_gas_used_lag1, low_lag5_ratio, PRICE_VS_SMA10, eth_btc_corr_7d, vol_regime_duration, ADX_14, INC_5, low_lag1, btc_intraday_range, low_lag7, VIX_ETH_Vol_Cross_lag1, ada_volume_ratio_20d, sp500_SP500_lag1, sentiment_sum, low_lag1_ratio, news_count_lag1, return_lag2, RSI_OVERBOUGHT\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 2]\n",
      "Training data shape: (840, 381)\n",
      "Selected Features\n",
      "DPO_20, VOLUME_CHANGE_5, low_lag2_ratio, btc_return_lag5, eth_btc_corr_7d, bnb_volume_ratio_20d, volume_lag3, VOLUME_CHANGE, btc_return_lag10, btc_body_ratio, volume_lag5, sentiment_ma3, news_volume_change, eth_active_addresses, return_lag1, GAP, eth_btc_corr_3d, Acceleration_Momentum, bnb_return, bnb_volume_change, sol_return, ada_volume_change, dot_volume_ratio_20d, eth_btc_volume_corr_30d, eth_large_eth_transfers_lag1, eth_contract_events_lag1, bull_bear_ratio, volume_percentile_90d, low_lag1, eth_avg_block_difficulty, PRICE_VS_SMA10, RSI_30, news_volume_ma14, vol_regime_duration, Liquidity_Risk, RSI_percentile_60d, HIGH_CLOSE_RANGE, eth_btc_spread, sentiment_sum, sp500_SP500_lag1\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 3]\n",
      "Training data shape: (1080, 381)\n",
      "Selected Features\n",
      "DPO_20, eth_btc_corr_3d, btc_return_lag5, high_lag5_ratio, eth_btc_corr_7d, MACDH_12_26_9, volume_lag5, sentiment_acceleration, eth_avg_gas_price, GAP, btc_return_lag1, btc_dominance, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, sol_return, doge_volume_ratio_20d, eth_btc_volume_ratio, eth_large_eth_transfers_lag1, price_percentile_250d, HIGH_CLOSE_RANGE, EMA_CROSS_SIGNAL, btc_intraday_range, bull_bear_ratio, RSI_percentile_60d, eth_avg_block_difficulty, bull_bear_ratio_lag1, PRICE_VS_SMA10, sentiment_sum, eth_intraday_range, HIGH_LOW_RANGE, DISTANCE_FROM_LOW, VTXM_14, eth_btc_spread, eth_btc_volcorr_30d, vol_regime_duration, funding_fundingRate_lag1, BTC_Weighted_Impact, VOLUME_CHANGE, CCI_14\n",
      "Selected 40 features for this fold\n",
      "Scaling completed for Fold 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4 (walk_forward)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 4]\n",
      "Training data shape: (1320, 381)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_case[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_method[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_complete_pipeline_corrected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_method\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_case\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_start_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2025-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 2025 고정\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_method[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtvt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# TVT 방식\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_robust\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[5], line 554\u001b[0m, in \u001b[0;36mbuild_complete_pipeline_corrected\u001b[0;34m(df_raw, train_start_date, final_test_start, method, target_type, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m     result \u001b[38;5;241m=\u001b[39m process_single_split(\n\u001b[1;32m    548\u001b[0m         splits, \n\u001b[1;32m    549\u001b[0m         target_type\u001b[38;5;241m=\u001b[39mtarget_type,  \n\u001b[1;32m    550\u001b[0m         top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m    551\u001b[0m         fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    555\u001b[0m         process_single_split(\n\u001b[1;32m    556\u001b[0m             fold, \n\u001b[1;32m    557\u001b[0m             target_type\u001b[38;5;241m=\u001b[39mtarget_type,  \n\u001b[1;32m    558\u001b[0m             top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m    559\u001b[0m             fold_idx\u001b[38;5;241m=\u001b[39mfold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    560\u001b[0m         ) \n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m splits\n\u001b[1;32m    562\u001b[0m     ]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[5], line 555\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    547\u001b[0m     result \u001b[38;5;241m=\u001b[39m process_single_split(\n\u001b[1;32m    548\u001b[0m         splits, \n\u001b[1;32m    549\u001b[0m         target_type\u001b[38;5;241m=\u001b[39mtarget_type,  \n\u001b[1;32m    550\u001b[0m         top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m    551\u001b[0m         fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 555\u001b[0m         \u001b[43mprocess_single_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfold_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m splits\n\u001b[1;32m    562\u001b[0m     ]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[5], line 412\u001b[0m, in \u001b[0;36mprocess_single_split\u001b[0;34m(split_data, target_type, top_n, fold_idx)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Feature Selection for Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 412\u001b[0m selected_features, selection_stats \u001b[38;5;241m=\u001b[39m \u001b[43mselect_features_multi_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features for this fold\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m X_train_sel \u001b[38;5;241m=\u001b[39m X_train[selected_features]\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mselect_features_multi_target\u001b[0;34m(X_train, y_train, target_type, top_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_features_multi_target\u001b[39m(X_train, y_train, target_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m'\u001b[39m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         selected, stats \u001b[38;5;241m=\u001b[39m \u001b[43mselect_features_verified\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext_direction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m         selected, stats \u001b[38;5;241m=\u001b[39m select_features_verified(\n\u001b[1;32m     13\u001b[0m             X_train, \n\u001b[1;32m     14\u001b[0m             y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_log_return\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     15\u001b[0m             task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m             top_n\u001b[38;5;241m=\u001b[39mtop_n\n\u001b[1;32m     17\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[5], line 166\u001b[0m, in \u001b[0;36mselect_features_verified\u001b[0;34m(X_train, y_train, task, top_n, verbose)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[1;32m    160\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    161\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    162\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m    163\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[0;32m--> 166\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m rf_importances \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m    168\u001b[0m rf_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(rf_importances)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:top_n]\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:486\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    475\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    478\u001b[0m ]\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[1;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for target_case in target_cases:\n",
    "    for split_method in split_methods:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Experiment: {target_case['name']} x {split_method['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = build_complete_pipeline_corrected(\n",
    "            df_merged, train_start_date,\n",
    "            method=split_method['method'],\n",
    "            target_type=target_case['target_type'],\n",
    "            test_start_date='2025-01-01'  # 2025 고정\n",
    "        )\n",
    "        \n",
    "        if split_method['method'] == 'tvt':\n",
    "            # TVT 방식\n",
    "            X_train = result['train']['X_robust']\n",
    "            X_val = result['val']['X_robust']\n",
    "            X_test = result['test']['X_robust']\n",
    "            test_returns = result['test']['y']['next_log_return'].values  \n",
    "            test_dates = result['test']['dates'].values \n",
    "            \n",
    "            if len(target_case['outputs']) == 1:\n",
    "                y_train = result['train']['y'][target_case['outputs'][0]].values\n",
    "                y_val = result['val']['y'][target_case['outputs'][0]].values\n",
    "                y_test = result['test']['y'][target_case['outputs'][0]].values\n",
    "                ml_models = ML_MODELS_CLASSIFICATION\n",
    "                dl_models = DL_MODELS_CLASSIFICATION\n",
    "                task = 'classification'\n",
    "            \n",
    "            evaluator = ModelEvaluator()\n",
    "            train_all_models(\n",
    "                X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                test_returns, test_dates, evaluator,\n",
    "                ml_models=ml_models, dl_models=dl_models, task=task\n",
    "            )\n",
    "            \n",
    "\n",
    "            summary_df = evaluator.get_summary_dataframe()\n",
    "            predictions_dict = evaluator.get_predictions_dict()  \n",
    "            \n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = summary_df\n",
    "\n",
    "            save_summary_csv(\n",
    "                summary_df, predictions_dict,  \n",
    "                target_case['name'], split_method['name'], task\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            fold_results = []\n",
    "            fold_predictions = []\n",
    "\n",
    "            for fold_idx, fold in enumerate(result, start=1):\n",
    "                fold_type = fold.get('fold_type', 'walk_forward')  # ← fold_type 추출\n",
    "\n",
    "                print(f\"\\n  Processing Fold {fold_idx}/{len(result)} ({fold_type})\")  # ← 표시\n",
    "\n",
    "                X_train = fold['train']['X_robust']\n",
    "                X_val = fold['val']['X_robust']\n",
    "                X_test = fold['test']['X_robust']\n",
    "                test_returns = fold['test']['y']['next_log_return'].values  \n",
    "                test_dates = fold['test']['dates'].values  \n",
    "\n",
    "                if len(target_case['outputs']) == 1:\n",
    "                    y_train = fold['train']['y'][target_case['outputs'][0]].values  \n",
    "                    y_val = fold['val']['y'][target_case['outputs'][0]].values\n",
    "                    y_test = fold['test']['y'][target_case['outputs'][0]].values\n",
    "                    ml_models = ML_MODELS_CLASSIFICATION\n",
    "                    dl_models = DL_MODELS_CLASSIFICATION\n",
    "                    task = 'classification'\n",
    "\n",
    "                evaluator = ModelEvaluator()\n",
    "                train_all_models(\n",
    "                    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator,\n",
    "                    ml_models=ml_models, dl_models=dl_models, task=task\n",
    "                )\n",
    "\n",
    "                fold_summary = evaluator.get_summary_dataframe()\n",
    "                fold_pred_dict = evaluator.get_predictions_dict()  \n",
    "\n",
    "                fold_results.append((fold_summary, fold_type))\n",
    "                fold_predictions.append((fold_pred_dict, fold_type))\n",
    "\n",
    "                print(f\"  Fold {fold_idx} ({fold_type}) completed\")\n",
    "\n",
    "            print(f\"\\n  Aggregating {len(fold_results)} folds...\")\n",
    "            detailed_df, avg_df = save_walk_forward_results(\n",
    "                fold_results, fold_predictions,\n",
    "                target_case['name'], task\n",
    "            )\n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = avg_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
