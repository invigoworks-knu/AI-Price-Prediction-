{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a552629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.5\n",
      "1.7.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import sklearn\n",
    "\n",
    "print(xgboost.__version__)   # 3.0.5\n",
    "print(sklearn.__version__)   # 1.0.x 이하이면 호환 문제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a3426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETHEREUM PRICE PREDICTION - NO LEAKAGE PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Step 1: Loading raw datasets...\n",
      "Macro: (3199, 51), 2017-01-01 00:00:00 to 2025-10-05 00:00:00\n",
      "\n",
      "Step 2: Creating sentiment features...\n",
      "\n",
      "Step 3: Unifying date range...\n",
      "Unified range: 2020-01-01 00:00:00 to 2025-10-04 00:00:00\n",
      "\n",
      "Step 4: Split raw data FIRST (70-15-15)...\n",
      "Train raw: (1472, 51), 2020-01-01 00:00:00 to 2024-01-11 00:00:00\n",
      "Val raw: (315, 51), 2024-01-12 00:00:00 to 2024-11-21 00:00:00\n",
      "Test raw: (316, 51), 2024-11-22 00:00:00 to 2025-10-03 00:00:00\n",
      "\n",
      "Step 5: Processing TRAIN set independently...\n",
      "Train with features: (1472, 265)\n",
      "\n",
      "Step 6: Processing VAL set (using train+val for rolling windows)...\n",
      "Val with features: (315, 265)\n",
      "\n",
      "Step 7: Processing TEST set (using train+val+test for rolling windows)...\n",
      "Test with features: (323, 265)\n",
      "\n",
      "Step 8: Removing NaN rows from each set...\n",
      "After NaN removal:\n",
      "Train: (1411, 265)\n",
      "Val: (314, 265)\n",
      "Test: (322, 265)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas_ta as ta\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def standardize_date_column(df):\n",
    "    date_cols = [col for col in df.columns if col.lower() == 'date']\n",
    "    if not date_cols:\n",
    "        return df\n",
    "\n",
    "    date_col = date_cols[0]\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()\n",
    "\n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(file_name, base_dir='./macro_data'):\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = standardize_date_column(df)\n",
    "    return df\n",
    "\n",
    "def create_sentiment_features_with_lags(news_df):\n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x))\n",
    "    ).reset_index()\n",
    "\n",
    "    sentiment_agg['sentiment_std'] = sentiment_agg['sentiment_std'].fillna(0)\n",
    "\n",
    "    for lag in [1, 3, 5, 7]:\n",
    "        sentiment_agg[f'sentiment_mean_lag{lag}'] = sentiment_agg['sentiment_mean'].shift(lag)\n",
    "        sentiment_agg[f'positive_ratio_lag{lag}'] = sentiment_agg['positive_ratio'].shift(lag)\n",
    "        sentiment_agg[f'news_count_lag{lag}'] = sentiment_agg['news_count'].shift(lag)\n",
    "\n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "def calculate_technical_indicators(df, price_col='ETH_Close', volume_col='ETH_Volume', high_col=None, low_col=None):\n",
    "    \"\"\"\n",
    "    pandas_ta 기반 기술적 지표 계산 (논문 검증됨)\n",
    "    참고: \"Cryptocurrency Price Forecasting Using XGBoost Regressor\" (2024)\n",
    "    \"\"\"\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    df['returns'] = df[price_col].pct_change()\n",
    "    df['log_returns'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    \n",
    "    \n",
    "    # RSI (Momentum) - 논문에서 중요도 높음\n",
    "    for length in [14, 30]:\n",
    "        rsi_series = ta.rsi(df[price_col], length=length)\n",
    "        df[f'rsi_{length}'] = rsi_series.shift(1)  \n",
    "    \n",
    "    # MACD (Momentum & Trend)\n",
    "    macd_df = ta.macd(df[price_col], fast=12, slow=26, signal=9)\n",
    "    if macd_df is not None and not macd_df.empty:\n",
    "        for col in macd_df.columns:\n",
    "            df[col] = macd_df[col].shift(1)  \n",
    "\n",
    "    bb_df = ta.bbands(df[price_col], length=20, std=2)\n",
    "    if bb_df is not None and not bb_df.empty:\n",
    "        bb_cols = list(bb_df.columns)\n",
    "        for col in bb_cols:\n",
    "            df[col] = bb_df[col].shift(1)  \n",
    "        \n",
    "        # BB 파생 지표 계산 (논문 권장)\n",
    "        bb_middle = bb_df.iloc[:, 1] if len(bb_df.columns) >= 2 else bb_df.iloc[:, 0]\n",
    "        bb_upper = bb_df.iloc[:, 0]\n",
    "        bb_lower = bb_df.iloc[:, 2] if len(bb_df.columns) >= 3 else bb_df.iloc[:, 1]\n",
    "        \n",
    "        df['bb_width_20'] = ((bb_upper - bb_lower) / bb_middle).shift(1)\n",
    "        df['bb_position_20'] = ((df[price_col].shift(1) - bb_lower.shift(1)) / \n",
    "                                (bb_upper.shift(1) - bb_lower.shift(1)))\n",
    "    \n",
    "    # ADX (Trend Strength)\n",
    "    if high_col and low_col:\n",
    "        adx_df = ta.adx(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if adx_df is not None and not adx_df.empty:\n",
    "            for col in adx_df.columns:\n",
    "                df[col] = adx_df[col].shift(1)  \n",
    "    \n",
    "    # ATR (Volatility) - 논문에서 중요도 높음\n",
    "    if high_col and low_col:\n",
    "        atr_series = ta.atr(df[high_col], df[low_col], df[price_col], length=14)\n",
    "        if atr_series is not None:\n",
    "            df['atr_14'] = atr_series.shift(1) \n",
    "    \n",
    "    # MFI (Volume)\n",
    "    if high_col and low_col and volume_col:\n",
    "        mfi_series = ta.mfi(df[high_col], df[low_col], df[price_col], df[volume_col], length=14)\n",
    "        if mfi_series is not None:\n",
    "            df['mfi_14'] = mfi_series.shift(1)  \n",
    "    \n",
    "    # CCI (Momentum)\n",
    "    if high_col and low_col:\n",
    "        cci_series = ta.cci(df[high_col], df[low_col], df[price_col], length=20)\n",
    "        if cci_series is not None:\n",
    "            df['cci_20'] = cci_series.shift(1)  \n",
    "    \n",
    "    # OBV (Volume)\n",
    "    obv_series = ta.obv(df[price_col], df[volume_col])\n",
    "    if obv_series is not None:\n",
    "        df['obv'] = obv_series.shift(1) \n",
    "    \n",
    "    # VWAP\n",
    "    if high_col and low_col:\n",
    "        vwap_series = ta.vwap(df[high_col], df[low_col], df[price_col], df[volume_col])\n",
    "        if vwap_series is not None:\n",
    "            df['vwap'] = vwap_series.shift(1)\n",
    "    \n",
    "    # SMA/EMA (Trend) - 다양한 기간\n",
    "    for window in [7, 14, 30, 60]:\n",
    "        sma = ta.sma(df[price_col], length=window)\n",
    "        ema = ta.ema(df[price_col], length=window)\n",
    "        if sma is not None:\n",
    "            df[f'sma_{window}'] = sma.shift(1)  \n",
    "        if ema is not None:\n",
    "            df[f'ema_{window}'] = ema.shift(1) \n",
    "        \n",
    "        # Volatility & Volume indicators\n",
    "        df[f'volatility_{window}'] = df['returns'].shift(1).rolling(window=window).std()\n",
    "        df[f'volume_sma_{window}'] = df[volume_col].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_sma_{window}'] = df['returns'].shift(1).rolling(window=window).mean()\n",
    "        df[f'returns_ema_{window}'] = df['returns'].shift(1).ewm(span=window, adjust=False).mean()\n",
    "        df[f'cumulative_returns_{window}'] = (1 + df['returns'].shift(1)).rolling(window=window).apply(lambda x: x.prod(), raw=True) - 1\n",
    "    \n",
    "    # Momentum & ROC\n",
    "    for window in [10, 20]:\n",
    "        df[f'momentum_{window}'] = df[price_col].shift(1) - df[price_col].shift(window + 1)\n",
    "        df[f'roc_{window}'] = ((df[price_col].shift(1) - df[price_col].shift(window + 1)) / \n",
    "                               df[price_col].shift(window + 1)) * 100\n",
    "    \n",
    "    # Stochastic & Williams %R\n",
    "    if high_col in df.columns and low_col in df.columns:\n",
    "        high_roll = df[high_col].shift(1).rolling(window=14).max()\n",
    "        low_roll = df[low_col].shift(1).rolling(window=14).min()\n",
    "        df['stochastic_14'] = 100 * (df[price_col].shift(1) - low_roll) / (high_roll - low_roll)\n",
    "        df['williams_r_14'] = -100 * (high_roll - df[price_col].shift(1)) / (high_roll - low_roll)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 3, 7, 14]:\n",
    "        df[f'price_lag_{lag}'] = df[price_col].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df[volume_col].shift(lag)\n",
    "        df[f'returns_lag_{lag}'] = df['returns'].shift(lag)\n",
    "    \n",
    "    # Regime indicators\n",
    "    df['volatility_30'] = df['returns'].shift(1).rolling(30).std()\n",
    "    df['volatility_regime'] = (df['volatility_30'] > df['volatility_30'].shift(1).rolling(60).mean()).astype(int)\n",
    "    df['price_trend'] = (df['sma_14'] > df['sma_60']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_external_features(base_df, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df):\n",
    "    merged_df = base_df.copy()\n",
    "\n",
    "    eth_onchain_df = standardize_date_column(eth_onchain_df)\n",
    "    onchain_col_orig = [c for c in eth_onchain_df.columns if c != 'date']\n",
    "    for col in onchain_col_orig:\n",
    "        new_col = f'onchain_{col}'\n",
    "        eth_onchain_df.rename(columns={col: new_col}, inplace=True)\n",
    "        for lag in [1,2,3]:\n",
    "            eth_onchain_df[f'{new_col}_lag{lag}'] = eth_onchain_df[new_col].shift(lag)\n",
    "\n",
    "    merged_df = merged_df.merge(eth_onchain_df, on='date', how='left')\n",
    "\n",
    "    for df, prefix, lag_list in data_with_lags:\n",
    "        df_renamed = df.copy()\n",
    "        df_renamed = standardize_date_column(df_renamed)\n",
    "\n",
    "        orig_cols = [c for c in df_renamed.columns if c != 'date']\n",
    "        for col in orig_cols:\n",
    "            df_renamed.rename(columns={col: f'{prefix}_{col}'}, inplace=True)\n",
    "\n",
    "        renamed_cols = [c for c in df_renamed.columns if c != 'date']\n",
    "        for col in renamed_cols:\n",
    "            for lag in lag_list:\n",
    "                df_renamed[f'{col}_lag{lag}'] = df_renamed[col].shift(lag)\n",
    "\n",
    "        merged_df = merged_df.merge(df_renamed, on='date', how='left')\n",
    "\n",
    "    merged_df = merged_df.merge(sentiment_features, on='date', how='left')\n",
    "\n",
    "    google_trends_df = standardize_date_column(google_trends_df)\n",
    "    trend_cols = [c for c in google_trends_df.columns if c != 'date']\n",
    "    for col in trend_cols:\n",
    "        google_trends_df[f'{col}_lag7'] = google_trends_df[col].shift(7)\n",
    "        google_trends_df[f'{col}_lag14'] = google_trends_df[col].shift(14)\n",
    "    merged_df = merged_df.merge(google_trends_df.add_prefix('trends_').rename(columns={'trends_date': 'date'}),\n",
    "                              on='date', how='left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ETHEREUM PRICE PREDICTION - NO LEAKAGE PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nStep 1: Loading raw datasets...\")\n",
    "macro_df = load_and_standardize_data('macro_crypto_data.csv')\n",
    "news_df = load_and_standardize_data('news_data.csv')\n",
    "eth_onchain_df = load_and_standardize_data('eth_onchain.csv')\n",
    "fear_greed_df = load_and_standardize_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_and_standardize_data('usdt_eth_mcap.csv')\n",
    "usdt_total_mcap_df = load_and_standardize_data('usdt_total_mcap.csv')\n",
    "aave_tvl_df = load_and_standardize_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_and_standardize_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_and_standardize_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_and_standardize_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_and_standardize_data('eth_funding_rate.csv')\n",
    "sp500_df = load_and_standardize_data('SP500.csv')\n",
    "vix_df = load_and_standardize_data('VIX.csv')\n",
    "gold_df = load_and_standardize_data('GOLD.csv')\n",
    "dxy_df = load_and_standardize_data('DXY.csv')\n",
    "google_trends_df = load_and_standardize_data('ethereum_google_trends_weekly_2017_2025_scaled.csv')\n",
    "\n",
    "print(f\"Macro: {macro_df.shape}, {macro_df['date'].min()} to {macro_df['date'].max()}\")\n",
    "\n",
    "print(\"\\nStep 2: Creating sentiment features...\")\n",
    "sentiment_features = create_sentiment_features_with_lags(news_df)\n",
    "\n",
    "print(\"\\nStep 3: Unifying date range...\")\n",
    "common_start = macro_df['date'].min()\n",
    "common_end = macro_df['date'].max()\n",
    "for df in [news_df, eth_onchain_df, fear_greed_df]:\n",
    "    common_start = max(common_start, df['date'].min())\n",
    "    common_end = min(common_end, df['date'].max())\n",
    "\n",
    "macro_df = macro_df[(macro_df['date'] >= common_start) & (macro_df['date'] <= common_end)].reset_index(drop=True)\n",
    "print(f\"Unified range: {common_start} to {common_end}\")\n",
    "\n",
    "print(\"\\nStep 4: Split raw data FIRST (70-15-15)...\")\n",
    "train_size = int(len(macro_df) * 0.7)\n",
    "val_size = int(len(macro_df) * 0.15)\n",
    "\n",
    "train_raw = macro_df.iloc[:train_size].copy()\n",
    "val_raw = macro_df.iloc[train_size:train_size+val_size].copy()\n",
    "test_raw = macro_df.iloc[train_size+val_size:].copy()\n",
    "\n",
    "print(f\"Train raw: {train_raw.shape}, {train_raw['date'].min()} to {train_raw['date'].max()}\")\n",
    "print(f\"Val raw: {val_raw.shape}, {val_raw['date'].min()} to {val_raw['date'].max()}\")\n",
    "print(f\"Test raw: {test_raw.shape}, {test_raw['date'].min()} to {test_raw['date'].max()}\")\n",
    "\n",
    "data_with_lags = [\n",
    "    (fear_greed_df, 'fg', [1]),\n",
    "    (usdt_eth_mcap_df, 'usdt_eth', [1]),\n",
    "    (usdt_total_mcap_df, 'usdt_total', [1]),\n",
    "    (aave_tvl_df, 'aave', [1, 3, 7]),\n",
    "    (lido_tvl_df, 'lido', [1, 3, 7]),\n",
    "    (makerdao_tvl_df, 'maker', [1, 3, 7]),\n",
    "    (eth_chain_tvl_df, 'chain_tvl', [1, 3, 7]),\n",
    "    (eth_funding_df, 'funding', [1]),\n",
    "    (sp500_df, 'sp500', [1]),\n",
    "    (vix_df, 'vix', [1]),\n",
    "    (gold_df, 'gold', [1]),\n",
    "    (dxy_df, 'dxy', [1])\n",
    "]\n",
    "\n",
    "print(\"\\nStep 5: Processing TRAIN set independently...\")\n",
    "eth_cols = [col for col in train_raw.columns if col.startswith('ETH_')]\n",
    "train_eth = train_raw[['date'] + eth_cols].copy()\n",
    "has_high_low = 'ETH_High' in train_eth.columns and 'ETH_Low' in train_eth.columns\n",
    "train_eth = calculate_technical_indicators(train_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                          'ETH_High' if has_high_low else None,\n",
    "                                          'ETH_Low' if has_high_low else None)\n",
    "\n",
    "btc_cols = [col for col in train_raw.columns if col.startswith('BTC_')]\n",
    "if btc_cols:\n",
    "    train_btc = train_raw[['date'] + btc_cols].copy()\n",
    "    has_btc_hl = 'BTC_High' in train_btc.columns and 'BTC_Low' in train_btc.columns\n",
    "    train_btc = calculate_technical_indicators(train_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                              'BTC_High' if has_btc_hl else None,\n",
    "                                              'BTC_Low' if has_btc_hl else None)\n",
    "    train_btc = train_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = train_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = train_btc['BTC_BTC_Close'].shift(1)\n",
    "    train_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    train_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    train_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    train_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    train_eth = train_eth.merge(train_btc[['date'] + [col for col in train_btc.columns if col != 'date']],\n",
    "                                on='date', how='left')\n",
    "\n",
    "altcoins = ['BNB', 'ADA']\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in train_raw.columns:\n",
    "        coin_shifted = train_raw[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = train_eth['ETH_Close'].shift(1)\n",
    "        train_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        train_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "train_df = merge_external_features(train_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "train_df['target_next_log_return'] = np.log(train_df['ETH_Close'] / train_df['ETH_Close'].shift(1)).shift(-1)\n",
    "train_df['target_direction'] = (train_df['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "print(f\"Train with features: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 6: Processing VAL set (using train+val for rolling windows)...\")\n",
    "combined_for_val = pd.concat([train_raw, val_raw]).reset_index(drop=True)\n",
    "val_eth = combined_for_val[['date'] + eth_cols].copy()\n",
    "val_eth = calculate_technical_indicators(val_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                        'ETH_High' if has_high_low else None,\n",
    "                                        'ETH_Low' if has_high_low else None)\n",
    "\n",
    "if btc_cols:\n",
    "    val_btc = combined_for_val[['date'] + btc_cols].copy()\n",
    "    val_btc = calculate_technical_indicators(val_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                            'BTC_High' if has_btc_hl else None,\n",
    "                                            'BTC_Low' if has_btc_hl else None)\n",
    "    val_btc = val_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = val_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = val_btc['BTC_BTC_Close'].shift(1)\n",
    "    val_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    val_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    val_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    val_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    val_eth = val_eth.merge(val_btc[['date'] + [col for col in val_btc.columns if col != 'date']],\n",
    "                           on='date', how='left')\n",
    "\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in combined_for_val.columns:\n",
    "        coin_shifted = combined_for_val[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = val_eth['ETH_Close'].shift(1)\n",
    "        val_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        val_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "val_df_full = merge_external_features(val_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "val_df_full['target_next_log_return'] = np.log(val_df_full['ETH_Close'] / val_df_full['ETH_Close'].shift(1)).shift(-1)\n",
    "val_df_full['target_direction'] = (val_df_full['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "val_df = val_df_full.iloc[len(train_raw):].reset_index(drop=True)\n",
    "print(f\"Val with features: {val_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 7: Processing TEST set (using train+val+test for rolling windows)...\")\n",
    "combined_for_test = pd.concat([train_raw, val_raw, test_raw]).reset_index(drop=True)\n",
    "test_eth = combined_for_test[['date'] + eth_cols].copy()\n",
    "test_eth = calculate_technical_indicators(test_eth, 'ETH_Close', 'ETH_Volume',\n",
    "                                         'ETH_High' if has_high_low else None,\n",
    "                                         'ETH_Low' if has_high_low else None)\n",
    "\n",
    "if btc_cols:\n",
    "    test_btc = combined_for_test[['date'] + btc_cols].copy()\n",
    "    test_btc = calculate_technical_indicators(test_btc, 'BTC_Close', 'BTC_Volume',\n",
    "                                             'BTC_High' if has_btc_hl else None,\n",
    "                                             'BTC_Low' if has_btc_hl else None)\n",
    "    test_btc = test_btc.add_prefix('BTC_').rename(columns={'BTC_date': 'date'})\n",
    "\n",
    "    eth_shifted = test_eth['ETH_Close'].shift(1)\n",
    "    btc_shifted = test_btc['BTC_BTC_Close'].shift(1)\n",
    "    test_btc['btc_eth_correlation'] = eth_shifted.rolling(30).corr(btc_shifted)\n",
    "    test_btc['btc_dominance'] = btc_shifted / (btc_shifted + eth_shifted)\n",
    "    test_btc['eth_btc_ratio'] = eth_shifted / btc_shifted\n",
    "    test_btc['eth_btc_ratio_sma_30'] = (eth_shifted / btc_shifted).rolling(30).mean()\n",
    "\n",
    "    test_eth = test_eth.merge(test_btc[['date'] + [col for col in test_btc.columns if col != 'date']],\n",
    "                             on='date', how='left')\n",
    "\n",
    "for coin in altcoins:\n",
    "    if f'{coin}_Close' in combined_for_test.columns:\n",
    "        coin_shifted = combined_for_test[f'{coin}_Close'].shift(1)\n",
    "        eth_shifted = test_eth['ETH_Close'].shift(1)\n",
    "        test_eth[f'{coin.lower()}_eth_ratio'] = coin_shifted / eth_shifted\n",
    "        test_eth[f'{coin.lower()}_eth_correlation'] = eth_shifted.rolling(30).corr(coin_shifted)\n",
    "\n",
    "test_df_full = merge_external_features(test_eth, data_with_lags, sentiment_features, google_trends_df, eth_onchain_df)\n",
    "test_df_full['target_next_log_return'] = np.log(test_df_full['ETH_Close'] / test_df_full['ETH_Close'].shift(1)).shift(-1)\n",
    "test_df_full['target_direction'] = (test_df_full['target_next_log_return'] > 0).astype(int)\n",
    "\n",
    "test_df = test_df_full.iloc[len(train_raw)+len(val_raw):].reset_index(drop=True)\n",
    "print(f\"Test with features: {test_df.shape}\")\n",
    "\n",
    "print(\"\\nStep 8: Removing NaN rows from each set...\")\n",
    "max_lag = 60\n",
    "train_df = train_df.iloc[max_lag:-1].reset_index(drop=True)\n",
    "val_df = val_df.iloc[:-1].reset_index(drop=True)\n",
    "test_df = test_df.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "print(f\"After NaN removal:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] Unsupervised filtering (variance/correlation) ...\n",
      "\n",
      "[Phase 2] Nested cross-validation: supervised feature selection & tuning ...\n",
      "\n",
      "Nested CV finished.\n",
      "All folds selected features: [['BTC_DMP_14', 'BTC_bb_position_20', 'volume_sma_7', 'BTC_returns', 'positive_ratio', 'volume_lag_7', 'dxy_DXY', 'returns', 'BTC_returns_lag_7', 'bb_width_20', 'DMN_14', 'BTC_returns_lag_14', 'returns_lag_3', 'returns_ema_7', 'BTC_volume_lag_3', 'BTC_returns_lag_1', 'returns_sma_60', 'obv', 'returns_sma_7', 'stochastic_14', 'BTC_BTC_Open'], ['BTC_returns_ema_14', 'BTC_BTC_Volume', 'volume_lag_1', 'returns_lag_7', 'dxy_DXY', 'BTC_MACD_12_26_9', 'BTC_volume_sma_7', 'BTC_obv', 'BTC_bb_position_20', 'funding_fundingRate', 'volume_sma_7', 'returns', 'negative_ratio', 'bnb_eth_ratio', 'BTC_returns', 'BTC_rsi_14', 'BTC_bb_width_20', 'BTC_returns_ema_7', 'BTC_ADX_14', 'BTC_mfi_14', 'bb_position_20', 'sentiment_mean_lag1', 'vix_VIX', 'BTC_DMN_14', 'BTC_roc_20', 'BTC_atr_14', 'bnb_eth_correlation', 'ada_eth_ratio', 'ADX_14', 'BTC_rsi_30', 'BTC_volume_lag_1', 'lido_lido_eth_tvl', 'bb_width_20', 'news_count_lag5', 'returns_ema_30', 'BTC_momentum_20', 'returns_sma_7', 'BBL_20', 'BTC_volatility_14', 'fg_fear_greed', 'volatility_14', 'BTC_returns_lag_3', 'usdt_total_totalUnreleased', 'volume_lag_3', 'sp500_SP500', 'volume_lag_7'], ['positive_ratio_lag1', 'bb_width_20', 'BTC_returns_ema_7', 'bb_position_20', 'BTC_obv', 'usdt_total_totalUnreleased', 'BTC_bb_position_20', 'btc_eth_correlation', 'sp500_SP500', 'BTC_returns_lag_1', 'volume_sma_7', 'dxy_DXY', 'BTC_rsi_14', 'volume_sma_60', 'BTC_atr_14', 'returns_sma_60', 'BTC_MACD_12_26_9', 'volatility_30', 'returns', 'BTC_returns', 'aave_aave_eth_tvl', 'BTC_returns_sma_7', 'returns_ema_7', 'sentiment_mean', 'news_count', 'BTC_volatility_30', 'BTC_stochastic_14', 'BBL_20', 'returns_sma_7', 'returns_lag_3', 'BTC_returns_sma_60', 'volume_lag_3'], ['bnb_eth_ratio', 'positive_ratio_lag7', 'BTC_returns_lag_1', 'returns', 'volume_sma_60', 'BTC_obv', 'BTC_rsi_14', 'BTC_bb_position_20', 'BTC_cci_20', 'BTC_atr_14', 'DMP_14', 'positive_ratio', 'BTC_ADX_14', 'BTC_returns_sma_30', 'returns_ema_14', 'BTC_returns_sma_14', 'positive_ratio_lag1', 'bb_width_20', 'BTC_returns_ema_30', 'BTC_returns_sma_7', 'returns_ema_7', 'rsi_14', 'mfi_14', 'BTC_returns_ema_7', 'BTC_volume_sma_7', 'BTC_BTC_Volume', 'BTC_DMN_14', 'BTC_stochastic_14', 'momentum_20', 'returns_lag_7', 'bb_position_20', 'volume_lag_1', 'BTC_roc_10', 'BTC_volatility_7', 'sentiment_mean_lag7', 'BTC_MACDH_12_26_9', 'sentiment_mean', 'BBL_20', 'BTC_volume_lag_1', 'BTC_mfi_14', 'aave_aave_eth_tvl', 'btc_eth_correlation', 'sp500_SP500', 'sentiment_std', 'sentiment_mean_lag3', 'negative_ratio', 'sentiment_mean_lag1', 'volume_lag_7', 'BTC_MACD_12_26_9', 'ada_eth_ratio'], ['volume_sma_60', 'BTC_obv', 'bnb_eth_ratio', 'volume_sma_7', 'BTC_atr_14', 'returns', 'volume_lag_1', 'BTC_volatility_7', 'BTC_BTC_Volume', 'BTC_BTC_Open', 'BTC_cci_20', 'returns_ema_14', 'BTC_volume_sma_7', 'bb_width_20', 'BTC_bb_position_20', 'BTC_returns_ema_7', 'BTC_volatility_14', 'BTC_volume_lag_1', 'BTC_returns_sma_30', 'usdt_eth_totalCirculating', 'returns_ema_7', 'volume_lag_14', 'mfi_14', 'BTC_MACDH_12_26_9', 'volume_lag_3', 'BTC_rsi_14', 'gold_GOLD', 'returns_lag_7', 'positive_ratio', 'positive_ratio_lag1', 'dxy_DXY', 'BTC_volume_lag_14', 'news_count', 'obv', 'volume_lag_7', 'BTC_volume_lag_3', 'rsi_14', 'BBL_20', 'BTC_ADX_14', 'sentiment_mean_lag7', 'BTC_returns_sma_14', 'BTC_returns_ema_30', 'stochastic_14', 'lido_lido_eth_tvl', 'volatility_7', 'BTC_returns_sma_60', 'bb_position_20', 'ada_eth_correlation', 'usdt_total_totalUnreleased']]\n",
      "\n",
      "Best params from each fold: [{'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}]\n",
      "\n",
      "Consensus selected features (appearing in majority folds):\n",
      "['BTC_bb_position_20', 'volume_sma_7', 'BTC_returns', 'positive_ratio', 'volume_lag_7', 'dxy_DXY', 'returns', 'bb_width_20', 'returns_ema_7', 'BTC_returns_lag_1', 'returns_sma_7', 'BTC_BTC_Volume', 'volume_lag_1', 'returns_lag_7', 'BTC_MACD_12_26_9', 'BTC_volume_sma_7', 'BTC_obv', 'bnb_eth_ratio', 'BTC_rsi_14', 'BTC_returns_ema_7', 'BTC_ADX_14', 'bb_position_20', 'BTC_atr_14', 'BTC_volume_lag_1', 'BBL_20', 'usdt_total_totalUnreleased', 'volume_lag_3', 'sp500_SP500', 'positive_ratio_lag1', 'volume_sma_60']\n",
      "\n",
      "[Step 3] Scaling selected features only (no leakage)...\n",
      "\n",
      "Final feature selection complete. Model ready: 30 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''\n",
    "최신 논문(특히 2025년도 \"Optimizing Forecast Accuracy in Cryptocurrency Markets: \n",
    "Evaluating Feature Selection Techniques for Technical Indicators\") \n",
    "및 여러 비교 논문들은 supervised+unsupervised 방법을 조합하되 \n",
    "반드시 nested cross-validation과 시계열 보존(roll-forward 방식)으로 \n",
    "데이터 누수를 막는 것을 기준으로 하고 있음. 따라서, 데이터 누수를 막기 위해서\n",
    "\n",
    "'''\n",
    "# [0] 데이터 분할 및 전처리(기술적지표, 외부지표 등 생성 단계는 기존 코드 동일)\n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume', 'target_next_log_return', 'target_direction']\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# [1] Unsupervised 필터 (variance, correlation) - 전체 dataset/validation/test에는 사용 가능\n",
    "print(\"\\n[Phase 1] Unsupervised filtering (variance/correlation) ...\")\n",
    "to_drop = [c for c in feature_cols if train_df[c].isnull().sum()>len(train_df)*0.5 or train_df[c].nunique()<=1]\n",
    "feature_cols = [c for c in feature_cols if c not in to_drop]\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "for col in feature_cols:\n",
    "    train_df[col] = train_df[col].interpolate().fillna(train_df[col].median())\n",
    "    val_df[col] = val_df[col].interpolate().fillna(train_df[col].median())\n",
    "    test_df[col] = test_df[col].interpolate().fillna(train_df[col].median())\n",
    "corr_matrix = train_df[feature_cols].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column]>0.95)]\n",
    "feature_cols = [c for c in feature_cols if c not in to_drop_corr]\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "\n",
    "# [2] Nested CV 기반 supervised feature selection + 모델 튜닝\n",
    "print(\"\\n[Phase 2] Nested cross-validation: supervised feature selection & tuning ...\")\n",
    "\n",
    "X = train_df[feature_cols].values\n",
    "y = train_df['target_next_log_return'].values\n",
    "\n",
    "tscv_outer = TimeSeriesSplit(n_splits=5)\n",
    "final_selected_features = []\n",
    "best_param_list = []\n",
    "\n",
    "for train_index, valid_index in tscv_outer.split(X):\n",
    "    # Split indices for fold\n",
    "    X_train_cv, X_valid_cv = X[train_index], X[valid_index]\n",
    "    y_train_cv, y_valid_cv = y[train_index], y[valid_index]\n",
    "\n",
    "    # [A] Supervised feature selection (MI + RFECV) - 내부CV만 사용\n",
    "    mi_scores = mutual_info_regression(X_train_cv, y_train_cv, random_state=42, n_neighbors=5)\n",
    "    mi_rank_idx = np.argsort(mi_scores)[::-1][:60]  # top 60개\n",
    "    mi_features_idx = [feature_cols[i] for i in mi_rank_idx]\n",
    "    X_train_mi = X_train_cv[:, mi_rank_idx]\n",
    "\n",
    "    # RFECV with GridSearch for best model in fold\n",
    "    xgb = XGBRegressor(tree_method='gpu_hist', n_jobs=-1, random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    rfecv = RFECV(estimator=xgb, step=1, cv=TimeSeriesSplit(3), scoring='neg_mean_squared_error', min_features_to_select=20)\n",
    "    \n",
    "    # RFECV 실행\n",
    "    rfecv.fit(X_train_mi, y_train_cv)\n",
    "    selected_idx = [i for i, s in enumerate(rfecv.support_) if s]\n",
    "    selected_features_this_fold = [mi_features_idx[i] for i in selected_idx]\n",
    "    final_selected_features.append(selected_features_this_fold)\n",
    "\n",
    "    # 최적 파라미터\n",
    "    grid_search = GridSearchCV(xgb, param_grid, scoring='neg_mean_squared_error', cv=TimeSeriesSplit(3), n_jobs=-1)\n",
    "    grid_search.fit(X_train_mi[:,selected_idx], y_train_cv)\n",
    "    best_param_list.append(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nNested CV finished.\\nAll folds selected features: {final_selected_features}\")\n",
    "print(f\"\\nBest params from each fold: {best_param_list}\")\n",
    "\n",
    "# 교차검증에서 가장 많이 선택된 feature만 retain\n",
    "\n",
    "flat_features = [f for featlist in final_selected_features for f in featlist]\n",
    "feature_freq = Counter(flat_features)\n",
    "selected_final_features = [f for f, cnt in feature_freq.items() if cnt > (len(final_selected_features)//2)]\n",
    "print(f\"\\nConsensus selected features (appearing in majority folds):\\n{selected_final_features}\")\n",
    "\n",
    "print(\"\\n[Step 3] Scaling selected features only (no leakage)...\")\n",
    "scaler = StandardScaler()\n",
    "train_df[selected_final_features] = scaler.fit_transform(train_df[selected_final_features])\n",
    "val_df[selected_final_features] = scaler.transform(val_df[selected_final_features])\n",
    "test_df[selected_final_features] = scaler.transform(test_df[selected_final_features])\n",
    "\n",
    "print(f\"\\nFinal feature selection complete. Model ready: {len(selected_final_features)} features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2205bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# print(\"\\nStep 9: Advanced Feature Selection (논문 기반)...\")\n",
    "# print(\"Reference: 'Optimizing Forecast Accuracy in Cryptocurrency Markets' (2025)\")\n",
    "\n",
    "# exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "#                 'target_next_log_return', 'target_direction']\n",
    "# feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# # PHASE 1: 결측치 제거\n",
    "# print(\"\\n[Phase 1] Variance-based filtering...\")\n",
    "\n",
    "# cols_to_drop = []\n",
    "# for col in feature_cols:\n",
    "#     if train_df[col].isnull().sum() > len(train_df) * 0.5:\n",
    "#         cols_to_drop.append(col)\n",
    "\n",
    "# if cols_to_drop:\n",
    "#     print(f\"Dropping {len(cols_to_drop)} features with >50% missing\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in cols_to_drop]\n",
    "#     train_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# for col in feature_cols:\n",
    "#     train_df[col] = train_df[col].fillna(method='ffill').fillna(train_df[col].median())\n",
    "#     train_median = train_df[col].median()\n",
    "#     val_df[col] = val_df[col].fillna(method='ffill').fillna(train_median)\n",
    "#     test_df[col] = test_df[col].fillna(method='ffill').fillna(train_median)\n",
    "\n",
    "# print(f\"Features after Phase 1: {len(feature_cols)}\")\n",
    "\n",
    "# # PHASE 2: 상관관계 0.95 이상 중복 제거\n",
    "# print(\"\\n[Phase 2] Correlation-based redundancy removal...\")\n",
    "# corr_matrix = train_df[feature_cols].corr().abs()\n",
    "# upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "\n",
    "# if to_drop_corr:\n",
    "#     print(f\"Dropping {len(to_drop_corr)} highly correlated features (>0.95)\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in to_drop_corr]\n",
    "#     train_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=to_drop_corr, inplace=True, errors='ignore')\n",
    "\n",
    "# print(f\"Features after Phase 2: {len(feature_cols)}\")\n",
    "\n",
    "# # PHASE 3: Mutual Information (Top 50~60)\n",
    "# print(\"\\n[Phase 3] Mutual Information feature selection...\")\n",
    "# print(\"Reference: MI effectively captures non-linear relationships in crypto markets\")\n",
    "\n",
    "# X_train = train_df[feature_cols].values\n",
    "# y_train = train_df['target_next_log_return'].values\n",
    "# X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "# y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=5)\n",
    "# mi_scores_series = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=False)\n",
    "# n_mi_features = min(60, len(feature_cols))\n",
    "# top_mi_features = mi_scores_series.head(n_mi_features).index.tolist()\n",
    "# print(f\"Selected top {len(top_mi_features)} features by MI\")\n",
    "# print(f\"Top 10 MI scores:\\n{mi_scores_series.head(10)}\")\n",
    "\n",
    "# X_train_mi = train_df[top_mi_features].values\n",
    "# X_train_mi = np.nan_to_num(X_train_mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# # ====================하이퍼파라미터 자동 탐색 ====================\n",
    "# print(\"\\nStep 9.1: XGBoost Hyperparameter Optimization (Optuna, 논문 권장)\")\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#         'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1.0),\n",
    "#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 2.0),\n",
    "#         'tree_method': 'gpu_hist',\n",
    "#         'random_state': 42,\n",
    "#         'n_jobs': -1\n",
    "#     }\n",
    "    \n",
    "#     model = XGBRegressor(**params)\n",
    "#     score = cross_val_score(model, X_train_mi, y_train, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#     return -score.mean()\n",
    "\n",
    "\n",
    "# # Pruner 적용\n",
    "# study = optuna.create_study(\n",
    "#     direction='minimize',\n",
    "#     pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "# )\n",
    "# study.optimize(objective, n_trials=50)  \n",
    "\n",
    "# print(\"Best XGBoost params:\", study.best_trial.params)\n",
    "\n",
    "# best_xgb = XGBRegressor(**study.best_trial.params)\n",
    "\n",
    "# # PHASE 4: RFECV with 최적 하이퍼파라미터 적용\n",
    "# print(\"\\n[Phase 4] Recursive Feature Elimination with Cross-Validation... (Optuna 최적 파라미터 적용)\")\n",
    "\n",
    "# selector = RFECV(\n",
    "#     estimator=best_xgb,\n",
    "#     step=1,\n",
    "#     cv=5,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     min_features_to_select=20,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# print(\"Training RFECV... (this may take a few minutes with GPU & Optuna-tuned params)\")\n",
    "# selector.fit(X_train_mi, y_train)\n",
    "# selected_features = [top_mi_features[i] for i in range(len(top_mi_features)) if selector.support_[i]]\n",
    "# print(f\"\\nOptimal number of features: {len(selected_features)}\")\n",
    "# print(f\"Feature reduction: {100 * (1 - len(selected_features) / len(feature_cols)):.1f}%\")\n",
    "# print(f\"\\nSelected features:\\n{selected_features}\")\n",
    "\n",
    "# # PHASE 5: Feature Importance 분석\n",
    "# print(\"\\n[Phase 5] XGBoost Feature Importance analysis...\")\n",
    "# feature_importance = pd.Series(\n",
    "#     selector.estimator_.feature_importances_,\n",
    "#     index=selected_features\n",
    "# ).sort_values(ascending=False)\n",
    "# print(f\"\\nTop 15 most important features:\")\n",
    "# print(feature_importance.head(15))\n",
    "\n",
    "# # ====================  카테고리별 상세 분류 ====================\n",
    "\n",
    "# def categorize_feature(f):\n",
    "#     if any(x in f for x in ['rsi', 'macd', 'momentum', 'roc', 'cci', 'stochastic', 'williams']):\n",
    "#         return 'Momentum'\n",
    "#     if any(x in f for x in ['volatility', 'atr', 'bb_', 'bbands']):\n",
    "#         return 'Volatility'\n",
    "#     if any(x in f for x in ['volume', 'obv', 'mfi']):\n",
    "#         return 'Volume'\n",
    "#     if any(x in f for x in ['sma', 'ema', 'trend']):\n",
    "#         return 'Trend'\n",
    "#     if 'onchain_' in f:\n",
    "#         return 'On-chain'\n",
    "#     if any(x in f for x in ['sp500', 'vix', 'gold', 'dxy']):\n",
    "#         return 'Macro'\n",
    "#     if any(x in f for x in ['sentiment', 'news', 'positive_ratio', 'negative_ratio']):\n",
    "#         return 'Sentiment'\n",
    "#     if any(x in f for x in ['aave', 'lido', 'maker', 'chain_tvl', 'funding']):\n",
    "#         return 'External'\n",
    "#     return 'Other'\n",
    "\n",
    "# category_map = {f: categorize_feature(f) for f in selected_features}\n",
    "\n",
    "# category_counts = Counter(category_map.values())\n",
    "# print(\"\\nFeature category breakdown (detailed):\")\n",
    "# for cat, count in category_counts.items():\n",
    "#     print(f\"{cat}: {count}\")\n",
    "\n",
    "# print(\"\\nDetailed feature list by category:\")\n",
    "# for cat in sorted(set(category_map.values())):\n",
    "#     print(f\"\\n[{cat}]\")\n",
    "#     for f in [k for k, v in category_map.items() if v == cat]:\n",
    "#         print(f\"  - {f}\")\n",
    "\n",
    "# # ==================== Step 10: Scaling ====================\n",
    "# print(\"\\n[Step 10] Scaling selected features only...\")\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# train_df[selected_features] = scaler.fit_transform(train_df[selected_features])\n",
    "# val_df[selected_features] = scaler.transform(val_df[selected_features])\n",
    "# test_df[selected_features] = scaler.transform(test_df[selected_features])\n",
    "\n",
    "# print(f\"\\n{'='*70}\")\n",
    "# print(f\"FINAL PREPROCESSED DATASETS\")\n",
    "# print(f\"{'='*70}\")\n",
    "# print(f\"Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "# print(f\"Val: {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "# print(f\"Test: {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "# print(f\"Selected Features: {len(selected_features)}\")\n",
    "# print(f\"\\nFeature selection complete. Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5902d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# 퍼플렉시티 버전인데 일단 주석 ##########\n",
    "# print(\"\\nStep 9: Feature selection and missing value handling...\")\n",
    "# exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "#                 'target_next_log_return', 'target_direction']\n",
    "# feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# cols_to_drop = []\n",
    "# for col in feature_cols:\n",
    "#     if train_df[col].isnull().sum() > len(train_df) * 0.5:\n",
    "#         cols_to_drop.append(col)\n",
    "\n",
    "# if cols_to_drop:\n",
    "#     print(f\"Dropping {len(cols_to_drop)} features with >50% missing in train\")\n",
    "#     feature_cols = [c for c in feature_cols if c not in cols_to_drop]\n",
    "#     train_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     val_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "#     test_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# for col in feature_cols:\n",
    "#     train_df[col] = train_df[col].fillna(method='ffill').fillna(train_df[col].median())\n",
    "#     train_median = train_df[col].median()\n",
    "\n",
    "#     val_df[col] = val_df[col].fillna(method='ffill').fillna(train_median)\n",
    "#     test_df[col] = test_df[col].fillna(method='ffill').fillna(train_median)\n",
    "\n",
    "# print(\"\\nStep 10: Scaling with StandardScaler...\")\n",
    "# scaler = StandardScaler()\n",
    "# train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "# val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "# test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# print(f\"\\nFinal datasets:\")\n",
    "# print(f\"Train: {train_df.shape}, {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "# print(f\"Val: {val_df.shape}, {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "# print(f\"Test: {test_df.shape}, {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "# print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d29d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################    클로드 버전 #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0738b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 11: Feature Selection...\n",
      "Initial features: 208\n",
      "After variance filter: 208 features\n",
      "After correlation filter: 127 features\n",
      "\n",
      "Final selected features: 30\n",
      "\n",
      "Top 30 most important:\n",
      "  BTC_obv: 0.0551\n",
      "  returns_lag_3: 0.0467\n",
      "  returns: 0.0431\n",
      "  macd_signal_12_26_9: 0.0413\n",
      "  BTC_BTC_Volume: 0.0244\n",
      "  btc_dominance: 0.0237\n",
      "  volume_lag_3: 0.0222\n",
      "  lido_lido_eth_tvl: 0.0201\n",
      "  BTC_roc_20: 0.0170\n",
      "  BTC_returns: 0.0152\n",
      "  volume_sma_7: 0.0151\n",
      "  BTC_returns_lag_14: 0.0141\n",
      "  volume_lag_14: 0.0137\n",
      "  btc_eth_correlation: 0.0135\n",
      "  BTC_macd_12_26: 0.0131\n",
      "  BTC_mfi_14: 0.0119\n",
      "  dxy_DXY: 0.0116\n",
      "  returns_sma_60: 0.0111\n",
      "  vix_VIX: 0.0110\n",
      "  BTC_returns_ema_7: 0.0107\n",
      "  BTC_macd_hist_12_26_9: 0.0106\n",
      "  sentiment_mean: 0.0106\n",
      "  returns_ema_7: 0.0105\n",
      "  BTC_returns_lag_7: 0.0099\n",
      "  bnb_eth_ratio: 0.0098\n",
      "  BTC_returns_sma_14: 0.0097\n",
      "  funding_fundingRate_lag1: 0.0096\n",
      "  BTC_volatility_7: 0.0096\n",
      "  vwap: 0.0093\n",
      "  BTC_returns_lag_1: 0.0092\n",
      "\n",
      "Final dataset shapes:\n",
      "Train: (1411, 33)\n",
      "Val: (314, 33)\n",
      "Test: (322, 33)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStep 11: Feature Selection...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "                'target_next_log_return', 'target_direction']\n",
    "all_features = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Initial features: {len(all_features)}\")\n",
    "\n",
    "# Stage 1: Variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(train_df[all_features])\n",
    "features_after_variance = [feat for feat, selected in zip(all_features, selector.get_support()) if selected]\n",
    "print(f\"After variance filter: {len(features_after_variance)} features\")\n",
    "\n",
    "# Stage 2: Correlation filter\n",
    "corr_matrix = train_df[features_after_variance].corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]\n",
    "features_after_corr = [f for f in features_after_variance if f not in to_drop]\n",
    "print(f\"After correlation filter: {len(features_after_corr)} features\")\n",
    "\n",
    "# Stage 3: Tree-based importance\n",
    "X_train = train_df[features_after_corr].values\n",
    "y_train = train_df['target_next_log_return'].values\n",
    "mask = ~np.isnan(y_train)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'feature': features_after_corr,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "TOP_K = 30\n",
    "selected_features = importances.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nFinal selected features: {len(selected_features)}\")\n",
    "print(f\"\\nTop {TOP_K} most important:\")\n",
    "for idx, row in importances.head(TOP_K).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Apply selection to all datasets\n",
    "train_df = train_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "val_df = val_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "test_df = test_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "\n",
    "print(f\"\\nFinal dataset shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "\n",
    "targets = {\n",
    "    'log_return': 'target_next_log_return',\n",
    "    'direction': 'target_direction'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 퍼플렉시티 버전 ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9809b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Step 11: Advanced Feature Selection\n",
      "======================================================================\n",
      "\n",
      "Initial features: 208\n",
      "\n",
      "[Stage 1] Variance Threshold Filter...\n",
      "  Remaining: 208 features\n",
      "\n",
      "[Stage 2] Correlation Filter (threshold=0.95)...\n",
      "  Dropped 81 highly correlated features\n",
      "  Remaining: 127 features\n",
      "\n",
      "[Stage 3] Random Forest Feature Importance...\n",
      "\n",
      "[Stage 4] Top-K Selection (K=30)...\n",
      "  Cumulative importance at K=30: 52.82%\n",
      "  Features needed for 95% importance: 105\n",
      "\n",
      "======================================================================\n",
      "Selected 30 Features\n",
      "======================================================================\n",
      "\n",
      "Category Distribution:\n",
      "  BTC                 : 13 ( 43.3%)\n",
      "  Momentum            :  5 ( 16.7%)\n",
      "  Volume              :  5 ( 16.7%)\n",
      "  Market Structure    :  3 ( 10.0%)\n",
      "  Technical           :  1 (  3.3%)\n",
      "  DeFi/OnChain        :  1 (  3.3%)\n",
      "  Macro               :  1 (  3.3%)\n",
      "  Other               :  1 (  3.3%)\n",
      "\n",
      " Top 30 Most Important Features:\n",
      "Rank   Feature                             Importance   Category            \n",
      "---------------------------------------------------------------------------\n",
      "1      BTC_obv                             0.056564    BTC                 \n",
      "2      returns                             0.046740    Momentum            \n",
      "3      returns_lag_3                       0.043536    Momentum            \n",
      "4      macd_signal_12_26_9                 0.032343    Technical           \n",
      "5      btc_dominance                       0.023450    Market Structure    \n",
      "6      BTC_BTC_Volume                      0.021518    BTC                 \n",
      "7      BTC_returns                         0.018410    BTC                 \n",
      "8      volume_lag_3                        0.017045    Volume              \n",
      "9      lido_lido_eth_tvl                   0.016676    DeFi/OnChain        \n",
      "10     returns_sma_60                      0.015583    Momentum            \n",
      "11     returns_ema_7                       0.015508    Momentum            \n",
      "12     volume_sma_7                        0.014788    Volume              \n",
      "13     returns_lag_1                       0.014636    Momentum            \n",
      "14     volume_lag_14                       0.013902    Volume              \n",
      "15     BTC_returns_lag_14                  0.013847    BTC                 \n",
      "16     BTC_roc_20                          0.013339    BTC                 \n",
      "17     BTC_macd_12_26                      0.012726    BTC                 \n",
      "18     btc_eth_correlation                 0.012622    Market Structure    \n",
      "19     dxy_DXY                             0.012394    Macro               \n",
      "20     BTC_returns_lag_7                   0.011336    BTC                 \n",
      "21     volume_lag_7                        0.010723    Volume              \n",
      "22     BTC_returns_ema_7                   0.010668    BTC                 \n",
      "23     BTC_returns_lag_1                   0.010474    BTC                 \n",
      "24     bnb_eth_ratio                       0.010133    Market Structure    \n",
      "25     BTC_returns_lag_3                   0.010116    BTC                 \n",
      "26     BTC_momentum_10                     0.010009    BTC                 \n",
      "27     funding_fundingRate_lag1            0.009978    Other               \n",
      "28     BTC_cci_20                          0.009886    BTC                 \n",
      "29     volume_lag_1                        0.009616    Volume              \n",
      "30     BTC_mfi_14                          0.009606    BTC                 \n",
      "\n",
      "======================================================================\n",
      "Critical Feature Check\n",
      "======================================================================\n",
      "  ⚠ RSI: Not in top 30, best rank: 17\n",
      "  ⚠ ADX: Not in top 30, best rank: 34\n",
      "  ⚠ Bollinger Bands: Not in top 30, best rank: 22\n",
      "  ⚠ Stochastic: Not in top 30, best rank: 26\n",
      "\n",
      "======================================================================\n",
      "Applying Feature Selection to All Datasets\n",
      "======================================================================\n",
      "\n",
      "Final Dataset Shapes:\n",
      "  Train: (1411, 33)\n",
      "  Val:   (314, 33)\n",
      "  Test:  (322, 33)\n",
      "\n",
      "======================================================================\n",
      "Feature Selection Completed Successfully!\n",
      "======================================================================\n",
      "  TOP_K: 30\n",
      "  USE_RFECV: False\n",
      "  CUMULATIVE_THRESHOLD: 0.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 11: Advanced Feature Selection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TOP_K = 30  \n",
    "USE_RFECV = False \n",
    "CUMULATIVE_THRESHOLD = 0.95 \n",
    "exclude_cols = ['date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'ETH_Volume',\n",
    "                'target_next_log_return', 'target_direction']\n",
    "all_features = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nInitial features: {len(all_features)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 1: Variance Threshold\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 1] Variance Threshold Filter...\")\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(train_df[all_features])\n",
    "features_after_variance = [feat for feat, selected in zip(all_features, selector.get_support()) if selected]\n",
    "print(f\"  Remaining: {len(features_after_variance)} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 2: Correlation Filter (Removes redundant features)\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 2] Correlation Filter (threshold=0.95)...\")\n",
    "corr_matrix = train_df[features_after_variance].corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]\n",
    "features_after_corr = [f for f in features_after_variance if f not in to_drop]\n",
    "print(f\"  Dropped {len(to_drop)} highly correlated features\")\n",
    "print(f\"  Remaining: {len(features_after_corr)} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 3: Tree-based Feature Importance\n",
    "# ============================================================================\n",
    "print(\"\\n[Stage 3] Random Forest Feature Importance...\")\n",
    "X_train = train_df[features_after_corr].values\n",
    "y_train = train_df['target_next_log_return'].values\n",
    "mask = ~np.isnan(y_train)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,  # Increased for stability\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'feature': features_after_corr,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ============================================================================\n",
    "# Stage 4: Feature Selection Strategy\n",
    "# ============================================================================\n",
    "if USE_RFECV:\n",
    "    print(\"\\n[Stage 4] RFECV - Finding Optimal Feature Count...\")\n",
    "    print(\"  (This may take several minutes...)\")\n",
    "\n",
    "    rfecv = RFECV(\n",
    "        estimator=RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        step=1,\n",
    "        cv=TimeSeriesSplit(n_splits=3),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    rfecv.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "    selected_features = [features_after_corr[i] for i in range(len(features_after_corr)) if rfecv.support_[i]]\n",
    "    print(f\"  RFECV selected {len(selected_features)} optimal features\")\n",
    "    print(f\"  (Ignoring TOP_K={TOP_K} parameter)\")\n",
    "\n",
    "else:\n",
    "    # Method 1: Top K selection\n",
    "    print(f\"\\n[Stage 4] Top-K Selection (K={TOP_K})...\")\n",
    "    selected_features = importances.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "    # Calculate cumulative importance for reference\n",
    "    importances['cumulative'] = importances['importance'].cumsum()\n",
    "    importances['cumulative_pct'] = importances['cumulative'] / importances['importance'].sum()\n",
    "\n",
    "    cum_at_k = importances.iloc[TOP_K-1]['cumulative_pct']\n",
    "    print(f\"  Cumulative importance at K={TOP_K}: {cum_at_k:.2%}\")\n",
    "\n",
    "    # Show how many features needed for 95% cumulative importance\n",
    "    features_for_95 = len(importances[importances['cumulative_pct'] <= CUMULATIVE_THRESHOLD])\n",
    "    print(f\"  Features needed for {CUMULATIVE_THRESHOLD:.0%} importance: {features_for_95}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Display Top Features with Category Analysis\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Selected {len(selected_features)} Features\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Categorize features\n",
    "def categorize_feature(feat):\n",
    "    if feat.startswith('BTC_'):\n",
    "        return 'BTC'\n",
    "    elif any(x in feat for x in ['returns', 'log_returns', 'momentum', 'roc']):\n",
    "        return 'Momentum'\n",
    "    elif any(x in feat for x in ['volume', 'obv']):\n",
    "        return 'Volume'\n",
    "    elif any(x in feat for x in ['sma', 'ema', 'macd', 'rsi', 'bb_', 'atr', 'adx']):\n",
    "        return 'Technical'\n",
    "    elif any(x in feat for x in ['tvl', 'onchain', 'lido', 'aave', 'maker']):\n",
    "        return 'DeFi/OnChain'\n",
    "    elif any(x in feat for x in ['sentiment', 'news']):\n",
    "        return 'Sentiment'\n",
    "    elif any(x in feat for x in ['vix', 'sp500', 'gold', 'dxy']):\n",
    "        return 'Macro'\n",
    "    elif any(x in feat for x in ['correlation', 'ratio', 'dominance']):\n",
    "        return 'Market Structure'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Category distribution\n",
    "categories = {}\n",
    "for feat in selected_features:\n",
    "    cat = categorize_feature(feat)\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nCategory Distribution:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: -x[1]):\n",
    "    pct = count / len(selected_features) * 100\n",
    "    print(f\"  {cat:20s}: {count:2d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n Top {min(30, len(selected_features))} Most Important Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<35} {'Importance':<12} {'Category':<20}\")\n",
    "print(\"-\" * 75)\n",
    "for idx, row in importances.head(min(30, len(selected_features))).iterrows():\n",
    "    feat = row['feature']\n",
    "    if feat in selected_features:\n",
    "        rank = list(selected_features).index(feat) + 1\n",
    "        cat = categorize_feature(feat)\n",
    "        print(f\"{rank:<6} {feat:<35} {row['importance']:.6f}    {cat:<20}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check for Missing Critical Features\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Critical Feature Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "critical_features = {\n",
    "    'RSI': [f for f in features_after_corr if 'rsi' in f.lower()],\n",
    "    'ADX': [f for f in features_after_corr if 'adx' in f.lower()],\n",
    "    'Bollinger Bands': [f for f in features_after_corr if 'bb_' in f],\n",
    "    'Stochastic': [f for f in features_after_corr if 'stochastic' in f],\n",
    "}\n",
    "\n",
    "for indicator, features in critical_features.items():\n",
    "    if features:\n",
    "        selected_count = len([f for f in features if f in selected_features])\n",
    "        if selected_count > 0:\n",
    "            print(f\"  ✓ {indicator}: {selected_count}/{len(features)} selected\")\n",
    "        else:\n",
    "            # Find rank of best feature in this category\n",
    "            ranks = [importances[importances['feature']==f].index[0] for f in features if f in importances['feature'].values]\n",
    "            if ranks:\n",
    "                best_rank = min(ranks) + 1\n",
    "                print(f\"  ⚠ {indicator}: Not in top {len(selected_features)}, best rank: {best_rank}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {indicator}: Not computed\")\n",
    "\n",
    "# ============================================================================\n",
    "# Apply Selection to All Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Applying Feature Selection to All Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_df = train_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "val_df = val_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "test_df = test_df[['date'] + selected_features + ['target_next_log_return', 'target_direction']].copy()\n",
    "\n",
    "print(f\"\\nFinal Dataset Shapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val:   {val_df.shape}\")\n",
    "print(f\"  Test:  {test_df.shape}\")\n",
    "\n",
    "\n",
    "importances[importances['feature'].isin(selected_features)].to_csv('feature_importance.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Feature Selection Completed Successfully!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"  TOP_K: {TOP_K}\")\n",
    "print(f\"  USE_RFECV: {USE_RFECV}\")\n",
    "print(f\"  CUMULATIVE_THRESHOLD: {CUMULATIVE_THRESHOLD}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "978aee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                        0\n",
       "BTC_obv                     0\n",
       "returns                     0\n",
       "returns_lag_3               0\n",
       "macd_signal_12_26_9         0\n",
       "btc_dominance               0\n",
       "BTC_BTC_Volume              0\n",
       "BTC_returns                 0\n",
       "volume_lag_3                0\n",
       "lido_lido_eth_tvl           0\n",
       "returns_sma_60              0\n",
       "returns_ema_7               0\n",
       "volume_sma_7                0\n",
       "returns_lag_1               0\n",
       "volume_lag_14               0\n",
       "BTC_returns_lag_14          0\n",
       "BTC_roc_20                  0\n",
       "BTC_macd_12_26              0\n",
       "btc_eth_correlation         0\n",
       "dxy_DXY                     0\n",
       "BTC_returns_lag_7           0\n",
       "volume_lag_7                0\n",
       "BTC_returns_ema_7           0\n",
       "BTC_returns_lag_1           0\n",
       "bnb_eth_ratio               0\n",
       "BTC_returns_lag_3           0\n",
       "BTC_momentum_10             0\n",
       "funding_fundingRate_lag1    0\n",
       "BTC_cci_20                  0\n",
       "volume_lag_1                0\n",
       "BTC_mfi_14                  0\n",
       "target_next_log_return      0\n",
       "target_direction            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d541cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
