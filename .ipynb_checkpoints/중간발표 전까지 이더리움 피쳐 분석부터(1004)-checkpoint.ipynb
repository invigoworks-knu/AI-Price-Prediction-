{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbf7c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: tqdm in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from optuna) (1.4.39)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: tomli in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m190.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.5 colorlog-6.9.0 optuna-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-binance\n",
    "# !pip install defillama2\n",
    "# !pip install python-dotenv\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6044a75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnalysisIndicators', 'BasePandasObject', 'PandasObject', 'accbands', 'ad', 'adosc', 'adx', 'ao', 'apo', 'aroon', 'atr', 'bbands', 'bop', 'cci', 'cmf', 'cmo', 'combination', 'coppock', 'core', 'decreasing', 'dema', 'df_error_analysis', 'donchian', 'dpo', 'efi', 'ema', 'eom', 'fibonacci', 'fwma', 'get_drift', 'get_offset', 'hl2', 'hlc3', 'hma', 'ichimoku', 'increasing', 'kc', 'kst', 'kurtosis', 'linreg', 'log_return', 'macd', 'mad', 'massi', 'math', 'median', 'mfi', 'midpoint', 'midprice', 'mom', 'momentum', 'mul', 'name', 'natr', 'np', 'nvi', 'obv', 'ohlc4', 'overlap', 'pascals_triangle', 'pd', 'percent_return', 'performance', 'ppo', 'pvi', 'pvol', 'pvt', 'pwma', 'qstick', 'quantile', 'reduce', 'rma', 'roc', 'rsi', 'sflt', 'signed_series', 'skew', 'sma', 'statistics', 'stdev', 'stoch', 't3', 'tema', 'time', 'trend', 'trima', 'trix', 'true_range', 'tsi', 'uo', 'utils', 'variance', 'verify_series', 'volatility', 'volume', 'vortex', 'vp', 'vwap', 'vwma', 'weights', 'willr', 'wma', 'zero', 'zlma', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "# pandas_ta 모듈 안의 모든 함수(지표) 목록 확인\n",
    "all_indicators = [f for f in dir(ta) if not f.startswith(\"_\")]\n",
    "print(all_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e207eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./macro_data/news_data.csv' 파일 로딩 중...\n",
      "날짜별 평균 감성 점수 계산 중...\n",
      "성공! 전처리된 파일이 './macro_data/news_sentiment_daily.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = './macro_data' \n",
    "RAW_NEWS_FILE = 'news_data.csv'\n",
    "PROCESSED_NEWS_FILE = 'news_sentiment_daily.csv'\n",
    "# --- 설정 끝 ---\n",
    "\n",
    "def create_daily_sentiment_file():\n",
    "    \"\"\"\n",
    "    라벨이 지정된 뉴스 데이터를 읽어, 일별 평균 감성 점수 파일을 생성합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 원본 뉴스 파일 로드 (컬럼명을 소문자 'date'로 수정)\n",
    "        file_path = os.path.join(DATA_DIR, RAW_NEWS_FILE)\n",
    "        print(f\"'{file_path}' 파일 로딩 중...\")\n",
    "        news_df = pd.read_csv(file_path, usecols=['date', 'label']) # 'Date' -> 'date'\n",
    "        \n",
    "        # 2. 'date' 컬럼을 datetime 형식으로 변환 (소문자로 수정)\n",
    "        news_df['date'] = pd.to_datetime(news_df['date']) # 'Date' -> 'date'\n",
    "        \n",
    "        # 3. 날짜별로 그룹화하여 하루의 평균 감성 점수 계산 (소문자로 수정)\n",
    "        print(\"날짜별 평균 감성 점수 계산 중...\")\n",
    "        daily_sentiment = news_df.groupby('date')['label'].mean().reset_index() # 'Date' -> 'date'\n",
    "        \n",
    "        # 4. 컬럼 이름을 최종 파이프라인과 맞추기 위해 'Date'로 변경\n",
    "        #    (이후의 모든 데이터와 컬럼명을 통일하기 위함)\n",
    "        daily_sentiment.rename(columns={'date': 'Date', 'label': 'news_sentiment_score'}, inplace=True)\n",
    "        \n",
    "        # 5. 새로운 CSV 파일로 저장\n",
    "        output_path = os.path.join(DATA_DIR, PROCESSED_NEWS_FILE)\n",
    "        daily_sentiment.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"성공! 전처리된 파일이 '{output_path}'에 저장되었습니다.\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"오류: CSV 파일에 'date' 또는 'label' 컬럼이 없는 것 같습니다. 컬럼명을 확인해주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"알 수 없는 오류 발생: {e}\")\n",
    "\n",
    "# 스크립트 실행\n",
    "if __name__ == '__main__':\n",
    "    create_daily_sentiment_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c199e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A6000\n",
      "VRAM: 47.5 GB\n",
      "실행 환경: GPU\n",
      "TensorFlow 버전: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "from binance.client import Client\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'macro_data'  # 모든 CSV가 있는 폴더\n",
    "\n",
    "\n",
    "DEVICE = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n",
    "\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"실행 환경: {DEVICE}\")\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1단계: 모든 데이터 로딩\n",
    "# =============================================================================\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"모든 CSV 파일을 로드하고 딕셔너리로 반환\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    try:\n",
    "        data['macro_crypto'] = pd.read_csv(os.path.join(DATA_DIR, 'macro_crypto_data.csv'))\n",
    "        data['fear_greed'] = pd.read_csv(os.path.join(DATA_DIR, 'fear_greed.csv'))\n",
    "        data['news'] = pd.read_csv(os.path.join(DATA_DIR, 'news_sentiment_daily.csv'))\n",
    "        data['sp500'] = pd.read_csv(os.path.join(DATA_DIR, 'SP500.csv'))\n",
    "        data['vix'] = pd.read_csv(os.path.join(DATA_DIR, 'VIX.csv'))\n",
    "        data['gold'] = pd.read_csv(os.path.join(DATA_DIR, 'GOLD.csv'))\n",
    "        data['dxy'] = pd.read_csv(os.path.join(DATA_DIR, 'DXY.csv'))\n",
    "        data['eth_funding'] = pd.read_csv(os.path.join(DATA_DIR, 'eth_funding_rate.csv'))\n",
    "        data['eth_chain_tvl'] = pd.read_csv(os.path.join(DATA_DIR, 'eth_chain_tvl.csv'))\n",
    "        data['makerdao_tvl'] = pd.read_csv(os.path.join(DATA_DIR, 'makerdao_eth_tvl.csv'))\n",
    "        data['lido_tvl'] = pd.read_csv(os.path.join(DATA_DIR, 'lido_eth_tvl.csv'))\n",
    "        data['aave_tvl'] = pd.read_csv(os.path.join(DATA_DIR, 'aave_eth_tvl.csv'))\n",
    "        data['usdt_total'] = pd.read_csv(os.path.join(DATA_DIR, 'usdt_total_mcap.csv'))\n",
    "        data['usdt_eth'] = pd.read_csv(os.path.join(DATA_DIR, 'usdt_eth_mcap.csv'))\n",
    "        data['google_trends_weekly'] = pd.read_csv(os.path.join(DATA_DIR, 'ethereum_google_trends_weekly_2017_2025.csv'))\n",
    "        data['google_trends_detailed'] = pd.read_csv(os.path.join(DATA_DIR, 'ethereum_google_trends_weekly_detailed.csv'))\n",
    "        data['google_trends_5y'] = pd.read_csv(os.path.join(DATA_DIR, 'ethereum_trends_5years.csv'))\n",
    "        data['eth_onchain'] = pd.read_csv(os.path.join(DATA_DIR, 'eth_onchain.csv'))  # 추가\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "data = load_all_data()\n",
    "if data is None:\n",
    "    print(\"데이터 로딩 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cc0dfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e49c1383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[macro_crypto] Shape: (3199, 51), Missing: 32405\n",
      "[fear_greed] Shape: (2799, 2), Missing: 0\n",
      "[news] Shape: (2061, 2), Missing: 0\n",
      "[sp500] Shape: (2201, 2), Missing: 0\n",
      "[vix] Shape: (2201, 2), Missing: 0\n",
      "[gold] Shape: (2202, 2), Missing: 0\n",
      "[dxy] Shape: (2203, 2), Missing: 0\n",
      "[eth_funding] Shape: (2139, 2), Missing: 0\n",
      "[eth_chain_tvl] Shape: (2929, 2), Missing: 0\n",
      "[makerdao_tvl] Shape: (2466, 2), Missing: 0\n",
      "[lido_tvl] Shape: (1750, 2), Missing: 0\n",
      "[aave_tvl] Shape: (1964, 2), Missing: 0\n",
      "[usdt_total] Shape: (2867, 6), Missing: 6617\n",
      "[usdt_eth] Shape: (2867, 6), Missing: 1631\n",
      "[google_trends_weekly] Shape: (457, 2), Missing: 0\n",
      "[google_trends_detailed] Shape: (457, 2), Missing: 0\n",
      "[google_trends_5y] Shape: (70, 2), Missing: 0\n",
      "[eth_onchain] Shape: (3711, 11), Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2단계: 데이터 기본 탐색\n",
    "# =============================================================================\n",
    "\n",
    "def explore_data(data):\n",
    "    \"\"\"각 데이터셋의 기본 정보 출력 (간략화)\"\"\"\n",
    "    for name, df in data.items():\n",
    "        print(f\"[{name}] Shape: {df.shape}, Missing: {df.isnull().sum().sum()}\")  \n",
    "    \n",
    "explore_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e6f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 퍼플렉시티 버전 ###################################\n",
    "############# 퍼플렉시티 버전 ###################################3\n",
    "############# 퍼플렉시티 버전 ###################################3\n",
    "############# 퍼플렉시티 버전 ###################################3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a1abad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터 로딩 중...\n",
      "================================================================================\n",
      "✓ macro_crypto: 3199 rows, 2017-01-01 ~ 2025-10-04\n",
      "✓ fear_greed: 2799 rows, 2018-02-01 ~ 2025-10-04\n",
      "✓ news: 2061 rows, 2020-01-01 ~ 2025-10-03\n",
      "✓ sp500: 2201 rows, 2017-01-03 ~ 2025-10-03\n",
      "✓ vix: 2201 rows, 2017-01-03 ~ 2025-10-03\n",
      "✓ gold: 2202 rows, 2017-01-03 ~ 2025-10-03\n",
      "✓ dxy: 2203 rows, 2017-01-03 ~ 2025-10-03\n",
      "✓ eth_funding: 2139 rows, 2019-11-27 ~ 2025-10-04\n",
      "✓ eth_chain_tvl: 2929 rows, 2017-09-27 ~ 2025-10-03\n",
      "✓ makerdao_tvl: 2466 rows, 2019-01-04 ~ 2025-10-03\n",
      "✓ lido_tvl: 1750 rows, 2020-12-19 ~ 2025-10-03\n",
      "✓ aave_tvl: 1964 rows, 2020-05-20 ~ 2025-10-03\n",
      "✓ usdt_total: 2867 rows, 2017-11-29 ~ 2025-10-04\n",
      "✓ usdt_eth: 2867 rows, 2017-11-29 ~ 2025-10-04\n",
      "✓ google_trends_weekly: 457 rows, 2017-01-01 ~ 2025-09-28\n",
      "✓ google_trends_detailed: 457 rows, 2017-01-01 ~ 2025-09-28\n",
      "✓ eth_onchain: 3711 rows, 2015-08-07 ~ 2025-10-03\n",
      "\\n공통 가능 기간: 2020-12-19 ~ 2025-09-28\n",
      "공통 기간: 1743일 (4.78년)\n",
      "\\n================================================================================\n",
      "데이터셋별 상세 정보 (시작일 순)\n",
      "================================================================================\n",
      "eth_onchain               | 2015-08-07 ~ 2025-10-03 | 3711 rows |  10 features\n",
      "macro_crypto              | 2017-01-01 ~ 2025-10-04 | 3199 rows |  50 features\n",
      "google_trends_weekly      | 2017-01-01 ~ 2025-09-28 |  457 rows |   1 features\n",
      "google_trends_detailed    | 2017-01-01 ~ 2025-09-28 |  457 rows |   1 features\n",
      "sp500                     | 2017-01-03 ~ 2025-10-03 | 2201 rows |   1 features\n",
      "vix                       | 2017-01-03 ~ 2025-10-03 | 2201 rows |   1 features\n",
      "gold                      | 2017-01-03 ~ 2025-10-03 | 2202 rows |   1 features\n",
      "dxy                       | 2017-01-03 ~ 2025-10-03 | 2203 rows |   1 features\n",
      "eth_chain_tvl             | 2017-09-27 ~ 2025-10-03 | 2929 rows |   1 features\n",
      "usdt_total                | 2017-11-29 ~ 2025-10-04 | 2867 rows |   5 features\n",
      "usdt_eth                  | 2017-11-29 ~ 2025-10-04 | 2867 rows |   5 features\n",
      "fear_greed                | 2018-02-01 ~ 2025-10-04 | 2799 rows |   1 features\n",
      "makerdao_tvl              | 2019-01-04 ~ 2025-10-03 | 2466 rows |   1 features\n",
      "eth_funding               | 2019-11-27 ~ 2025-10-04 | 2139 rows |   1 features\n",
      "news                      | 2020-01-01 ~ 2025-10-03 | 2061 rows |   1 features\n",
      "aave_tvl                  | 2020-05-20 ~ 2025-10-03 | 1964 rows |   1 features\n",
      "lido_tvl                  | 2020-12-19 ~ 2025-10-03 | 1750 rows |   1 features\n",
      "\\n================================================================================\n",
      "시나리오 구성 완료\n",
      "================================================================================\n",
      "\\n[scenario1_short_term]\n",
      "  설명: 단기 예측 (1-30일): 온체인+감성 중심, 2021년 이후\n",
      "  시작일: 2021-01-01\n",
      "  데이터셋: 15개\n",
      "  포함: eth_onchain, fear_greed, news, eth_funding, eth_chain_tvl...\n",
      "\\n[scenario2_long_term]\n",
      "  설명: 장기 예측 (30-180일): 매크로+온체인 중심, 2018년 이후\n",
      "  시작일: 2018-01-01\n",
      "  데이터셋: 11개\n",
      "  포함: macro_crypto, eth_onchain, sp500, vix, gold...\n",
      "\\n병합 중: 단기 예측 (1-30일): 온체인+감성 중심, 2021년 이후\n",
      "기간: 2021-01-01 ~ 2025-10-03\n",
      "  ✓ eth_onchain: 10 features 추가\n",
      "  ✓ fear_greed: 1 features 추가\n",
      "  ✓ news: 1 features 추가\n",
      "  ✓ eth_funding: 1 features 추가\n",
      "  ✓ eth_chain_tvl: 1 features 추가\n",
      "  ✓ makerdao_tvl: 1 features 추가\n",
      "  ✓ lido_tvl: 1 features 추가\n",
      "  ✓ aave_tvl: 1 features 추가\n",
      "  ✓ sp500: 1 features 추가\n",
      "  ✓ vix: 1 features 추가\n",
      "  ✓ gold: 1 features 추가\n",
      "  ✓ dxy: 1 features 추가\n",
      "  ✓ usdt_total: 5 features 추가\n",
      "  ✓ usdt_eth: 5 features 추가\n",
      "  ✓ macro_crypto: 50 features 추가\n",
      "\\n병합 완료:\n",
      "  - 총 rows: 1737\n",
      "  - 총 features: 81\n",
      "  - 결측치: 3474\n",
      "\\n================================================================================\n",
      "시계열 분할 완료 (Train-Val-Test 모두 보존)\n",
      "================================================================================\n",
      "Train: 1206 rows (69.4%) | 2021-01-01 ~ 2024-04-20\n",
      "  ↓ GAP: 7일\n",
      "Val:    258 rows (14.9%) | 2024-04-28 ~ 2025-01-10\n",
      "  ↓ GAP: 7일\n",
      "Test:   259 rows (14.9%) | 2025-01-18 ~ 2025-10-03\n",
      "\\n총 사용: 1723/1737 rows (99.2%)\n",
      "\\n✓ 인덱스 재조정 후:\n",
      "  Train: 1206 rows\n",
      "  Val:   258 rows\n",
      "  Test:  0 rows\n",
      "\\n피처 엔지니어링 완료:\n",
      "  - 총 features: 103\n",
      "  - 수익률 기반: 21\n",
      "  - Target: 다음날 수익률 (target_next_return)\n",
      "\\n================================================================================\n",
      "최종 데이터셋 Shape\n",
      "================================================================================\n",
      "X_train: (1206, 103), y_train_return: (1206,), y_train_direction: (1206,)\n",
      "X_val:   (258, 103), y_val_return: (258,), y_val_direction: (258,)\n",
      "X_test:  (0, 103), y_test_return: (0,), y_test_direction: (0,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# 완벽한 데이터 누수 방지 파이프라인 - 최종 버전\n",
    "# 특징:\n",
    "# 1. 수익률(returns) 기반 예측 (비정상 → 정상 시계열 변환)\n",
    "# 2. Train-Val-Test 모두 보존\n",
    "# 3. GAP 설정으로 데이터 누수 완벽 차단\n",
    "# 4. 논문 기반 피처 엔지니어링\n",
    "# =============================================================================\n",
    "\n",
    "class TimeSeriesDataPipeline:\n",
    "    \"\"\"시계열 데이터 누수 방지 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='macro_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.datasets = {}\n",
    "        self.coverage = {}\n",
    "        self.scenarios = {}\n",
    "        \n",
    "    def load_and_standardize_dates(self):\n",
    "        \"\"\"모든 데이터 로드 후 날짜 컬럼 표준화 (Timezone 제거)\"\"\"\n",
    "        \n",
    "        file_map = {\n",
    "            'macro_crypto': 'macro_crypto_data.csv',\n",
    "            'fear_greed': 'fear_greed.csv',\n",
    "            'news': 'news_sentiment_daily.csv',\n",
    "            'sp500': 'SP500.csv',\n",
    "            'vix': 'VIX.csv',\n",
    "            'gold': 'GOLD.csv',\n",
    "            'dxy': 'DXY.csv',\n",
    "            'eth_funding': 'eth_funding_rate.csv',\n",
    "            'eth_chain_tvl': 'eth_chain_tvl.csv',\n",
    "            'makerdao_tvl': 'makerdao_eth_tvl.csv',\n",
    "            'lido_tvl': 'lido_eth_tvl.csv',\n",
    "            'aave_tvl': 'aave_eth_tvl.csv',\n",
    "            'usdt_total': 'usdt_total_mcap.csv',\n",
    "            'usdt_eth': 'usdt_eth_mcap.csv',\n",
    "            'google_trends_weekly': 'ethereum_google_trends_weekly_2017_2025.csv',\n",
    "            'google_trends_detailed': 'ethereum_google_trends_weekly_detailed.csv',\n",
    "            'eth_onchain': 'eth_onchain.csv'\n",
    "        }\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"데이터 로딩 중...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for name, filename in file_map.items():\n",
    "            try:\n",
    "                filepath = os.path.join(self.data_dir, filename)\n",
    "                if not os.path.exists(filepath):\n",
    "                    print(f\"⚠ {name}: 파일 없음 ({filename})\")\n",
    "                    continue\n",
    "                    \n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # 날짜 컬럼 찾기\n",
    "                date_col = None\n",
    "                for col in df.columns:\n",
    "                    if col.lower() in ['date', 'timestamp', 'time']:\n",
    "                        date_col = col\n",
    "                        break\n",
    "                \n",
    "                if date_col:\n",
    "                    # Timezone 정보 제거\n",
    "                    df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                    if df['date'].dt.tz is not None:\n",
    "                        df['date'] = df['date'].dt.tz_localize(None)\n",
    "                    \n",
    "                    # NaT 제거\n",
    "                    df = df.dropna(subset=['date'])\n",
    "                    \n",
    "                    if date_col != 'date':\n",
    "                        df = df.drop(columns=[date_col])\n",
    "                    \n",
    "                    df = df.sort_values('date').reset_index(drop=True)\n",
    "                    self.datasets[name] = df\n",
    "                    print(f\"✓ {name}: {len(df)} rows, {df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "                else:\n",
    "                    print(f\"✗ {name}: 날짜 컬럼을 찾을 수 없습니다\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ {name}: 로딩 실패 - {e}\")\n",
    "        \n",
    "        return self.datasets\n",
    "    \n",
    "    def analyze_date_coverage(self):\n",
    "        \"\"\"각 데이터셋의 날짜 커버리지 분석\"\"\"\n",
    "        \n",
    "        if not self.datasets:\n",
    "            raise ValueError(\"먼저 load_and_standardize_dates()를 실행하세요\")\n",
    "        \n",
    "        for name, df in self.datasets.items():\n",
    "            self.coverage[name] = {\n",
    "                'start': df['date'].min(),\n",
    "                'end': df['date'].max(),\n",
    "                'rows': len(df),\n",
    "                'features': df.shape[1] - 1,\n",
    "                'days': (df['date'].max() - df['date'].min()).days\n",
    "            }\n",
    "        \n",
    "        # 공통 기간 계산\n",
    "        latest_start = max([info['start'] for info in self.coverage.values()])\n",
    "        earliest_end = min([info['end'] for info in self.coverage.values()])\n",
    "        \n",
    "        print(f\"\\\\n공통 가능 기간: {latest_start.strftime('%Y-%m-%d')} ~ {earliest_end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"공통 기간: {(earliest_end - latest_start).days}일 ({(earliest_end - latest_start).days/365:.2f}년)\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        print(\"데이터셋별 상세 정보 (시작일 순)\")\n",
    "        print(\"=\"*80)\n",
    "        for name, info in sorted(self.coverage.items(), key=lambda x: x[1]['start']):\n",
    "            print(f\"{name:25} | {info['start'].strftime('%Y-%m-%d')} ~ {info['end'].strftime('%Y-%m-%d')} | {info['rows']:4} rows | {info['features']:3} features\")\n",
    "        \n",
    "        return self.coverage, latest_start, earliest_end\n",
    "    \n",
    "    def create_scenarios(self):\n",
    "        \"\"\"논문 기반 2가지 시나리오 생성\"\"\"\n",
    "        \n",
    "        scenario1_start = pd.to_datetime('2021-01-01')\n",
    "        scenario1_datasets = [\n",
    "            'eth_onchain', 'fear_greed', 'news', 'eth_funding',\n",
    "            'eth_chain_tvl', 'makerdao_tvl', 'lido_tvl', 'aave_tvl',\n",
    "            'sp500', 'vix', 'gold', 'dxy',\n",
    "            'usdt_total', 'usdt_eth', 'macro_crypto'\n",
    "        ]\n",
    "        \n",
    "        scenario2_start = pd.to_datetime('2018-01-01')\n",
    "        scenario2_datasets = [\n",
    "            'macro_crypto', 'eth_onchain', 'sp500', 'vix', 'gold', 'dxy',\n",
    "            'eth_chain_tvl', 'fear_greed', 'makerdao_tvl',\n",
    "            'usdt_total', 'usdt_eth'\n",
    "        ]\n",
    "        \n",
    "        self.scenarios = {\n",
    "            'scenario1_short_term': {\n",
    "                'start_date': scenario1_start,\n",
    "                'datasets': [d for d in scenario1_datasets if d in self.datasets],\n",
    "                'description': '단기 예측 (1-30일): 온체인+감성 중심, 2021년 이후'\n",
    "            },\n",
    "            'scenario2_long_term': {\n",
    "                'start_date': scenario2_start,\n",
    "                'datasets': [d for d in scenario2_datasets if d in self.datasets],\n",
    "                'description': '장기 예측 (30-180일): 매크로+온체인 중심, 2018년 이후'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        print(\"시나리오 구성 완료\")\n",
    "        print(\"=\"*80)\n",
    "        for name, info in self.scenarios.items():\n",
    "            print(f\"\\\\n[{name}]\")\n",
    "            print(f\"  설명: {info['description']}\")\n",
    "            print(f\"  시작일: {info['start_date'].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  데이터셋: {len(info['datasets'])}개\")\n",
    "            print(f\"  포함: {', '.join(info['datasets'][:5])}...\")\n",
    "        \n",
    "        return self.scenarios\n",
    "    \n",
    "    def merge_datasets_no_leakage(self, scenario_name, end_date=None):\n",
    "        \"\"\"데이터 누수 없이 병합\"\"\"\n",
    "        \n",
    "        if scenario_name not in self.scenarios:\n",
    "            raise ValueError(f\"시나리오 '{scenario_name}'이 존재하지 않습니다\")\n",
    "        \n",
    "        scenario = self.scenarios[scenario_name]\n",
    "        start_date = scenario['start_date']\n",
    "        \n",
    "        if end_date is None:\n",
    "            end_date = min([self.datasets[name]['date'].max() for name in scenario['datasets']])\n",
    "        \n",
    "        # 전체 날짜 범위 생성\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        merged_df = pd.DataFrame({'date': date_range})\n",
    "        \n",
    "        print(f\"\\\\n병합 중: {scenario['description']}\")\n",
    "        print(f\"기간: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        for name in scenario['datasets']:\n",
    "            df = self.datasets[name].copy()\n",
    "            df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(f\"  ⚠ {name}: 해당 기간에 데이터 없음\")\n",
    "                continue\n",
    "            \n",
    "            # 컬럼명 충돌 방지\n",
    "            cols_to_rename = {col: f\"{name}_{col}\" for col in df.columns if col != 'date'}\n",
    "            df = df.rename(columns=cols_to_rename)\n",
    "            \n",
    "            merged_df = pd.merge(merged_df, df, on='date', how='left')\n",
    "            print(f\"  ✓ {name}: {df.shape[1]-1} features 추가\")\n",
    "        \n",
    "        # Forward fill (과거 값으로 채움)\n",
    "        merged_df = merged_df.ffill()\n",
    "        merged_df = merged_df.bfill()\n",
    "        \n",
    "        print(f\"\\\\n병합 완료:\")\n",
    "        print(f\"  - 총 rows: {len(merged_df)}\")\n",
    "        print(f\"  - 총 features: {merged_df.shape[1]-1}\")\n",
    "        print(f\"  - 결측치: {merged_df.isnull().sum().sum()}\")\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    def time_series_split(self, df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, gap_days=7):\n",
    "        \"\"\"시간 순서 기반 분할 + GAP (Test 데이터 보존 보장)\"\"\"\n",
    "        \n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        total_rows = len(df)\n",
    "        \n",
    "        # 실제 사용 가능한 데이터 계산 (갭 고려)\n",
    "        usable_rows = total_rows - (2 * gap_days)\n",
    "        \n",
    "        if usable_rows <= 0:\n",
    "            raise ValueError(f\"데이터가 너무 적습니다. 최소 {2*gap_days + 100}일 이상 필요\")\n",
    "        \n",
    "        # Train 크기 계산\n",
    "        train_size = int(usable_rows * train_ratio)\n",
    "        val_size = int(usable_rows * val_ratio)\n",
    "        test_size = usable_rows - train_size - val_size\n",
    "        \n",
    "        # 인덱스 범위 계산\n",
    "        train_start = 0\n",
    "        train_end = train_start + train_size\n",
    "        \n",
    "        val_start = train_end + gap_days\n",
    "        val_end = val_start + val_size\n",
    "        \n",
    "        test_start = val_end + gap_days\n",
    "        test_end = total_rows\n",
    "        \n",
    "        train_idx = list(range(train_start, train_end))\n",
    "        val_idx = list(range(val_start, val_end))\n",
    "        test_idx = list(range(test_start, test_end))\n",
    "        \n",
    "        # 검증\n",
    "        assert len(train_idx) > 0, \"Train 데이터가 비어있습니다\"\n",
    "        assert len(val_idx) > 0, \"Validation 데이터가 비어있습니다\"\n",
    "        assert len(test_idx) > 0, \"Test 데이터가 비어있습니다\"\n",
    "        \n",
    "        train_dates = df.loc[train_idx, 'date']\n",
    "        val_dates = df.loc[val_idx, 'date']\n",
    "        test_dates = df.loc[test_idx, 'date']\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        print(\"시계열 분할 완료 (Train-Val-Test 모두 보존)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Train: {len(train_idx):4} rows ({len(train_idx)/total_rows*100:.1f}%) | {train_dates.min().strftime('%Y-%m-%d')} ~ {train_dates.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  ↓ GAP: {gap_days}일\")\n",
    "        print(f\"Val:   {len(val_idx):4} rows ({len(val_idx)/total_rows*100:.1f}%) | {val_dates.min().strftime('%Y-%m-%d')} ~ {val_dates.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  ↓ GAP: {gap_days}일\")\n",
    "        print(f\"Test:  {len(test_idx):4} rows ({len(test_idx)/total_rows*100:.1f}%) | {test_dates.min().strftime('%Y-%m-%d')} ~ {test_dates.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"\\\\n총 사용: {len(train_idx) + len(val_idx) + len(test_idx)}/{total_rows} rows ({(len(train_idx) + len(val_idx) + len(test_idx))/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        return train_idx, val_idx, test_idx\n",
    "    \n",
    "    def engineer_features_with_returns(self, df, train_idx, val_idx, test_idx, target_col='macro_crypto_ETH_Close'):\n",
    "        \"\"\"수익률 기반 피처 엔지니어링 (논문 표준)\"\"\"\n",
    "        \n",
    "        df = df.copy()\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # 1. 가격 데이터 확인\n",
    "        if target_col not in df.columns:\n",
    "            print(f\"⚠ 타겟 컬럼 '{target_col}'이 없습니다. 사용 가능한 컬럼:\")\n",
    "            price_cols = [col for col in df.columns if 'ETH' in col.upper() and ('close' in col.lower() or 'Close' in col)]\n",
    "            for col in price_cols[:5]:\n",
    "                print(f\"  - {col}\")\n",
    "            if price_cols:\n",
    "                target_col = price_cols[0]\n",
    "                print(f\"  → '{target_col}' 사용\")\n",
    "            else:\n",
    "                raise ValueError(\"이더리움 가격 컬럼을 찾을 수 없습니다\")\n",
    "        \n",
    "        # 2. 수익률 계산 (정상 시계열로 변환)\n",
    "        df['price_return'] = df[target_col].pct_change()\n",
    "        df['log_return'] = np.log(df[target_col] / df[target_col].shift(1))\n",
    "        \n",
    "        # 3. Target 변수 (다음날 수익률 예측)\n",
    "        df['target_next_return'] = df['price_return'].shift(-1)\n",
    "        df['target_next_price'] = df[target_col].shift(-1)  # 가격도 함께 저장\n",
    "        \n",
    "        # 방향 예측 (Classification)\n",
    "        df['target_direction'] = (df['target_next_return'] > 0).astype(int)\n",
    "        \n",
    "        # 4. Lag Features (수익률 기반)\n",
    "        for lag in [1, 3, 7, 14, 30]:\n",
    "            df[f'return_lag_{lag}'] = df['price_return'].shift(lag)\n",
    "            df[f'log_return_lag_{lag}'] = df['log_return'].shift(lag)\n",
    "        \n",
    "        # 5. Rolling Features (수익률 기반)\n",
    "        for window in [7, 14, 30]:\n",
    "            df[f'return_ma_{window}'] = df['price_return'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'return_std_{window}'] = df['price_return'].rolling(window=window, min_periods=1).std()\n",
    "            df[f'return_skew_{window}'] = df['price_return'].rolling(window=window, min_periods=3).skew()\n",
    "        \n",
    "        # 6. 변동성 지표\n",
    "        df['volatility_7d'] = df['price_return'].rolling(window=7, min_periods=1).std()\n",
    "        df['volatility_30d'] = df['price_return'].rolling(window=30, min_periods=1).std()\n",
    "        \n",
    "        # 결측치 제거\n",
    "        df = df.dropna(subset=['target_next_return']).reset_index(drop=True)\n",
    "        \n",
    "        # 7. 인덱스 재조정 (날짜 기준으로 매핑)\n",
    "        original_dates = {\n",
    "            'train': df.iloc[train_idx]['date'].values if max(train_idx) < len(df) else [],\n",
    "            'val': df.iloc[val_idx]['date'].values if len(val_idx) > 0 and max(val_idx) < len(df) else [],\n",
    "            'test': df.iloc[test_idx]['date'].values if len(test_idx) > 0 and max(test_idx) < len(df) else []\n",
    "        }\n",
    "        \n",
    "        new_train_idx = df[df['date'].isin(original_dates['train'])].index.tolist()\n",
    "        new_val_idx = df[df['date'].isin(original_dates['val'])].index.tolist()\n",
    "        new_test_idx = df[df['date'].isin(original_dates['test'])].index.tolist()\n",
    "        \n",
    "        print(f\"\\\\n✓ 인덱스 재조정 후:\")\n",
    "        print(f\"  Train: {len(new_train_idx)} rows\")\n",
    "        print(f\"  Val:   {len(new_val_idx)} rows\")\n",
    "        print(f\"  Test:  {len(new_test_idx)} rows\")\n",
    "        \n",
    "        # 8. 스케일링\n",
    "        feature_cols = [col for col in df.columns if col not in [\n",
    "            'date', 'target_next_return', 'target_next_price', 'target_direction', target_col\n",
    "        ]]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        if len(new_train_idx) > 0:\n",
    "            df.loc[new_train_idx, feature_cols] = scaler.fit_transform(df.loc[new_train_idx, feature_cols])\n",
    "        if len(new_val_idx) > 0:\n",
    "            df.loc[new_val_idx, feature_cols] = scaler.transform(df.loc[new_val_idx, feature_cols])\n",
    "        if len(new_test_idx) > 0:\n",
    "            df.loc[new_test_idx, feature_cols] = scaler.transform(df.loc[new_test_idx, feature_cols])\n",
    "        \n",
    "        print(f\"\\\\n피처 엔지니어링 완료:\")\n",
    "        print(f\"  - 총 features: {len(feature_cols)}\")\n",
    "        print(f\"  - 수익률 기반: {len([c for c in feature_cols if 'return' in c])}\")\n",
    "        print(f\"  - Target: 다음날 수익률 (target_next_return)\")\n",
    "        \n",
    "        return df, new_train_idx, new_val_idx, new_test_idx, scaler, feature_cols\n",
    "    \n",
    "    def prepare_final_datasets(self, df, train_idx, val_idx, test_idx, feature_cols):\n",
    "        \"\"\"최종 데이터셋 반환\"\"\"\n",
    "        \n",
    "        X_train = df.loc[train_idx, feature_cols].values\n",
    "        y_train_return = df.loc[train_idx, 'target_next_return'].values\n",
    "        y_train_price = df.loc[train_idx, 'target_next_price'].values\n",
    "        y_train_direction = df.loc[train_idx, 'target_direction'].values\n",
    "        \n",
    "        X_val = df.loc[val_idx, feature_cols].values\n",
    "        y_val_return = df.loc[val_idx, 'target_next_return'].values\n",
    "        y_val_price = df.loc[val_idx, 'target_next_price'].values\n",
    "        y_val_direction = df.loc[val_idx, 'target_direction'].values\n",
    "        \n",
    "        X_test = df.loc[test_idx, feature_cols].values\n",
    "        y_test_return = df.loc[test_idx, 'target_next_return'].values\n",
    "        y_test_price = df.loc[test_idx, 'target_next_price'].values\n",
    "        y_test_direction = df.loc[test_idx, 'target_direction'].values\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        print(\"최종 데이터셋 Shape\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"X_train: {X_train.shape}, y_train_return: {y_train_return.shape}, y_train_direction: {y_train_direction.shape}\")\n",
    "        print(f\"X_val:   {X_val.shape}, y_val_return: {y_val_return.shape}, y_val_direction: {y_val_direction.shape}\")\n",
    "        print(f\"X_test:  {X_test.shape}, y_test_return: {y_test_return.shape}, y_test_direction: {y_test_direction.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train, 'y_train_return': y_train_return, 'y_train_price': y_train_price, 'y_train_direction': y_train_direction,\n",
    "            'X_val': X_val, 'y_val_return': y_val_return, 'y_val_price': y_val_price, 'y_val_direction': y_val_direction,\n",
    "            'X_test': X_test, 'y_test_return': y_test_return, 'y_test_price': y_test_price, 'y_test_direction': y_test_direction,\n",
    "            'feature_cols': feature_cols,\n",
    "            'dataframe': df,\n",
    "            'train_idx': train_idx,\n",
    "            'val_idx': val_idx,\n",
    "            'test_idx': test_idx\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 실행 예제\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 파이프라인 초기화\n",
    "    pipeline = TimeSeriesDataPipeline(data_dir='macro_data')\n",
    "    \n",
    "    # 1. 데이터 로딩\n",
    "    datasets = pipeline.load_and_standardize_dates()\n",
    "    \n",
    "    # 2. 커버리지 분석\n",
    "    coverage, latest_start, earliest_end = pipeline.analyze_date_coverage()\n",
    "    \n",
    "    # 3. 시나리오 생성\n",
    "    scenarios = pipeline.create_scenarios()\n",
    "    \n",
    "    # 4. 시나리오 1 실행 (단기 예측)\n",
    "    merged_df = pipeline.merge_datasets_no_leakage('scenario1_short_term')\n",
    "    \n",
    "    # 5. 시계열 분할 (GAP 7일)\n",
    "    train_idx, val_idx, test_idx = pipeline.time_series_split(\n",
    "        merged_df, \n",
    "        train_ratio=0.7, \n",
    "        val_ratio=0.15, \n",
    "        test_ratio=0.15,\n",
    "        gap_days=7\n",
    "    )\n",
    "    \n",
    "    # 6. 피처 엔지니어링 (수익률 기반)\n",
    "    df_final, train_idx, val_idx, test_idx, scaler, feature_cols = pipeline.engineer_features_with_returns(\n",
    "        merged_df, train_idx, val_idx, test_idx\n",
    "    )\n",
    "    \n",
    "    # 7. 최종 데이터셋\n",
    "    final_data = pipeline.prepare_final_datasets(df_final, train_idx, val_idx, test_idx, feature_cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b987c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## GEMINI version ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded1e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a2caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6181d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Claude version ###########################\n",
    "################################## Claude version ###########################\n",
    "################################## Claude version ###########################\n",
    "################################## Claude version ###########################\n",
    "################################## Claude version ###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57846435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "데이터 누수 방지 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "외부 데이터 병합 (Raw 데이터만)\n",
      "================================================================================\n",
      "기간: 2020-12-19 ~ 2025-10-03\n",
      "샘플: 1749개\n",
      "\n",
      "외부 데이터 병합 중...\n",
      "  - Fear&Greed: 1개 컬럼\n",
      "  - SP500: 1개 컬럼\n",
      "  - VIX: 1개 컬럼\n",
      "  - GOLD: 1개 컬럼\n",
      "  - DXY: 1개 컬럼\n",
      "  - Funding: 1개 컬럼\n",
      "  - ChainTVL: 1개 컬럼\n",
      "  - Makerdao: 1개 컬럼\n",
      "  - Lido: 1개 컬럼\n",
      "  - Aave: 1개 컬럼\n",
      "  - USDT_Total: 5개 컬럼\n",
      "  - USDT_ETH: 5개 컬럼\n",
      "  - GoogleTrends: 1개 컬럼\n",
      "  - OnChain: 10개 컬럼\n",
      "\n",
      "✓ Raw 데이터 병합 완료: 82개 컬럼\n",
      "\n",
      "================================================================================\n",
      "시계열 데이터 분할\n",
      "================================================================================\n",
      "Train: 1224개\n",
      "Val:   262개\n",
      "Test:  263개\n",
      "================================================================================\n",
      "\n",
      "Feature Engineering 중...\n",
      "   Target Leakage 방지: 기술적 지표 1일 shift 적용...\n",
      "   Target Leakage 방지: 기술적 지표 1일 shift 적용...\n",
      "   Target Leakage 방지: 기술적 지표 1일 shift 적용...\n",
      "✓ 특징 생성 완료: 207개 컬럼\n",
      "\n",
      "초기 200일 제거 (Train만)...\n",
      "  Train: 1024개 (lookback=200)\n",
      "  Val:   212개 (lookback=50)\n",
      "  Test:  213개 (lookback=50)\n",
      "\n",
      "결측치 처리...\n",
      "  특징 선택 (Train 기준): 193개 (제거: 8개)\n",
      "  Train: 1024개 샘플 (제거: 컬럼 0개, 행 0개)\n",
      "  Val: 212개 샘플 (제거: 컬럼 0개, 행 0개)\n",
      "  Test: 213개 샘플 (제거: 컬럼 5개, 행 0개)\n",
      "\n",
      "  공통 특징: 188개\n",
      "\n",
      "================================================================================\n",
      "완료!\n",
      "최종 특징 개수: 188개\n",
      "Train: X(1024, 188), y(1024,)\n",
      "Val:   X(212, 188), y(212,)\n",
      "Test:  X(213, 188), y(213,)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# =============================================================================\n",
    "# 1단계: 외부 데이터 병합 (파생 변수 생성 제외)\n",
    "# =============================================================================\n",
    "\n",
    "def merge_external_data_only(data, eth_data):\n",
    "    \"\"\"\n",
    "    외부 데이터만 병합 (파생 변수/Lag 생성 없음)\n",
    "    나중에 각 train/val/test에서 독립적으로 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"외부 데이터 병합 (Raw 데이터만)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    merged_df = eth_data.copy()\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date']).dt.tz_localize(None)\n",
    "    \n",
    "    # 날짜 범위 결정\n",
    "    date_ranges = {}\n",
    "    for key, df in data.items():\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            temp_df = df.copy()\n",
    "            temp_df[date_col] = pd.to_datetime(temp_df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            temp_df = temp_df.dropna(subset=[date_col])\n",
    "            if len(temp_df) > 0:\n",
    "                date_ranges[key] = {\n",
    "                    'start': temp_df[date_col].min(),\n",
    "                    'end': temp_df[date_col].max()\n",
    "                }\n",
    "    \n",
    "    start_dates = [info['start'] for info in date_ranges.values()]\n",
    "    filter_start = max(start_dates)\n",
    "    filter_end = \"2025-10-03\"\n",
    "    \n",
    "    merged_df = merged_df[(merged_df['Date'] >= filter_start) & (merged_df['Date'] <= filter_end)]\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"기간: {filter_start.date()} ~ {filter_end}\")\n",
    "    print(f\"샘플: {len(merged_df)}개\")\n",
    "    \n",
    "    # 병합 함수\n",
    "    def merge_with_ffill(merged_df, df, name=\"\"):\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) == 0:\n",
    "            return merged_df, 0\n",
    "        \n",
    "        date_col = date_cols[0]\n",
    "        df = df.copy()\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        df = df.rename(columns={date_col: 'Date'})\n",
    "        df = df[(df['Date'] >= filter_start) & (df['Date'] <= filter_end)]\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return merged_df, 0\n",
    "        \n",
    "        value_cols = [col for col in df.columns if col != 'Date']\n",
    "        before_cols = set(merged_df.columns)\n",
    "        merged_df = merged_df.merge(df, on='Date', how='left')\n",
    "        new_cols = list(set(merged_df.columns) - before_cols)\n",
    "        \n",
    "        for col in new_cols:\n",
    "            merged_df[col] = merged_df[col].fillna(method='ffill')\n",
    "        \n",
    "        print(f\"  - {name}: {len(value_cols)}개 컬럼\")\n",
    "        return merged_df, len(value_cols)\n",
    "    \n",
    "    # 각 데이터 소스 병합\n",
    "    print(\"\\n외부 데이터 병합 중...\")\n",
    "    \n",
    "    # Fear & Greed\n",
    "    if 'fear_greed' in data:\n",
    "        merged_df, _ = merge_with_ffill(merged_df, data['fear_greed'], \"Fear&Greed\")\n",
    "    \n",
    "    # 전통 금융\n",
    "    for name in ['sp500', 'vix', 'gold', 'dxy']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{name.upper()}_{col}\" for col in value_cols})\n",
    "                merged_df, _ = merge_with_ffill(merged_df, df, name.upper())\n",
    "    \n",
    "    # Funding Rate\n",
    "    if 'eth_funding' in data:\n",
    "        df = data['eth_funding'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"ETH_Funding_{col}\" for col in value_cols})\n",
    "            merged_df, _ = merge_with_ffill(merged_df, df, \"Funding\")\n",
    "    \n",
    "    # Chain TVL\n",
    "    if 'eth_chain_tvl' in data:\n",
    "        df = data['eth_chain_tvl'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"ETH_Chain_{col}\" for col in value_cols})\n",
    "            merged_df, _ = merge_with_ffill(merged_df, df, \"ChainTVL\")\n",
    "    \n",
    "    # DeFi TVL\n",
    "    for name in ['makerdao_tvl', 'lido_tvl', 'aave_tvl']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                protocol_name = name.split('_')[0].capitalize()\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{protocol_name}_{col}\" for col in value_cols})\n",
    "                merged_df, _ = merge_with_ffill(merged_df, df, protocol_name)\n",
    "    \n",
    "    # USDT\n",
    "    for name in ['usdt_total', 'usdt_eth']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                prefix = \"USDT_Total\" if 'total' in name else \"USDT_ETH\"\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{prefix}_{col}\" for col in value_cols})\n",
    "                merged_df, _ = merge_with_ffill(merged_df, df, prefix)\n",
    "    \n",
    "    # Google Trends\n",
    "    if 'google_trends_weekly' in data:\n",
    "        df = data['google_trends_weekly'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'week' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"Google_Trends_{col}\" for col in value_cols})\n",
    "            merged_df, _ = merge_with_ffill(merged_df, df, \"GoogleTrends\")\n",
    "    \n",
    "    # On-Chain\n",
    "    if 'eth_onchain' in data and len(data['eth_onchain']) > 0:\n",
    "        df = data['eth_onchain'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"OnChain_{col}\" for col in value_cols})\n",
    "            merged_df, _ = merge_with_ffill(merged_df, df, \"OnChain\")\n",
    "    \n",
    "    print(f\"\\n✓ Raw 데이터 병합 완료: {len(merged_df.columns)}개 컬럼\")\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2단계: 시계열 분할\n",
    "# =============================================================================\n",
    "\n",
    "def timeseries_train_val_test_split(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"시계열 순서 유지하면서 분할\"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"시계열 데이터 분할\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {len(train_df)}개\")\n",
    "    print(f\"Val:   {len(val_df)}개\")\n",
    "    print(f\"Test:  {len(test_df)}개\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3단계: Feature Engineering Transformer (누수 방지)\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    기술적 지표 + 파생 변수 + Lag Features 생성\n",
    "    각 데이터셋에서 독립적으로 계산\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lookback_max=200):\n",
    "        self.lookback_max = lookback_max\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit()을 먼저 호출하세요\")\n",
    "        \n",
    "        df = X.copy()\n",
    "        \n",
    "        # ========================================\n",
    "        # A. 기술적 지표 (OHLCV 기반)\n",
    "        # ========================================\n",
    "        required_cols = ['ETH_Open', 'ETH_High', 'ETH_Low', 'ETH_Close', 'ETH_Volume']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            raise ValueError(f\"필수 OHLCV 컬럼 없음\")\n",
    "        \n",
    "        df.rename(columns={\n",
    "            'ETH_Open': 'open', 'ETH_High': 'high', 'ETH_Low': 'low',\n",
    "            'ETH_Close': 'close', 'ETH_Volume': 'volume'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # RSI\n",
    "        for period in [9, 14, 21, 30]:\n",
    "            delta = df['close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "            rs = gain / loss\n",
    "            df[f'RSI_{period}'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        ema_12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "        ema_26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "        df['MACD_12_26_9'] = ema_12 - ema_26\n",
    "        df['MACDs_12_26_9'] = df['MACD_12_26_9'].ewm(span=9, adjust=False).mean()\n",
    "        df['MACDh_12_26_9'] = df['MACD_12_26_9'] - df['MACDs_12_26_9']\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        for period in [10, 20, 30]:\n",
    "            sma = df['close'].rolling(window=period).mean()\n",
    "            std = df['close'].rolling(window=period).std()\n",
    "            df[f'BBL_{period}'] = sma - (2 * std)\n",
    "            df[f'BBU_{period}'] = sma + (2 * std)\n",
    "        \n",
    "        # SMA/EMA\n",
    "        for period in [5, 10, 20, 50, 100, 200]:\n",
    "            df[f'SMA_{period}'] = df['close'].rolling(window=period).mean()\n",
    "            df[f'EMA_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "        \n",
    "        # ATR\n",
    "        for period in [7, 14]:\n",
    "            high_low = df['high'] - df['low']\n",
    "            high_close = (df['high'] - df['close'].shift()).abs()\n",
    "            low_close = (df['low'] - df['close'].shift()).abs()\n",
    "            tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "            df[f'ATR_{period}'] = tr.rolling(window=period).mean()\n",
    "        \n",
    "        # OBV\n",
    "        obv = [0]\n",
    "        for i in range(1, len(df)):\n",
    "            if df['close'].iloc[i] > df['close'].iloc[i-1]:\n",
    "                obv.append(obv[-1] + df['volume'].iloc[i])\n",
    "            elif df['close'].iloc[i] < df['close'].iloc[i-1]:\n",
    "                obv.append(obv[-1] - df['volume'].iloc[i])\n",
    "            else:\n",
    "                obv.append(obv[-1])\n",
    "        df['OBV'] = obv\n",
    "        \n",
    "        # ========================================\n",
    "        # D. TARGET LEAKAGE 방지: 모든 기술적 지표 shift(1)\n",
    "        # ========================================\n",
    "        # Day T 예측 시 Day T-1까지의 정보만 사용\n",
    "        print(\"   Target Leakage 방지: 기술적 지표 1일 shift 적용...\")\n",
    "        \n",
    "        technical_indicators = [\n",
    "            # 이동평균 (현재 행 포함하므로 shift 필수)\n",
    "            'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100', 'SMA_200',\n",
    "            'EMA_5', 'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100', 'EMA_200',\n",
    "            \n",
    "            # Bollinger Bands (이동평균 기반이므로 shift 필수)\n",
    "            'BBL_10', 'BBL_20', 'BBL_30',\n",
    "            'BBU_10', 'BBU_20', 'BBU_30',\n",
    "            \n",
    "            # RSI (현재 행 diff 포함하므로 shift 필수)\n",
    "            'RSI_9', 'RSI_14', 'RSI_21', 'RSI_30',\n",
    "            \n",
    "            # MACD (EMA 기반이므로 shift 필수)\n",
    "            'MACD_12_26_9', 'MACDs_12_26_9', 'MACDh_12_26_9',\n",
    "            \n",
    "            # ATR (현재 행 high/low 포함하므로 shift 필수)\n",
    "            'ATR_7', 'ATR_14',\n",
    "            \n",
    "            # OBV (현재 행 volume 포함하므로 shift 필수)\n",
    "            'OBV'\n",
    "        ]\n",
    "        \n",
    "        for indicator in technical_indicators:\n",
    "            if indicator in df.columns:\n",
    "                df[indicator] = df[indicator].shift(1)\n",
    "        \n",
    "        df.rename(columns={\n",
    "            'open': 'ETH_Open', 'high': 'ETH_High', 'low': 'ETH_Low',\n",
    "            'close': 'ETH_Close', 'volume': 'ETH_Volume'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # ========================================\n",
    "        # B. 외부 데이터 파생 변수 (shift 적용)\n",
    "        # ========================================\n",
    "        \n",
    "        # Google Trends 파생\n",
    "        trend_cols = [col for col in df.columns if 'Google_Trends' in col \n",
    "                      and 'Change' not in col and 'MA' not in col and 'lag' not in col]\n",
    "        if len(trend_cols) > 0:\n",
    "            col = trend_cols[0]\n",
    "            df[f'{col}_Change_7d'] = df[col].pct_change(7).shift(1)  # shift 추가\n",
    "            df[f'{col}_MA_7'] = df[col].rolling(7).mean().shift(1)   # shift 추가\n",
    "        \n",
    "        # Fear & Greed 파생\n",
    "        fg_cols = [col for col in df.columns if col.lower() == 'value' \n",
    "                   or ('fear' in col.lower() and 'Change' not in col and 'lag' not in col)]\n",
    "        if len(fg_cols) > 0:\n",
    "            col = fg_cols[0]\n",
    "            df['FearGreed_Change_1d'] = df[col].diff().shift(1)       # shift 추가\n",
    "            df['FearGreed_MA_7'] = df[col].rolling(7).mean().shift(1) # shift 추가\n",
    "        \n",
    "        # TVL 파생\n",
    "        tvl_cols = [col for col in df.columns if ('TVL' in col or 'tvl' in col) \n",
    "                    and 'Change' not in col and 'lag' not in col]\n",
    "        for col in tvl_cols[:3]:\n",
    "            df[f'{col}_Change_7d'] = df[col].pct_change(7).shift(1)   # shift 추가\n",
    "        \n",
    "        # ========================================\n",
    "        # C. Lag Features\n",
    "        # ========================================\n",
    "        lag_periods = [1, 3, 7]\n",
    "        external_cols = [col for col in df.columns \n",
    "                         if any(x in col for x in ['SP500', 'VIX', 'GOLD', 'DXY', \n",
    "                                                    'OnChain', 'Funding', 'value',\n",
    "                                                    'USDT', 'Google_Trends',\n",
    "                                                    'Makerdao', 'Lido', 'Aave', 'ETH_Chain'])\n",
    "                         and 'Change' not in col and 'MA' not in col and 'lag' not in col]\n",
    "        \n",
    "        for col in external_cols:\n",
    "            for lag in lag_periods:\n",
    "                df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4단계: 최종 파이프라인\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_without_leakage(data, eth_ohlcv):\n",
    "    \"\"\"\n",
    "    완전한 데이터 누수 방지 파이프라인\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"데이터 누수 방지 파이프라인 시작\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. 외부 데이터 병합 (파생 변수 없이)\n",
    "    raw_merged = merge_external_data_only(data, eth_ohlcv)\n",
    "    \n",
    "    # 2. 시계열 분할\n",
    "    train_df, val_df, test_df = timeseries_train_val_test_split(raw_merged)\n",
    "    \n",
    "    # 3. 각 셋에 독립적으로 Feature Engineering\n",
    "    print(\"Feature Engineering 중...\")\n",
    "    \n",
    "    fe_transformer = FeatureEngineeringTransformer(lookback_max=200)\n",
    "    fe_transformer.fit(train_df)\n",
    "    \n",
    "    train_df = fe_transformer.transform(train_df)\n",
    "    val_df = fe_transformer.transform(val_df)\n",
    "    test_df = fe_transformer.transform(test_df)\n",
    "    \n",
    "    print(f\"✓ 특징 생성 완료: {len(train_df.columns)}개 컬럼\")\n",
    "    \n",
    "    # 4. Lookback 기간 제거 (Train만 적용)\n",
    "    lookback = 200\n",
    "    print(f\"\\n초기 {lookback}일 제거 (Train만)...\")\n",
    "    train_df = train_df.iloc[lookback:].reset_index(drop=True)\n",
    "    \n",
    "    # Val/Test는 최소 lookback만큼만 제거 (데이터 손실 최소화)\n",
    "    min_lookback = min(50, len(val_df) // 3, len(test_df) // 3)\n",
    "    val_df = val_df.iloc[min_lookback:].reset_index(drop=True)\n",
    "    test_df = test_df.iloc[min_lookback:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)}개 (lookback={lookback})\")\n",
    "    print(f\"  Val:   {len(val_df)}개 (lookback={min_lookback})\")\n",
    "    print(f\"  Test:  {len(test_df)}개 (lookback={min_lookback})\")\n",
    "    \n",
    "    # 5. X, y 분리\n",
    "    features_to_exclude = ['Date', 'ETH_Open', 'ETH_High', 'ETH_Low', \n",
    "                           'ETH_Close', 'ETH_Volume']\n",
    "    feature_cols = [col for col in train_df.columns if col not in features_to_exclude]\n",
    "    \n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df['ETH_Close'].copy()\n",
    "    \n",
    "    X_val = val_df[feature_cols].copy()\n",
    "    y_val = val_df['ETH_Close'].copy()\n",
    "    \n",
    "    X_test = test_df[feature_cols].copy()\n",
    "    y_test = test_df['ETH_Close'].copy()\n",
    "    \n",
    "    # 6. 결측치 처리 (Train 기준으로 특징 선택)\n",
    "    print(\"\\n결측치 처리...\")\n",
    "    \n",
    "    # 6-1. Train에서 유효한 특징 선택\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "    missing_ratio_train = X_train.isnull().sum() / len(X_train)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    valid_features = missing_ratio_train[missing_ratio_train < threshold].index.tolist()\n",
    "    \n",
    "    print(f\"  특징 선택 (Train 기준): {len(valid_features)}개 (제거: {len(X_train.columns) - len(valid_features)}개)\")\n",
    "    \n",
    "    # 6-2. 모든 셋에 동일한 특징만 적용\n",
    "    X_train = X_train[valid_features]\n",
    "    X_val = X_val[valid_features]\n",
    "    X_test = X_test[valid_features]\n",
    "    \n",
    "    # 6-3. 각 셋에서 결측치 처리 (더 관대한 방식)\n",
    "    def interpolate_missing(X_set, y_set, set_name):\n",
    "        X_set = X_set.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # 먼저 컬럼별로 결측치가 너무 많은지 체크\n",
    "        col_missing = X_set.isnull().sum() / len(X_set)\n",
    "        cols_to_keep = col_missing[col_missing < 0.8].index.tolist()  # 80% 이상 결측은 제거\n",
    "        X_set = X_set[cols_to_keep]\n",
    "        \n",
    "        # Interpolation (더 적극적)\n",
    "        X_set = X_set.interpolate(method='linear', limit_direction='both', limit=None)\n",
    "        X_set = X_set.fillna(method='ffill', limit=None)\n",
    "        X_set = X_set.fillna(method='bfill', limit=None)\n",
    "        \n",
    "        # 마지막으로 컬럼 평균으로 채우기\n",
    "        X_set = X_set.fillna(X_set.mean())\n",
    "        \n",
    "        # 여전히 NaN 있는 행만 제거\n",
    "        valid_indices = X_set.dropna().index\n",
    "        X_set = X_set.loc[valid_indices]\n",
    "        y_set = y_set.loc[valid_indices]\n",
    "        \n",
    "        removed_cols = len(valid_features) - len(cols_to_keep)\n",
    "        removed_rows = len(X_set.index.union(valid_indices)) - len(valid_indices)\n",
    "        \n",
    "        print(f\"  {set_name}: {len(X_set)}개 샘플 (제거: 컬럼 {removed_cols}개, 행 {removed_rows}개)\")\n",
    "        return X_set, y_set, cols_to_keep\n",
    "    \n",
    "    X_train, y_train, train_cols = interpolate_missing(X_train, y_train, \"Train\")\n",
    "    X_val, y_val, val_cols = interpolate_missing(X_val, y_val, \"Val\")\n",
    "    X_test, y_test, test_cols = interpolate_missing(X_test, y_test, \"Test\")\n",
    "    \n",
    "    # 6-4. 공통 특징으로 정렬\n",
    "    common_features = list(set(train_cols) & set(val_cols) & set(test_cols))\n",
    "    print(f\"\\n  공통 특징: {len(common_features)}개\")\n",
    "    \n",
    "    X_train = X_train[common_features]\n",
    "    X_val = X_val[common_features]\n",
    "    X_test = X_test[common_features]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"완료!\")\n",
    "    print(f\"최종 특징 개수: {len(common_features)}개\")\n",
    "    print(f\"Train: X{X_train.shape}, y{y_train.shape}\")\n",
    "    print(f\"Val:   X{X_val.shape}, y{y_val.shape}\")\n",
    "    print(f\"Test:  X{X_test.shape}, y{y_test.shape}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 사용 예시\n",
    "# =============================================================================\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) =  prepare_data_without_leakage(data, data['macro_crypto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7c399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6b76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558fd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32c46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703da024",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## 클로드 ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c3e7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 193)\n",
      "  Val:   (262, 193)\n",
      "  Test:  (263, 193)\n",
      "\n",
      "[Step 1/3] Feature Selection...\n",
      "\n",
      "================================================================================\n",
      "Feature Selection (Train Only)\n",
      "================================================================================\n",
      "\n",
      "1. Random Forest 기반 특징 중요도...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "2. Mutual Information 계산...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "   교집합 11개 → 합집합 사용\n",
      "\n",
      "✓ 최종 선택: 30개 특징\n",
      "\n",
      "Top 10 특징:\n",
      "   1. EMA_5\n",
      "   2. SMA_5\n",
      "   3. EMA_10\n",
      "   4. BTC_Close\n",
      "   5. BBL_10\n",
      "   6. BBU_10\n",
      "   7. SMA_10\n",
      "   8. BTC_High\n",
      "   9. EMA_20\n",
      "   10. BBL_20\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "Hyperparameter Tuning (Train → Val)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능\n",
      "\n",
      "총 조합: 108개\n",
      "Train 크기: 1024\n",
      "Val 크기: 262\n",
      "  [10/108] 현재 최고 - R²: 0.9485, MAPE: 0.0287\n",
      "  [20/108] 현재 최고 - R²: 0.9485, MAPE: 0.0287\n",
      "  [30/108] 현재 최고 - R²: 0.9485, MAPE: 0.0287\n",
      "  [40/108] 현재 최고 - R²: 0.9485, MAPE: 0.0287\n",
      "  [50/108] 현재 최고 - R²: 0.9498, MAPE: 0.0283\n",
      "  [60/108] 현재 최고 - R²: 0.9498, MAPE: 0.0283\n",
      "  [70/108] 현재 최고 - R²: 0.9498, MAPE: 0.0283\n",
      "  [80/108] 현재 최고 - R²: 0.9498, MAPE: 0.0283\n",
      "  [90/108] 현재 최고 - R²: 0.9506, MAPE: 0.0281\n",
      "  [100/108] 현재 최고 - R²: 0.9506, MAPE: 0.0281\n",
      "  [108/108] 현재 최고 - R²: 0.9506, MAPE: 0.0281\n",
      "\n",
      "================================================================================\n",
      "최적 파라미터:\n",
      "  n_estimators: 300\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.1\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 1.0\n",
      "\n",
      "Val 성능: R² 0.9506, MAPE 0.0281\n",
      "================================================================================\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.007707 0.999434  21.335791  16.196025\n",
      "  Val 0.028059 0.950570 108.389781  83.408560\n",
      " Test 0.074152 0.951501 207.813190 175.666763\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.0489\n",
      "  Train → Test R² 하락: 0.0479\n",
      "  ✓ 과적합 없음, 일반화 성능 양호\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                    feature  importance\n",
      "                      SMA_5    0.577602\n",
      "                      EMA_5    0.371297\n",
      "    ETH_Chain_eth_chain_tvl    0.021961\n",
      "                  BTC_Close    0.006442\n",
      "          Lido_lido_eth_tvl    0.006346\n",
      "                  SOL_Close    0.001880\n",
      "                   BTC_High    0.001693\n",
      "                    EMA_100    0.001493\n",
      "USDT_Total_totalCirculating    0.001459\n",
      "                    SMA_200    0.000859\n",
      "                    BTC_Low    0.000746\n",
      "                     BBU_30    0.000630\n",
      "                     BBU_10    0.000580\n",
      "    USDT_ETH_totalMintedUSD    0.000572\n",
      "                     SMA_10    0.000572\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 193)\n",
      "  Val:   (262, 193)\n",
      "  Test:  (263, 193)\n",
      "\n",
      "[Step 1/3] Feature Selection...\n",
      "\n",
      "================================================================================\n",
      "Feature Selection (Train Only)\n",
      "================================================================================\n",
      "\n",
      "1. Random Forest 기반 특징 중요도...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "2. Mutual Information 계산...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "   교집합 11개 → 합집합 사용\n",
      "\n",
      "✓ 최종 선택: 30개 특징\n",
      "\n",
      "Top 10 특징:\n",
      "   1. EMA_5\n",
      "   2. SMA_5\n",
      "   3. EMA_10\n",
      "   4. BTC_Close\n",
      "   5. BBL_10\n",
      "   6. BBU_10\n",
      "   7. SMA_10\n",
      "   8. BTC_High\n",
      "   9. EMA_20\n",
      "   10. BBL_20\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning 생략, 기본값 사용\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.006574 0.999593  18.089424  13.777562\n",
      "  Val 0.034943 0.925408 133.149588 108.139134\n",
      " Test 0.060043 0.963018 181.467280 151.387253\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.0742\n",
      "  Train → Test R² 하락: 0.0366\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                    feature  importance\n",
      "                      EMA_5    0.759145\n",
      "                      SMA_5    0.179930\n",
      "          Lido_lido_eth_tvl    0.027036\n",
      "                  BTC_Close    0.004644\n",
      "    ETH_Chain_eth_chain_tvl    0.003100\n",
      "                   BTC_High    0.002670\n",
      "                  SOL_Close    0.002643\n",
      "USDT_Total_totalCirculating    0.002226\n",
      "                     SMA_20    0.001775\n",
      "                    EMA_100    0.001648\n",
      "                     BBU_30    0.001353\n",
      "                    BTC_Low    0.001331\n",
      "                     BBU_10    0.001208\n",
      "                     EMA_50    0.000980\n",
      "                     SMA_10    0.000887\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 193)\n",
      "  Val:   (262, 193)\n",
      "  Test:  (263, 193)\n",
      "\n",
      "[Step 1/3] Feature Selection 생략, 모든 특징 사용\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "Hyperparameter Tuning (Train → Val)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능\n",
      "\n",
      "총 조합: 108개\n",
      "Train 크기: 1024\n",
      "Val 크기: 262\n",
      "  [10/108] 현재 최고 - R²: 0.9382, MAPE: 0.0341\n",
      "  [20/108] 현재 최고 - R²: 0.9409, MAPE: 0.0333\n",
      "  [30/108] 현재 최고 - R²: 0.9409, MAPE: 0.0333\n",
      "  [40/108] 현재 최고 - R²: 0.9409, MAPE: 0.0333\n",
      "  [50/108] 현재 최고 - R²: 0.9414, MAPE: 0.0334\n",
      "  [60/108] 현재 최고 - R²: 0.9414, MAPE: 0.0334\n",
      "  [70/108] 현재 최고 - R²: 0.9414, MAPE: 0.0334\n",
      "  [80/108] 현재 최고 - R²: 0.9414, MAPE: 0.0334\n",
      "  [90/108] 현재 최고 - R²: 0.9415, MAPE: 0.0334\n",
      "  [100/108] 현재 최고 - R²: 0.9415, MAPE: 0.0334\n",
      "  [108/108] 현재 최고 - R²: 0.9415, MAPE: 0.0334\n",
      "\n",
      "================================================================================\n",
      "최적 파라미터:\n",
      "  n_estimators: 300\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.1\n",
      "  subsample: 1.0\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "Val 성능: R² 0.9415, MAPE 0.0334\n",
      "================================================================================\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.004334 0.999827  11.779884   9.070123\n",
      "  Val 0.033444 0.941481 117.934948  99.038977\n",
      " Test 0.064641 0.958410 192.441699 167.648099\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.0583\n",
      "  Train → Test R² 하락: 0.0414\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                    feature  importance\n",
      "                      EMA_5    0.564335\n",
      "                      SMA_5    0.294018\n",
      "          Lido_lido_eth_tvl    0.036061\n",
      "                  BNB_Close    0.020418\n",
      "                    BTC_Low    0.013360\n",
      "                  BTC_Close    0.011577\n",
      "                   BTC_High    0.008969\n",
      "    ETH_Chain_eth_chain_tvl    0.006073\n",
      "                        OBV    0.005993\n",
      "                     BBU_20    0.004303\n",
      "                  SOL_Close    0.002164\n",
      "                    VIX_VIX    0.001685\n",
      "                      RSI_9    0.001535\n",
      "                  DOT_Close    0.001486\n",
      "OnChain_avg_block_size_lag1    0.001455\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 193)\n",
      "  Val:   (262, 193)\n",
      "  Test:  (263, 193)\n",
      "\n",
      "[Step 1/3] Feature Selection 생략, 모든 특징 사용\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning 생략, 기본값 사용\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.003088 0.999917   8.181412   6.390620\n",
      "  Val 0.035077 0.917333 140.171351 110.926610\n",
      " Test 0.042784 0.974497 150.696741 121.238011\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.0826\n",
      "  Train → Test R² 하락: 0.0254\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                     feature  importance\n",
      "                       SMA_5    0.757796\n",
      "                       EMA_5    0.195651\n",
      "           Lido_lido_eth_tvl    0.021233\n",
      "                   BTC_Close    0.001998\n",
      "                    BTC_High    0.001837\n",
      "                   BNB_Close    0.001520\n",
      "                      EMA_10    0.000952\n",
      "                   SOL_Close    0.000793\n",
      "                         OBV    0.000782\n",
      "                     SMA_100    0.000711\n",
      "                     BNB_Low    0.000580\n",
      "                     VIX_VIX    0.000531\n",
      "                    DOGE_Low    0.000511\n",
      "                      BBU_10    0.000509\n",
      "ETH_Chain_eth_chain_tvl_lag3    0.000432\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "Test R²: 0.9745\n",
      "Test MAPE: 0.0428\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Feature Selection (Train만 사용)\n",
    "# =============================================================================\n",
    "\n",
    "def feature_selection_train_only(X_train, y_train, top_n=30):\n",
    "    \"\"\"\n",
    "    Train 데이터만으로 특징 선택\n",
    "    Val 데이터는 절대 사용하지 않음\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Feature Selection (Train Only)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Random Forest Importance\n",
    "    print(\"\\n1. Random Forest 기반 특징 중요도...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    rf_top_features = rf_importance.head(top_n)['feature'].tolist()\n",
    "    print(f\"   Top {len(rf_top_features)}개 특징 선택\")\n",
    "    \n",
    "    # 2. Mutual Information\n",
    "    print(\"\\n2. Mutual Information 계산...\")\n",
    "    mi_scores = mutual_info_regression(X_train, y_train, random_state=42)\n",
    "    \n",
    "    mi_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'mi_score': mi_scores\n",
    "    }).sort_values('mi_score', ascending=False)\n",
    "    \n",
    "    mi_top_features = mi_importance.head(top_n)['feature'].tolist()\n",
    "    print(f\"   Top {len(mi_top_features)}개 특징 선택\")\n",
    "    \n",
    "    # 3. 교집합 특징 선택\n",
    "    common_features = list(set(rf_top_features) & set(mi_top_features))\n",
    "    \n",
    "    if len(common_features) < 15:\n",
    "        # 교집합이 너무 적으면 합집합 사용\n",
    "        print(f\"\\n   교집합 {len(common_features)}개 → 합집합 사용\")\n",
    "        combined_features = list(set(rf_top_features) | set(mi_top_features))\n",
    "        \n",
    "        # 중요도 점수 합산\n",
    "        feature_scores = {}\n",
    "        for feat in combined_features:\n",
    "            rf_score = rf_importance[rf_importance['feature'] == feat]['importance'].values\n",
    "            mi_score = mi_importance[mi_importance['feature'] == feat]['mi_score'].values\n",
    "            \n",
    "            rf_val = rf_score[0] if len(rf_score) > 0 else 0\n",
    "            mi_val = mi_score[0] if len(mi_score) > 0 else 0\n",
    "            \n",
    "            feature_scores[feat] = rf_val + mi_val\n",
    "        \n",
    "        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_features = [f[0] for f in sorted_features[:top_n]]\n",
    "    else:\n",
    "        selected_features = common_features[:top_n]\n",
    "    \n",
    "    print(f\"\\n✓ 최종 선택: {len(selected_features)}개 특징\")\n",
    "    print(\"\\nTop 10 특징:\")\n",
    "    for i, feat in enumerate(selected_features[:10], 1):\n",
    "        print(f\"   {i}. {feat}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Hyperparameter Tuning (Train 학습, Val 검증)\n",
    "# =============================================================================\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val, features):\n",
    "    \"\"\"\n",
    "    Train으로 학습, Val로 검증\n",
    "    Grid Search 방식\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Hyperparameter Tuning (Train → Val)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # GPU 체크\n",
    "    try:\n",
    "        test_model = XGBRegressor(tree_method='gpu_hist', n_estimators=10)\n",
    "        test_model.fit(X_train[features].head(100), y_train.head(100))\n",
    "        print(\"✓ GPU 사용 가능\")\n",
    "        tree_method = 'gpu_hist'\n",
    "    except:\n",
    "        print(\"✗ GPU 사용 불가 → CPU 사용\")\n",
    "        tree_method = 'hist'\n",
    "    \n",
    "    # 탐색 공간\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    keys = list(param_grid.keys())\n",
    "    all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "    \n",
    "    print(f\"\\n총 조합: {len(all_combinations)}개\")\n",
    "    print(f\"Train 크기: {len(X_train)}\")\n",
    "    print(f\"Val 크기: {len(X_val)}\")\n",
    "    \n",
    "    best_params = None\n",
    "    best_r2 = -np.inf\n",
    "    best_mape = np.inf\n",
    "    \n",
    "    for idx, combination in enumerate(all_combinations):\n",
    "        params = dict(zip(keys, combination))\n",
    "        params['tree_method'] = tree_method\n",
    "        params['random_state'] = 42\n",
    "        params['n_jobs'] = -1\n",
    "        \n",
    "        # Train으로만 학습\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train[features], y_train, verbose=False)\n",
    "        \n",
    "        # Val로 검증\n",
    "        y_val_pred = model.predict(X_val[features])\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "        \n",
    "        # R² 우선, MAPE 보조\n",
    "        if r2 > best_r2 or (r2 == best_r2 and mape < best_mape):\n",
    "            best_r2 = r2\n",
    "            best_mape = mape\n",
    "            best_params = params.copy()\n",
    "        \n",
    "        if (idx + 1) % 10 == 0 or idx == len(all_combinations) - 1:\n",
    "            print(f\"  [{idx+1}/{len(all_combinations)}] 현재 최고 - R²: {best_r2:.4f}, MAPE: {best_mape:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"최적 파라미터:\")\n",
    "    for k, v in best_params.items():\n",
    "        if k not in ['tree_method', 'random_state', 'n_jobs']:\n",
    "            print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nVal 성능: R² {best_r2:.4f}, MAPE {best_mape:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 최종 모델 학습 및 평가\n",
    "# =============================================================================\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                       features, params):\n",
    "    \"\"\"\n",
    "    Train으로 학습, Val/Test로 평가\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"최종 모델 학습 및 평가\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 모델 학습 (Train만)\n",
    "    print(\"\\nTrain 데이터로 학습 중...\")\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train[features], y_train, verbose=False)\n",
    "    \n",
    "    # 예측\n",
    "    y_train_pred = model.predict(X_train[features])\n",
    "    y_val_pred = model.predict(X_val[features])\n",
    "    y_test_pred = model.predict(X_test[features])\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    def calculate_metrics(y_true, y_pred, set_name):\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        \n",
    "        return {\n",
    "            'set': set_name,\n",
    "            'MAPE': mape,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae\n",
    "        }\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, 'Train')\n",
    "    val_metrics = calculate_metrics(y_val, y_val_pred, 'Val')\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, 'Test')\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"성능 평가 결과\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_df = pd.DataFrame([train_metrics, val_metrics, test_metrics])\n",
    "    print(\"\\n\" + results_df.to_string(index=False))\n",
    "    \n",
    "    # 과적합 체크\n",
    "    print(\"\\n과적합 분석:\")\n",
    "    r2_drop_val = train_metrics['R²'] - val_metrics['R²']\n",
    "    r2_drop_test = train_metrics['R²'] - test_metrics['R²']\n",
    "    \n",
    "    print(f\"  Train → Val R² 하락: {r2_drop_val:.4f}\")\n",
    "    print(f\"  Train → Test R² 하락: {r2_drop_test:.4f}\")\n",
    "    \n",
    "    if r2_drop_val > 0.1:\n",
    "        print(\"  ⚠️ Val 과적합 가능성 있음\")\n",
    "    if r2_drop_test > 0.1:\n",
    "        print(\"  ⚠️ Test 과적합 가능성 있음\")\n",
    "    if r2_drop_val < 0.05 and r2_drop_test < 0.05:\n",
    "        print(\"  ✓ 과적합 없음, 일반화 성능 양호\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Feature Importance Top 15\")\n",
    "    print(\"=\"*80)\n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    return model, {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics\n",
    "    }, importance_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 전체 파이프라인\n",
    "# =============================================================================\n",
    "\n",
    "def run_proper_pipeline(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                       select_features=True, tune_hyperparams=True, top_n=30):\n",
    "    \"\"\"\n",
    "    데이터 누수 없는 올바른 ML 파이프라인\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    select_features : bool\n",
    "        특징 선택 수행 여부\n",
    "    tune_hyperparams : bool\n",
    "        하이퍼파라미터 튜닝 수행 여부\n",
    "    top_n : int\n",
    "        선택할 특징 개수\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"올바른 ML 파이프라인 시작\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n데이터 크기:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Val:   {X_val.shape}\")\n",
    "    print(f\"  Test:  {X_test.shape}\")\n",
    "    \n",
    "    # Step 1: Feature Selection (Train만 사용)\n",
    "    if select_features:\n",
    "        print(\"\\n[Step 1/3] Feature Selection...\")\n",
    "        selected_features = feature_selection_train_only(X_train, y_train, top_n=top_n)\n",
    "    else:\n",
    "        print(\"\\n[Step 1/3] Feature Selection 생략, 모든 특징 사용\")\n",
    "        selected_features = X_train.columns.tolist()\n",
    "    \n",
    "    # Step 2: Hyperparameter Tuning (Train 학습, Val 검증)\n",
    "    if tune_hyperparams:\n",
    "        print(\"\\n[Step 2/3] Hyperparameter Tuning...\")\n",
    "        best_params = hyperparameter_tuning(\n",
    "            X_train, y_train, X_val, y_val, selected_features\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[Step 2/3] Hyperparameter Tuning 생략, 기본값 사용\")\n",
    "        best_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 1.0,\n",
    "            'colsample_bytree': 1.0,\n",
    "            'tree_method': 'hist',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    \n",
    "    # Step 3: 최종 학습 및 평가\n",
    "    print(\"\\n[Step 3/3] 최종 모델 학습 및 평가...\")\n",
    "    model, metrics, importance_df = train_and_evaluate(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        selected_features, best_params\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"파이프라인 완료!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'features': selected_features,\n",
    "        'params': best_params,\n",
    "        'metrics': metrics,\n",
    "        'importance': importance_df,\n",
    "        'predictions': {\n",
    "            'train': model.predict(X_train[selected_features]),\n",
    "            'val': model.predict(X_val[selected_features]),\n",
    "            'test': model.predict(X_test[selected_features])\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d50451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 188)\n",
      "  Val:   (212, 188)\n",
      "  Test:  (213, 188)\n",
      "\n",
      "[Step 1/3] Feature Selection...\n",
      "\n",
      "================================================================================\n",
      "Feature Selection (Train Only)\n",
      "================================================================================\n",
      "\n",
      "1. Random Forest 기반 특징 중요도...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "2. Mutual Information 계산...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "   교집합 11개 → 합집합 사용\n",
      "\n",
      "✓ 최종 선택: 30개 특징\n",
      "\n",
      "Top 10 특징:\n",
      "   1. EMA_5\n",
      "   2. SMA_5\n",
      "   3. BTC_Close\n",
      "   4. EMA_10\n",
      "   5. SMA_10\n",
      "   6. BTC_High\n",
      "   7. BBL_10\n",
      "   8. ETH_Chain_eth_chain_tvl\n",
      "   9. BBU_10\n",
      "   10. BTC_Low\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "Hyperparameter Tuning (Train → Val)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능\n",
      "\n",
      "총 조합: 108개\n",
      "Train 크기: 1024\n",
      "Val 크기: 212\n",
      "  [10/108] 현재 최고 - R²: 0.8685, MAPE: 0.0506\n",
      "  [20/108] 현재 최고 - R²: 0.8685, MAPE: 0.0506\n",
      "  [30/108] 현재 최고 - R²: 0.8685, MAPE: 0.0506\n",
      "  [40/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [50/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [60/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [70/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [80/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [90/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [100/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "  [108/108] 현재 최고 - R²: 0.8722, MAPE: 0.0480\n",
      "\n",
      "================================================================================\n",
      "최적 파라미터:\n",
      "  n_estimators: 200\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.01\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 1.0\n",
      "\n",
      "Val 성능: R² 0.8722, MAPE 0.0480\n",
      "================================================================================\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.058004 0.968631 158.842280 122.617215\n",
      "  Val 0.047973 0.872223 172.776453 140.476810\n",
      " Test 0.112851 0.889887 342.682078 294.876986\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.0964\n",
      "  Train → Test R² 하락: 0.0787\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                    feature  importance\n",
      "                      SMA_5    0.391446\n",
      "    ETH_Chain_eth_chain_tvl    0.300156\n",
      "                      EMA_5    0.187738\n",
      "          Lido_lido_eth_tvl    0.020359\n",
      "                   BTC_High    0.016040\n",
      "                    BTC_Low    0.013286\n",
      "                  BTC_Close    0.012486\n",
      "                     BBU_20    0.009642\n",
      "                     EMA_10    0.007047\n",
      "                  SOL_Close    0.005384\n",
      "    USDT_ETH_totalMintedUSD    0.004793\n",
      "                   SOL_High    0.004262\n",
      "                     BBL_10    0.004233\n",
      "USDT_Total_totalCirculating    0.004227\n",
      "  USDT_ETH_totalCirculating    0.003904\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "Test R²: 0.8899\n",
      "Test MAPE: 0.1129\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 188)\n",
      "  Val:   (212, 188)\n",
      "  Test:  (213, 188)\n",
      "\n",
      "[Step 1/3] Feature Selection...\n",
      "\n",
      "================================================================================\n",
      "Feature Selection (Train Only)\n",
      "================================================================================\n",
      "\n",
      "1. Random Forest 기반 특징 중요도...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "2. Mutual Information 계산...\n",
      "   Top 30개 특징 선택\n",
      "\n",
      "   교집합 11개 → 합집합 사용\n",
      "\n",
      "✓ 최종 선택: 30개 특징\n",
      "\n",
      "Top 10 특징:\n",
      "   1. EMA_5\n",
      "   2. SMA_5\n",
      "   3. BTC_Close\n",
      "   4. EMA_10\n",
      "   5. SMA_10\n",
      "   6. BTC_High\n",
      "   7. BBL_10\n",
      "   8. ETH_Chain_eth_chain_tvl\n",
      "   9. BBU_10\n",
      "   10. BTC_Low\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning 생략, 기본값 사용\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.007898 0.999399  21.983312  16.508056\n",
      "  Val 0.067078 0.763371 235.121349 192.479178\n",
      " Test 0.133455 0.864628 379.959015 324.036448\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.2360\n",
      "  Train → Test R² 하락: 0.1348\n",
      "  ⚠️ Val 과적합 가능성 있음\n",
      "  ⚠️ Test 과적합 가능성 있음\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                    feature  importance\n",
      "                      EMA_5    0.498942\n",
      "                      SMA_5    0.316173\n",
      "    ETH_Chain_eth_chain_tvl    0.111476\n",
      "          Lido_lido_eth_tvl    0.018798\n",
      "                   BTC_High    0.011004\n",
      "                    BTC_Low    0.010791\n",
      "                  BTC_Close    0.007614\n",
      "                     BBU_20    0.004892\n",
      "                  SOL_Close    0.004062\n",
      "USDT_Total_totalCirculating    0.001853\n",
      "                     EMA_50    0.001420\n",
      "                     SMA_20    0.001390\n",
      "                    SMA_200    0.001348\n",
      "                     BBL_20    0.001209\n",
      "                    EMA_100    0.000913\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "Test R²: 0.8646\n",
      "Test MAPE: 0.1335\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 188)\n",
      "  Val:   (212, 188)\n",
      "  Test:  (213, 188)\n",
      "\n",
      "[Step 1/3] Feature Selection 생략, 모든 특징 사용\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning...\n",
      "\n",
      "================================================================================\n",
      "Hyperparameter Tuning (Train → Val)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능\n",
      "\n",
      "총 조합: 108개\n",
      "Train 크기: 1024\n",
      "Val 크기: 212\n",
      "  [10/108] 현재 최고 - R²: 0.7914, MAPE: 0.0662\n",
      "  [20/108] 현재 최고 - R²: 0.7914, MAPE: 0.0662\n",
      "  [30/108] 현재 최고 - R²: 0.7914, MAPE: 0.0662\n",
      "  [40/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [50/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [60/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [70/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [80/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [90/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [100/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "  [108/108] 현재 최고 - R²: 0.8043, MAPE: 0.0631\n",
      "\n",
      "================================================================================\n",
      "최적 파라미터:\n",
      "  n_estimators: 200\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.01\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 1.0\n",
      "\n",
      "Val 성능: R² 0.8043, MAPE 0.0631\n",
      "================================================================================\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n",
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.057514 0.970078 155.135190 120.858551\n",
      "  Val 0.063092 0.804257 213.845618 179.853692\n",
      " Test 0.145269 0.829592 426.301564 378.630704\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.1658\n",
      "  Train → Test R² 하락: 0.1405\n",
      "  ⚠️ Val 과적합 가능성 있음\n",
      "  ⚠️ Test 과적합 가능성 있음\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                      feature  importance\n",
      " ETH_Chain_eth_chain_tvl_lag1    0.428898\n",
      "      ETH_Chain_eth_chain_tvl    0.157047\n",
      "                        SMA_5    0.140340\n",
      "                        EMA_5    0.086054\n",
      "                    BNB_Close    0.036881\n",
      "                     BNB_High    0.012928\n",
      "       Lido_lido_eth_tvl_lag1    0.009744\n",
      "USDT_ETH_totalUnreleased_lag1    0.008758\n",
      "            Lido_lido_eth_tvl    0.008600\n",
      "                      BTC_Low    0.008260\n",
      "USDT_ETH_totalUnreleased_lag3    0.006898\n",
      "     USDT_ETH_totalUnreleased    0.006804\n",
      "                     BTC_High    0.006173\n",
      "                    BTC_Close    0.006147\n",
      "       OnChain_avg_block_size    0.005891\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "Test R²: 0.8296\n",
      "Test MAPE: 0.1453\n",
      "\n",
      "================================================================================\n",
      "올바른 ML 파이프라인 시작\n",
      "================================================================================\n",
      "\n",
      "데이터 크기:\n",
      "  Train: (1024, 188)\n",
      "  Val:   (212, 188)\n",
      "  Test:  (213, 188)\n",
      "\n",
      "[Step 1/3] Feature Selection 생략, 모든 특징 사용\n",
      "\n",
      "[Step 2/3] Hyperparameter Tuning 생략, 기본값 사용\n",
      "\n",
      "[Step 3/3] 최종 모델 학습 및 평가...\n",
      "\n",
      "================================================================================\n",
      "최종 모델 학습 및 평가\n",
      "================================================================================\n",
      "\n",
      "Train 데이터로 학습 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "  set     MAPE       R²       RMSE        MAE\n",
      "Train 0.004585 0.999802  12.626731   9.569993\n",
      "  Val 0.083635 0.635416 291.847875 233.797677\n",
      " Test 0.167715 0.808304 452.145358 400.105728\n",
      "\n",
      "과적합 분석:\n",
      "  Train → Val R² 하락: 0.3644\n",
      "  Train → Test R² 하락: 0.1915\n",
      "  ⚠️ Val 과적합 가능성 있음\n",
      "  ⚠️ Test 과적합 가능성 있음\n",
      "\n",
      "================================================================================\n",
      "Feature Importance Top 15\n",
      "================================================================================\n",
      "                     feature  importance\n",
      "                       SMA_5    0.466349\n",
      "                       EMA_5    0.349221\n",
      "     ETH_Chain_eth_chain_tvl    0.045749\n",
      "ETH_Chain_eth_chain_tvl_lag1    0.029860\n",
      "                     BTC_Low    0.016231\n",
      "           Lido_lido_eth_tvl    0.013899\n",
      "                    BTC_High    0.009154\n",
      "                   BNB_Close    0.006614\n",
      "                      BBU_20    0.005988\n",
      "                     SOL_Low    0.005721\n",
      "                   BTC_Close    0.004575\n",
      "                   SOL_Close    0.003832\n",
      "           Aave_aave_eth_tvl    0.002553\n",
      "                     EMA_100    0.002545\n",
      "                         OBV    0.001729\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "Test R²: 0.8083\n",
      "Test MAPE: 0.1677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 사용 예시\n",
    "# =============================================================================\n",
    "\n",
    "# 옵션 A: 전체 파이프라인 (약 20-30분)\n",
    "results = run_proper_pipeline(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    select_features=True,\n",
    "    tune_hyperparams=True,\n",
    "    top_n=30\n",
    ")\n",
    "\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\nTest R²: {results['metrics']['test']['R²']:.4f}\")\n",
    "print(f\"Test MAPE: {results['metrics']['test']['MAPE']:.4f}\")\n",
    "\n",
    "# 옵션 B: 특징 선택만 (약 5분)\n",
    "results = run_proper_pipeline(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    select_features=True,\n",
    "    tune_hyperparams=False,\n",
    "    top_n=30\n",
    ")\n",
    "\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\nTest R²: {results['metrics']['test']['R²']:.4f}\")\n",
    "print(f\"Test MAPE: {results['metrics']['test']['MAPE']:.4f}\")\n",
    "\n",
    "# 옵션 C: 튜닝만 (약 15분)\n",
    "results = run_proper_pipeline(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    select_features=False,\n",
    "    tune_hyperparams=True\n",
    ")\n",
    "\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\nTest R²: {results['metrics']['test']['R²']:.4f}\")\n",
    "print(f\"Test MAPE: {results['metrics']['test']['MAPE']:.4f}\")\n",
    "\n",
    "# 옵션 D: 빠른 테스트 (약 1분)\n",
    "results = run_proper_pipeline(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    select_features=False,\n",
    "    tune_hyperparams=False\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\nTest R²: {results['metrics']['test']['R²']:.4f}\")\n",
    "print(f\"Test MAPE: {results['metrics']['test']['MAPE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc125dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6bb4427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:42:17,819] A new study created in memory with name: no-name-68ca2fd4-0009-4da8-9771-61619336f0e9\n",
      "[I 2025-10-05 11:42:18,550] Trial 0 finished with value: 256.1380580529646 and parameters: {'max_depth': 7, 'learning_rate': 0.19483273179517838, 'n_estimators': 417, 'subsample': 0.8327864462932906, 'colsample_bytree': 0.512137607073484, 'gamma': 0.6280213398724088, 'reg_alpha': 0.3055170271694374, 'reg_lambda': 0.49546572144409884}. Best is trial 0 with value: 256.1380580529646.\n",
      "[I 2025-10-05 11:42:19,405] Trial 1 finished with value: 329.02512129787044 and parameters: {'max_depth': 5, 'learning_rate': 0.25762177593158386, 'n_estimators': 719, 'subsample': 0.852893512500718, 'colsample_bytree': 0.7037335915300083, 'gamma': 4.099740510337839, 'reg_alpha': 0.1389487774456255, 'reg_lambda': 4.870670159742948}. Best is trial 0 with value: 256.1380580529646.\n",
      "[I 2025-10-05 11:42:20,477] Trial 2 finished with value: 298.60861617909325 and parameters: {'max_depth': 9, 'learning_rate': 0.22518445146056174, 'n_estimators': 746, 'subsample': 0.8896655923836232, 'colsample_bytree': 0.5527739102815392, 'gamma': 2.4161155789410955, 'reg_alpha': 1.9261087067287725, 'reg_lambda': 3.0212387706539383}. Best is trial 0 with value: 256.1380580529646.\n",
      "[I 2025-10-05 11:42:21,944] Trial 3 finished with value: 231.29625415635232 and parameters: {'max_depth': 9, 'learning_rate': 0.10392143171886258, 'n_estimators': 593, 'subsample': 0.5456598082791182, 'colsample_bytree': 0.7236121447741772, 'gamma': 1.0368563416372656, 'reg_alpha': 2.442828602778663, 'reg_lambda': 0.4527399697049983}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:22,850] Trial 4 finished with value: 271.3002104150251 and parameters: {'max_depth': 10, 'learning_rate': 0.15133791546827938, 'n_estimators': 308, 'subsample': 0.853390655203947, 'colsample_bytree': 0.7498776028026499, 'gamma': 1.809310142356653, 'reg_alpha': 0.6238504903757863, 'reg_lambda': 2.338072321365508}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:23,487] Trial 5 finished with value: 251.4138783276727 and parameters: {'max_depth': 9, 'learning_rate': 0.20923211639873718, 'n_estimators': 336, 'subsample': 0.8860915052003591, 'colsample_bytree': 0.6276685603570238, 'gamma': 4.665129853626394, 'reg_alpha': 4.620570182822286, 'reg_lambda': 1.4640344874669657}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:24,733] Trial 6 finished with value: 255.1081141034522 and parameters: {'max_depth': 4, 'learning_rate': 0.024874565947784102, 'n_estimators': 815, 'subsample': 0.8011042497743059, 'colsample_bytree': 0.832531161719053, 'gamma': 4.2988325963215726, 'reg_alpha': 4.5175604142391625, 'reg_lambda': 4.537988457131651}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:25,482] Trial 7 finished with value: 236.6615781421837 and parameters: {'max_depth': 4, 'learning_rate': 0.24674175483377564, 'n_estimators': 591, 'subsample': 0.8774980010285485, 'colsample_bytree': 0.884437107478398, 'gamma': 0.7129464696302734, 'reg_alpha': 0.6026686901550277, 'reg_lambda': 0.355579760953908}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:26,639] Trial 8 finished with value: 260.00594440271664 and parameters: {'max_depth': 11, 'learning_rate': 0.18869919170517876, 'n_estimators': 546, 'subsample': 0.6642579701910283, 'colsample_bytree': 0.6898964404763939, 'gamma': 2.336056421387002, 'reg_alpha': 4.099817107142637, 'reg_lambda': 0.8802207385626171}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:27,382] Trial 9 finished with value: 273.22695684148584 and parameters: {'max_depth': 12, 'learning_rate': 0.2000278983277682, 'n_estimators': 308, 'subsample': 0.996416394871821, 'colsample_bytree': 0.7549119635310519, 'gamma': 4.756589037364668, 'reg_alpha': 3.390412949964431, 'reg_lambda': 2.588373600698932}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:27,826] Trial 10 finished with value: 247.75238757833998 and parameters: {'max_depth': 7, 'learning_rate': 0.090905784632823, 'n_estimators': 112, 'subsample': 0.5009470135146297, 'colsample_bytree': 0.9803688858853643, 'gamma': 1.4853573892235687, 'reg_alpha': 2.19971231353144, 'reg_lambda': 1.6238194597962652}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:29,311] Trial 11 finished with value: 257.4422402446915 and parameters: {'max_depth': 5, 'learning_rate': 0.1277916697527785, 'n_estimators': 972, 'subsample': 0.6635647986963399, 'colsample_bytree': 0.8907880852809948, 'gamma': 0.05799294376096975, 'reg_alpha': 1.3522838767013858, 'reg_lambda': 0.009487239499761047}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:30,029] Trial 12 finished with value: 238.60047863232867 and parameters: {'max_depth': 3, 'learning_rate': 0.08055091274970777, 'n_estimators': 583, 'subsample': 0.5294485466954711, 'colsample_bytree': 0.8621729715102658, 'gamma': 1.0154566778533185, 'reg_alpha': 3.001247515357406, 'reg_lambda': 1.0205163051888535}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:31,018] Trial 13 finished with value: 281.8565587135068 and parameters: {'max_depth': 8, 'learning_rate': 0.28053523267120783, 'n_estimators': 533, 'subsample': 0.6846073269341547, 'colsample_bytree': 0.9882381061742239, 'gamma': 0.06267603542371747, 'reg_alpha': 1.1951998775335198, 'reg_lambda': 0.024236879392743826}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:31,929] Trial 14 finished with value: 293.89646329475164 and parameters: {'max_depth': 6, 'learning_rate': 0.2996191559174724, 'n_estimators': 647, 'subsample': 0.5830431498919361, 'colsample_bytree': 0.802131166092706, 'gamma': 3.050092609421087, 'reg_alpha': 2.728125063164188, 'reg_lambda': 3.5831165736635917}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:33,052] Trial 15 finished with value: 245.11186063752973 and parameters: {'max_depth': 3, 'learning_rate': 0.09817715047738246, 'n_estimators': 922, 'subsample': 0.9879647713440171, 'colsample_bytree': 0.9109127205507866, 'gamma': 0.9152681147562947, 'reg_alpha': 1.6074408381720144, 'reg_lambda': 1.6638026524884673}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:34,637] Trial 16 finished with value: 262.4691709409892 and parameters: {'max_depth': 8, 'learning_rate': 0.042101422547404954, 'n_estimators': 451, 'subsample': 0.7539143789992226, 'colsample_bytree': 0.6557493531704384, 'gamma': 3.432424075615853, 'reg_alpha': 3.8131250554316853, 'reg_lambda': 0.4622114239507818}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:36,191] Trial 17 finished with value: 241.84894231644626 and parameters: {'max_depth': 10, 'learning_rate': 0.14439130013591267, 'n_estimators': 838, 'subsample': 0.937883360539594, 'colsample_bytree': 0.781283276821312, 'gamma': 1.7061950783194402, 'reg_alpha': 0.997309355264657, 'reg_lambda': 2.16940030245376}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:36,671] Trial 18 finished with value: 259.47452170442403 and parameters: {'max_depth': 6, 'learning_rate': 0.06530851644781191, 'n_estimators': 147, 'subsample': 0.5854781088462934, 'colsample_bytree': 0.9317753035826281, 'gamma': 0.5777549698345881, 'reg_alpha': 2.35760602866982, 'reg_lambda': 0.9426256347058947}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:38,055] Trial 19 finished with value: 322.14027910886983 and parameters: {'max_depth': 12, 'learning_rate': 0.2475081216643372, 'n_estimators': 691, 'subsample': 0.7389082963765772, 'colsample_bytree': 0.5925890179160832, 'gamma': 1.330659918863922, 'reg_alpha': 0.7596372708623127, 'reg_lambda': 3.748421970032085}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:38,891] Trial 20 finished with value: 264.73200632204896 and parameters: {'max_depth': 5, 'learning_rate': 0.12259287891076716, 'n_estimators': 459, 'subsample': 0.7771013568643016, 'colsample_bytree': 0.7180732482460412, 'gamma': 2.0059987302587787, 'reg_alpha': 1.801500274835419, 'reg_lambda': 0.521574658256353}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:39,652] Trial 21 finished with value: 257.9116667814898 and parameters: {'max_depth': 3, 'learning_rate': 0.07632970526263315, 'n_estimators': 616, 'subsample': 0.5000133726419705, 'colsample_bytree': 0.8602199562719399, 'gamma': 0.8941935157234764, 'reg_alpha': 3.010260434443349, 'reg_lambda': 1.242862486849485}. Best is trial 3 with value: 231.29625415635232.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:42:40,575] Trial 22 finished with value: 246.37865918515968 and parameters: {'max_depth': 4, 'learning_rate': 0.1733026792906167, 'n_estimators': 612, 'subsample': 0.5890179680588623, 'colsample_bytree': 0.8394279742320879, 'gamma': 1.1932112790228713, 'reg_alpha': 3.135368576633834, 'reg_lambda': 0.9618957458483361}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:41,249] Trial 23 finished with value: 259.11573757861396 and parameters: {'max_depth': 3, 'learning_rate': 0.11037731635311876, 'n_estimators': 522, 'subsample': 0.5700701875664472, 'colsample_bytree': 0.935954241159481, 'gamma': 0.3565449478216539, 'reg_alpha': 3.6242428134338374, 'reg_lambda': 1.9788779039885664}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:42,453] Trial 24 finished with value: 269.65512592371687 and parameters: {'max_depth': 4, 'learning_rate': 0.052602066060921354, 'n_estimators': 779, 'subsample': 0.5394646567835633, 'colsample_bytree': 0.868385419763286, 'gamma': 0.9009966593733818, 'reg_alpha': 2.7560298993702284, 'reg_lambda': 0.4521403664562455}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:44,084] Trial 25 finished with value: 255.87183555105076 and parameters: {'max_depth': 6, 'learning_rate': 0.022284760984794504, 'n_estimators': 605, 'subsample': 0.6267171235206491, 'colsample_bytree': 0.8007735381593832, 'gamma': 2.938211752778191, 'reg_alpha': 2.1593185335494836, 'reg_lambda': 1.1312854020239933}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:45,626] Trial 26 finished with value: 264.89849009409016 and parameters: {'max_depth': 9, 'learning_rate': 0.08140483741791733, 'n_estimators': 677, 'subsample': 0.7283249980539194, 'colsample_bytree': 0.7552224545531242, 'gamma': 1.2413877518451764, 'reg_alpha': 4.1363509490681345, 'reg_lambda': 0.25035214818562945}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:46,294] Trial 27 finished with value: 315.25335096738985 and parameters: {'max_depth': 4, 'learning_rate': 0.1697335848088342, 'n_estimators': 390, 'subsample': 0.5380933335332357, 'colsample_bytree': 0.9473932516253264, 'gamma': 0.43848127019357486, 'reg_alpha': 3.2357382050474754, 'reg_lambda': 0.6792609955485629}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:46,922] Trial 28 finished with value: 261.6089012845477 and parameters: {'max_depth': 3, 'learning_rate': 0.13650764014577918, 'n_estimators': 493, 'subsample': 0.6270575181059096, 'colsample_bytree': 0.819234666007509, 'gamma': 1.9461210706361527, 'reg_alpha': 2.6914183616401703, 'reg_lambda': 1.3441694239837207}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:48,051] Trial 29 finished with value: 261.5040650042727 and parameters: {'max_depth': 7, 'learning_rate': 0.11359246879645828, 'n_estimators': 579, 'subsample': 0.9326301183726304, 'colsample_bytree': 0.8889246027558119, 'gamma': 0.7631123829057995, 'reg_alpha': 0.38024685635538535, 'reg_lambda': 2.7117386850793292}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:48,964] Trial 30 finished with value: 256.4582172191198 and parameters: {'max_depth': 8, 'learning_rate': 0.05970268550458867, 'n_estimators': 212, 'subsample': 0.8152640485160804, 'colsample_bytree': 0.6663147964778585, 'gamma': 1.5285072014151853, 'reg_alpha': 1.5116812466547014, 'reg_lambda': 1.9205514803689472}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:50,470] Trial 31 finished with value: 263.5013954950665 and parameters: {'max_depth': 10, 'learning_rate': 0.14833132625468282, 'n_estimators': 853, 'subsample': 0.9439768085075534, 'colsample_bytree': 0.780964925758506, 'gamma': 1.8348139186795493, 'reg_alpha': 1.0644440502928931, 'reg_lambda': 0.7446623054332342}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:52,339] Trial 32 finished with value: 272.4071410539597 and parameters: {'max_depth': 10, 'learning_rate': 0.09936003535495562, 'n_estimators': 881, 'subsample': 0.9500173770790025, 'colsample_bytree': 0.7351645272385586, 'gamma': 1.1261134881504007, 'reg_alpha': 0.21329696009781385, 'reg_lambda': 3.1761021465411714}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:53,647] Trial 33 finished with value: 272.77447111231606 and parameters: {'max_depth': 11, 'learning_rate': 0.22748139617116128, 'n_estimators': 720, 'subsample': 0.8992716436464571, 'colsample_bytree': 0.7818201292686519, 'gamma': 1.5034066547712193, 'reg_alpha': 0.9417908707487723, 'reg_lambda': 2.1332836503314168}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:55,080] Trial 34 finished with value: 286.5157625803962 and parameters: {'max_depth': 11, 'learning_rate': 0.15991363629082372, 'n_estimators': 776, 'subsample': 0.8577504213799866, 'colsample_bytree': 0.8704497292161117, 'gamma': 2.1628155318227917, 'reg_alpha': 0.03324581964918982, 'reg_lambda': 0.1416825487487168}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:55,923] Trial 35 finished with value: 297.39032492325435 and parameters: {'max_depth': 9, 'learning_rate': 0.1442942448924856, 'n_estimators': 382, 'subsample': 0.9064347785886735, 'colsample_bytree': 0.7005692851010313, 'gamma': 0.4699394818536322, 'reg_alpha': 0.5134971839985191, 'reg_lambda': 0.35964199729641144}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:57,597] Trial 36 finished with value: 261.25771255619713 and parameters: {'max_depth': 10, 'learning_rate': 0.18549229962627603, 'n_estimators': 988, 'subsample': 0.8623298868643986, 'colsample_bytree': 0.836443128079373, 'gamma': 1.6546801263375126, 'reg_alpha': 1.8930568274005632, 'reg_lambda': 1.5802741177147615}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:42:58,424] Trial 37 finished with value: 290.19456447114686 and parameters: {'max_depth': 5, 'learning_rate': 0.2168495658761671, 'n_estimators': 651, 'subsample': 0.8232621331812473, 'colsample_bytree': 0.5461283777525819, 'gamma': 2.649247301576385, 'reg_alpha': 0.8322951382892334, 'reg_lambda': 2.2905782230558183}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:43:01,976] Trial 38 finished with value: 231.58829031982026 and parameters: {'max_depth': 9, 'learning_rate': 0.01079662653646643, 'n_estimators': 740, 'subsample': 0.5446193513165173, 'colsample_bytree': 0.7907158391072665, 'gamma': 1.1559232352122368, 'reg_alpha': 4.840177406262491, 'reg_lambda': 1.111122873416134}. Best is trial 3 with value: 231.29625415635232.\n",
      "[I 2025-10-05 11:43:05,257] Trial 39 finished with value: 210.32372860630363 and parameters: {'max_depth': 9, 'learning_rate': 0.036912085606611286, 'n_estimators': 747, 'subsample': 0.5387394139971816, 'colsample_bytree': 0.7281507418558079, 'gamma': 0.2186690768200945, 'reg_alpha': 4.646201276284195, 'reg_lambda': 1.150087453879106}. Best is trial 39 with value: 210.32372860630363.\n",
      "[I 2025-10-05 11:43:08,737] Trial 40 finished with value: 259.6100186243332 and parameters: {'max_depth': 9, 'learning_rate': 0.01546871438851008, 'n_estimators': 748, 'subsample': 0.6176387618371068, 'colsample_bytree': 0.6083619611783724, 'gamma': 0.3157102946040128, 'reg_alpha': 4.970667504954238, 'reg_lambda': 0.6861283846834485}. Best is trial 39 with value: 210.32372860630363.\n",
      "[I 2025-10-05 11:43:11,972] Trial 41 finished with value: 222.59063517375452 and parameters: {'max_depth': 9, 'learning_rate': 0.0374723215427495, 'n_estimators': 711, 'subsample': 0.5383621711503581, 'colsample_bytree': 0.6706993991952952, 'gamma': 0.009500098287739456, 'reg_alpha': 4.536538140083966, 'reg_lambda': 1.1723122454460322}. Best is trial 39 with value: 210.32372860630363.\n",
      "[I 2025-10-05 11:43:14,746] Trial 42 finished with value: 222.92948342459027 and parameters: {'max_depth': 8, 'learning_rate': 0.03918264485613676, 'n_estimators': 709, 'subsample': 0.5616013406620514, 'colsample_bytree': 0.6796993264066481, 'gamma': 0.15868798220264696, 'reg_alpha': 4.625529271709984, 'reg_lambda': 1.3479366082737647}. Best is trial 39 with value: 210.32372860630363.\n",
      "[I 2025-10-05 11:43:17,803] Trial 43 finished with value: 210.41522882620532 and parameters: {'max_depth': 8, 'learning_rate': 0.04396570199727609, 'n_estimators': 791, 'subsample': 0.5560170784904517, 'colsample_bytree': 0.66896853615397, 'gamma': 0.02271893251482454, 'reg_alpha': 4.6012757325313425, 'reg_lambda': 1.4314984665528576}. Best is trial 39 with value: 210.32372860630363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:43:20,798] Trial 44 finished with value: 231.42827028865406 and parameters: {'max_depth': 8, 'learning_rate': 0.04354856890410882, 'n_estimators': 821, 'subsample': 0.5194806452884998, 'colsample_bytree': 0.673480989483876, 'gamma': 0.192570842315649, 'reg_alpha': 4.392612089162666, 'reg_lambda': 1.430776314557369}. Best is trial 39 with value: 210.32372860630363.\n",
      "[I 2025-10-05 11:43:23,091] Trial 45 finished with value: 207.27962330027498 and parameters: {'max_depth': 7, 'learning_rate': 0.035531734946711684, 'n_estimators': 693, 'subsample': 0.5605681834406134, 'colsample_bytree': 0.641859327898104, 'gamma': 0.10431187404128393, 'reg_alpha': 4.67007001599775, 'reg_lambda': 1.5964331772493294}. Best is trial 45 with value: 207.27962330027498.\n",
      "[I 2025-10-05 11:43:25,994] Trial 46 finished with value: 243.45619460213265 and parameters: {'max_depth': 7, 'learning_rate': 0.027364439833010974, 'n_estimators': 901, 'subsample': 0.5639358116714439, 'colsample_bytree': 0.6362359842738387, 'gamma': 0.04004852495983047, 'reg_alpha': 4.662030086380194, 'reg_lambda': 1.7399118574831804}. Best is trial 45 with value: 207.27962330027498.\n",
      "[I 2025-10-05 11:43:28,389] Trial 47 finished with value: 214.80857480739277 and parameters: {'max_depth': 7, 'learning_rate': 0.037813839757516406, 'n_estimators': 784, 'subsample': 0.6074277051824307, 'colsample_bytree': 0.5915477064930275, 'gamma': 0.6900047892513701, 'reg_alpha': 4.261661050633932, 'reg_lambda': 1.9134773610031401}. Best is trial 45 with value: 207.27962330027498.\n",
      "[I 2025-10-05 11:43:30,851] Trial 48 finished with value: 237.13152911080533 and parameters: {'max_depth': 7, 'learning_rate': 0.03217546054688064, 'n_estimators': 793, 'subsample': 0.6098486533017184, 'colsample_bytree': 0.5823175044555462, 'gamma': 0.6549174940798045, 'reg_alpha': 4.259488122370392, 'reg_lambda': 1.8285950947550518}. Best is trial 45 with value: 207.27962330027498.\n",
      "[I 2025-10-05 11:43:32,877] Trial 49 finished with value: 229.4230258331908 and parameters: {'max_depth': 7, 'learning_rate': 0.06655423754055524, 'n_estimators': 929, 'subsample': 0.6685959144792026, 'colsample_bytree': 0.5101843315091436, 'gamma': 0.2935787603053422, 'reg_alpha': 3.8721109084010803, 'reg_lambda': 2.64129716691643}. Best is trial 45 with value: 207.27962330027498.\n",
      "[I 2025-10-05 11:43:53,458] A new study created in memory with name: no-name-d37686b1-9f7e-42d0-bdd1-2302df0f3573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: {'RMSE': 539.9471927351657, 'MAE': 492.0964544591769, 'R2': 0.7266250720117118}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:43:56,533] Trial 0 finished with value: 249.78318759171697 and parameters: {'num_leaves': 141, 'learning_rate': 0.11331651561557163, 'n_estimators': 250, 'feature_fraction': 0.8422776966806111, 'bagging_fraction': 0.5007968086027135}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:02,302] Trial 1 finished with value: 271.1556795126557 and parameters: {'num_leaves': 62, 'learning_rate': 0.12423955232269165, 'n_estimators': 451, 'feature_fraction': 0.898074149752724, 'bagging_fraction': 0.9391414718400146}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:04,156] Trial 2 finished with value: 280.6958094349701 and parameters: {'num_leaves': 83, 'learning_rate': 0.15319716700620176, 'n_estimators': 201, 'feature_fraction': 0.5500257230799741, 'bagging_fraction': 0.665992655775897}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:08,621] Trial 3 finished with value: 303.0226037446485 and parameters: {'num_leaves': 135, 'learning_rate': 0.26140753904975245, 'n_estimators': 452, 'feature_fraction': 0.593319781298023, 'bagging_fraction': 0.6379096401286188}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:10,452] Trial 4 finished with value: 309.8447975915995 and parameters: {'num_leaves': 62, 'learning_rate': 0.27052484535166355, 'n_estimators': 176, 'feature_fraction': 0.6157030473561882, 'bagging_fraction': 0.7669956292720146}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:11,487] Trial 5 finished with value: 271.5926480007379 and parameters: {'num_leaves': 144, 'learning_rate': 0.1675913082597085, 'n_estimators': 103, 'feature_fraction': 0.5374855033808152, 'bagging_fraction': 0.7428890705417739}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:17,313] Trial 6 finished with value: 289.31955605540884 and parameters: {'num_leaves': 52, 'learning_rate': 0.24559822461262917, 'n_estimators': 418, 'feature_fraction': 0.9828508912931422, 'bagging_fraction': 0.8138652672572155}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:20,191] Trial 7 finished with value: 279.37626972148314 and parameters: {'num_leaves': 53, 'learning_rate': 0.052353549185717725, 'n_estimators': 317, 'feature_fraction': 0.5820411903284455, 'bagging_fraction': 0.698984612148621}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:22,405] Trial 8 finished with value: 265.86860778395476 and parameters: {'num_leaves': 76, 'learning_rate': 0.03896465493141734, 'n_estimators': 248, 'feature_fraction': 0.5717317384931385, 'bagging_fraction': 0.6844401035978485}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:26,444] Trial 9 finished with value: 273.3505968100738 and parameters: {'num_leaves': 132, 'learning_rate': 0.0777994777978118, 'n_estimators': 361, 'feature_fraction': 0.7444648806861784, 'bagging_fraction': 0.871026148064941}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:29,789] Trial 10 finished with value: 286.7238164590486 and parameters: {'num_leaves': 113, 'learning_rate': 0.19017884594134327, 'n_estimators': 273, 'feature_fraction': 0.799162126717271, 'bagging_fraction': 0.5349151805770025}. Best is trial 0 with value: 249.78318759171697.\n",
      "[I 2025-10-05 11:44:31,532] Trial 11 finished with value: 232.5188458585215 and parameters: {'num_leaves': 27, 'learning_rate': 0.010171747381404191, 'n_estimators': 242, 'feature_fraction': 0.7117060821650345, 'bagging_fraction': 0.5090363037706751}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:32,830] Trial 12 finished with value: 273.3248693890357 and parameters: {'num_leaves': 20, 'learning_rate': 0.09361181769834998, 'n_estimators': 198, 'feature_fraction': 0.7228316294195041, 'bagging_fraction': 0.5000786208570439}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:36,303] Trial 13 finished with value: 258.2073688734756 and parameters: {'num_leaves': 106, 'learning_rate': 0.012498620390384671, 'n_estimators': 338, 'feature_fraction': 0.8274131536877032, 'bagging_fraction': 0.5841768114462758}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:37,355] Trial 14 finished with value: 265.1841187996472 and parameters: {'num_leaves': 29, 'learning_rate': 0.10913215724660393, 'n_estimators': 113, 'feature_fraction': 0.7034854484274325, 'bagging_fraction': 0.5840636191117531}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:40,649] Trial 15 finished with value: 263.1733643001658 and parameters: {'num_leaves': 118, 'learning_rate': 0.2089418161407367, 'n_estimators': 256, 'feature_fraction': 0.8836791955536454, 'bagging_fraction': 0.5792982476290339}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:44,061] Trial 16 finished with value: 255.58776388479998 and parameters: {'num_leaves': 97, 'learning_rate': 0.014100076385711705, 'n_estimators': 384, 'feature_fraction': 0.6721678365488764, 'bagging_fraction': 0.5025460266176652}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:46,638] Trial 17 finished with value: 259.4009255829891 and parameters: {'num_leaves': 43, 'learning_rate': 0.06362572078470148, 'n_estimators': 226, 'feature_fraction': 0.8063270998733563, 'bagging_fraction': 0.6144534387319742}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:48,457] Trial 18 finished with value: 285.9686689242205 and parameters: {'num_leaves': 97, 'learning_rate': 0.29793002487341347, 'n_estimators': 162, 'feature_fraction': 0.6511608703589838, 'bagging_fraction': 0.9747962768631153}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:52,478] Trial 19 finished with value: 284.0206169862933 and parameters: {'num_leaves': 147, 'learning_rate': 0.12954959154487672, 'n_estimators': 288, 'feature_fraction': 0.9791779796518782, 'bagging_fraction': 0.5469410786584642}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:44:58,310] Trial 20 finished with value: 296.68898281169106 and parameters: {'num_leaves': 127, 'learning_rate': 0.2180981128872681, 'n_estimators': 492, 'feature_fraction': 0.7810088125559662, 'bagging_fraction': 0.8580655864700424}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:01,676] Trial 21 finished with value: 261.97636788461637 and parameters: {'num_leaves': 97, 'learning_rate': 0.011749614536056946, 'n_estimators': 381, 'feature_fraction': 0.6686466828513559, 'bagging_fraction': 0.5021315208063784}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:05,266] Trial 22 finished with value: 267.3479492180987 and parameters: {'num_leaves': 74, 'learning_rate': 0.03044936190207747, 'n_estimators': 321, 'feature_fraction': 0.8485762816201555, 'bagging_fraction': 0.5281621775158302}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:09,078] Trial 23 finished with value: 293.2662198930172 and parameters: {'num_leaves': 38, 'learning_rate': 0.08433728930217367, 'n_estimators': 380, 'feature_fraction': 0.6909162952348037, 'bagging_fraction': 0.5591868365395325}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:12,190] Trial 24 finished with value: 280.1599106273923 and parameters: {'num_leaves': 99, 'learning_rate': 0.044307562059288624, 'n_estimators': 293, 'feature_fraction': 0.7510208768833383, 'bagging_fraction': 0.6235178467036755}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:14,445] Trial 25 finished with value: 285.3030444429339 and parameters: {'num_leaves': 120, 'learning_rate': 0.065576416099901, 'n_estimators': 230, 'feature_fraction': 0.6344810933608686, 'bagging_fraction': 0.5005305721488299}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:16,047] Trial 26 finished with value: 245.33866331069567 and parameters: {'num_leaves': 89, 'learning_rate': 0.013587363268238449, 'n_estimators': 138, 'feature_fraction': 0.9208461035768744, 'bagging_fraction': 0.6042219028158474}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:17,940] Trial 27 finished with value: 288.67466684263366 and parameters: {'num_leaves': 87, 'learning_rate': 0.09698405518666446, 'n_estimators': 144, 'feature_fraction': 0.928181589484517, 'bagging_fraction': 0.6061505828187556}. Best is trial 11 with value: 232.5188458585215.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:45:19,797] Trial 28 finished with value: 257.1145274229824 and parameters: {'num_leaves': 68, 'learning_rate': 0.14024647534872983, 'n_estimators': 136, 'feature_fraction': 0.935611336064747, 'bagging_fraction': 0.6475952660604258}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:21,275] Trial 29 finished with value: 263.11710036733234 and parameters: {'num_leaves': 20, 'learning_rate': 0.0323213663359628, 'n_estimators': 210, 'feature_fraction': 0.8718105112743987, 'bagging_fraction': 0.5605874980236645}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:23,551] Trial 30 finished with value: 271.65369439144285 and parameters: {'num_leaves': 54, 'learning_rate': 0.11667465001425693, 'n_estimators': 169, 'feature_fraction': 0.9337067162144848, 'bagging_fraction': 0.7169927608187556}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:26,805] Trial 31 finished with value: 276.8616637446452 and parameters: {'num_leaves': 86, 'learning_rate': 0.026415554229205558, 'n_estimators': 408, 'feature_fraction': 0.5059581904111297, 'bagging_fraction': 0.5327976825975757}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:29,135] Trial 32 finished with value: 238.59778952691875 and parameters: {'num_leaves': 107, 'learning_rate': 0.010428742021672304, 'n_estimators': 245, 'feature_fraction': 0.7506554085602907, 'bagging_fraction': 0.5162723159893957}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:31,901] Trial 33 finished with value: 285.29550663502755 and parameters: {'num_leaves': 107, 'learning_rate': 0.05638221605123204, 'n_estimators': 253, 'feature_fraction': 0.7764954028179191, 'bagging_fraction': 0.5864272627890565}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:34,330] Trial 34 finished with value: 266.6866468036148 and parameters: {'num_leaves': 79, 'learning_rate': 0.1610165620880866, 'n_estimators': 193, 'feature_fraction': 0.851719855651171, 'bagging_fraction': 0.6494976524720937}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:36,924] Trial 35 finished with value: 261.58248345858556 and parameters: {'num_leaves': 135, 'learning_rate': 0.02854530749262414, 'n_estimators': 227, 'feature_fraction': 0.8964080620510179, 'bagging_fraction': 0.5294194623424088}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:39,918] Trial 36 finished with value: 283.7937823859161 and parameters: {'num_leaves': 64, 'learning_rate': 0.07251993653724113, 'n_estimators': 273, 'feature_fraction': 0.7503397680524921, 'bagging_fraction': 0.5489444499249825}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:42,177] Trial 37 finished with value: 279.2740272032834 and parameters: {'num_leaves': 140, 'learning_rate': 0.05163174509318105, 'n_estimators': 179, 'feature_fraction': 0.9561427176809135, 'bagging_fraction': 0.7736105825932748}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:43,940] Trial 38 finished with value: 266.65401603399124 and parameters: {'num_leaves': 88, 'learning_rate': 0.1786735332861811, 'n_estimators': 140, 'feature_fraction': 0.8302276840789038, 'bagging_fraction': 0.6735750136572187}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:47,339] Trial 39 finished with value: 242.6002727691499 and parameters: {'num_leaves': 126, 'learning_rate': 0.0101954122377249, 'n_estimators': 317, 'feature_fraction': 0.9105448657389632, 'bagging_fraction': 0.6084645249734134}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:50,947] Trial 40 finished with value: 266.3170184636922 and parameters: {'num_leaves': 124, 'learning_rate': 0.02016813229249153, 'n_estimators': 322, 'feature_fraction': 0.9157657588952903, 'bagging_fraction': 0.7322745903657639}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:55,222] Trial 41 finished with value: 271.1479740699919 and parameters: {'num_leaves': 141, 'learning_rate': 0.04014613531473896, 'n_estimators': 340, 'feature_fraction': 0.9685015367244176, 'bagging_fraction': 0.6095217511562085}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:45:58,575] Trial 42 finished with value: 272.5409961246138 and parameters: {'num_leaves': 112, 'learning_rate': 0.04040446326373595, 'n_estimators': 270, 'feature_fraction': 0.9993136582399529, 'bagging_fraction': 0.5236511613857222}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:01,882] Trial 43 finished with value: 259.06245882688467 and parameters: {'num_leaves': 130, 'learning_rate': 0.020995157397872465, 'n_estimators': 300, 'feature_fraction': 0.9000562839507448, 'bagging_fraction': 0.5645638745583008}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:04,358] Trial 44 finished with value: 287.0303631276151 and parameters: {'num_leaves': 117, 'learning_rate': 0.05017784386719455, 'n_estimators': 239, 'feature_fraction': 0.7224885482095409, 'bagging_fraction': 0.691387392265287}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:06,609] Trial 45 finished with value: 239.7740583738164 and parameters: {'num_leaves': 105, 'learning_rate': 0.011522242329042354, 'n_estimators': 211, 'feature_fraction': 0.8622381481670998, 'bagging_fraction': 0.593171436668974}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:08,900] Trial 46 finished with value: 239.5189694923626 and parameters: {'num_leaves': 104, 'learning_rate': 0.011751075357150802, 'n_estimators': 210, 'feature_fraction': 0.8684481632839121, 'bagging_fraction': 0.6356218576650046}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:10,727] Trial 47 finished with value: 239.17009749423804 and parameters: {'num_leaves': 104, 'learning_rate': 0.010012531224942112, 'n_estimators': 212, 'feature_fraction': 0.6089229743237631, 'bagging_fraction': 0.6377515683717632}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:12,680] Trial 48 finished with value: 263.7306362573557 and parameters: {'num_leaves': 105, 'learning_rate': 0.034696145400519254, 'n_estimators': 211, 'feature_fraction': 0.6203487084539832, 'bagging_fraction': 0.635607604439036}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:14,297] Trial 49 finished with value: 269.57505003906624 and parameters: {'num_leaves': 109, 'learning_rate': 0.02428713286022484, 'n_estimators': 190, 'feature_fraction': 0.534702857915383, 'bagging_fraction': 0.6645341166271858}. Best is trial 11 with value: 232.5188458585215.\n",
      "[I 2025-10-05 11:46:15,526] A new study created in memory with name: no-name-ba09dce3-91b4-4812-b5ba-0cff8232c011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM: {'RMSE': 395.3028816527848, 'MAE': 354.7635744343099, 'R2': 0.853473536757033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 11:46:18,234] Trial 0 finished with value: 2409.615525616529 and parameters: {'seq_len': 43, 'hidden_dim': 106, 'num_layers': 3, 'lr': 0.008598489119324317}. Best is trial 0 with value: 2409.615525616529.\n",
      "[I 2025-10-05 11:46:21,262] Trial 1 finished with value: 2540.9008937437648 and parameters: {'seq_len': 59, 'hidden_dim': 87, 'num_layers': 3, 'lr': 0.007146462882980957}. Best is trial 0 with value: 2409.615525616529.\n",
      "[I 2025-10-05 11:46:23,252] Trial 2 finished with value: 2354.881643361623 and parameters: {'seq_len': 10, 'hidden_dim': 113, 'num_layers': 3, 'lr': 0.0091739100854457}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:25,575] Trial 3 finished with value: 2850.4546514157096 and parameters: {'seq_len': 32, 'hidden_dim': 63, 'num_layers': 2, 'lr': 0.002956454513763345}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:28,199] Trial 4 finished with value: 2957.7569691828944 and parameters: {'seq_len': 52, 'hidden_dim': 63, 'num_layers': 1, 'lr': 0.0070906776971866835}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:30,755] Trial 5 finished with value: 2676.781422699798 and parameters: {'seq_len': 42, 'hidden_dim': 69, 'num_layers': 2, 'lr': 0.0057195747663663}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:33,159] Trial 6 finished with value: 2955.898699792294 and parameters: {'seq_len': 40, 'hidden_dim': 57, 'num_layers': 1, 'lr': 0.008507851831437471}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:35,691] Trial 7 finished with value: 2944.03619306397 and parameters: {'seq_len': 45, 'hidden_dim': 115, 'num_layers': 1, 'lr': 0.004829489413028316}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:38,579] Trial 8 finished with value: 2546.4426369037096 and parameters: {'seq_len': 58, 'hidden_dim': 90, 'num_layers': 2, 'lr': 0.006866766545965704}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:41,122] Trial 9 finished with value: 2696.322910796093 and parameters: {'seq_len': 41, 'hidden_dim': 46, 'num_layers': 2, 'lr': 0.007973318210097587}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:43,114] Trial 10 finished with value: 2965.9160878667635 and parameters: {'seq_len': 11, 'hidden_dim': 127, 'num_layers': 3, 'lr': 0.0005242885813847714}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:45,438] Trial 11 finished with value: 2385.745597674328 and parameters: {'seq_len': 25, 'hidden_dim': 105, 'num_layers': 3, 'lr': 0.009851664922196067}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:47,521] Trial 12 finished with value: 2810.2329611669365 and parameters: {'seq_len': 14, 'hidden_dim': 103, 'num_layers': 3, 'lr': 0.009972921645602775}. Best is trial 2 with value: 2354.881643361623.\n",
      "[I 2025-10-05 11:46:49,801] Trial 13 finished with value: 2227.983723145896 and parameters: {'seq_len': 22, 'hidden_dim': 126, 'num_layers': 3, 'lr': 0.009889826831230578}. Best is trial 13 with value: 2227.983723145896.\n",
      "[W 2025-10-05 11:46:52,017] Trial 14 failed with parameters: {'seq_len': 19, 'hidden_dim': 126, 'num_layers': 3, 'lr': 0.004074311368058088} because of the following error: TypeError('iteration over a 0-d array').\n",
      "Traceback (most recent call last):\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_190359/3901920407.py\", line 148, in objective\n",
      "    val_preds.extend(preds)\n",
      "TypeError: iteration over a 0-d array\n",
      "[W 2025-10-05 11:46:52,017] Trial 14 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 189\u001b[0m\n\u001b[1;32m    186\u001b[0m preds_lgb \u001b[38;5;241m=\u001b[39m model_lgb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluate(y_test, preds_lgb))\n\u001b[0;32m--> 189\u001b[0m model_lstm, best_params_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# LSTM Test 예측\u001b[39;00m\n\u001b[1;32m    192\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m best_params_lstm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[16], line 153\u001b[0m, in \u001b[0;36mtrain_lstm_optuna\u001b[0;34m(X_train, y_train, X_val, y_val, n_trials, max_epochs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(val_targets, val_preds))\n\u001b[1;32m    152\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# 최적 하이퍼파라미터로 전체 train+val 학습 후 test 예측 가능\u001b[39;00m\n\u001b[1;32m    156\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    257\u001b[0m ):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 148\u001b[0m, in \u001b[0;36mtrain_lstm_optuna.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    146\u001b[0m         X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    147\u001b[0m         preds \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mval_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m         val_targets\u001b[38;5;241m.\u001b[39mextend(y_batch\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(val_targets, val_preds))\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7bdb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334a79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2735093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d7fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "완전 자동화 파이프라인 시작\n",
      "================================================================================\n",
      "Train+Val: 1486, Test: 263 (Test 완전 격리)\n",
      "\n",
      "[1/3] CV 안정성 검증 중... (Train+Val만)\n",
      "\\n================================================================================\n",
      "Time Series CV 안정성 검증 (5 folds)\n",
      "================================================================================\n",
      "\\nFold 1/5: Train 251, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 2/5: Train 498, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 3/5: Train 745, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 4/5: Train 992, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 5/5: Train 1239, Val 247\n",
      "  선택: 25개\n",
      "\\n================================================================================\n",
      "안정성 검증 결과\n",
      "================================================================================\n",
      "안정적인 특징: 25개\n",
      "\\n빈도 Top 10:\n",
      "                feature  frequency  stability_score\n",
      "                 WMA_10          5              1.0\n",
      "                 BBM_30          5              1.0\n",
      "                 WMA_20          5              1.0\n",
      "        rolling_mean_14          5              1.0\n",
      "ETH_Chain_eth_chain_tvl          5              1.0\n",
      "                 BBP_30          4              0.8\n",
      "                 WMA_30          4              0.8\n",
      "         rolling_min_30          4              0.8\n",
      "            SP500_SP500          4              0.8\n",
      "         rolling_max_30          4              0.8\n",
      "\n",
      "[2/3] 하이퍼파라미터 튜닝 중... (Train+Val만)\n",
      "\n",
      "================================================================================\n",
      "하이퍼파라미터 튜닝 (Grid Search)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능! → 'gpu_hist' 모드\n",
      "\n",
      "총 조합 수: 48개\n",
      "[5/48] 현재 최고 R²: 0.6871 | MAPE: 0.0711\n",
      "[10/48] 현재 최고 R²: 0.7443 | MAPE: 0.0688\n",
      "[15/48] 현재 최고 R²: 0.7443 | MAPE: 0.0688\n",
      "[20/48] 현재 최고 R²: 0.7443 | MAPE: 0.0688\n",
      "[25/48] 현재 최고 R²: 0.7443 | MAPE: 0.0688\n",
      "[30/48] 현재 최고 R²: 0.7512 | MAPE: 0.0592\n",
      "[35/48] 현재 최고 R²: 0.7694 | MAPE: 0.0646\n",
      "[40/48] 현재 최고 R²: 0.7694 | MAPE: 0.0646\n",
      "[45/48] 현재 최고 R²: 0.7694 | MAPE: 0.0646\n",
      "[48/48] 현재 최고 R²: 0.7694 | MAPE: 0.0646\n",
      "\n",
      "================================================================================\n",
      "최적 평균 R²: 0.7694\n",
      "최적 평균 MAPE: 0.0646\n",
      "최적 파라미터:\n",
      "  n_estimators: 200\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.05\n",
      "  subsample: 1.0\n",
      "  colsample_bytree: 1.0\n",
      "\n",
      "재분할: Train 1224, Val 262\n",
      "\n",
      "[3/3] 최종 모델 학습 중...\n",
      "\\n================================================================================\n",
      "최종 모델 학습\n",
      "================================================================================\n",
      "\\nTrain: MAPE 0.0098 | R² 0.9989\n",
      "Val:   MAPE 0.0201 | R² 0.9762\n",
      "\\nTop 10 중요 특징:\n",
      "                       feature  importance\n",
      "       ETH_Chain_eth_chain_tvl    0.592596\n",
      "                        WMA_10    0.333654\n",
      "             price_position_30    0.019706\n",
      "                        BBP_10    0.008856\n",
      "Makerdao_makerdao_eth_tvl_lag1    0.008122\n",
      "                        CCI_14    0.005260\n",
      "             price_position_14    0.004821\n",
      "                rolling_max_30    0.003733\n",
      "                   SP500_SP500    0.003528\n",
      "                        WMA_20    0.002312\n",
      "\n",
      "[4/3] 테스트셋 성능 평가 (완전히 unseen data)\n",
      "================================================================================\n",
      "Test: MAPE 0.0402 | R² 0.9837\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "완전 자동화 파이프라인 시작\n",
      "================================================================================\n",
      "Train+Val: 1486, Test: 263 (Test 완전 격리)\n",
      "\n",
      "[1/3] CV 안정성 검증 중... (Train+Val만)\n",
      "\\n================================================================================\n",
      "Time Series CV 안정성 검증 (5 folds)\n",
      "================================================================================\n",
      "\\nFold 1/5: Train 251, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 2/5: Train 498, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 3/5: Train 745, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 4/5: Train 992, Val 247\n",
      "  선택: 25개\n",
      "\\nFold 5/5: Train 1239, Val 247\n",
      "  선택: 25개\n",
      "\\n================================================================================\n",
      "안정성 검증 결과\n",
      "================================================================================\n",
      "안정적인 특징: 25개\n",
      "\\n빈도 Top 10:\n",
      "                feature  frequency  stability_score\n",
      "                 WMA_10          5              1.0\n",
      "                 BBM_30          5              1.0\n",
      "                 WMA_20          5              1.0\n",
      "        rolling_mean_14          5              1.0\n",
      "ETH_Chain_eth_chain_tvl          5              1.0\n",
      "                 BBP_30          4              0.8\n",
      "                 WMA_30          4              0.8\n",
      "         rolling_min_30          4              0.8\n",
      "            SP500_SP500          4              0.8\n",
      "         rolling_max_30          4              0.8\n",
      "\n",
      "[2/3] 기본 파라미터 사용\n",
      "\n",
      "재분할: Train 1224, Val 262\n",
      "\n",
      "[3/3] 최종 모델 학습 중...\n",
      "\\n================================================================================\n",
      "최종 모델 학습\n",
      "================================================================================\n",
      "\\nTrain: MAPE 0.0027 | R² 0.9999\n",
      "Val:   MAPE 0.0160 | R² 0.9808\n",
      "\\nTop 10 중요 특징:\n",
      "                feature  importance\n",
      "                 WMA_10    0.824426\n",
      "ETH_Chain_eth_chain_tvl    0.122901\n",
      "                 BBP_10    0.008825\n",
      "      price_position_30    0.006217\n",
      "         rolling_min_30    0.005525\n",
      "         rolling_max_30    0.005114\n",
      "      price_position_14    0.004709\n",
      "                 CCI_14    0.003273\n",
      "        rolling_mean_14    0.002845\n",
      "                 BBP_30    0.002347\n",
      "\n",
      "[4/3] 테스트셋 성능 평가 (완전히 unseen data)\n",
      "================================================================================\n",
      "Test: MAPE 0.0296 | R² 0.9893\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "완전 자동화 파이프라인 시작\n",
      "================================================================================\n",
      "Train+Val: 1486, Test: 263 (Test 완전 격리)\n",
      "\n",
      "[1/3] 기본 특징 선택 중...\n",
      "\\n================================================================================\n",
      "특징 선택 파이프라인 (개선 버전)\n",
      "  초기 특징: 196개\n",
      "  목표: 25개 (논문 권장)\n",
      "  방법: 가중 투표 (MI:2, RFE:2, SHAP:1)\n",
      "================================================================================\n",
      "\n",
      "[Method 1] Mutual Information 기반 특징 선택...\n",
      "  상위 30개 특징 선택 완료\n",
      "  Top 5: ['WMA_10', 'rolling_max_30', 'WMA_20', 'rolling_mean_14', 'ETH_Chain_eth_chain_tvl']\n",
      "\n",
      "[Method 2] RFE 기반 특징 선택...\n",
      "  상위 30개 특징 선택 완료\n",
      "  Top 5: ['MACD_5_35_5', 'MACDh_5_35_5', 'ROC_5', 'ROC_10', 'ROC_20']\n",
      "\n",
      "[Method 3] SHAP 기반 특징 선택...\n",
      "  SHAP 값 계산 중...\n",
      "  SHAP 값으로 선형 회귀...\n",
      "  통계적으로 유의한 특징: 78개\n",
      "  상위 30개 선택 완료\n",
      "  Top 5: ['USDT_Total_totalCirculatingUSD', 'GOLD_GOLD_lag1', 'Google_Trends_ethereum_lag7', 'USDT_Total_totalCirculatingUSD_lag3', 'USDT_ETH_totalBridgedToUSD_lag7']\n",
      "\\n================================================================================\n",
      "앙상블 특징 선택 (가중 투표)\n",
      "================================================================================\n",
      "\\n가중 투표 결과:\n",
      "  5점 (MI+RFE+SHAP): 1개\n",
      "  4점 (두 개 조합): 8개\n",
      "  3점: 7개\n",
      "  2점 이하: 57개\n",
      "\\n  MI/RFE 필수 조건: 22개 제거 (SHAP 단독)\n",
      "\\n가중 점수 >= 3: 16개\n",
      "  -> 16개 + 9개 추가\n",
      "\\n최종 선택: 25개\n",
      "\\n상위 10개:\n",
      "                     feature  weighted_score  in_MI  in_RFE  in_SHAP\n",
      "                      BBM_30               5   True    True     True\n",
      "                      WMA_10               4   True    True    False\n",
      "                      WMA_20               4   True    True    False\n",
      "             rolling_mean_14               4   True    True    False\n",
      "     ETH_Chain_eth_chain_tvl               4   True    True    False\n",
      "              rolling_min_30               4   True    True    False\n",
      "                      WMA_30               4   True    True    False\n",
      "              rolling_max_30               4   True    True    False\n",
      " USDT_Total_totalCirculating               4   True    True    False\n",
      "USDT_ETH_totalMintedUSD_lag1               3   True   False     True\n",
      "\n",
      "================================================================================\n",
      "카테고리별 특징 밸런싱\n",
      "================================================================================\n",
      "\n",
      "현재 구성:\n",
      "  기술적 지표: 12개\n",
      "  외부 데이터: 13개\n",
      "\n",
      "밸런싱 불필요 (이미 적절한 비율)\n",
      "\\n================================================================================\n",
      "파이프라인 완료\n",
      "  최종: 25개\n",
      "  샘플/특징 비율: 49.0\n",
      "================================================================================\n",
      "\n",
      "[2/3] 하이퍼파라미터 튜닝 중... (Train+Val만)\n",
      "\n",
      "================================================================================\n",
      "하이퍼파라미터 튜닝 (Grid Search)\n",
      "================================================================================\n",
      "✓ GPU 사용 가능! → 'gpu_hist' 모드\n",
      "\n",
      "총 조합 수: 48개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/48] 현재 최고 R²: 0.6953 | MAPE: 0.0689\n",
      "[10/48] 현재 최고 R²: 0.7658 | MAPE: 0.0639\n",
      "[15/48] 현재 최고 R²: 0.7658 | MAPE: 0.0639\n",
      "[20/48] 현재 최고 R²: 0.7658 | MAPE: 0.0639\n",
      "[25/48] 현재 최고 R²: 0.7658 | MAPE: 0.0639\n",
      "[30/48] 현재 최고 R²: 0.7658 | MAPE: 0.0639\n",
      "[35/48] 현재 최고 R²: 0.7865 | MAPE: 0.0602\n",
      "[40/48] 현재 최고 R²: 0.7865 | MAPE: 0.0602\n",
      "[45/48] 현재 최고 R²: 0.7865 | MAPE: 0.0602\n",
      "[48/48] 현재 최고 R²: 0.7865 | MAPE: 0.0602\n",
      "\n",
      "================================================================================\n",
      "최적 평균 R²: 0.7865\n",
      "최적 평균 MAPE: 0.0602\n",
      "최적 파라미터:\n",
      "  n_estimators: 200\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.05\n",
      "  subsample: 1.0\n",
      "  colsample_bytree: 1.0\n",
      "\n",
      "재분할: Train 1224, Val 262\n",
      "\n",
      "[3/3] 최종 모델 학습 중...\n",
      "\\n================================================================================\n",
      "최종 모델 학습\n",
      "================================================================================\n",
      "\\nTrain: MAPE 0.0099 | R² 0.9990\n",
      "Val:   MAPE 0.0182 | R² 0.9792\n",
      "\\nTop 10 중요 특징:\n",
      "                         feature  importance\n",
      "                          WMA_10    0.523981\n",
      "         ETH_Chain_eth_chain_tvl    0.383006\n",
      "                          BBP_10    0.013929\n",
      "     USDT_Total_totalCirculating    0.009135\n",
      "                      fear_greed    0.008693\n",
      "               price_position_14    0.007987\n",
      "                  rolling_max_30    0.007136\n",
      "                          BBP_20    0.005847\n",
      "Google_Trends_ethereum_Change_7d    0.005145\n",
      "                          BBM_30    0.004797\n",
      "\n",
      "[4/3] 테스트셋 성능 평가 (완전히 unseen data)\n",
      "================================================================================\n",
      "Test: MAPE 0.0379 | R² 0.9866\n",
      "\n",
      "================================================================================\n",
      "파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "완전 자동화 파이프라인 시작\n",
      "================================================================================\n",
      "Train+Val: 1486, Test: 263 (Test 완전 격리)\n",
      "\n",
      "[1/3] 기본 특징 선택 중...\n",
      "\\n================================================================================\n",
      "특징 선택 파이프라인 (개선 버전)\n",
      "  초기 특징: 196개\n",
      "  목표: 25개 (논문 권장)\n",
      "  방법: 가중 투표 (MI:2, RFE:2, SHAP:1)\n",
      "================================================================================\n",
      "\n",
      "[Method 1] Mutual Information 기반 특징 선택...\n",
      "  상위 30개 특징 선택 완료\n",
      "  Top 5: ['WMA_10', 'rolling_max_30', 'WMA_20', 'rolling_mean_14', 'ETH_Chain_eth_chain_tvl']\n",
      "\n",
      "[Method 2] RFE 기반 특징 선택...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb269af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e92f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f22b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d9664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################초기 버전 #################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a58247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3단계: 기술적 지표 생성\n",
    "# =============================================================================\n",
    "\n",
    "def generate_technical_indicators(df):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"기술적 지표 생성 시작 (논문 검증된 지표만)...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OHLCV 준비\n",
    "    # -------------------------------------------------------------------------\n",
    "    eth_data = df[['Date','ETH_Open','ETH_High','ETH_Low','ETH_Close','ETH_Volume']].copy()\n",
    "    eth_data['Date'] = pd.to_datetime(eth_data['Date'])\n",
    "    eth_data = eth_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    eth_data.rename(columns={\n",
    "        'ETH_Open': 'open',\n",
    "        'ETH_High': 'high',\n",
    "        'ETH_Low': 'low',\n",
    "        'ETH_Close': 'close',\n",
    "        'ETH_Volume': 'volume'\n",
    "    }, inplace=True)\n",
    "\n",
    "    initial_cols = len(eth_data.columns)\n",
    "\n",
    "    # ======================================================================\n",
    "    # 1. 모멘텀 지표 (논문에서 가장 많이 사용됨)\n",
    "    # ======================================================================\n",
    "    print(\"\\n1. 모멘텀 지표 생성 중...\")\n",
    "\n",
    "    # RSI - 논문에서 가장 많이 언급됨\n",
    "    for period in [9, 14, 21, 30]:\n",
    "        delta = eth_data['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        eth_data[f'RSI_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD - RSI 다음으로 많이 사용됨\n",
    "    # 12-26-9 설정\n",
    "    ema_12 = eth_data['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = eth_data['close'].ewm(span=26, adjust=False).mean()\n",
    "    eth_data['MACD_12_26_9'] = ema_12 - ema_26\n",
    "    eth_data['MACDs_12_26_9'] = eth_data['MACD_12_26_9'].ewm(span=9, adjust=False).mean()\n",
    "    eth_data['MACDh_12_26_9'] = eth_data['MACD_12_26_9'] - eth_data['MACDs_12_26_9']\n",
    "    \n",
    "    # 5-35-5 설정 (추가 변형)\n",
    "    ema_5 = eth_data['close'].ewm(span=5, adjust=False).mean()\n",
    "    ema_35 = eth_data['close'].ewm(span=35, adjust=False).mean()\n",
    "    eth_data['MACD_5_35_5'] = ema_5 - ema_35\n",
    "    eth_data['MACDs_5_35_5'] = eth_data['MACD_5_35_5'].ewm(span=5, adjust=False).mean()\n",
    "    eth_data['MACDh_5_35_5'] = eth_data['MACD_5_35_5'] - eth_data['MACDs_5_35_5']\n",
    "    \n",
    "    # ROC (Rate of Change)\n",
    "    for period in [5, 10, 20, 30]:\n",
    "        eth_data[f'ROC_{period}'] = eth_data['close'].pct_change(period) * 100\n",
    "    \n",
    "    # Momentum (MOM)\n",
    "    for period in [10, 20, 30]:\n",
    "        eth_data[f'MOM_{period}'] = eth_data['close'].diff(period)\n",
    "    \n",
    "    # Stochastic Oscillator (%K, %D)\n",
    "    for period in [14, 30, 200]:\n",
    "        low_min = eth_data['low'].rolling(window=period).min()\n",
    "        high_max = eth_data['high'].rolling(window=period).max()\n",
    "        eth_data[f'%K_{period}'] = 100 * ((eth_data['close'] - low_min) / (high_max - low_min))\n",
    "        eth_data[f'%D_{period}'] = eth_data[f'%K_{period}'].rolling(window=3).mean()\n",
    "    \n",
    "    # Williams %R\n",
    "    for period in [14, 28]:\n",
    "        high_max = eth_data['high'].rolling(window=period).max()\n",
    "        low_min = eth_data['low'].rolling(window=period).min()\n",
    "        eth_data[f'WILLR_{period}'] = -100 * ((high_max - eth_data['close']) / (high_max - low_min))\n",
    "    \n",
    "    # CCI (Commodity Channel Index)\n",
    "    for period in [14, 20]:\n",
    "        tp = (eth_data['high'] + eth_data['low'] + eth_data['close']) / 3\n",
    "        sma_tp = tp.rolling(window=period).mean()\n",
    "        mad = (tp - sma_tp).abs().rolling(window=period).mean()\n",
    "        eth_data[f'CCI_{period}'] = (tp - sma_tp) / (0.015 * mad)\n",
    "    \n",
    "    # MFI (Money Flow Index)\n",
    "    tp = (eth_data['high'] + eth_data['low'] + eth_data['close']) / 3\n",
    "    mf = tp * eth_data['volume']\n",
    "    \n",
    "    period = 14\n",
    "    pos_mf = (mf.where(tp.diff() > 0, 0)).rolling(window=period).sum()\n",
    "    neg_mf = (mf.where(tp.diff() < 0, 0)).rolling(window=period).sum()\n",
    "    mfi = 100 - (100 / (1 + pos_mf / neg_mf))\n",
    "    eth_data['MFI_14'] = mfi\n",
    "\n",
    "    print(f\"   → {len(eth_data.columns)-initial_cols}개 모멘텀 지표 생성 완료\")\n",
    "\n",
    "    # ======================================================================\n",
    "    # 2. 변동성 지표\n",
    "    # ======================================================================\n",
    "    print(\"\\n2. 변동성 지표 생성 중...\")\n",
    "    vol_start = len(eth_data.columns)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    for period in [10, 20, 30]:\n",
    "        sma = eth_data['close'].rolling(window=period).mean()\n",
    "        std = eth_data['close'].rolling(window=period).std()\n",
    "        eth_data[f'BBL_{period}'] = sma - (2 * std)\n",
    "        eth_data[f'BBM_{period}'] = sma\n",
    "        eth_data[f'BBU_{period}'] = sma + (2 * std)\n",
    "        eth_data[f'BBW_{period}'] = (eth_data[f'BBU_{period}'] - eth_data[f'BBL_{period}']) / eth_data[f'BBM_{period}']\n",
    "        eth_data[f'BBP_{period}'] = (eth_data['close'] - eth_data[f'BBL_{period}']) / (eth_data[f'BBU_{period}'] - eth_data[f'BBL_{period}'])\n",
    "    \n",
    "    # ATR (Average True Range)\n",
    "    for period in [7, 14, 21]:\n",
    "        high_low = eth_data['high'] - eth_data['low']\n",
    "        high_close = (eth_data['high'] - eth_data['close'].shift()).abs()\n",
    "        low_close = (eth_data['low'] - eth_data['close'].shift()).abs()\n",
    "        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        eth_data[f'ATR_{period}'] = tr.rolling(window=period).mean()\n",
    "    \n",
    "    # Historical Volatility\n",
    "    for period in [10, 20, 30]:\n",
    "        returns = eth_data['close'].pct_change()\n",
    "        eth_data[f'HV_{period}'] = returns.rolling(window=period).std() * np.sqrt(252)\n",
    "\n",
    "    print(f\"   → {len(eth_data.columns)-vol_start}개 변동성 지표 생성 완료\")\n",
    "\n",
    "    # ======================================================================\n",
    "    # 3. 거래량 지표\n",
    "    # ======================================================================\n",
    "    print(\"\\n3. 거래량 지표 생성 중...\")\n",
    "    volu_start = len(eth_data.columns)\n",
    "\n",
    "    # OBV (On Balance Volume)\n",
    "    obv = [0]\n",
    "    for i in range(1, len(eth_data)):\n",
    "        if eth_data['close'].iloc[i] > eth_data['close'].iloc[i-1]:\n",
    "            obv.append(obv[-1] + eth_data['volume'].iloc[i])\n",
    "        elif eth_data['close'].iloc[i] < eth_data['close'].iloc[i-1]:\n",
    "            obv.append(obv[-1] - eth_data['volume'].iloc[i])\n",
    "        else:\n",
    "            obv.append(obv[-1])\n",
    "    eth_data['OBV'] = obv\n",
    "    \n",
    "    # AD (Accumulation/Distribution)\n",
    "    clv = ((eth_data['close'] - eth_data['low']) - (eth_data['high'] - eth_data['close'])) / (eth_data['high'] - eth_data['low'])\n",
    "    clv = clv.fillna(0)\n",
    "    eth_data['AD'] = (clv * eth_data['volume']).cumsum()\n",
    "    \n",
    "    # CMF (Chaikin Money Flow)\n",
    "    period = 20\n",
    "    mfv = clv * eth_data['volume']\n",
    "    eth_data['CMF_20'] = mfv.rolling(window=period).sum() / eth_data['volume'].rolling(window=period).sum()\n",
    "    \n",
    "    # Volume ROC\n",
    "    for period in [5, 10, 20]:\n",
    "        eth_data[f'VROC_{period}'] = eth_data['volume'].pct_change(period) * 100\n",
    "    \n",
    "    # Volume MA\n",
    "    for period in [5, 10, 20]:\n",
    "        eth_data[f'Volume_MA_{period}'] = eth_data['volume'].rolling(period).mean()\n",
    "    \n",
    "    # Volume Ratio\n",
    "    for window in [5, 10, 20]:\n",
    "        eth_data[f'Volume_Ratio_{window}'] = eth_data['volume'] / eth_data['volume'].rolling(window).mean()\n",
    "\n",
    "    print(f\"   → {len(eth_data.columns)-volu_start}개 거래량 지표 생성 완료\")\n",
    "\n",
    "    # ======================================================================\n",
    "    # 4. 추세 지표\n",
    "    # ======================================================================\n",
    "    print(\"\\n4. 추세 지표 생성 중...\")\n",
    "    trend_start = len(eth_data.columns)\n",
    "\n",
    "    # SMA (Simple Moving Average)\n",
    "    for period in [5, 10, 20, 50, 100, 200]:\n",
    "        eth_data[f'SMA_{period}'] = eth_data['close'].rolling(window=period).mean()\n",
    "    \n",
    "    # EMA (Exponential Moving Average)\n",
    "    for period in [5, 10, 20, 50, 100, 200]:\n",
    "        eth_data[f'EMA_{period}'] = eth_data['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # WMA (Weighted Moving Average)\n",
    "    def wma(series, period):\n",
    "        weights = np.arange(1, period + 1)\n",
    "        return series.rolling(period).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n",
    "    \n",
    "    for period in [10, 20, 30]:\n",
    "        eth_data[f'WMA_{period}'] = wma(eth_data['close'], period)\n",
    "    \n",
    "    # ADX (Average Directional Index)\n",
    "    period = 14\n",
    "    high_low = eth_data['high'] - eth_data['low']\n",
    "    high_close = (eth_data['high'] - eth_data['close'].shift()).abs()\n",
    "    low_close = (eth_data['low'] - eth_data['close'].shift()).abs()\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=period).mean()\n",
    "    \n",
    "    up_move = eth_data['high'].diff()\n",
    "    down_move = -eth_data['low'].diff()\n",
    "    \n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)\n",
    "    \n",
    "    plus_di = 100 * (pd.Series(plus_dm).rolling(window=period).mean() / atr)\n",
    "    minus_di = 100 * (pd.Series(minus_dm).rolling(window=period).mean() / atr)\n",
    "    \n",
    "    dx = 100 * ((plus_di - minus_di).abs() / (plus_di + minus_di))\n",
    "    eth_data['ADX_14'] = dx.rolling(window=period).mean()\n",
    "    eth_data['DI+_14'] = plus_di\n",
    "    eth_data['DI-_14'] = minus_di\n",
    "\n",
    "    print(f\"   → {len(eth_data.columns)-trend_start}개 추세 지표 생성 완료\")\n",
    "\n",
    "    # ======================================================================\n",
    "    # 5. 추가 파생 변수\n",
    "    # ======================================================================\n",
    "    print(\"\\n5. 추가 파생 변수 생성 중...\")\n",
    "    derived_start = len(eth_data.columns)\n",
    "\n",
    "    # Log Returns\n",
    "    eth_data['log_return'] = np.log(eth_data['close'] / eth_data['close'].shift(1))\n",
    "    \n",
    "    # Price Changes\n",
    "    for period in [1, 3, 5, 7, 14]:\n",
    "        eth_data[f'price_change_{period}d'] = eth_data['close'].pct_change(period)\n",
    "    \n",
    "    # Rolling Statistics\n",
    "    for window in [7, 14, 30]:\n",
    "        eth_data[f'rolling_mean_{window}'] = eth_data['close'].rolling(window).mean()\n",
    "        eth_data[f'rolling_std_{window}'] = eth_data['close'].rolling(window).std()\n",
    "        eth_data[f'rolling_min_{window}'] = eth_data['close'].rolling(window).min()\n",
    "        eth_data[f'rolling_max_{window}'] = eth_data['close'].rolling(window).max()\n",
    "    \n",
    "    # Price Position (현재가가 최근 N일 범위에서 어디쯤인지)\n",
    "    for window in [14, 30, 60]:\n",
    "        roll_min = eth_data['close'].rolling(window).min()\n",
    "        roll_max = eth_data['close'].rolling(window).max()\n",
    "        eth_data[f'price_position_{window}'] = (eth_data['close'] - roll_min) / (roll_max - roll_min + 1e-10)\n",
    "    \n",
    "    # High-Low 비율\n",
    "    eth_data['high_low_ratio'] = eth_data['high'] / eth_data['low']\n",
    "    \n",
    "    # Close 위치 (일간 범위 내)\n",
    "    eth_data['close_location'] = (eth_data['close'] - eth_data['low']) / (eth_data['high'] - eth_data['low'] + 1e-10)\n",
    "\n",
    "    print(f\"   → {len(eth_data.columns)-derived_start}개 파생 변수 생성 완료\")\n",
    "\n",
    "    # 컬럼명 복원\n",
    "    eth_data.rename(columns={\n",
    "        'open':'ETH_Open',\n",
    "        'high':'ETH_High',\n",
    "        'low':'ETH_Low',\n",
    "        'close':'ETH_Close',\n",
    "        'volume':'ETH_Volume'\n",
    "    }, inplace=True)\n",
    "\n",
    "    total_indicators = len(eth_data.columns) - 6  # Date+OHLCV 제외\n",
    "    print(f\"\\n✓ 총 {total_indicators}개의 기술적 지표 생성 완료!\")\n",
    "    \n",
    "\n",
    "    return eth_data\n",
    "\n",
    "\n",
    "# 기술적 지표 생성\n",
    "eth_with_indicators = generate_technical_indicators(data['macro_crypto'])\n",
    "\n",
    "\n",
    "def merge_all_features(data, eth_with_indicators):\n",
    "    \"\"\"\n",
    "    모든 CSV 파일의 데이터를 eth_with_indicators에 병합\n",
    "    - 자동으로 최적 날짜 범위 찾기\n",
    "    - Forward fill 자동 적용\n",
    "    - Lag features 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"외부 데이터 병합 시작...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 날짜 범위 분석\n",
    "    print(\"\\\\n데이터 날짜 범위 분석 중...\")\n",
    "    \n",
    "    date_ranges = {}\n",
    "    for key, df in data.items():\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            temp_df = df.copy()\n",
    "            temp_df[date_col] = pd.to_datetime(temp_df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            temp_df = temp_df.dropna(subset=[date_col])\n",
    "            \n",
    "            if len(temp_df) > 0:\n",
    "                date_ranges[key] = {\n",
    "                    'start': temp_df[date_col].min(),\n",
    "                    'end': temp_df[date_col].max(),\n",
    "                    'rows': len(temp_df)\n",
    "                }\n",
    "    \n",
    "    print(f\"\\\\n{'Dataset':<25} {'Start':<12} {'End':<12} {'Rows':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for key, info in sorted(date_ranges.items(), key=lambda x: x[1]['start']):\n",
    "        print(f\"{key:<25} {str(info['start'].date()):<12} {str(info['end'].date()):<12} {info['rows']:<10,}\")\n",
    "    \n",
    "    start_dates = [info['start'] for info in date_ranges.values()]\n",
    "    end_dates = [info['end'] for info in date_ranges.values()]\n",
    "    \n",
    "    filter_start = max(start_dates)\n",
    "    #filter_end = min(end_dates)\n",
    "    filter_end=\"2025-10-03\"\n",
    "    \n",
    "    #print(f\"\\\\n선택된 기간: {filter_start.date()} ~ {filter_end.date()}\")\n",
    "    #print(f\"예상 샘플: {(filter_end - filter_start).days}일\")\n",
    "    \n",
    "    # 기준 데이터 필터링\n",
    "    merged_df = eth_with_indicators.copy()\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date']).dt.tz_localize(None)\n",
    "    original_rows = len(merged_df)\n",
    "    merged_df = merged_df[(merged_df['Date'] >= filter_start) & (merged_df['Date'] <= filter_end)]\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\\\nETH 기술적 지표: {original_rows}행 -> {len(merged_df)}행\")\n",
    "    initial_cols = len(merged_df.columns)\n",
    "    \n",
    "    # 병합 헬퍼 함수\n",
    "    def merge_with_ffill(merged_df, df, name=\"\"):\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        \n",
    "        if len(date_cols) == 0:\n",
    "            print(f\"   -> {name}: 날짜 컬럼 없음\")\n",
    "            return merged_df, 0\n",
    "        \n",
    "        date_col = date_cols[0]\n",
    "        df = df.copy()\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        df = df.rename(columns={date_col: 'Date'})\n",
    "        df = df[(df['Date'] >= filter_start) & (df['Date'] <= filter_end)]\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(f\"   -> {name}: 데이터 없음\")\n",
    "            return merged_df, 0\n",
    "        \n",
    "        value_cols = [col for col in df.columns if col != 'Date']\n",
    "        before_cols = set(merged_df.columns)\n",
    "        merged_df = merged_df.merge(df, on='Date', how='left')\n",
    "        new_cols = list(set(merged_df.columns) - before_cols)\n",
    "        \n",
    "        for col in new_cols:\n",
    "            merged_df[col] = merged_df[col].fillna(method='ffill')\n",
    "        \n",
    "        missing_pct = merged_df[new_cols].isnull().sum().sum() / (len(merged_df) * len(new_cols)) * 100\n",
    "        print(f\"   -> {name}: {len(value_cols)}개 컬럼 (결측치 {missing_pct:.1f}%)\")\n",
    "        \n",
    "        return merged_df, len(value_cols)\n",
    "    \n",
    "    # 1. Fear & Greed Index\n",
    "    print(\"\\\\n1. Fear & Greed Index\")\n",
    "    if 'fear_greed' in data:\n",
    "        merged_df, n_cols = merge_with_ffill(merged_df, data['fear_greed'], \"Fear&Greed\")\n",
    "    \n",
    "    # 2. 전통 금융 지표\n",
    "    print(\"\\\\n2. 전통 금융 지표\")\n",
    "    for name in ['sp500', 'vix', 'gold', 'dxy']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{name.upper()}_{col}\" for col in value_cols})\n",
    "                merged_df, n_cols = merge_with_ffill(merged_df, df, name.upper())\n",
    "    \n",
    "    # 3. ETH Funding Rate\n",
    "    print(\"\\\\n3. ETH Funding Rate\")\n",
    "    if 'eth_funding' in data:\n",
    "        df = data['eth_funding'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"ETH_Funding_{col}\" for col in value_cols})\n",
    "            merged_df, n_cols = merge_with_ffill(merged_df, df, \"Funding\")\n",
    "    \n",
    "    # 4. ETH Chain TVL\n",
    "    print(\"\\\\n4. ETH Chain TVL\")\n",
    "    if 'eth_chain_tvl' in data:\n",
    "        df = data['eth_chain_tvl'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"ETH_Chain_{col}\" for col in value_cols})\n",
    "            merged_df, n_cols = merge_with_ffill(merged_df, df, \"ChainTVL\")\n",
    "    \n",
    "    # 5. DeFi TVL\n",
    "    print(\"\\\\n5. DeFi TVL\")\n",
    "    for name in ['makerdao_tvl', 'lido_tvl', 'aave_tvl']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                protocol_name = name.split('_')[0].capitalize()\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{protocol_name}_{col}\" for col in value_cols})\n",
    "                merged_df, n_cols = merge_with_ffill(merged_df, df, protocol_name)\n",
    "    \n",
    "    # 6. USDT Market Cap\n",
    "    print(\"\\\\n6. USDT Market Cap\")\n",
    "    for name in ['usdt_total', 'usdt_eth']:\n",
    "        if name in data:\n",
    "            df = data[name].copy()\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            if len(date_cols) > 0:\n",
    "                date_col = date_cols[0]\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "                df = df.rename(columns={date_col: 'Date'})\n",
    "                prefix = \"USDT_Total\" if 'total' in name else \"USDT_ETH\"\n",
    "                value_cols = [col for col in df.columns if col != 'Date']\n",
    "                df = df.rename(columns={col: f\"{prefix}_{col}\" for col in value_cols})\n",
    "                merged_df, n_cols = merge_with_ffill(merged_df, df, prefix)\n",
    "    \n",
    "    # 7. Google Trends\n",
    "    print(\"\\\\n7. Google Trends\")\n",
    "    if 'google_trends_weekly' in data:\n",
    "        df = data['google_trends_weekly'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'week' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"Google_Trends_{col}\" for col in value_cols})\n",
    "            merged_df, n_cols = merge_with_ffill(merged_df, df, \"GoogleTrends\")\n",
    "    \n",
    "    # 8. 뉴스 감성 데이터\n",
    "    print(\"\\\\n8. 뉴스 감성 데이터\")\n",
    "    if 'news' in data:\n",
    "        news = data['news'].copy()\n",
    "        date_cols = [col for col in news.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            news[date_col] = pd.to_datetime(news[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            news = news.rename(columns={date_col: 'Date'})\n",
    "            news = news[(news['Date'] >= filter_start) & (news['Date'] <= filter_end)]\n",
    "            \n",
    "            if len(news) > 0:\n",
    "                news['Date'] = news['Date'].dt.date\n",
    "                sentiment_cols = [col for col in news.columns if 'sentiment' in col.lower() or 'score' in col.lower()]\n",
    "                \n",
    "                if len(sentiment_cols) > 0:\n",
    "                    news_daily = news.groupby('Date').agg({\n",
    "                        sentiment_cols[0]: ['mean', 'std', 'count']\n",
    "                    }).reset_index()\n",
    "                    news_daily.columns = ['Date', 'News_Sentiment_Mean', 'News_Sentiment_Std', 'News_Count']\n",
    "                    news_daily['Date'] = pd.to_datetime(news_daily['Date'])\n",
    "                    merged_df, n_cols = merge_with_ffill(merged_df, news_daily, \"News\")\n",
    "                else:\n",
    "                    news_daily = news.groupby('Date').size().reset_index(name='News_Count')\n",
    "                    news_daily['Date'] = pd.to_datetime(news_daily['Date'])\n",
    "                    merged_df, n_cols = merge_with_ffill(merged_df, news_daily, \"News\")\n",
    "    \n",
    "    # 9. ETH On-Chain Data\n",
    "    print(\"\\\\n9. ETH On-Chain Data\")\n",
    "    if 'eth_onchain' in data and len(data['eth_onchain']) > 0:\n",
    "        df = data['eth_onchain'].copy()\n",
    "        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if len(date_cols) > 0:\n",
    "            date_col = date_cols[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.tz_localize(None)\n",
    "            df = df.rename(columns={date_col: 'Date'})\n",
    "            value_cols = [col for col in df.columns if col != 'Date']\n",
    "            df = df.rename(columns={col: f\"OnChain_{col}\" for col in value_cols})\n",
    "            merged_df, n_cols = merge_with_ffill(merged_df, df, \"OnChain\")\n",
    "        else:\n",
    "            print(\"   -> 날짜 컬럼 없음\")\n",
    "    else:\n",
    "        print(\"   -> 데이터 없음\")\n",
    "    \n",
    "    # 10. 파생 변수 및 Lag Features 생성\n",
    "    print(\"\\\\n10. 파생 변수 및 Lag Features 생성\")\n",
    "    derived_count = 0\n",
    "    \n",
    "    # Google Trends 파생 변수\n",
    "    trend_cols = [col for col in merged_df.columns if 'Google_Trends' in col and 'Change' not in col and 'MA' not in col and 'lag' not in col]\n",
    "    if len(trend_cols) > 0:\n",
    "        trend_col = trend_cols[0]\n",
    "        merged_df[f'{trend_col}_Change_7d'] = merged_df[trend_col].pct_change(7)\n",
    "        merged_df[f'{trend_col}_MA_7'] = merged_df[trend_col].rolling(7).mean()\n",
    "        derived_count += 2\n",
    "    \n",
    "    # Fear & Greed 파생 변수\n",
    "    fg_cols = [col for col in merged_df.columns if col.lower() == 'value' or ('fear' in col.lower() and 'Change' not in col and 'lag' not in col)]\n",
    "    if len(fg_cols) > 0:\n",
    "        fg_col = fg_cols[0]\n",
    "        merged_df['FearGreed_Change_1d'] = merged_df[fg_col].diff()\n",
    "        merged_df['FearGreed_MA_7'] = merged_df[fg_col].rolling(7).mean()\n",
    "        derived_count += 2\n",
    "    \n",
    "    # TVL 파생 변수\n",
    "    tvl_cols = [col for col in merged_df.columns if ('TVL' in col or 'tvl' in col) and 'Change' not in col and 'lag' not in col]\n",
    "    for col in tvl_cols[:3]:\n",
    "        merged_df[f'{col}_Change_7d'] = merged_df[col].pct_change(7)\n",
    "        derived_count += 1\n",
    "    \n",
    "    print(f\"   -> 파생 변수: {derived_count}개\")\n",
    "    \n",
    "    # Lag Features 생성\n",
    "    lag_periods = [1, 3, 7]\n",
    "    external_cols = [col for col in merged_df.columns \n",
    "                     if any(x in col for x in ['SP500', 'VIX', 'GOLD', 'DXY', \n",
    "                                                'OnChain', 'Funding', 'value',\n",
    "                                                'USDT', 'Google_Trends', 'News',\n",
    "                                                'Makerdao', 'Lido', 'Aave', 'ETH_Chain'])\n",
    "                     and 'Change' not in col and 'MA' not in col and 'lag' not in col]\n",
    "    \n",
    "    lag_count = 0\n",
    "    for col in external_cols:\n",
    "        for lag in lag_periods:\n",
    "            merged_df[f'{col}_lag{lag}'] = merged_df[col].shift(lag)\n",
    "            lag_count += 1\n",
    "    \n",
    "    print(f\"   -> Lag Features: {lag_count}개 ({len(external_cols)}개 컬럼 x {len(lag_periods)}개 lag)\")\n",
    "    \n",
    "    # 최종 정리\n",
    "    total_added = len(merged_df.columns) - initial_cols\n",
    "    features = [col for col in merged_df.columns if col not in ['Date', 'ETH_Open', 'ETH_High', 'ETH_Low', 'ETH_Close', 'ETH_Volume']]\n",
    "    missing_pct = merged_df[features].isnull().sum().sum() / (len(merged_df) * len(features)) * 100\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(f\"완료: {total_added}개 특징 추가\")\n",
    "    print(f\"  기존: {initial_cols}개 -> 최종: {len(merged_df.columns)}개\")\n",
    "    print(f\"  샘플: {len(merged_df)}개\")\n",
    "    print(f\"  기간: {merged_df['Date'].min().date()} ~ {merged_df['Date'].max().date()}\")\n",
    "    print(f\"  결측치: {missing_pct:.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# 외부 데이터 병합 (날짜 필터링 + forward fill 포함)\n",
    "merged_data = merge_all_features(data, eth_with_indicators)\n",
    "\n",
    "print(f\"\\\\n병합 후 데이터 shape: {merged_data.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"데이터 준비\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 데이터 준비 단계에서\n",
    "features_to_exclude = ['Date', 'ETH_Open', 'ETH_High', 'ETH_Low', \n",
    "                       'ETH_Close', 'ETH_Volume'] \n",
    "# 특징 컬럼만 추출\n",
    "all_feature_cols = [col for col in merged_data.columns if col not in features_to_exclude]\n",
    "\n",
    "# 무한대 값을 NaN으로 변환\n",
    "merged_data[all_feature_cols] = merged_data[all_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 결측치 통계\n",
    "missing_ratio = merged_data[all_feature_cols].isnull().sum() / len(merged_data)\n",
    "\n",
    "print(f\"\\n전체 특징: {len(all_feature_cols)}개\")\n",
    "print(f\"평균 결측치 비율: {missing_ratio.mean()*100:.2f}%\")\n",
    "print(f\"결측치 > 5% 인 특징: {(missing_ratio > 0.05).sum()}개\")\n",
    "\n",
    "# 결측치가 많은 컬럼만 제거 (threshold 완화)\n",
    "threshold = 0.5  # 30% → 50%로 완화\n",
    "valid_features = missing_ratio[missing_ratio < threshold].index.tolist()\n",
    "removed_features = missing_ratio[missing_ratio >= threshold].index.tolist()\n",
    "\n",
    "print(f\"\\n결측치 {threshold*100}% 이상 컬럼 제거:\")\n",
    "print(f\"  유지: {len(valid_features)}개\")\n",
    "print(f\"  제거: {len(removed_features)}개\")\n",
    "\n",
    "if len(removed_features) > 0:\n",
    "    print(f\"\\\\n제거된 컬럼:\")\n",
    "    for col in removed_features[:10]:  # 상위 10개만 출력\n",
    "        print(f\"  - {col} (결측치 {missing_ratio[col]*100:.1f}%)\")\n",
    "    if len(removed_features) > 10:\n",
    "        print(f\"  ... 외 {len(removed_features)-10}개\")\n",
    "\n",
    "# X, y 분리\n",
    "X = merged_data[valid_features].copy()\n",
    "y = merged_data['ETH_Close'].copy()\n",
    "\n",
    "# Interpolation으로 남은 결측치 처리\n",
    "print(f\"\\n결측치 처리 (interpolation + ffill/bfill)...\")\n",
    "print(f\"  처리 전 결측치: {X.isnull().sum().sum()}개\")\n",
    "\n",
    "X = X.interpolate(method='linear', limit_direction='both')\n",
    "X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(f\"  처리 후 결측치: {X.isnull().sum().sum()}개\")\n",
    "\n",
    "# 그래도 남은 결측치가 있으면 제거\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    before = len(X)\n",
    "    valid_indices = X.dropna().index\n",
    "    X = X.loc[valid_indices]\n",
    "    y = y.loc[valid_indices]\n",
    "    after = len(X)\n",
    "    print(f\"  결측치 있는 행 제거: {before}행 → {after}행 (제거: {before-after}행)\")\n",
    "else:\n",
    "    valid_indices = X.index\n",
    "\n",
    "print(f\"\\n최종 데이터셋:\")\n",
    "print(f\"  특징 개수: {X.shape[1]}개\")\n",
    "print(f\"  샘플 개수: {X.shape[0]}개\")\n",
    "print(f\"  결측치: {X.isnull().sum().sum()}개\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4단계: 특징 선택 - Method 1: Mutual Information\n",
    "# =============================================================================\n",
    "\n",
    "def select_features_mi(X, y, k=20):\n",
    "    \"\"\"\n",
    "    Mutual Information 기반 특징 선택\n",
    "    비선형 관계를 잘 포착하는 장점\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    \n",
    "    print(\"\\n[Method 1] Mutual Information 기반 특징 선택...\")\n",
    "    \n",
    "    # MI 계산\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42, n_neighbors=5)\n",
    "    \n",
    "    # 점수를 DataFrame으로 정리\n",
    "    mi_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mi_score': mi_scores\n",
    "    }).sort_values('mi_score', ascending=False)\n",
    "    \n",
    "    # 상위 k개 선택\n",
    "    top_features = mi_df.head(k)['feature'].tolist()\n",
    "    \n",
    "    print(f\"  상위 {k}개 특징 선택 완료\")\n",
    "    print(f\"  Top 5: {top_features[:5]}\")\n",
    "    \n",
    "    return top_features, mi_df\n",
    "\n",
    "# =============================================================================\n",
    "# 5단계: 특징 선택 - Method 2: RFE (Recursive Feature Elimination)\n",
    "# =============================================================================\n",
    "\n",
    "def select_features_rfe(X, y, k=20):\n",
    "    \"\"\"\n",
    "    RFE 기반 특징 선택\n",
    "    특징 간 상호작용을 고려하는 장점\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    print(\"\\n[Method 2] RFE 기반 특징 선택...\")\n",
    "    \n",
    "    # XGBoost 모델 사용\n",
    "    model = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # RFE 실행\n",
    "    rfe = RFE(estimator=model, n_features_to_select=k, step=5)\n",
    "    rfe.fit(X, y)\n",
    "    \n",
    "    # 선택된 특징\n",
    "    selected_features = X.columns[rfe.support_].tolist()\n",
    "    \n",
    "    # 특징 중요도 정리\n",
    "    rfe_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'selected': rfe.support_,\n",
    "        'ranking': rfe.ranking_\n",
    "    }).sort_values('ranking')\n",
    "    \n",
    "    print(f\"  상위 {k}개 특징 선택 완료\")\n",
    "    print(f\"  Top 5: {selected_features[:5]}\")\n",
    "    \n",
    "    return selected_features, rfe_df\n",
    "\n",
    "# =============================================================================\n",
    "# 6단계: 특징 선택 - Method 3: SHAP 기반\n",
    "# =============================================================================\n",
    "\n",
    "def select_features_shap(X_train, y_train, X_val, y_val, k=20, p_value_threshold=0.05):\n",
    "    \"\"\"\n",
    "    SHAP 값 기반 특징 선택\n",
    "    해석 가능성과 통계적 유의성을 동시에 고려\n",
    "    \n",
    "    수정사항: SHAP 값이 모두 동일한 특징 필터링 추가\n",
    "    \"\"\"\n",
    "    import shap\n",
    "    from xgboost import XGBRegressor\n",
    "    from scipy import stats\n",
    "    \n",
    "    print(\"\\n[Method 3] SHAP 기반 특징 선택...\")\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # SHAP 값 계산\n",
    "    print(\"  SHAP 값 계산 중...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_val)\n",
    "    \n",
    "    # SHAP 값으로 타겟 회귀\n",
    "    print(\"  SHAP 값으로 선형 회귀...\")\n",
    "    shap_df = pd.DataFrame(shap_values, columns=X_val.columns)\n",
    "    \n",
    "    selected_features = []\n",
    "    feature_scores = []\n",
    "    \n",
    "    for col in shap_df.columns:\n",
    "        shap_col = shap_df[col]\n",
    "        \n",
    "        # SHAP 값이 모두 동일한 경우 스킵 (분산이 0에 가까움)\n",
    "        if shap_col.std() < 1e-10 or shap_col.nunique() == 1:\n",
    "            continue\n",
    "        \n",
    "        # NaN이나 Inf 체크\n",
    "        if shap_col.isnull().any() or np.isinf(shap_col).any():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 선형 회귀: y_val ~ shap_value\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(shap_col, y_val)\n",
    "            \n",
    "            # 양의 계수 & 통계적으로 유의한 특징만 선택\n",
    "            if slope > 0 and p_value < p_value_threshold:\n",
    "                selected_features.append(col)\n",
    "                feature_scores.append({\n",
    "                    'feature': col,\n",
    "                    'coefficient': slope,\n",
    "                    'p_value': p_value,\n",
    "                    'r_squared': r_value**2,\n",
    "                    'shap_mean_abs': shap_col.abs().mean()\n",
    "                })\n",
    "        except Exception as e:\n",
    "            # 계산 실패한 특징은 스킵\n",
    "            continue\n",
    "    \n",
    "    # 계수 크기 기준으로 정렬\n",
    "    if len(feature_scores) > 0:\n",
    "        shap_result_df = pd.DataFrame(feature_scores).sort_values('coefficient', ascending=False)\n",
    "        top_features = shap_result_df.head(k)['feature'].tolist()\n",
    "    else:\n",
    "        # SHAP 회귀가 실패한 경우, SHAP 절대값 평균으로 폴백\n",
    "        print(\"  경고: SHAP 선형 회귀 실패. 절대값 평균으로 선택...\")\n",
    "        shap_importance = shap_df.abs().mean().sort_values(ascending=False)\n",
    "        top_features = shap_importance.head(k).index.tolist()\n",
    "        \n",
    "        shap_result_df = pd.DataFrame({\n",
    "            'feature': shap_importance.index,\n",
    "            'shap_mean_abs': shap_importance.values,\n",
    "            'coefficient': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'r_squared': np.nan\n",
    "        })\n",
    "    \n",
    "    print(f\"  통계적으로 유의한 특징: {len(selected_features)}개\")\n",
    "    print(f\"  상위 {k}개 선택 완료\")\n",
    "    if len(top_features) > 0:\n",
    "        print(f\"  Top 5: {top_features[:5]}\")\n",
    "    \n",
    "    return top_features, shap_result_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7단계: 앙상블 특징 선택\n",
    "# =============================================================================\n",
    "\n",
    "def ensemble_feature_selection(mi_features, rfe_features, shap_features, \n",
    "                                target_count=25, min_votes=2):\n",
    "    \"\"\"\n",
    "    3가지 방법의 결과를 앙상블 (가중 투표 + 안정성 강화)\n",
    "    \n",
    "    개선사항:\n",
    "      1. 가중 투표: MI(2) + RFE(2) + SHAP(1)\n",
    "      2. MI 또는 RFE 필수 조건 (SHAP 단독 제거)\n",
    "      3. 점수 순으로 정확히 target_count 선택\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"앙상블 특징 선택 (가중 투표)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 가중 투표\n",
    "    weighted_votes = Counter()\n",
    "    \n",
    "    for feat in mi_features:\n",
    "        weighted_votes[feat] += 2\n",
    "    \n",
    "    for feat in rfe_features:\n",
    "        weighted_votes[feat] += 2\n",
    "    \n",
    "    for feat in shap_features:\n",
    "        weighted_votes[feat] += 1\n",
    "    \n",
    "    # 투표 결과 정리\n",
    "    vote_df = pd.DataFrame([\n",
    "        {\n",
    "            'feature': feat,\n",
    "            'weighted_score': score,\n",
    "            'normalized_score': score / 5.0,\n",
    "            'in_MI': feat in mi_features,\n",
    "            'in_RFE': feat in rfe_features,\n",
    "            'in_SHAP': feat in shap_features,\n",
    "            'vote_count': sum([feat in mi_features, feat in rfe_features, feat in shap_features])\n",
    "        }\n",
    "        for feat, score in weighted_votes.items()\n",
    "    ]).sort_values('weighted_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\\\n가중 투표 결과:\")\n",
    "    print(f\"  5점 (MI+RFE+SHAP): {len(vote_df[vote_df['weighted_score'] == 5])}개\")\n",
    "    print(f\"  4점 (두 개 조합): {len(vote_df[vote_df['weighted_score'] == 4])}개\")\n",
    "    print(f\"  3점: {len(vote_df[vote_df['weighted_score'] == 3])}개\")\n",
    "    print(f\"  2점 이하: {len(vote_df[vote_df['weighted_score'] <= 2])}개\")\n",
    "    \n",
    "    # MI 또는 RFE 필수 조건\n",
    "    vote_df_filtered = vote_df[vote_df['in_MI'] | vote_df['in_RFE']].copy()\n",
    "    \n",
    "    removed = len(vote_df) - len(vote_df_filtered)\n",
    "    if removed > 0:\n",
    "        print(f\"\\\\n  MI/RFE 필수 조건: {removed}개 제거 (SHAP 단독)\")\n",
    "    \n",
    "    # 가중 점수 >= 3\n",
    "    high_score_features = vote_df_filtered[vote_df_filtered['weighted_score'] >= 3]['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\\\n가중 점수 >= 3: {len(high_score_features)}개\")\n",
    "    \n",
    "    # target_count에 맞춰 조정\n",
    "    if len(high_score_features) >= target_count:\n",
    "        final_features = vote_df_filtered.head(target_count)['feature'].tolist()\n",
    "        print(f\"  -> 상위 {target_count}개 선택\")\n",
    "    else:\n",
    "        needed = target_count - len(high_score_features)\n",
    "        low_score_features = vote_df_filtered[vote_df_filtered['weighted_score'] < 3]['feature'].tolist()[:needed]\n",
    "        final_features = high_score_features + low_score_features\n",
    "        print(f\"  -> {len(high_score_features)}개 + {needed}개 추가\")\n",
    "    \n",
    "    print(f\"\\\\n최종 선택: {len(final_features)}개\")\n",
    "    print(\"\\\\n상위 10개:\")\n",
    "    print(vote_df_filtered.head(10)[['feature', 'weighted_score', 'in_MI', 'in_RFE', 'in_SHAP']].to_string(index=False))\n",
    "    \n",
    "    return final_features, vote_df_filtered\n",
    "\n",
    "def balance_feature_categories(final_features, vote_df, \n",
    "                                target_total=25,\n",
    "                                max_tech=15,\n",
    "                                min_external=10):\n",
    "    \"\"\"\n",
    "    카테고리별 특징 개수 밸런싱\n",
    "    \n",
    "    논문 권장:\n",
    "      - 총 25개\n",
    "      - 기술적 지표 최대 15개\n",
    "      - 외부 데이터 최소 10개\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"카테고리별 특징 밸런싱\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 특징 분류\n",
    "    tech_keywords = ['RSI', 'MACD', 'BB', 'SMA', 'EMA', 'ATR', 'OBV', 'rolling', \n",
    "                     'ROC', 'MOM', 'ADX', 'CCI', 'WILLR', 'MFI', '%K', '%D', \n",
    "                     'Volume', 'CMF', 'VROC', 'WMA', 'HV', 'AD']\n",
    "    \n",
    "    tech_features = [f for f in final_features if any(kw in f for kw in tech_keywords)]\n",
    "    external_features = [f for f in final_features if f not in tech_features]\n",
    "    \n",
    "    print(f\"\\n현재 구성:\")\n",
    "    print(f\"  기술적 지표: {len(tech_features)}개\")\n",
    "    print(f\"  외부 데이터: {len(external_features)}개\")\n",
    "    \n",
    "    # 조정 필요 여부 확인\n",
    "    if len(tech_features) > max_tech or len(external_features) < min_external:\n",
    "        print(f\"\\n밸런싱 필요 (목표: 기술 <= {max_tech}, 외부 >= {min_external})\")\n",
    "        \n",
    "        # vote_df에서 투표 수 높은 순으로 재선택\n",
    "        vote_df_sorted = vote_df.copy()\n",
    "        \n",
    "        # 기술적 지표: 투표 순으로 max_tech개\n",
    "        tech_vote_df = vote_df_sorted[vote_df_sorted['feature'].isin(\n",
    "            [f for f in vote_df_sorted['feature'] if any(kw in f for kw in tech_keywords)]\n",
    "        )]\n",
    "        selected_tech = tech_vote_df.head(max_tech)['feature'].tolist()\n",
    "        \n",
    "        # 외부 데이터: 투표 순으로 선택\n",
    "        external_vote_df = vote_df_sorted[vote_df_sorted['feature'].isin(\n",
    "            [f for f in vote_df_sorted['feature'] if f not in tech_vote_df['feature'].tolist()]\n",
    "        )]\n",
    "        \n",
    "        # 목표 개수 맞춤\n",
    "        needed_external = max(min_external, target_total - len(selected_tech))\n",
    "        selected_external = external_vote_df.head(needed_external)['feature'].tolist()\n",
    "        \n",
    "        balanced_features = selected_tech + selected_external\n",
    "        \n",
    "        print(f\"\\n밸런싱 후:\")\n",
    "        print(f\"  기술적 지표: {len(selected_tech)}개\")\n",
    "        print(f\"  외부 데이터: {len(selected_external)}개\")\n",
    "        print(f\"  총: {len(balanced_features)}개\")\n",
    "        \n",
    "        return balanced_features\n",
    "    else:\n",
    "        print(\"\\n밸런싱 불필요 (이미 적절한 비율)\")\n",
    "        return final_features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 메인 특징 선택 파이프라인\n",
    "# =============================================================================\n",
    "\n",
    "def feature_selection_pipeline(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    논문 기반 안정적인 특징 선택 파이프라인\n",
    "    \"\"\"\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"특징 선택 파이프라인 (개선 버전)\")\n",
    "    print(f\"  초기 특징: {X_train.shape[1]}개\")\n",
    "    print(f\"  목표: 25개 (논문 권장)\")\n",
    "    print(f\"  방법: 가중 투표 (MI:2, RFE:2, SHAP:1)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1단계: 3가지 방법으로 30개씩 선택\n",
    "    mi_features, mi_df = select_features_mi(X_train, y_train, k=30)\n",
    "    rfe_features, rfe_df = select_features_rfe(X_train, y_train, k=30)\n",
    "    shap_features, shap_df = select_features_shap(X_train, y_train, X_val, y_val, k=30)\n",
    "    \n",
    "    # 2단계: 가중 앙상블\n",
    "    final_features, vote_df = ensemble_feature_selection(\n",
    "        mi_features, rfe_features, shap_features, \n",
    "        target_count=25, \n",
    "        min_votes=2\n",
    "    )\n",
    "    \n",
    "    # 3단계: 카테고리 밸런싱\n",
    "    balanced_features = balance_feature_categories(\n",
    "        final_features, \n",
    "        vote_df,\n",
    "        target_total=25,\n",
    "        max_tech=15,\n",
    "        min_external=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"파이프라인 완료\")\n",
    "    print(f\"  최종: {len(balanced_features)}개\")\n",
    "    print(f\"  샘플/특징 비율: {len(X_train) / len(balanced_features):.1f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return balanced_features, vote_df, mi_df, rfe_df, shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f34300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_feature_selection_cv(X, y, n_splits=5, target_count=25):\n",
    "    \"\"\"CV로 안정적인 특징 선택\"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    from collections import Counter\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(f\"Time Series CV 안정성 검증 ({n_splits} folds)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    all_selected_features = []\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\\\nFold {fold+1}/{n_splits}: Train {len(train_idx)}, Val {len(val_idx)}\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        # 출력 숨기기\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "        \n",
    "        features, _, _, _, _ = feature_selection_pipeline(\n",
    "            X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "        )\n",
    "        \n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "        all_selected_features.extend(features)\n",
    "        fold_results.append(features)\n",
    "        print(f\"  선택: {len(features)}개\")\n",
    "    \n",
    "    # 빈도 계산\n",
    "    feature_frequency = Counter(all_selected_features)\n",
    "    \n",
    "    stability_df = pd.DataFrame([\n",
    "        {\n",
    "            'feature': feat,\n",
    "            'frequency': count,\n",
    "            'stability_score': count / n_splits\n",
    "        }\n",
    "        for feat, count in feature_frequency.items()\n",
    "    ]).sort_values('frequency', ascending=False)\n",
    "    \n",
    "    stable_features = stability_df.head(target_count)['feature'].tolist()\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"안정성 검증 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"안정적인 특징: {len(stable_features)}개\")\n",
    "    print(f\"\\\\n빈도 Top 10:\")\n",
    "    print(stability_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return stable_features, stability_df, fold_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_tuning_cv(X, y, features, n_splits=5):\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"하이퍼파라미터 튜닝 (Grid Search)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # GPU 체크\n",
    "    try:\n",
    "        test_model = XGBRegressor(tree_method='gpu_hist', n_estimators=10)\n",
    "        test_model.fit(X[features].head(100), y.head(100))\n",
    "        print(\"✓ GPU 사용 가능! → 'gpu_hist' 모드\")\n",
    "        tree_method = 'gpu_hist'\n",
    "    except:\n",
    "        print(\"✗ GPU 사용 불가 → 'hist' 모드\")\n",
    "        tree_method = 'hist'\n",
    "\n",
    "    # Grid 탐색 범위 (확장 가능)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    print(f\"\\n총 조합 수: {len(all_combinations)}개\")\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "    best_r2 = -np.inf\n",
    "\n",
    "    for idx, combination in enumerate(all_combinations):\n",
    "        params = dict(zip(keys, combination))\n",
    "        params['tree_method'] = tree_method\n",
    "        params['random_state'] = 42\n",
    "        params['n_jobs'] = -1\n",
    "\n",
    "        cv_mapes = []\n",
    "        cv_r2s = []\n",
    "\n",
    "        for fold_i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            X_train_fold = X.iloc[train_idx][features]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            X_val_fold = X.iloc[val_idx][features]\n",
    "            y_val_fold = y.iloc[val_idx]\n",
    "\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            mape = mean_absolute_percentage_error(y_val_fold, y_pred)\n",
    "            r2 = r2_score(y_val_fold, y_pred)\n",
    "\n",
    "            cv_mapes.append(mape)\n",
    "            cv_r2s.append(r2)\n",
    "\n",
    "        avg_mape = np.mean(cv_mapes)\n",
    "        avg_r2 = np.mean(cv_r2s)\n",
    "\n",
    "        # 이중 조건: R² 우선 → MAPE 보조 기준\n",
    "        if (avg_r2 > best_r2) or (avg_r2 == best_r2 and avg_mape < best_score):\n",
    "            best_r2 = avg_r2\n",
    "            best_score = avg_mape\n",
    "            best_params = params.copy()\n",
    "\n",
    "        # 중간 로그\n",
    "        if (idx + 1) % 5 == 0 or idx == len(all_combinations) - 1:\n",
    "            print(f\"[{idx+1}/{len(all_combinations)}] 현재 최고 R²: {best_r2:.4f} | MAPE: {best_score:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"최적 평균 R²: {best_r2:.4f}\")\n",
    "    print(f\"최적 평균 MAPE: {best_score:.4f}\")\n",
    "    print(\"최적 파라미터:\")\n",
    "    for k, v in best_params.items():\n",
    "        if k not in ['tree_method', 'random_state', 'n_jobs']:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_final_model(X_train, y_train, X_val, y_val, features, best_params):\n",
    "    \"\"\"최종 모델 학습 및 평가\"\"\"\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"최종 모델 학습\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train[features], y_train, verbose=False)\n",
    "    \n",
    "    # 예측\n",
    "    y_train_pred = model.predict(X_train[features])\n",
    "    y_val_pred = model.predict(X_val[features])\n",
    "    \n",
    "    # 평가\n",
    "    train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "    val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\\\nTrain: MAPE {train_mape:.4f} | R² {train_r2:.4f}\")\n",
    "    print(f\"Val:   MAPE {val_mape:.4f} | R² {val_r2:.4f}\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\\\nTop 10 중요 특징:\")\n",
    "    print(importance.head(10).to_string(index=False))\n",
    "    \n",
    "    return model, {\n",
    "        'train_mape': train_mape,\n",
    "        'val_mape': val_mape,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2\n",
    "    }\n",
    "\n",
    "def run_complete_pipeline(X, y, use_cv=True, tune=True):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"완전 자동화 파이프라인 시작\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    n = len(X)\n",
    "    train_val_end = int(n * 0.85)  # Train+Val = 85%\n",
    "    \n",
    "    X_train_val = X.iloc[:train_val_end].copy()\n",
    "    y_train_val = y.iloc[:train_val_end].copy()\n",
    "    X_test = X.iloc[train_val_end:].copy()\n",
    "    y_test = y.iloc[train_val_end:].copy()\n",
    "\n",
    "    print(f\"Train+Val: {len(X_train_val)}, Test: {len(X_test)} (Test 완전 격리)\")\n",
    "\n",
    "    # 2. Feature Selection (Train+Val만 사용)\n",
    "    if use_cv:\n",
    "        print(\"\\n[1/3] CV 안정성 검증 중... (Train+Val만)\")\n",
    "        features, _, _ = stable_feature_selection_cv(\n",
    "            X_train_val, y_train_val,  \n",
    "            n_splits=5, \n",
    "            target_count=25\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[1/3] 기본 특징 선택 중...\")\n",
    "        train_end = int(len(X_train_val) * 0.824) \n",
    "        X_train = X_train_val.iloc[:train_end]\n",
    "        y_train = y_train_val.iloc[:train_end]\n",
    "        X_val = X_train_val.iloc[train_end:]\n",
    "        y_val = y_train_val.iloc[train_end:]\n",
    "        features, _, _, _, _ = feature_selection_pipeline(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # 3. 하이퍼파라미터 튜닝 (Train+Val만 사용)\n",
    "    if tune:\n",
    "        print(\"\\n[2/3] 하이퍼파라미터 튜닝 중... (Train+Val만)\")\n",
    "        best_params = hyperparameter_tuning_cv(\n",
    "            X_train_val, y_train_val,  \n",
    "            features, \n",
    "            n_splits=5\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[2/3] 기본 파라미터 사용\")\n",
    "        best_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "    # 4. Train/Val 재분할 (최종 학습용)\n",
    "    train_end = int(len(X_train_val) * 0.824)  # 70/85 비율\n",
    "    X_train = X_train_val.iloc[:train_end]\n",
    "    y_train = y_train_val.iloc[:train_end]\n",
    "    X_val = X_train_val.iloc[train_end:]\n",
    "    y_val = y_train_val.iloc[train_end:]\n",
    "\n",
    "    print(f\"\\n재분할: Train {len(X_train)}, Val {len(X_val)}\")\n",
    "\n",
    "    # 5. 최종 모델 학습 (Train으로만 학습)\n",
    "    print(\"\\n[3/3] 최종 모델 학습 중...\")\n",
    "    model, metrics = train_final_model(X_train, y_train, X_val, y_val, features, best_params)\n",
    "    \n",
    "    # 6. Test 평가 (처음이자 마지막)\n",
    "    from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "    y_test_pred = model.predict(X_test[features])\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    print(\"\\n[4/3] 테스트셋 성능 평가 (완전히 unseen data)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Test: MAPE {test_mape:.4f} | R² {test_r2:.4f}\")\n",
    "    \n",
    "    # 성능 하락 확인\n",
    "    r2_drop = metrics['val_r2'] - test_r2\n",
    "    if r2_drop > 0.1:\n",
    "        print(f\"\\n⚠️ Val R²({metrics['val_r2']:.4f}) → Test R²({test_r2:.4f}) 하락폭: {r2_drop:.4f}\")\n",
    "        print(\"   이는 정상입니다. 이전 99% 성능이 데이터 누수 때문이었습니다.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"파이프라인 완료!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'features': features,\n",
    "        'params': best_params,\n",
    "        'metrics': {\n",
    "            **metrics,\n",
    "            'test_mape': test_mape,\n",
    "            'test_r2': test_r2\n",
    "        },\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# 옵션 A: 완전 자동 (CV + 튜닝) - 약 20분\n",
    "results = run_complete_pipeline(X, y, use_cv=True, tune=True)\n",
    "\n",
    "# 옵션 B: CV만 (튜닝 생략) - 약 7분\n",
    "results = run_complete_pipeline(X, y, use_cv=True, tune=False)\n",
    "\n",
    "# 옵션 C: 튜닝만 (CV 생략) - 약 17분\n",
    "results = run_complete_pipeline(X, y, use_cv=False, tune=True)\n",
    "\n",
    "# 옵션 D: 기본 (빠름) - 약 2분\n",
    "results = run_complete_pipeline(X, y, use_cv=False, tune=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"검증 MAPE: {results['metrics']['val_mape']:.4f}\")\n",
    "print(f\"검증 R²: {results['metrics']['val_r2']:.4f}\")\n",
    "print(f\"테스트 MAPE: {results['metrics']['test_mape']:.4f}\")\n",
    "print(f\"테스트 R²: {results['metrics']['test_r2']:.4f}\")\n",
    "\n",
    "print(f\"특징: {len(results['features'])}개\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af0074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ea2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### 퍼플렉 시티 버전 ###################################################\n",
    "\n",
    "# =========================================================================\n",
    "# Train/Validation 분할\n",
    "# =========================================================================\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]}개\")\n",
    "print(f\"Validation set: {X_val.shape[0]}개\")\n",
    "\n",
    "# =========================================================================\n",
    "# 특징 선택\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"특징 선택 시작...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_features, vote_df, mi_df, rfe_df, shap_df = feature_selection_pipeline(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 최종 특징 카테고리 분석 \n",
    "# =========================================================================\n",
    "\n",
    "# 카테고리 분석\n",
    "categories = {\n",
    "    '기술적지표': [],\n",
    "    'Fear&Greed': [],\n",
    "    '전통금융': [],\n",
    "    'DeFi': [],\n",
    "    'USDT': [],\n",
    "    'GoogleTrends': [],\n",
    "    '뉴스': [],\n",
    "    'FundingRate': [],\n",
    "    '온체인': [],\n",
    "    '파생변수': []  # 새로 추가\n",
    "}\n",
    "\n",
    "tech_keywords = ['RSI', 'MACD', 'BB', 'SMA', 'EMA', 'ATR', 'OBV', 'rolling', \n",
    "                 'ROC', 'MOM', 'ADX', 'CCI', 'WILLR', 'MFI', '%K', '%D', \n",
    "                 'Volume', 'CMF', 'VROC', 'WMA', 'HV', 'AD']\n",
    "\n",
    "# 파생 변수 키워드 추가\n",
    "derived_keywords = ['price_change', 'price_position', 'log_return', \n",
    "                    'volatility', 'momentum', 'trend']\n",
    "\n",
    "for feat in final_features:\n",
    "    if any(kw in feat for kw in tech_keywords):\n",
    "        categories['기술적지표'].append(feat)\n",
    "    elif any(kw in feat for kw in derived_keywords):  \n",
    "        categories['파생변수'].append(feat)\n",
    "    elif 'OnChain' in feat:\n",
    "        categories['온체인'].append(feat)\n",
    "    elif any(x in feat.lower() for x in ['fear', 'greed']) or feat.lower() == 'value':\n",
    "        categories['Fear&Greed'].append(feat)\n",
    "    elif any(x in feat for x in ['SP500', 'VIX', 'GOLD', 'DXY']):\n",
    "        categories['전통금융'].append(feat)\n",
    "    elif any(x in feat for x in ['TVL', 'Makerdao', 'Lido', 'Aave', 'Chain']):\n",
    "        categories['DeFi'].append(feat)\n",
    "    elif 'USDT' in feat:\n",
    "        categories['USDT'].append(feat)\n",
    "    elif 'Google' in feat or 'Trends' in feat:\n",
    "        categories['GoogleTrends'].append(feat)\n",
    "    elif 'News' in feat:\n",
    "        categories['뉴스'].append(feat)\n",
    "    elif 'Funding' in feat:\n",
    "        categories['FundingRate'].append(feat)\n",
    "    else:\n",
    "        categories['파생변수'].append(feat)  \n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n선택된 특징: {len(final_features)}개\")\n",
    "for cat, feats in categories.items():\n",
    "    if len(feats) > 0:\n",
    "        print(f\"  {cat}: {len(feats)}개\")\n",
    "        for f in feats:\n",
    "            print(f\"    - {f}\")\n",
    "\n",
    "            \n",
    "            \n",
    "# 유효한 특징만 포함된 전체 데이터\n",
    "merged_clean = merged_data.loc[valid_indices, ['Date', 'ETH_Close'] + valid_features].copy()\n",
    "\n",
    "# 최종 선택 특징만\n",
    "final_df = merged_data.loc[valid_indices, ['Date', 'ETH_Close'] + final_features].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)\n",
    "print(f\"총 {len(final_features)}개 특징 선택됨\")\n",
    "print(f\"  - 기술적지표: {len(categories['기술적지표'])}개\")\n",
    "print(f\"  - Fear&Greed: {len(categories['Fear&Greed'])}개\")\n",
    "print(f\"  - 전통금융: {len(categories['전통금융'])}개\")\n",
    "print(f\"  - DeFi: {len(categories['DeFi'])}개\")\n",
    "print(f\"  - USDT: {len(categories['USDT'])}개\")\n",
    "print(f\"  - GoogleTrends: {len(categories['GoogleTrends'])}개\")\n",
    "print(f\"  - 뉴스: {len(categories['뉴스'])}개\")\n",
    "print(f\"  - FundingRate: {len(categories['FundingRate'])}개\")\n",
    "print(f\"  - 온체인: {len(categories['온체인'])}개\")  \n",
    "print(f\"  - 파생변수: {len(categories['파생변수'])}개\")  \n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de33aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca43acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4dcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################아래는클로드 기반 ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6eef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Feature Selection Pipeline - Based on Validated Research\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression, RFE\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def select_features_lasso(X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Lasso (L1 regularization) based feature selection\n",
    "    Reference: Tibshirani (1996), \"Regression Shrinkage and Selection via the Lasso\"\n",
    "    \n",
    "    Strong theoretical foundation for sparse feature selection\n",
    "    Automatically performs variable selection by shrinking coefficients to zero\n",
    "    \"\"\"\n",
    "    lasso = LassoCV(cv=cv, random_state=42, n_jobs=-1, max_iter=10000)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    coef_abs = np.abs(lasso.coef_)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'coefficient': lasso.coef_,\n",
    "        'abs_coefficient': coef_abs\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    non_zero_features = feature_importance[feature_importance['abs_coefficient'] > 0]['feature'].tolist()\n",
    "    \n",
    "    return non_zero_features, feature_importance\n",
    "\n",
    "def select_features_random_forest(X, y, n_estimators=500, max_features='sqrt'):\n",
    "    \"\"\"\n",
    "    Random Forest feature importance\n",
    "    Reference: Breiman (2001), \"Random Forests\"\n",
    "    \n",
    "    Robust measure of feature importance based on impurity decrease\n",
    "    Handles non-linear relationships and interactions\n",
    "    \"\"\"\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True\n",
    "    )\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def select_features_xgboost(X, y, n_estimators=500):\n",
    "    \"\"\"\n",
    "    XGBoost feature importance (gain-based)\n",
    "    Reference: Chen & Guestrin (2016), \"XGBoost: A Scalable Tree Boosting System\"\n",
    "    \n",
    "    More accurate importance measure than Random Forest\n",
    "    Directly optimizes prediction error\n",
    "    \"\"\"\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def select_features_mutual_information(X, y, n_neighbors=3):\n",
    "    \"\"\"\n",
    "    Mutual Information regression\n",
    "    Reference: Kraskov et al. (2004), \"Estimating mutual information\"\n",
    "    \n",
    "    Non-parametric method that captures non-linear dependencies\n",
    "    Does not assume any functional form\n",
    "    \"\"\"\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42, n_neighbors=n_neighbors)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mi_score': mi_scores\n",
    "    }).sort_values('mi_score', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def stability_selection(X, y, n_bootstrap=100, threshold=0.6, sample_fraction=0.75):\n",
    "    \"\"\"\n",
    "    Stability Selection\n",
    "    Reference: Meinshausen & Bühlmann (2010), \"Stability selection\"\n",
    "    \n",
    "    Improves reproducibility by selecting features that are consistently important\n",
    "    across multiple bootstrap samples. Reduces false discovery rate.\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    n_features = X.shape[1]\n",
    "    selection_counts = np.zeros(n_features)\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        sample_indices = np.random.choice(n_samples, size=int(n_samples * sample_fraction), replace=False)\n",
    "        X_boot = X.iloc[sample_indices]\n",
    "        y_boot = y.iloc[sample_indices]\n",
    "        \n",
    "        lasso = LassoCV(cv=3, random_state=42+i, n_jobs=-1, max_iter=5000)\n",
    "        lasso.fit(X_boot, y_boot)\n",
    "        \n",
    "        selected = np.abs(lasso.coef_) > 0\n",
    "        selection_counts += selected\n",
    "    \n",
    "    selection_frequency = selection_counts / n_bootstrap\n",
    "    \n",
    "    stable_features = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'selection_frequency': selection_frequency\n",
    "    }).sort_values('selection_frequency', ascending=False)\n",
    "    \n",
    "    stable_features_filtered = stable_features[stable_features['selection_frequency'] >= threshold]\n",
    "    \n",
    "    return stable_features_filtered['feature'].tolist(), stable_features\n",
    "\n",
    "def ensemble_feature_selection_validated(X_train, y_train, target_features=25):\n",
    "    \"\"\"\n",
    "    Validated ensemble approach combining multiple proven methods\n",
    "    \n",
    "    Methodology:\n",
    "    1. Lasso: Theoretical foundation for sparse selection\n",
    "    2. Random Forest: Robust to overfitting, handles interactions\n",
    "    3. XGBoost: State-of-the-art gradient boosting\n",
    "    4. Mutual Information: Non-parametric, captures non-linearity\n",
    "    5. Stability Selection: Improves reproducibility\n",
    "    \n",
    "    Final selection: Intersection + top-ranked union approach\n",
    "    Reference: Saeys et al. (2007), \"A review of feature selection techniques in bioinformatics\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Feature Selection - Validated Research-Based Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Initial features: {X_train.shape[1]}\")\n",
    "    print(f\"Target features: {target_features}\")\n",
    "    \n",
    "    lasso_features, lasso_importance = select_features_lasso(X_train, y_train)\n",
    "    print(f\"\\nLasso: {len(lasso_features)} non-zero features\")\n",
    "    \n",
    "    rf_importance = select_features_random_forest(X_train, y_train)\n",
    "    rf_top_features = rf_importance.head(50)['feature'].tolist()\n",
    "    print(f\"Random Forest: Top 50 features selected\")\n",
    "    \n",
    "    xgb_importance = select_features_xgboost(X_train, y_train)\n",
    "    xgb_top_features = xgb_importance.head(50)['feature'].tolist()\n",
    "    print(f\"XGBoost: Top 50 features selected\")\n",
    "    \n",
    "    mi_importance = select_features_mutual_information(X_train, y_train)\n",
    "    mi_top_features = mi_importance.head(50)['feature'].tolist()\n",
    "    print(f\"Mutual Information: Top 50 features selected\")\n",
    "    \n",
    "    stable_features, stability_df = stability_selection(X_train, y_train, n_bootstrap=100, threshold=0.6)\n",
    "    print(f\"Stability Selection: {len(stable_features)} stable features (threshold=0.6)\")\n",
    "    \n",
    "    all_methods = {\n",
    "        'lasso': set(lasso_features),\n",
    "        'rf': set(rf_top_features),\n",
    "        'xgb': set(xgb_top_features),\n",
    "        'mi': set(mi_top_features),\n",
    "        'stability': set(stable_features)\n",
    "    }\n",
    "    \n",
    "    feature_votes = {}\n",
    "    for feature in X_train.columns:\n",
    "        votes = sum([feature in method_features for method_features in all_methods.values()])\n",
    "        if votes >= 3:\n",
    "            feature_votes[feature] = votes\n",
    "    \n",
    "    print(f\"\\nConsensus features (voted by >=3 methods): {len(feature_votes)}\")\n",
    "    \n",
    "    consensus_features = sorted(feature_votes.keys(), key=lambda x: feature_votes[x], reverse=True)\n",
    "    \n",
    "    if len(consensus_features) >= target_features:\n",
    "        final_features = consensus_features[:target_features]\n",
    "    else:\n",
    "        remaining_needed = target_features - len(consensus_features)\n",
    "        \n",
    "        additional_candidates = []\n",
    "        for feature in X_train.columns:\n",
    "            if feature not in consensus_features and feature_votes.get(feature, 0) >= 2:\n",
    "                additional_candidates.append(feature)\n",
    "        \n",
    "        rank_scores = {}\n",
    "        for feature in additional_candidates:\n",
    "            score = 0\n",
    "            \n",
    "            if feature in lasso_features:\n",
    "                lasso_rank = list(lasso_importance['feature']).index(feature)\n",
    "                score += 1.0 / (lasso_rank + 1)\n",
    "            \n",
    "            rf_rank = list(rf_importance['feature']).index(feature)\n",
    "            score += 1.0 / (rf_rank + 1)\n",
    "            \n",
    "            xgb_rank = list(xgb_importance['feature']).index(feature)\n",
    "            score += 1.0 / (xgb_rank + 1)\n",
    "            \n",
    "            mi_rank = list(mi_importance['feature']).index(feature)\n",
    "            score += 1.0 / (mi_rank + 1)\n",
    "            \n",
    "            if feature in stable_features:\n",
    "                stability_rank = list(stability_df['feature']).index(feature)\n",
    "                score += 1.0 / (stability_rank + 1)\n",
    "            \n",
    "            rank_scores[feature] = score\n",
    "        \n",
    "        additional_features = sorted(rank_scores.keys(), key=lambda x: rank_scores[x], reverse=True)[:remaining_needed]\n",
    "        final_features = consensus_features + additional_features\n",
    "    \n",
    "    vote_summary = pd.DataFrame([\n",
    "        {\n",
    "            'feature': feat,\n",
    "            'votes': feature_votes.get(feat, 0),\n",
    "            'in_lasso': feat in all_methods['lasso'],\n",
    "            'in_rf': feat in all_methods['rf'],\n",
    "            'in_xgb': feat in all_methods['xgb'],\n",
    "            'in_mi': feat in all_methods['mi'],\n",
    "            'in_stability': feat in all_methods['stability']\n",
    "        }\n",
    "        for feat in final_features\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Final selected features: {len(final_features)}\")\n",
    "    print(f\"  5 votes: {len(vote_summary[vote_summary['votes']==5])}\")\n",
    "    print(f\"  4 votes: {len(vote_summary[vote_summary['votes']==4])}\")\n",
    "    print(f\"  3 votes: {len(vote_summary[vote_summary['votes']==3])}\")\n",
    "    print(f\"  2 votes: {len(vote_summary[vote_summary['votes']==2])}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return final_features, vote_summary, lasso_importance, rf_importance, xgb_importance, mi_importance, stability_df\n",
    "\n",
    "def feature_selection_pipeline(X_train, y_train, X_val, y_val, target_features=25):\n",
    "    \"\"\"\n",
    "    Main pipeline wrapper\n",
    "    \"\"\"\n",
    "    final_features, vote_summary, lasso_df, rf_df, xgb_df, mi_df, stability_df = \\\n",
    "        ensemble_feature_selection_validated(X_train, y_train, target_features=target_features)\n",
    "    \n",
    "    return final_features, vote_summary, lasso_df, rf_df, xgb_df, mi_df, stability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 올바른 Train/Validation/Test 분할\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"데이터 준비 및 분할\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "features_to_exclude = ['Date', 'ETH_Open', 'ETH_High', 'ETH_Low', 'ETH_Close', 'ETH_Volume']\n",
    "all_feature_cols = [col for col in merged_data.columns if col not in features_to_exclude]\n",
    "\n",
    "merged_data[all_feature_cols] = merged_data[all_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "missing_ratio = merged_data[all_feature_cols].isnull().sum() / len(merged_data)\n",
    "\n",
    "print(f\"전체 특징: {len(all_feature_cols)}개\")\n",
    "print(f\"평균 결측치 비율: {missing_ratio.mean()*100:.2f}%\")\n",
    "\n",
    "threshold = 0.5\n",
    "valid_features = missing_ratio[missing_ratio < threshold].index.tolist()\n",
    "\n",
    "X = merged_data[valid_features].copy()\n",
    "y = merged_data['ETH_Close'].copy()\n",
    "\n",
    "X = X.interpolate(method='linear', limit_direction='both')\n",
    "X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    valid_indices = X.dropna().index\n",
    "    X = X.loc[valid_indices]\n",
    "    y = y.loc[valid_indices]\n",
    "else:\n",
    "    valid_indices = X.index\n",
    "\n",
    "dates = merged_data.loc[valid_indices, 'Date'].values\n",
    "\n",
    "print(f\"\\n최종 데이터셋: 특징 {X.shape[1]}개, 샘플 {X.shape[0]}개\")\n",
    "\n",
    "# Train: 60%, Validation: 20%, Test: 20%\n",
    "train_end = int(len(X) * 0.6)\n",
    "val_end = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:train_end]\n",
    "y_train = y.iloc[:train_end]\n",
    "dates_train = dates[:train_end]\n",
    "\n",
    "X_val = X.iloc[train_end:val_end]\n",
    "y_val = y.iloc[train_end:val_end]\n",
    "dates_val = dates[train_end:val_end]\n",
    "\n",
    "X_test = X.iloc[val_end:]\n",
    "y_test = y.iloc[val_end:]\n",
    "dates_test = dates[val_end:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]}개 ({dates_train[0]} ~ {dates_train[-1]})\")\n",
    "print(f\"Validation set: {X_val.shape[0]}개 ({dates_val[0]} ~ {dates_val[-1]})\")\n",
    "print(f\"Test set: {X_test.shape[0]}개 ({dates_test[0]} ~ {dates_test[-1]})\")\n",
    "\n",
    "# =============================================================================\n",
    "# Train 데이터로만 특징 선택 (데이터 누수 방지)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"특징 선택 (Train 데이터만 사용)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_features, vote_summary, lasso_df, rf_df, xgb_df, mi_df, stability_df = \\\n",
    "    feature_selection_pipeline(X_train, y_train, X_val, y_val, target_features=25)\n",
    "\n",
    "# =============================================================================\n",
    "# 선택된 특징으로 데이터 재구성\n",
    "# =============================================================================\n",
    "\n",
    "X_train_selected = X_train[final_features]\n",
    "X_val_selected = X_val[final_features]\n",
    "X_test_selected = X_test[final_features]\n",
    "\n",
    "print(f\"\\n선택된 특징으로 데이터 재구성 완료\")\n",
    "print(f\"Train: {X_train_selected.shape}\")\n",
    "print(f\"Validation: {X_val_selected.shape}\")\n",
    "print(f\"Test: {X_test_selected.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 카테고리 분석\n",
    "# =============================================================================\n",
    "\n",
    "categories = {\n",
    "    '기술적지표': [],\n",
    "    'Fear&Greed': [],\n",
    "    '전통금융': [],\n",
    "    'DeFi': [],\n",
    "    'USDT': [],\n",
    "    'GoogleTrends': [],\n",
    "    '뉴스': [],\n",
    "    'FundingRate': [],\n",
    "    '온체인': [],\n",
    "    '파생변수': []\n",
    "}\n",
    "\n",
    "tech_keywords = ['RSI', 'MACD', 'BB', 'SMA', 'EMA', 'ATR', 'OBV', 'rolling', \n",
    "                 'ROC', 'MOM', 'ADX', 'CCI', 'WILLR', 'MFI', '%K', '%D', \n",
    "                 'Volume', 'CMF', 'VROC', 'WMA', 'HV', 'AD']\n",
    "derived_keywords = ['price_change', 'price_position', 'log_return', \n",
    "                    'volatility', 'momentum', 'trend']\n",
    "\n",
    "for feat in final_features:\n",
    "    if any(kw in feat for kw in tech_keywords):\n",
    "        categories['기술적지표'].append(feat)\n",
    "    elif any(kw in feat for kw in derived_keywords):\n",
    "        categories['파생변수'].append(feat)\n",
    "    elif 'OnChain' in feat:\n",
    "        categories['온체인'].append(feat)\n",
    "    elif any(x in feat.lower() for x in ['fear', 'greed']) or feat.lower() == 'value':\n",
    "        categories['Fear&Greed'].append(feat)\n",
    "    elif any(x in feat for x in ['SP500', 'VIX', 'GOLD', 'DXY']):\n",
    "        categories['전통금융'].append(feat)\n",
    "    elif any(x in feat for x in ['TVL', 'Makerdao', 'Lido', 'Aave', 'Chain']):\n",
    "        categories['DeFi'].append(feat)\n",
    "    elif 'USDT' in feat:\n",
    "        categories['USDT'].append(feat)\n",
    "    elif 'Google' in feat or 'Trends' in feat:\n",
    "        categories['GoogleTrends'].append(feat)\n",
    "    elif 'News' in feat:\n",
    "        categories['뉴스'].append(feat)\n",
    "    elif 'Funding' in feat:\n",
    "        categories['FundingRate'].append(feat)\n",
    "    else:\n",
    "        categories['파생변수'].append(feat)\n",
    "\n",
    "print(f\"\\n선택된 특징: {len(final_features)}개\")\n",
    "for cat, feats in categories.items():\n",
    "    if len(feats) > 0:\n",
    "        print(f\"  {cat}: {len(feats)}개\")\n",
    "        for f in feats:\n",
    "            print(f\"    - {f}\")\n",
    "\n",
    "print(\"\\n투표 상세:\")\n",
    "print(vote_summary[['feature', 'votes', 'in_lasso', 'in_rf', 'in_xgb', 'in_mi', 'in_stability']].to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 모델 학습 및 평가 예시\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_val_scaled = scaler.transform(X_val_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_performance(y_true, y_pred, dataset_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} 성능:\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  MAE: ${mae:,.2f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"모델 평가 (XGBoost)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_metrics = evaluate_performance(y_train, y_train_pred, \"Train\")\n",
    "val_metrics = evaluate_performance(y_val, y_val_pred, \"Validation\")\n",
    "test_metrics = evaluate_performance(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"과적합 검증\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train vs Validation R² 차이: {abs(train_metrics['r2'] - val_metrics['r2']):.4f}\")\n",
    "print(f\"Train vs Test R² 차이: {abs(train_metrics['r2'] - test_metrics['r2']):.4f}\")\n",
    "\n",
    "if test_metrics['r2'] > 0.7:\n",
    "    print(\"\\n✓ Test set 성능 우수 - 실제 예측 능력 검증됨\")\n",
    "elif test_metrics['r2'] > 0.5:\n",
    "    print(\"\\n△ Test set 성능 보통 - 개선 필요\")\n",
    "else:\n",
    "    print(\"\\n✗ Test set 성능 낮음 - 과적합 의심\")\n",
    "\n",
    "# =============================================================================\n",
    "# 최종 데이터 저장\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'Date': dates_train,\n",
    "    'ETH_Close_actual': y_train.values,\n",
    "    'ETH_Close_pred': y_train_pred\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'Date': dates_val,\n",
    "    'ETH_Close_actual': y_val.values,\n",
    "    'ETH_Close_pred': y_val_pred\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'Date': dates_test,\n",
    "    'ETH_Close_actual': y_test.values,\n",
    "    'ETH_Close_pred': y_test_pred\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"특징 선택 및 평가 완료\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
