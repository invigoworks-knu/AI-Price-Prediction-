{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc722ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:03:32.495953: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-18 21:03:32.495997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-18 21:03:32.497410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-18 21:03:32.504619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-18 21:03:33.339075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING\n",
      "================================================================================\n",
      "(2976, 41) macro_crypto_data.csv\n",
      "(2231, 2) SP500.csv\n",
      "(2231, 2) VIX.csv\n",
      "(2233, 2) GOLD.csv\n",
      "(2234, 2) DXY.csv\n",
      "(2843, 2) fear_greed.csv\n",
      "(2183, 2) eth_funding_rate.csv\n",
      "(2911, 6) usdt_eth_mcap.csv\n",
      "(2009, 2) aave_eth_tvl.csv\n",
      "(1795, 2) lido_eth_tvl.csv\n",
      "(2511, 2) makerdao_eth_tvl.csv\n",
      "(2570, 2) uniswap_eth_tvl.csv\n",
      "(2103, 2) curve-dex_eth_tvl.csv\n",
      "(2974, 2) eth_chain_tvl.csv\n",
      "(1601, 5) layer2_tvl.csv\n",
      "Loaded 10 files\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT FEATURES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATA MERGING\n",
      "================================================================================\n",
      "Merged shape: (2346, 62)\n",
      "Missing before fill: 23,712\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUE HANDLING\n",
      "================================================================================\n",
      "Missing after fill: 0\n",
      "Shape: (2346, 62)\n",
      "Period: 2019-06-15 ~ 2025-11-14\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "import glob\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from collections import Counter\n",
    "from numba import jit\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE,\n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier, StackingClassifier, VotingClassifier,\n",
    "    AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor, RandomForestRegressor,\n",
    "    StackingRegressor, VotingRegressor\n",
    ")\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm.callback import early_stopping\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Flatten, Dropout, Activation,\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    Attention, MultiHeadAttention,\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    pass\n",
    "# ============================================================================\n",
    "# 환경 설정 및 경고 무시\n",
    "# ============================================================================\n",
    "\n",
    "# GPU 메모리 증가 허용 설정\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DATA_DIR_MAIN = './macro_data'\n",
    "DATA_DIR_NEW = './macro_data/macro_data'\n",
    "\n",
    "TRAIN_START_DATE = pd.to_datetime('2020-01-01')\n",
    "LOOKBACK_DAYS = 200\n",
    "LOOKBACK_START_DATE = TRAIN_START_DATE - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "\n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    print(df.shape,file_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"[Warning] {filename} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    return standardize_date_column(df, filename)\n",
    "\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [f\"{prefix}_{col}\" if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sentiment_features(news_df):\n",
    "    if news_df.empty:\n",
    "        return pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index().fillna(0)\n",
    "    \n",
    "    agg['sentiment_polarity'] = agg['positive_ratio'] - agg['negative_ratio']\n",
    "    agg['sentiment_intensity'] = agg['positive_ratio'] + agg['negative_ratio']\n",
    "    agg['sentiment_disagreement'] = agg['positive_ratio'] * agg['negative_ratio']\n",
    "    agg['bull_bear_ratio'] = agg['positive_ratio'] / (agg['negative_ratio'] + 1e-10)\n",
    "    agg['weighted_sentiment'] = agg['sentiment_mean'] * np.log1p(agg['news_count'])\n",
    "    agg['extremity_index'] = (agg['extreme_positive_count'] + agg['extreme_negative_count']) / (agg['news_count'] + 1e-10)\n",
    "    \n",
    "    for window in [3,7]:\n",
    "        agg[f'sentiment_ma{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        agg[f'sentiment_volatility_{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    agg['sentiment_trend'] = agg['sentiment_mean'].diff()\n",
    "    agg['sentiment_acceleration'] = agg['sentiment_trend'].diff()\n",
    "    agg['news_volume_change'] = agg['news_count'].pct_change()\n",
    "    \n",
    "    for window in [7, 14]:\n",
    "        agg[f'news_volume_ma{window}'] = agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return agg.fillna(0)\n",
    "\n",
    "\n",
    "def smart_fill_missing(df_merged):\n",
    "    REFERENCE_START_DATE = pd.to_datetime('2020-01-01')\n",
    "    \n",
    "    for col in df_merged.columns:\n",
    "        if col == 'date':\n",
    "            continue\n",
    "        \n",
    "        if df_merged[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        non_null_idx = df_merged[col].first_valid_index()\n",
    "        \n",
    "        if non_null_idx is None:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "            continue\n",
    "        \n",
    "        first_date = df_merged.loc[non_null_idx, 'date']\n",
    "        \n",
    "        before_mask = df_merged['date'] < first_date\n",
    "        after_mask = df_merged['date'] >= first_date\n",
    "        \n",
    "        df_merged.loc[before_mask, col] = df_merged.loc[before_mask, col].fillna(0)\n",
    "        df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(method='ffill')\n",
    "        \n",
    "        remaining = df_merged.loc[after_mask, col].isnull().sum()\n",
    "        if remaining > 0:\n",
    "            df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(0)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#news_df = load_csv(DATA_DIR_MAIN, 'news_data.csv')\n",
    "#eth_onchain_df = load_csv(DATA_DIR_MAIN, 'eth_onchain.csv')\n",
    "macro_df = load_csv(DATA_DIR_NEW, 'macro_crypto_data.csv')\n",
    "sp500_df = load_csv(DATA_DIR_NEW, 'SP500.csv')\n",
    "vix_df = load_csv(DATA_DIR_NEW, 'VIX.csv')\n",
    "gold_df = load_csv(DATA_DIR_NEW, 'GOLD.csv')\n",
    "dxy_df = load_csv(DATA_DIR_NEW, 'DXY.csv')\n",
    "fear_greed_df = load_csv(DATA_DIR_NEW, 'fear_greed.csv')\n",
    "eth_funding_df = load_csv(DATA_DIR_NEW, 'eth_funding_rate.csv')\n",
    "usdt_eth_mcap_df = load_csv(DATA_DIR_NEW, 'usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_csv(DATA_DIR_NEW, 'aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_csv(DATA_DIR_NEW, 'lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_csv(DATA_DIR_NEW, 'makerdao_eth_tvl.csv')\n",
    "uniswap_tvl_df = load_csv(DATA_DIR_NEW, 'uniswap_eth_tvl.csv')\n",
    "curve_tvl_df = load_csv(DATA_DIR_NEW, 'curve-dex_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_csv(DATA_DIR_NEW, 'eth_chain_tvl.csv')\n",
    "layer2_tvl_df = load_csv(DATA_DIR_NEW, 'layer2_tvl.csv')\n",
    "\n",
    "print(f\"Loaded {len([df for df in [fear_greed_df, eth_funding_df, usdt_eth_mcap_df, aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df, eth_chain_tvl_df, layer2_tvl_df] if not df.empty])} files\")\n",
    "\n",
    "all_dataframes = [\n",
    "    macro_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df, \n",
    "    sp500_df, vix_df, gold_df, dxy_df#,news_df, eth_onchain_df\n",
    "]\n",
    "\n",
    "last_dates = [\n",
    "    pd.to_datetime(df['date']).max() \n",
    "    for df in all_dataframes \n",
    "    if not df.empty and 'date' in df.columns\n",
    "]\n",
    "\n",
    "end_date = min(last_dates) if last_dates else pd.Timestamp.today()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "#sentiment_features = create_sentiment_features(news_df)\n",
    "#print(f\"Generated {sentiment_features.shape[1]-1} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA MERGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "uniswap_tvl_df = add_prefix(uniswap_tvl_df, 'uniswap')\n",
    "curve_tvl_df = add_prefix(curve_tvl_df, 'curve')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "layer2_tvl_df = add_prefix(layer2_tvl_df, 'l2')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=LOOKBACK_START_DATE, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df,  fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df,\n",
    "    sp500_df, vix_df, gold_df, dxy_df#,sentiment_features,eth_onchain_df,\n",
    "]\n",
    "\n",
    "for df in dataframes_to_merge:\n",
    "    if not df.empty:\n",
    "        df_merged = pd.merge(df_merged, df, on='date', how='left')\n",
    "\n",
    "print(f\"Merged shape: {df_merged.shape}\")\n",
    "print(f\"Missing before fill: {df_merged.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUE HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_merged = smart_fill_missing(df_merged)\n",
    "\n",
    "missing_after = df_merged.isnull().sum().sum()\n",
    "print(f\"Missing after fill: {missing_after:,}\")\n",
    "\n",
    "if missing_after > 0:\n",
    "    df_merged = df_merged.fillna(0)\n",
    "    print(f\"Remaining filled with 0\")\n",
    "\n",
    "lookback_df = df_merged[df_merged['date'] < TRAIN_START_DATE]\n",
    "cols_to_drop = [\n",
    "    col for col in lookback_df.columns \n",
    "    if lookback_df[col].isnull().all() and col != 'date'\n",
    "]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping {len(cols_to_drop)} fully missing columns\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Shape: {df_merged.shape}\")\n",
    "print(f\"Period: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"Missing: {df_merged.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e25a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_14'] > 100).astype(int)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "\n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "\n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)\n",
    "\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "\n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 5]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df,target_type,method):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        for period in [5, 20]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        \n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['eth_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        \n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['eth_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = df_transformed[col_name].rolling(30, min_periods=10).std().fillna(0)\n",
    "            \n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(btc_volume.pct_change())\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (eth_volume / (btc_volume + 1e-8)).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "            \n",
    "    ## raw_data 저장하기\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    base_dir=os.path.join('model_results',timestamp,'raw_data',target_type,method)\n",
    "    os.makedirs(base_dir,exist_ok=True)\n",
    "    df.to_csv(os.path.join(base_dir,\"raw_data_all_features.csv\"),index=False)        \n",
    "            \n",
    "            \n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [col for col in df_transformed.columns if 'return' in col.lower() and 'next' not in col]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650f6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = ['sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio', 'negative_ratio']\n",
    "    sentiment_ma_cols = [col for col in df.columns if 'sentiment' in col and ('_ma7' in col or '_volatility_7' in col)]\n",
    "    no_lag_patterns = ['_trend', '_acceleration', '_volume_change', 'news_volume_change', 'news_volume_ma']\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', \n",
    "                                  'eth_contract', 'eth_avg_gas', 'eth_total_gas', 'eth_avg_block'])]\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col or '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            if not any(pattern in col for pattern in no_lag_patterns):\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' not in df.columns:\n",
    "        return df_norm\n",
    "    \n",
    "    current_close = df['ETH_Close']\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_non_stationary_features(df):\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    prefixes_to_transform = [\n",
    "        'eth_', 'aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_',\n",
    "        'l2_', 'sp500_', 'gold_', 'dxy_', 'vix_', 'usdt_'\n",
    "    ]\n",
    "    \n",
    "    exclude_prefixes = ['fg_', 'funding_']\n",
    "    \n",
    "    exclude_keywords = [\n",
    "        '_pct_', '_ratio', '_lag', '_volatility', '_corr', '_beta', '_spread',\n",
    "        'eth_return', 'btc_return', 'eth_log_return' \n",
    "    ]\n",
    "    \n",
    "    cols_to_transform = []\n",
    "    for col in df_proc.columns:\n",
    "        if col.startswith(tuple(prefixes_to_transform)):\n",
    "            if not col.startswith(tuple(exclude_prefixes)):\n",
    "                if not any(keyword in col for keyword in exclude_keywords):\n",
    "                    cols_to_transform.append(col)\n",
    "                    \n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        df_proc[col] = df_proc[col].fillna(method='ffill').replace(0, 1e-9)\n",
    "\n",
    "        df_proc[f'{col}_pct_1d'] = df_proc[col].pct_change(1)\n",
    "        df_proc[f'{col}_pct_5d'] = df_proc[col].pct_change(5)\n",
    "        \n",
    "        ma_30 = df_proc[col].rolling(window=30, min_periods=10).mean()\n",
    "        df_proc[f'{col}_ma30_ratio'] = df_proc[col] / (ma_30 + 1e-9)\n",
    "        \n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "    df_proc = df_proc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    df_proc = df_proc.replace([np.inf, -np.inf], np.nan)\n",
    "    df_proc = df_proc.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "   \n",
    "    return df_proc    \n",
    "    \n",
    "    \n",
    "    \n",
    "def detect_swing_points_at_point(df_slice, lookback=2):\n",
    "    if len(df_slice) < lookback * 2 + 1:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_window = df_slice.copy()\n",
    "    \n",
    "    df_window['swing_high'] = (\n",
    "        (df_window['ETH_High'] > df_window['ETH_High'].shift(1)) &\n",
    "        (df_window['ETH_High'] > df_window['ETH_High'].shift(-1)) &\n",
    "        (df_window['ETH_High'] > df_window['ETH_High'].shift(lookback)) &\n",
    "        (df_window['ETH_High'] > df_window['ETH_High'].shift(-lookback))\n",
    "    )\n",
    "    \n",
    "    df_window['swing_low'] = (\n",
    "        (df_window['ETH_Low'] < df_window['ETH_Low'].shift(1)) &\n",
    "        (df_window['ETH_Low'] < df_window['ETH_Low'].shift(-1)) &\n",
    "        (df_window['ETH_Low'] < df_window['ETH_Low'].shift(lookback)) &\n",
    "        (df_window['ETH_Low'] < df_window['ETH_Low'].shift(-lookback))\n",
    "    )\n",
    "    \n",
    "    swing_highs = df_window[df_window['swing_high']][['ETH_High']].copy()\n",
    "    swing_highs['type'] = 'high'\n",
    "    swing_highs.rename(columns={'ETH_High': 'price'}, inplace=True)\n",
    "    \n",
    "    swing_lows = df_window[df_window['swing_low']][['ETH_Low']].copy()\n",
    "    swing_lows['type'] = 'low'\n",
    "    swing_lows.rename(columns={'ETH_Low': 'price'}, inplace=True)\n",
    "    \n",
    "    swings = pd.concat([swing_highs, swing_lows]).sort_index()\n",
    "    \n",
    "    return swings\n",
    "\n",
    "\n",
    "def analyze_trend_structure(swings, num_points=5):\n",
    "    if len(swings) < num_points:\n",
    "        return 0, 0.0\n",
    "    \n",
    "    recent_swings = swings.tail(num_points)\n",
    "    \n",
    "    highs = recent_swings[recent_swings['type'] == 'high']['price'].values\n",
    "    lows = recent_swings[recent_swings['type'] == 'low']['price'].values\n",
    "    \n",
    "    if len(highs) < 2 or len(lows) < 2:\n",
    "        return 0, 0.0\n",
    "    \n",
    "    high_trend = 1 if highs[-1] > highs[0] else -1\n",
    "    low_trend = 1 if lows[-1] > lows[0] else -1\n",
    "    \n",
    "    if high_trend == 1 and low_trend == 1:\n",
    "        trend_direction = 1\n",
    "    elif high_trend == -1 and low_trend == -1:\n",
    "        trend_direction = -1\n",
    "    else:\n",
    "        trend_direction = 0\n",
    "    \n",
    "    all_prices = recent_swings['price'].values\n",
    "    price_range = all_prices.max() - all_prices.min()\n",
    "    trend_slope = (all_prices[-1] - all_prices[0]) / len(all_prices)\n",
    "    trend_strength = abs(trend_slope) / (price_range + 1e-8)\n",
    "    \n",
    "    return trend_direction, trend_strength\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_triple_barrier_targets(\n",
    "    prices_close,\n",
    "    prices_high,\n",
    "    prices_low,\n",
    "    atr,\n",
    "    trend_directions,\n",
    "    trend_strengths,\n",
    "    lookahead_candles,\n",
    "    atr_multiplier_profit,\n",
    "    atr_multiplier_stop\n",
    "):\n",
    "    n = len(prices_close)\n",
    "    targets_raw = np.zeros(n, dtype=np.int32)\n",
    "    upper_barriers_arr = np.zeros(n, dtype=np.float64)\n",
    "    lower_barriers_arr = np.zeros(n, dtype=np.float64)\n",
    "    exit_reasons = np.zeros(n, dtype=np.int32) # 1:TP, 2:SL, 3:Timeout\n",
    "\n",
    "    for i in range(n - lookahead_candles):\n",
    "        current_atr = max(atr[i], 1e-8)\n",
    "        current_price = prices_close[i]\n",
    "\n",
    "        if np.isnan(current_atr) or current_price <= 0:\n",
    "            continue\n",
    "\n",
    "        upper_barrier = current_price + (current_atr * atr_multiplier_profit)\n",
    "        lower_barrier = current_price - (current_atr * atr_multiplier_stop)\n",
    "\n",
    "        upper_barriers_arr[i] = upper_barrier\n",
    "        lower_barriers_arr[i] = lower_barrier\n",
    "\n",
    "        profit_hit = False\n",
    "        stop_hit = False\n",
    "\n",
    "        for j in range(1, lookahead_candles + 1):\n",
    "            future_high = prices_high[i + j]\n",
    "            future_low = prices_low[i + j]\n",
    "\n",
    "            if future_high >= upper_barrier:\n",
    "                profit_hit = True\n",
    "                targets_raw[i] = 1\n",
    "                exit_reasons[i] = 1\n",
    "                break\n",
    "\n",
    "            if future_low <= lower_barrier:\n",
    "                stop_hit = True\n",
    "                targets_raw[i] = 2\n",
    "                exit_reasons[i] = 2\n",
    "                break\n",
    "\n",
    "        if not profit_hit and not stop_hit:\n",
    "            targets_raw[i] = 0\n",
    "            exit_reasons[i] = 3\n",
    "\n",
    "    return targets_raw, upper_barriers_arr, lower_barriers_arr, exit_reasons\n",
    "\n",
    "\n",
    "def create_targets(df, lookahead_candles=8, \n",
    "                   atr_multiplier_profit=1.5, \n",
    "                   atr_multiplier_stop=1.0,\n",
    "                   trend_window=120,\n",
    "                   trend_analysis_points=5):\n",
    "    \n",
    "    df_target = df.copy()\n",
    "    atr_col_name = 'ATR_14'\n",
    "    if atr_col_name not in df.columns:\n",
    "        raise ValueError(f\"'{atr_col_name}' feature is missing. Run calculate_technical_indicators first.\")\n",
    "\n",
    "    trend_directions = np.zeros(len(df_target), dtype=np.int32)\n",
    "    trend_strengths = np.zeros(len(df_target), dtype=np.float64)\n",
    "\n",
    "    for i in range(trend_window, len(df_target)):\n",
    "        start_idx = max(0, i - trend_window)\n",
    "        df_slice = df_target.iloc[start_idx:i+1]\n",
    "\n",
    "        swings = detect_swing_points_at_point(df_slice, lookback=2)\n",
    "        trend_dir, trend_str = analyze_trend_structure(swings, num_points=trend_analysis_points)\n",
    "        trend_directions[i] = trend_dir\n",
    "        trend_strengths[i] = trend_str\n",
    "\n",
    "    prices_close = df_target['ETH_Close'].to_numpy()\n",
    "    prices_high = df_target['ETH_High'].to_numpy()\n",
    "    prices_low = df_target['ETH_Low'].to_numpy()\n",
    "    atr = pd.Series(df_target[atr_col_name]).fillna(method='ffill').fillna(0).to_numpy()\n",
    "\n",
    "    targets_raw, upper_barriers, lower_barriers, exit_reasons = compute_triple_barrier_targets(\n",
    "        prices_close, prices_high, prices_low, atr,\n",
    "        trend_directions, trend_strengths,\n",
    "        lookahead_candles, atr_multiplier_profit, atr_multiplier_stop\n",
    "    )\n",
    "\n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "\n",
    "    df_target['next_open'] = next_open\n",
    "    df_target['next_close'] = next_close\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "\n",
    "    df_target['trend_direction'] = trend_directions\n",
    "    df_target['trend_strength'] = trend_strengths\n",
    "    df_target['next_direction'] = targets_raw\n",
    "    df_target['take_profit_price'] = pd.Series(upper_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['stop_loss_price'] = pd.Series(lower_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['exit_reason'] = exit_reasons\n",
    "\n",
    "    df_target['next_direction'].iloc[-lookahead_candles:] = np.nan\n",
    "\n",
    "    return df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38918df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=30):\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "\n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        if atr_col_name not in selected and atr_col_name in X_train.columns:\n",
    "            selected.pop()\n",
    "            selected.insert(0, atr_col_name)\n",
    "    \n",
    "    \n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=30, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date,\n",
    "                              final_test_start='2025-01-01',\n",
    "                              initial_train_size=800,\n",
    "                              val_size=150,\n",
    "                              test_size=150,\n",
    "                              step=150,\n",
    "                              gap_size=20,\n",
    "                              apply_gap_to_train_val=True):\n",
    "    \n",
    "    df_full_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_full_period = df_full_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "        \n",
    "    final_test_df = df_full_period[df_full_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    rolling_cutoff_date = final_test_start - pd.Timedelta(days=gap_size)\n",
    "    df_rolling_period = df_full_period[df_full_period['date'] < rolling_cutoff_date].copy().reset_index(drop=True)\n",
    "\n",
    "    total_rolling_days = len(df_rolling_period)\n",
    "    \n",
    "    if apply_gap_to_train_val:\n",
    "        min_required_days = initial_train_size + gap_size + val_size + gap_size + test_size\n",
    "    else:\n",
    "        min_required_days = initial_train_size + val_size + gap_size + test_size\n",
    "    \n",
    "    if total_rolling_days < min_required_days:\n",
    "        n_splits = 0\n",
    "    else:\n",
    "        n_splits = (total_rolling_days - min_required_days) // step + 1\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        test_end_idx = total_rolling_days - (fold_idx * step)\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        \n",
    "        if test_start_idx < 0: break\n",
    "        \n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        \n",
    "        if apply_gap_to_train_val:\n",
    "            train_end_idx = val_start_idx - gap_size\n",
    "        else:\n",
    "            train_end_idx = val_start_idx\n",
    "            \n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0: break\n",
    "        \n",
    "        train_fold = df_rolling_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_rolling_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_rolling_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold, 'val': val_fold, 'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1, 'fold_type': 'walk_forward_rolling'\n",
    "        })\n",
    "        \n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "\n",
    "    if len(final_test_df) > 0 and len(df_rolling_period) > 0:\n",
    "        \n",
    "        final_val_end_idx = len(df_rolling_period) \n",
    "        final_val_start_idx = final_val_end_idx - val_size\n",
    "        \n",
    "        if apply_gap_to_train_val:\n",
    "            final_train_end_idx = final_val_start_idx - gap_size\n",
    "        else:\n",
    "            final_train_end_idx = final_val_start_idx\n",
    "            \n",
    "        final_train_start_idx = max(0, final_train_end_idx - initial_train_size)\n",
    "        \n",
    "        final_train_data = df_rolling_period.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "        final_val_data = df_rolling_period.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None, trend_params=None):\n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(), train_start_date=train_df['date'].min(), is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(), train_start_date=val_df['date'].min(),\n",
    "        is_train=False, train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(), train_start_date=test_df['date'].min(),\n",
    "        is_train=False, train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_direction','next_log_return', 'next_close','next_open', \n",
    "                   'take_profit_price', 'stop_loss_price',\n",
    "                   'trend_direction', 'trend_strength', 'exit_reason']\n",
    "    \n",
    "    dropna_cols = ['next_direction','next_log_return', 'next_close','next_open', \n",
    "                   'take_profit_price', 'stop_loss_price']\n",
    "\n",
    "    train_processed = train_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=dropna_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                    if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, y_train, target_type=target_type, top_n=top_n\n",
    "    )\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    if trend_params is None:\n",
    "        trend_params = {}\n",
    "        \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_processed['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),\n",
    "            'dates': test_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'scaler': robust_scaler, \n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'missing_stats': missing_stats,  \n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx,\n",
    "            'trend_window': trend_params.get('trend_window', 120),\n",
    "            'trend_analysis_points': trend_params.get('trend_analysis_points', 5)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                      final_test_start='2025-01-01',\n",
    "                                      method='tvt', target_type='direction',\n",
    "                                      lookahead_candles=8, atr_multiplier_profit=1.5, \n",
    "                                      atr_multiplier_stop=1.0, **kwargs):\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = preprocess_non_stationary_features(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "    \n",
    "    trend_params = {\n",
    "        'trend_window': kwargs.get('trend_window', 120),\n",
    "        'trend_analysis_points': kwargs.get('trend_analysis_points', 5)\n",
    "    }\n",
    "    \n",
    "    df = create_targets(\n",
    "        df, \n",
    "        lookahead_candles=lookahead_candles,\n",
    "        atr_multiplier_profit=atr_multiplier_profit, \n",
    "        atr_multiplier_stop=atr_multiplier_stop,\n",
    "        **trend_params  \n",
    "    )\n",
    "    \n",
    "    df = remove_raw_prices_and_transform(df, target_type, method)\n",
    "    df = df.iloc[:-lookahead_candles]\n",
    "    \n",
    "    split_kwargs = {\n",
    "        'final_test_start': final_test_start,\n",
    "        'gap_size': lookahead_candles, \n",
    "        'apply_gap_to_train_val': True\n",
    "    }\n",
    "    \n",
    "    for key in ['initial_train_size', 'val_size', 'test_size', 'step']:\n",
    "        if key in kwargs:\n",
    "            split_kwargs[key] = kwargs[key]\n",
    "    \n",
    "    splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "\n",
    "    result = [\n",
    "        process_single_split(\n",
    "            fold, \n",
    "            target_type=target_type,  \n",
    "            top_n=kwargs.get('top_n', 30),\n",
    "            fold_idx=fold['fold_idx'],\n",
    "            trend_params=trend_params\n",
    "        ) \n",
    "        for fold in splits\n",
    "    ]\n",
    "    \n",
    "    summary_data = []\n",
    "    for fold in result:\n",
    "        f_idx = fold['stats']['fold_idx']\n",
    "        f_type = fold['stats']['fold_type']\n",
    "        tr_dates = fold['train']['dates']\n",
    "        val_dates = fold['val']['dates']\n",
    "        te_dates = fold['test']['dates']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Fold': f_idx,\n",
    "            'Type': f_type,\n",
    "            'Train_N': len(fold['train']['y']),\n",
    "            'Train_Start': tr_dates.min().date() if len(tr_dates)>0 else '-',\n",
    "            'Train_End': tr_dates.max().date() if len(tr_dates)>0 else '-',\n",
    "            'Val_N': len(fold['val']['y']),\n",
    "            'Val_Start': val_dates.min().date() if len(val_dates)>0 else '-',\n",
    "            'Val_End': val_dates.max().date() if len(val_dates)>0 else '-',\n",
    "            'Test_N': len(fold['test']['y']),\n",
    "            'Test_Start': te_dates.min().date() if len(te_dates)>0 else '-',\n",
    "            'Test_End': te_dates.max().date() if len(te_dates)>0 else '-'\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\" [FINAL PIPELINE SUMMARY] dropna() 이후 데이터 생존 현황\")\n",
    "    print(\"=\"*100)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e26b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "class TimeSeriesAugmentation:\n",
    "    @staticmethod\n",
    "    def jittering(X, sigma=0.02):\n",
    "        noise = np.random.normal(0, sigma, X.shape)\n",
    "        return X + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaling(X, sigma=0.1):\n",
    "        if len(X.shape) == 3:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], 1, X.shape[2]))\n",
    "        else:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], X.shape[1]))\n",
    "        return X * factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def magnitude_warping(X, sigma=0.2, num_knots=4):\n",
    "        if len(X.shape) == 3:\n",
    "            seq_len = X.shape[1]\n",
    "            orig_steps = np.linspace(0, seq_len - 1, num_knots + 2)\n",
    "            random_warps = np.random.normal(1, sigma, size=(X.shape[0], num_knots + 2, X.shape[2]))\n",
    "            \n",
    "            warped_X = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    warper = np.interp(np.arange(seq_len), orig_steps, random_warps[i, :, j])\n",
    "                    warped_X[i, :, j] = X[i, :, j] * warper\n",
    "            return warped_X\n",
    "        else:\n",
    "            return X * np.random.normal(1, sigma, X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_augmentation(X, method='jittering', **kwargs):\n",
    "        if method == 'jittering':\n",
    "            return TimeSeriesAugmentation.jittering(X, **kwargs)\n",
    "        elif method == 'scaling':\n",
    "            return TimeSeriesAugmentation.scaling(X, **kwargs)\n",
    "        elif method == 'magnitude_warping':\n",
    "            return TimeSeriesAugmentation.magnitude_warping(X, **kwargs)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 70),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 35),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_samples': trial.suggest_float('max_samples', 0.6, 0.8),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 40, 100),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.01),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.01),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'bootstrap': True,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False, n_jobs=1)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, class_weight='balanced')\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Random Forest] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 15, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 20.0, log=True),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 50, 100),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0, log=True),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.01, 1.0, log=True),\n",
    "                'path_smooth': trial.suggest_float('path_smooth', 0.0, 1.0),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.8),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.8),\n",
    "                'bagging_freq': 1,\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': 42,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='binary_logloss',\n",
    "                callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'verbose': -1,\n",
    "            'force_col_wise': True,\n",
    "            'bagging_freq': 1\n",
    "        })\n",
    "\n",
    "        final_model = LGBMClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[LightGBM] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return final_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        neg_count = (y_train == 0).sum()\n",
    "        pos_count = (y_train == 1).sum()\n",
    "        base_scale_pos_weight = neg_count / max(pos_count, 1)\n",
    "\n",
    "        def objective(trial):\n",
    "            scale_multiplier = trial.suggest_float('scale_multiplier', 0.8, 1.2)\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.8),\n",
    "                'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 2.0, 20.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 10, 30),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 2.0, log=True),\n",
    "                'max_delta_step': trial.suggest_float('max_delta_step', 0, 3),\n",
    "                'scale_pos_weight': base_scale_pos_weight * scale_multiplier,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'tree_method': 'hist',\n",
    "                'eval_metric': 'logloss'\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'scale_pos_weight': base_scale_pos_weight * best_params['scale_multiplier'],\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist',\n",
    "            'eval_metric': 'logloss'\n",
    "        })\n",
    "        del best_params['scale_multiplier']\n",
    "\n",
    "        final_model = XGBClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[XGBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f} | ScalePosWeight: {best_params['scale_pos_weight']:.2f}\")\n",
    "\n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def histgradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 25, 70),\n",
    "                'l2_regularization': trial.suggest_float('l2_regularization', 1.0, 20.0, log=True),\n",
    "                'max_bins': trial.suggest_int('max_bins', 128, 255),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 40),\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 20,\n",
    "                'validation_fraction': 0.1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = HistGradientBoostingClassifier(**params)\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        \n",
    "        best_model = HistGradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[HistGradientBoosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'C': trial.suggest_float('C', 0.01, 5.0, log=True),\n",
    "                'penalty': 'l2',\n",
    "                'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "                'class_weight': 'balanced',\n",
    "                'max_iter': 3000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = LogisticRegression(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_model = LogisticRegression(**study.best_params, class_weight='balanced')\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Logistic Regression] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 30, 100),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5),\n",
    "                'algorithm': 'SAMME',\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            base_max_depth = trial.suggest_int('base_max_depth', 1, 3)\n",
    "            base_min_samples_split = trial.suggest_int('base_min_samples_split', 30, 60)\n",
    "            base_min_samples_leaf = trial.suggest_int('base_min_samples_leaf', 15, 30)\n",
    "            \n",
    "            base_estimator = DecisionTreeClassifier(\n",
    "                max_depth=base_max_depth,\n",
    "                min_samples_split=base_min_samples_split,\n",
    "                min_samples_leaf=base_min_samples_leaf,\n",
    "                max_features='sqrt',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            model = AdaBoostClassifier(estimator=base_estimator, **param)\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            max_depth=best_params['base_max_depth'],\n",
    "            min_samples_split=best_params['base_min_samples_split'],\n",
    "            min_samples_leaf=best_params['base_min_samples_leaf'],\n",
    "            max_features='sqrt',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model = AdaBoostClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            algorithm='SAMME',\n",
    "            random_state=42\n",
    "        )\n",
    "        best_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[AdaBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'depth': trial.suggest_int('depth', 2, 4),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 8.0, 20.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'rsm': trial.suggest_float('rsm', 0.4, 0.7),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 40, 80),\n",
    "                'auto_class_weights': 'Balanced',\n",
    "                'random_seed': 42,\n",
    "                'verbose': False,\n",
    "                'early_stopping_rounds': 20\n",
    "            }\n",
    "            \n",
    "            model = CatBoostClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_val, y_val),\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=15)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        model = CatBoostClassifier(**study.best_params, auto_class_weights='Balanced', random_seed=42, verbose=False)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        train_acc = model.score(X_train, y_train)\n",
    "        val_acc = model.score(X_val, y_val)\n",
    "        print(f\"[CatBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 80),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 40),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 80),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.02),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.02),\n",
    "                'validation_fraction': 0.15,\n",
    "                'n_iter_no_change': 15,\n",
    "                'tol': 0.001,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=12),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_model = GradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Gradient Boosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        pos_count = (y_train == 1).sum()\n",
    "        scale_pos_weight = neg_count / max(pos_count, 1)\n",
    "\n",
    "        def objective(trial):\n",
    "            xgb_estimators = trial.suggest_int('xgb_estimators', 100, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_depth', 3, 5)\n",
    "            xgb_lr = trial.suggest_float('xgb_lr', 0.01, 0.05, log=True)\n",
    "\n",
    "            lgbm_estimators = trial.suggest_int('lgbm_estimators', 100, 200)\n",
    "            lgbm_depth = trial.suggest_int('lgbm_depth', 3, 5)\n",
    "            lgbm_lr = trial.suggest_float('lgbm_lr', 0.01, 0.05, log=True)\n",
    "\n",
    "            cat_estimators = trial.suggest_int('cat_estimators', 100, 200)\n",
    "            cat_depth = trial.suggest_int('cat_depth', 3, 5)\n",
    "            cat_lr = trial.suggest_float('cat_lr', 0.01, 0.05, log=True)\n",
    "\n",
    "            meta_C = trial.suggest_float('meta_C', 0.1, 2.0, log=True)\n",
    "\n",
    "            base_learners = [\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_estimators,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=xgb_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=3.0,\n",
    "                    min_child_weight=10,\n",
    "                    scale_pos_weight=scale_pos_weight,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    verbosity=0\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=lgbm_estimators,\n",
    "                    max_depth=lgbm_depth,\n",
    "                    learning_rate=lgbm_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=2.0,\n",
    "                    min_child_samples=60,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    force_col_wise=True\n",
    "                )),\n",
    "                ('cat', CatBoostClassifier(\n",
    "                    iterations=cat_estimators,\n",
    "                    depth=cat_depth,\n",
    "                    learning_rate=cat_lr,\n",
    "                    l2_leaf_reg=3.0,\n",
    "                    border_count=32,\n",
    "                    auto_class_weights='Balanced',\n",
    "                    random_state=42,\n",
    "                    verbose=False,\n",
    "                    task_type='CPU',\n",
    "                    thread_count=-1\n",
    "                ))\n",
    "            ]\n",
    "\n",
    "            meta_learner = LogisticRegression(\n",
    "                max_iter=3000, \n",
    "                C=meta_C, \n",
    "                class_weight='balanced', \n",
    "                random_state=42, \n",
    "                penalty='l2'\n",
    "            )\n",
    "\n",
    "            model = StackingClassifier(\n",
    "                estimators=base_learners,\n",
    "                final_estimator=meta_learner,\n",
    "                cv=5,\n",
    "                n_jobs=-1,\n",
    "                passthrough=False\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        base_learners = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_estimators'],\n",
    "                max_depth=best_params['xgb_depth'],\n",
    "                learning_rate=best_params['xgb_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=3.0,\n",
    "                min_child_weight=10,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=best_params['lgbm_estimators'],\n",
    "                max_depth=best_params['lgbm_depth'],\n",
    "                learning_rate=best_params['lgbm_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=60,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "                force_col_wise=True\n",
    "            )),\n",
    "            ('cat', CatBoostClassifier(\n",
    "                iterations=best_params['cat_estimators'],\n",
    "                depth=best_params['cat_depth'],\n",
    "                learning_rate=best_params['cat_lr'],\n",
    "                l2_leaf_reg=3.0,\n",
    "                border_count=32,\n",
    "                auto_class_weights='Balanced',\n",
    "                random_state=42,\n",
    "                verbose=False,\n",
    "                task_type='CPU',\n",
    "                thread_count=-1\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        meta_learner = LogisticRegression(\n",
    "            max_iter=3000, \n",
    "            C=best_params['meta_C'], \n",
    "            class_weight='balanced', \n",
    "            random_state=42, \n",
    "            penalty='l2'\n",
    "        )\n",
    "\n",
    "        best_model = StackingClassifier(\n",
    "            estimators=base_learners,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Stacking] Train: {train_acc:.4f} | Val: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return best_model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 48, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 24, 64, step=16)\n",
    "            units2 = trial.suggest_int('units2', 12, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0), input_shape=input_shape),\n",
    "                BatchNormalization(),\n",
    "                Bidirectional(LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)),\n",
    "                BatchNormalization(),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0), input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Bidirectional(LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)),\n",
    "            BatchNormalization(),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[BiLSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 56, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                GRU(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                GRU(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            GRU(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, class_weight=class_weight_dict, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[GRU] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc00393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models (8개)\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    #{'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 14, 'name': 'HistGradientBoosting', 'func': DirectionModels.histgradient_boosting, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# DL Models (3개)\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3bfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.predictions = {}\n",
    "        self.models = {} if save_models else None\n",
    "        self.save_models = save_models\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for p in pred:\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "\n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                    X_test, y_test_df, test_dates, model_name,\n",
    "                                    is_deep_learning=False):\n",
    "        \n",
    "        if is_deep_learning:\n",
    "            train_pred_proba = self._predict_model(model, X_train)\n",
    "            val_pred_proba = self._predict_model(model, X_val)\n",
    "            test_pred_proba = self._predict_model(model, X_test)\n",
    "            \n",
    "            if isinstance(test_pred_proba, list):\n",
    "                test_pred_proba = test_pred_proba[0]\n",
    "            if isinstance(train_pred_proba, list):\n",
    "                train_pred_proba = train_pred_proba[0]\n",
    "            if isinstance(val_pred_proba, list):\n",
    "                val_pred_proba = val_pred_proba[0]\n",
    "\n",
    "            train_pred = np.argmax(train_pred_proba, axis=1)\n",
    "            val_pred = np.argmax(val_pred_proba, axis=1)\n",
    "            test_pred = np.argmax(test_pred_proba, axis=1)\n",
    "            \n",
    "        else:\n",
    "            train_pred = model.predict(X_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "            else:\n",
    "                n_classes = 3\n",
    "                test_pred_proba = np.eye(n_classes)[test_pred]\n",
    "        \n",
    "        y_test_direction = y_test_df['next_direction'].values.astype(int)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test_direction, test_pred)\n",
    "        \n",
    "        test_prec = precision_score(y_test_direction, test_pred, average='weighted', zero_division=0)\n",
    "        test_rec = recall_score(y_test_direction, test_pred, average='weighted', zero_division=0)\n",
    "        test_f1 = f1_score(y_test_direction, test_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            test_roc_auc = roc_auc_score(y_test_direction, test_pred_proba, multi_class='ovr')\n",
    "        except:\n",
    "            test_roc_auc = 0.0\n",
    "\n",
    "        self._save_predictions(model_name, test_pred, test_pred_proba, y_test_df, test_dates)\n",
    "        \n",
    "        if self.save_models and self.models is not None:\n",
    "            self.models[model_name] = model\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name, \n",
    "            'Train_Accuracy': train_acc, 'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc, 'Test_Precision': test_prec, \n",
    "            'Test_Recall': test_rec, 'Test_F1': test_f1, \n",
    "            'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        gc.collect()\n",
    "        return self.results[-1]\n",
    "    \n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba, y_test_df, dates):\n",
    "        if pred_proba.shape[1] != 3:\n",
    "            padded_proba = np.zeros((len(pred_proba), 3))\n",
    "            padded_proba[:, :pred_proba.shape[1]] = pred_proba\n",
    "            pred_proba = padded_proba\n",
    "\n",
    "        prob_neutral = pred_proba[:, 0]\n",
    "        prob_long = pred_proba[:, 1]\n",
    "        prob_short = pred_proba[:, 2]\n",
    "        \n",
    "        trade_confidence = np.where(pred_direction == 1, prob_long, \n",
    "                                  np.where(pred_direction == 2, prob_short, 0.0))\n",
    "\n",
    "        y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': y_test_direction,\n",
    "            'actual_return': y_test_df['next_log_return'].values,\n",
    "            'take_profit_price': y_test_df['take_profit_price'].values,\n",
    "            'stop_loss_price': y_test_df['stop_loss_price'].values,\n",
    "            'pred_direction': pred_direction,\n",
    "            'prob_neutral': prob_neutral,\n",
    "            'prob_long': prob_long,\n",
    "            'prob_short': prob_short,\n",
    "            'trade_confidence': trade_confidence,\n",
    "            'is_correct': (pred_direction == y_test_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        self.predictions[model_name] = predictions_df\n",
    "    \n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_predictions_dict(self):\n",
    "        return self.predictions\n",
    "    \n",
    "    def get_models_dict(self):\n",
    "        return self.models if self.models is not None else {}\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        X_arr = np.array(Xs)\n",
    "        y_arr = np.array(ys)\n",
    "        del Xs, ys\n",
    "        gc.collect()\n",
    "        return X_arr, y_arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        tf.keras.backend.clear_session()\n",
    "        try:\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "    \n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train, y_train, X_val, y_val, X_test, y_test_df,\n",
    "                test_dates, model_config['name'], is_deep_learning=is_mlp\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                if is_mlp:\n",
    "                    self.clear_memory()\n",
    "                else:\n",
    "                    gc.collect()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            if model_config.get('name') == 'MLP':\n",
    "                self.clear_memory()\n",
    "            else:\n",
    "                gc.collect()\n",
    "    \n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            self.clear_memory()\n",
    "            \n",
    "            model = model_config['func'](X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape)\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                X_test_seq, y_test_df_seq, test_dates_seq,\n",
    "                model_config['name'], is_deep_learning=True\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                self.clear_memory()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, evaluator, lookback=30, ml_models=None, dl_models=None):\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    ml_success = 0\n",
    "    for model_config in ml_models:\n",
    "        if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "            ml_success += 1\n",
    "        gc.collect()\n",
    "    \n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test_direction, lookback)\n",
    "    \n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    y_test_df_seq = y_test_df.iloc[lookback:].reset_index(drop=True)\n",
    "    \n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    dl_success = 0\n",
    "    for model_config in dl_models:\n",
    "        trainer.clear_memory()\n",
    "        \n",
    "        if model_config['name'] in ['TabNet', 'StackingEnsemble', 'VotingHard', 'VotingSoft']:\n",
    "            if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "                dl_success += 1\n",
    "        else:\n",
    "            if trainer.train_dl_model(model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape):\n",
    "                dl_success += 1\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    del X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, y_test_df_seq, test_dates_seq\n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    return ml_success + dl_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8736210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data_once(pipeline_result, target_name, split_method):\n",
    "    raw_dir = os.path.join(RESULT_DIR, \"raw_data\", target_name, split_method)\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    \n",
    "    for fold_data in pipeline_result:\n",
    "        fold_idx = fold_data['stats']['fold_idx']\n",
    "        fold_type = fold_data['stats']['fold_type']\n",
    "        fold_dir = os.path.join(raw_dir, f\"fold_{fold_idx}_{fold_type}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        features = fold_data['stats']['selected_features']\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if 'X_raw' in fold_data[split]:\n",
    "                df = pd.DataFrame(fold_data[split]['X_raw'], columns=features)\n",
    "                df['date'] = fold_data[split]['dates']\n",
    "                \n",
    "                y_df = fold_data[split]['y']\n",
    "                df = pd.concat([df, y_df], axis=1)\n",
    "                \n",
    "                df.to_csv(os.path.join(fold_dir, f\"{split}_raw.csv\"), index=False)\n",
    "\n",
    "def check_fold_completed(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    \n",
    "    if not os.path.isdir(fold_dir):\n",
    "        return False\n",
    "    \n",
    "    required_files = [\n",
    "        \"fold_summary.csv\",\n",
    "        \"robust_scaler.pkl\",\n",
    "        \"standard_scaler.pkl\",\n",
    "        \"selected_features.pkl\",\n",
    "        \"inference_config.pkl\"\n",
    "    ]\n",
    "    \n",
    "    for file in required_files:\n",
    "        if not os.path.exists(os.path.join(fold_dir, file)):\n",
    "            return False\n",
    "    \n",
    "    model_files = [f for f in os.listdir(fold_dir) if f.endswith(('.pkl', '.h5')) \n",
    "                   and not any(x in f for x in ['scaler', 'features', 'stats', 'config'])]\n",
    "    \n",
    "    return len(model_files) > 0\n",
    "\n",
    "def save_fold_results(fold_idx, fold_type, evaluator, target_name, fold_data):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, model_obj in evaluator.get_models_dict().items():\n",
    "        try:\n",
    "            is_dl = isinstance(model_obj, tf.keras.Model)\n",
    "            ext = \".h5\" if is_dl else \".pkl\"\n",
    "            path = os.path.join(fold_dir, f\"{model_name}{ext}\")\n",
    "            \n",
    "            if is_dl: \n",
    "                model_obj.save(path)\n",
    "            else:\n",
    "                with open(path, 'wb') as f:\n",
    "                    pickle.dump(model_obj, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {model_name}: {e}\")\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"robust_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['robust_scaler'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"standard_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['standard_scaler'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"selected_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['selected_features'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"missing_stats.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['missing_stats'], f)\n",
    "    \n",
    "    inference_config = {\n",
    "        'selected_features': fold_data['stats']['selected_features'],\n",
    "        'feature_order': fold_data['stats']['selected_features'], \n",
    "        'target_cols': fold_data['stats']['target_cols'],\n",
    "        'target_type': fold_data['stats']['target_type'],\n",
    "        'fold_type': fold_type,\n",
    "        'fold_idx': fold_idx,\n",
    "        'trend_params': {\n",
    "            'trend_window': fold_data['stats'].get('trend_window', 120),\n",
    "            'trend_analysis_points': fold_data['stats'].get('trend_analysis_points', 5)\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(fold_dir, \"inference_config.pkl\"), 'wb') as f:\n",
    "        pickle.dump(inference_config, f)\n",
    "    \n",
    "    for model_name, pred_df in evaluator.get_predictions_dict().items():\n",
    "        pred_df.to_csv(os.path.join(fold_dir, f\"{model_name}_predictions.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "        \n",
    "    summary = evaluator.get_summary_dataframe()\n",
    "    summary.to_csv(os.path.join(fold_dir, \"fold_summary.csv\"), \n",
    "                   index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    return summary, evaluator.get_predictions_dict()\n",
    "\n",
    "def load_fold_results(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    if not os.path.exists(summary_path): \n",
    "        return None, None\n",
    "    summary = pd.read_csv(summary_path)\n",
    "    predictions = {f.replace('_predictions.csv', ''): pd.read_csv(os.path.join(fold_dir, f)) \n",
    "                   for f in os.listdir(fold_dir) if f.endswith('_predictions.csv')}\n",
    "    return summary, predictions\n",
    "\n",
    "def save_walk_forward_summary(all_fold_results, target_name):\n",
    "    if not all_fold_results: \n",
    "        return\n",
    "    detailed_df = pd.concat([df.assign(Fold=i+1, fold_type=ft) \n",
    "                             for i, (df, ft) in enumerate(all_fold_results)], ignore_index=True)\n",
    "    detailed_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_all_folds_detailed.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward_rolling'].copy()\n",
    "    if wf_data.empty: \n",
    "        return\n",
    "\n",
    "    numeric_cols = wf_data.select_dtypes(include=np.number).columns.drop('Fold', errors='ignore')\n",
    "    avg_results = wf_data.groupby('Model')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "    avg_results.columns = ['_'.join(col).strip() if col[1] else col[0] for col in avg_results.columns.values]\n",
    "    \n",
    "    sort_col = 'Test_Precision_mean' if 'Test_Precision_mean' in avg_results.columns else 'Test_Accuracy_mean'\n",
    "    avg_results = avg_results.sort_values(by=sort_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    avg_results.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_walk_forward_average.csv\"), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "\n",
    "def load_all_fold_results_from_disk(target_name):\n",
    "    fold_results_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name)\n",
    "    if not os.path.isdir(fold_results_dir):\n",
    "        return []\n",
    "    \n",
    "    fold_dirs = sorted([d for d in os.listdir(fold_results_dir) \n",
    "                        if os.path.isdir(os.path.join(fold_results_dir, d))])\n",
    "    \n",
    "    fold_results = []\n",
    "    for fold_dir_name in fold_dirs:\n",
    "        fold_parts = fold_dir_name.split('_')\n",
    "        fold_idx = int(fold_parts[1])\n",
    "        fold_type = '_'.join(fold_parts[2:])\n",
    "        \n",
    "        summary, _ = load_fold_results(target_name, fold_idx, fold_type)\n",
    "        if summary is not None:\n",
    "            fold_results.append((summary, fold_type))\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "def run_and_save_master_summary(result_dir):\n",
    "    summary_files = glob.glob(os.path.join(result_dir, \"*_walk_forward_average.csv\"))\n",
    "    if not summary_files: \n",
    "        return\n",
    "\n",
    "    master_list = []\n",
    "    for f in summary_files:\n",
    "        filename = os.path.basename(f)\n",
    "        parts = filename.replace('trial_', '').replace('_walk_forward_average.csv', '').split('_')\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        for p in parts:\n",
    "            if p.startswith('l'):\n",
    "                df['lookahead'] = int(p[1:])\n",
    "            elif p.startswith('p'):\n",
    "                df['profit_mult'] = float(p[1:])\n",
    "            elif p.startswith('s'):\n",
    "                df['stop_mult'] = float(p[1:])\n",
    "            elif p.startswith('tw'):\n",
    "                df['trend_window'] = int(p[2:])\n",
    "            elif p.startswith('tap'):\n",
    "                df['trend_analysis_points'] = int(p[3:])\n",
    "        \n",
    "        master_list.append(df)\n",
    "        \n",
    "    master_df = pd.concat(master_list, ignore_index=True)\n",
    "    \n",
    "    sort_col = 'Test_Precision_mean' if 'Test_Precision_mean' in master_df.columns else 'Test_Accuracy_mean'\n",
    "    master_df = master_df.sort_values(by=sort_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    master_df.to_csv(os.path.join(result_dir, \"_MASTER_SUMMARY_RESULTS.csv\"), \n",
    "                     index=False, encoding='utf-8-sig')\n",
    "\n",
    "def check_experiment_completed(target_name, result_dir):\n",
    "    fold_results_dir = os.path.join(result_dir, \"fold_results\", target_name)\n",
    "    if os.path.isdir(fold_results_dir):\n",
    "        fold_dirs = [d for d in os.listdir(fold_results_dir) if os.path.isdir(os.path.join(fold_results_dir, d))]\n",
    "        has_final_holdout = any('final_holdout' in d for d in fold_dirs)\n",
    "        \n",
    "        if has_final_holdout and len(fold_dirs) >= 2:\n",
    "            summary_path = os.path.join(result_dir, f\"{target_name}_walk_forward_average.csv\")\n",
    "            if os.path.exists(summary_path):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def record_optuna_result(trial_num, target_name, score, params):\n",
    "    summary_file = os.path.join(RESULT_DIR, \"optuna_trials_summary.csv\")\n",
    "    \n",
    "    row = {\n",
    "        'trial_number': trial_num,\n",
    "        'target_name': target_name,\n",
    "        'score': score,\n",
    "        **params\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(summary_file):\n",
    "        df.to_csv(summary_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(summary_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91342413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 21:03:36,383] A new study created in memory with name: no-name-61d33927-8ffc-4f70-9003-ccacfdfaccf9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 0: l7_p2.4_s1.3_tw122_tap4\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-18 21:03:40,297] Trial 0 failed with parameters: {'lookahead': 7, 'profit_mult': 2.4359285983328913, 'stop_mult': 1.3123957592679836, 'trend_window': 122, 'trend_analysis_points': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_4049469/4145282114.py\", line 34, in objective\n",
      "    pipeline_result = build_complete_pipeline_corrected(\n",
      "  File \"/tmp/ipykernel_4049469/83122026.py\", line 307, in build_complete_pipeline_corrected\n",
      "    df = add_enhanced_cross_crypto_features(df)\n",
      "  File \"/tmp/ipykernel_4049469/3512454316.py\", line 254, in add_enhanced_cross_crypto_features\n",
      "    high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py\", line 3798, in __getitem__\n",
      "    return self._getitem_bool_array(key)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py\", line 3853, in _getitem_bool_array\n",
      "    return self._take_with_is_copy(indexer, axis=0)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py\", line 3902, in _take_with_is_copy\n",
      "    result = self._take(indices=indices, axis=axis)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py\", line 3884, in _take\n",
      "    self._consolidate_inplace()\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py\", line 5980, in _consolidate_inplace\n",
      "    self._protect_consolidate(f)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py\", line 5968, in _protect_consolidate\n",
      "    result = f()\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py\", line 5978, in f\n",
      "    self._mgr = self._mgr.consolidate()\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 686, in consolidate\n",
      "    bm._consolidate_inplace()\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 1873, in _consolidate_inplace\n",
      "    self.blocks, self.refs = _consolidate_with_refs(self.blocks, self.refs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 2351, in _consolidate_with_refs\n",
      "    list(group_blocks), dtype=dtype, can_consolidate=_can_consolidate\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-18 21:03:40,305] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m    156\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m--> 157\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptuna optimization completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    257\u001b[0m ):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m pipeline_result \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_complete_pipeline_corrected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_START_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwalk_forward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdirection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_test_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2025-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlookahead_candles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookahead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43matr_multiplier_profit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_mult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43matr_multiplier_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_mult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrend_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrend_analysis_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend_analysis_points\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m raw_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RESULT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, trial_target_name)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(raw_dir):\n",
      "Cell \u001b[0;32mIn[4], line 307\u001b[0m, in \u001b[0;36mbuild_complete_pipeline_corrected\u001b[0;34m(df_raw, train_start_date, final_test_start, method, target_type, lookahead_candles, atr_multiplier_profit, atr_multiplier_stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m df \u001b[38;5;241m=\u001b[39m add_price_lag_features_first(df)\n\u001b[1;32m    306\u001b[0m df \u001b[38;5;241m=\u001b[39m calculate_technical_indicators(df)\n\u001b[0;32m--> 307\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43madd_enhanced_cross_crypto_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m df \u001b[38;5;241m=\u001b[39m add_volatility_regime_features(df)\n\u001b[1;32m    309\u001b[0m df \u001b[38;5;241m=\u001b[39m add_interaction_features(df)\n",
      "Cell \u001b[0;32mIn[2], line 254\u001b[0m, in \u001b[0;36madd_enhanced_cross_crypto_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_enhanced)):\n\u001b[1;32m    253\u001b[0m     window_data \u001b[38;5;241m=\u001b[39m df_enhanced\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m:i]\n\u001b[0;32m--> 254\u001b[0m     high_vol_data \u001b[38;5;241m=\u001b[39m \u001b[43mwindow_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhigh_vol_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    255\u001b[0m     low_vol_data \u001b[38;5;241m=\u001b[39m window_data[\u001b[38;5;241m~\u001b[39mhigh_vol_mask\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m:i]]\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(high_vol_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3853\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3851\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[1;32m   3852\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 3853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3896\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3900\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3902\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3903\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:3884\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take\u001b[39m(\n\u001b[1;32m   3874\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[1;32m   3875\u001b[0m     indices,\n\u001b[1;32m   3876\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   3877\u001b[0m     convert_indices: bool_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3878\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3880\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[1;32m   3881\u001b[0m \n\u001b[1;32m   3882\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3884\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m   3887\u001b[0m         indices,\n\u001b[1;32m   3888\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[1;32m   3889\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3890\u001b[0m         convert_indices\u001b[38;5;241m=\u001b[39mconvert_indices,\n\u001b[1;32m   3891\u001b[0m     )\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5980\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m():\n\u001b[1;32m   5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mconsolidate()\n\u001b[0;32m-> 5980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_protect_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5968\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f()\n\u001b[1;32m   5967\u001b[0m blocks_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m-> 5968\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m!=\u001b[39m blocks_before:\n\u001b[1;32m   5970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5978\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m():\n\u001b[0;32m-> 5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py:686\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    685\u001b[0m bm\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py:1873\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1871\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate_with_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/raid/invigoworks/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py:2351\u001b[0m, in \u001b[0;36m_consolidate_with_refs\u001b[0;34m(blocks, refs)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks_refs \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m   2349\u001b[0m     group_blocks, group_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(group_blocks_refs)))\n\u001b[1;32m   2350\u001b[0m     merged_blocks, consolidated \u001b[38;5;241m=\u001b[39m _merge_blocks(\n\u001b[0;32m-> 2351\u001b[0m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype, can_consolidate\u001b[38;5;241m=\u001b[39m_can_consolidate\n\u001b[1;32m   2352\u001b[0m     )\n\u001b[1;32m   2353\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m consolidated:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    lookahead = trial.suggest_int('lookahead', 5, 10)\n",
    "    p_mult = trial.suggest_float('profit_mult', 1.2, 2.5)\n",
    "    s_mult = trial.suggest_float('stop_mult', 0.8, 1.5)\n",
    "    trend_window = trial.suggest_int('trend_window', 80, 150)\n",
    "    trend_analysis_points = trial.suggest_int('trend_analysis_points', 4, 7)\n",
    "    \n",
    "    param_suffix = f\"l{lookahead}_p{p_mult:.1f}_s{s_mult:.1f}_tw{trend_window}_tap{trend_analysis_points}\"\n",
    "    trial_target_name = f\"trial_{trial.number}_{param_suffix}\"\n",
    "    \n",
    "    if check_experiment_completed(trial_target_name, RESULT_DIR):\n",
    "        print(f\"Trial {trial.number} already completed - skipping\")\n",
    "        summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "        existing_summary = pd.read_csv(summary_path)\n",
    "        \n",
    "        if 'Test_Precision_mean' in existing_summary.columns:\n",
    "            return existing_summary['Test_Precision_mean'].max()\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        pipeline_result = build_complete_pipeline_corrected(\n",
    "            df_raw=df_merged,\n",
    "            train_start_date=TRAIN_START_DATE,\n",
    "            final_test_start='2025-01-01',\n",
    "            method='walk_forward',\n",
    "            target_type='direction',\n",
    "            lookahead_candles=lookahead,\n",
    "            atr_multiplier_profit=p_mult,\n",
    "            atr_multiplier_stop=s_mult,\n",
    "            trend_window=trend_window,\n",
    "            trend_analysis_points=trend_analysis_points,\n",
    "            top_n=40\n",
    "        )\n",
    "        \n",
    "        save_raw_data_once(pipeline_result, trial_target_name, 'walk_forward')\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold_data in pipeline_result:\n",
    "            fold_idx = fold_data['stats']['fold_idx']\n",
    "            fold_type = fold_data['stats']['fold_type']\n",
    "            \n",
    "            if check_fold_completed(trial_target_name, fold_idx, fold_type):\n",
    "                print(f\"Fold {fold_idx} already completed\")\n",
    "                fold_summary, _ = load_fold_results(trial_target_name, fold_idx, fold_type)\n",
    "                if fold_summary is not None:\n",
    "                    fold_results.append((fold_summary, fold_type))\n",
    "                continue\n",
    "            \n",
    "            print(f\"Training Fold {fold_idx} ({fold_type})...\")\n",
    "            \n",
    "            evaluator = ModelEvaluator(save_models=True)\n",
    "            \n",
    "            train_all_models(\n",
    "                fold_data['train']['X_robust'], \n",
    "                fold_data['train']['y']['next_direction'].values.astype(int),\n",
    "                fold_data['val']['X_robust'], \n",
    "                fold_data['val']['y']['next_direction'].values.astype(int),\n",
    "                fold_data['test']['X_robust'], \n",
    "                fold_data['test']['y'],\n",
    "                fold_data['test']['dates'].values,\n",
    "                evaluator,\n",
    "                ml_models=ML_MODELS_CLASSIFICATION,\n",
    "                dl_models=DL_MODELS_CLASSIFICATION\n",
    "            )\n",
    "            \n",
    "            fold_summary, _ = save_fold_results(fold_idx, fold_type, evaluator, trial_target_name, fold_data)\n",
    "            fold_results.append((fold_summary, fold_type))\n",
    "            \n",
    "            del evaluator\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "        \n",
    "        save_walk_forward_summary(fold_results, trial_target_name)\n",
    "        \n",
    "        summary_path = os.path.join(RESULT_DIR, f\"{trial_target_name}_walk_forward_average.csv\")\n",
    "        if os.path.exists(summary_path):\n",
    "            wf_avg = pd.read_csv(summary_path)\n",
    "            best_score = wf_avg['Test_Precision_mean'].max()\n",
    "            \n",
    "            record_optuna_result(trial.number, trial_target_name, best_score, trial.params)\n",
    "            return best_score\n",
    "        \n",
    "        return 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} Failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
