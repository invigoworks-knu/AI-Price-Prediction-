{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8f4c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boruta\n",
      "  Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.1 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from boruta) (1.7.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from boruta) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from boruta) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /raid/invigoworks/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\n",
      "Installing collected packages: boruta\n",
      "Successfully installed boruta-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf2b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 감성 지표 생성 완료: 25개 (date 제외)\n",
      "\n",
      "병합 후 감성 지표 결측치 처리:\n",
      "  extreme_positive_count: 39개 → 0\n",
      "  extreme_negative_count: 39개 → 0\n",
      "Dropping columns with all NaN values during the lookback period (2020-06-02 to 2020-12-19):\n",
      "['usdt_totalBridgedToUSD']\n",
      "최종 데이터 shape: (1953, 95)\n",
      "날짜 범위: 2020-06-02 00:00:00 ~ 2025-10-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================ \n",
    "# 1. 날짜 파싱 및 CSV 로드 함수\n",
    "# ============================================================================ \n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    #print(df.shape)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    #print(df.shape)\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(filepath):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = standardize_date_column(df,filepath)\n",
    "    return df\n",
    "# ============================================================================ \n",
    "# 2. 데이터 로딩\n",
    "# ============================================================================ \n",
    "DATA_DIR = './macro_data'\n",
    "\n",
    "def load_from_macro_data(filename):\n",
    "    return load_and_standardize_data(os.path.join(DATA_DIR, filename))\n",
    "\n",
    "macro_df = load_from_macro_data('macro_crypto_data.csv')\n",
    "news_df = load_from_macro_data('news_data.csv')\n",
    "eth_onchain_df = load_from_macro_data('eth_onchain.csv')\n",
    "fear_greed_df = load_from_macro_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_from_macro_data('usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_from_macro_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_from_macro_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_from_macro_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_from_macro_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_from_macro_data('eth_funding_rate.csv')\n",
    "sp500_df = load_from_macro_data('SP500.csv')\n",
    "vix_df = load_from_macro_data('VIX.csv')\n",
    "gold_df = load_from_macro_data('GOLD.csv')\n",
    "dxy_df = load_from_macro_data('DXY.csv')\n",
    "\n",
    "# ============================================================================ \n",
    "# 3. 기준 날짜 설정 (Lido TVL 시작일 기준)\n",
    "# ============================================================================ \n",
    "train_start_date = pd.to_datetime('2020-12-19')\n",
    "lookback_start_date = train_start_date - timedelta(days=200)\n",
    "end_date= pd.to_datetime('2025-10-06')\n",
    "\n",
    "# ============================================================================ \n",
    "# 4. 뉴스 감성 피처 생성 \n",
    "# ============================================================================ \n",
    "def create_sentiment_features(news_df):\n",
    "    \"\"\"\n",
    "    한국어 뉴스 감성 지표 생성 (고급 버전)\n",
    "    출처: \"Cryptocurrency Price Prediction Model Based on Sentiment Analysis\" (2024)\n",
    "    \"\"\"\n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        # ===== 기본 통계 =====\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        \n",
    "        # ===== 추가 지표 =====\n",
    "        # 1. 극단 감성 카운트\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        \n",
    "        # 2. 총 감성 점수\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    # ===== 파생 지표 계산 =====\n",
    "    \n",
    "    # 1. Sentiment Polarity (극성 강도) - 핵심 지표!\n",
    "    sentiment_agg['sentiment_polarity'] = (\n",
    "        sentiment_agg['positive_ratio'] - sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 2. Sentiment Intensity (감성 강도) - 중립 제외한 강한 의견 비율\n",
    "    sentiment_agg['sentiment_intensity'] = (\n",
    "        sentiment_agg['positive_ratio'] + sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 3. Sentiment Disagreement (의견 불일치) - 극단 의견이 공존할 때 높음\n",
    "    sentiment_agg['sentiment_disagreement'] = (\n",
    "        sentiment_agg['positive_ratio'] * sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 4. Bull/Bear Ratio (상승/하락 비율)\n",
    "    sentiment_agg['bull_bear_ratio'] = (\n",
    "        sentiment_agg['positive_ratio'] / (sentiment_agg['negative_ratio'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # 5. Weighted Sentiment (뉴스 개수 가중)\n",
    "    sentiment_agg['weighted_sentiment'] = (\n",
    "        sentiment_agg['sentiment_mean'] * np.log1p(sentiment_agg['news_count'])\n",
    "    )\n",
    "    \n",
    "    # 6. Extremity Index (극단 감성 비율)\n",
    "    sentiment_agg['extremity_index'] = (\n",
    "        (sentiment_agg['extreme_positive_count'] + sentiment_agg['extreme_negative_count']) / \n",
    "        (sentiment_agg['news_count'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # ===== 시계열 파생 지표 (이동 평균) =====\n",
    "    \n",
    "    for window in [3, 7, 14]:\n",
    "        # 감성 이동 평균\n",
    "        sentiment_agg[f'sentiment_ma{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # 감성 변동성 (이동 표준편차)\n",
    "        sentiment_agg[f'sentiment_volatility_{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # 7. Sentiment Trend (감성 변화 방향)\n",
    "    sentiment_agg['sentiment_trend'] = sentiment_agg['sentiment_mean'].diff()\n",
    "    \n",
    "    # 8. Sentiment Acceleration (감성 변화 가속도)\n",
    "    sentiment_agg['sentiment_acceleration'] = sentiment_agg['sentiment_trend'].diff()\n",
    "    \n",
    "    # 9. News Volume Change (뉴스 양 변화율)\n",
    "    sentiment_agg['news_volume_change'] = sentiment_agg['news_count'].pct_change()\n",
    "    \n",
    "    # 10. News Volume MA (뉴스 양 이동 평균)\n",
    "    for window in [7, 14]:\n",
    "        sentiment_agg[f'news_volume_ma{window}'] = (\n",
    "            sentiment_agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ 감성 지표 생성 완료: {sentiment_agg.shape[1] - 1}개 (date 제외)\")\n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "sentiment_features = create_sentiment_features(news_df)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 5. 데이터 병합\n",
    "# ============================================================================ \n",
    "def add_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + col if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=lookback_start_date, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "# List of all dataframes to be merged\n",
    "dataframes_to_merge = [\n",
    "    macro_df, sentiment_features, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, eth_chain_tvl_df,\n",
    "    eth_funding_df, sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "# Sequentially merge all dataframes onto the master date range\n",
    "for df_to_merge in dataframes_to_merge:\n",
    "    df_merged = pd.merge(df_merged, df_to_merge, on='date', how='left')\n",
    "\n",
    "    \n",
    "################## 감정분석 결측치 따로 걍 처리하자..############################\n",
    "\n",
    "\n",
    "sentiment_cols = [col for col in df_merged.columns \n",
    "                 if any(x in col for x in ['extreme'])]\n",
    "\n",
    "print(f\"\\n병합 후 감성 지표 결측치 처리:\")\n",
    "for col in sentiment_cols:\n",
    "    missing_before = df_merged[col].isnull().sum()\n",
    "    if missing_before > 0:\n",
    "        df_merged[col] = df_merged[col].fillna(0)\n",
    "        print(f\"  {col}: {missing_before}개 → 0\")\n",
    "    \n",
    "    \n",
    "# ============================================================================\n",
    "# 5.1. lookback 기간 동안 모든 값이 결측치인 컬럼 제거\n",
    "# ============================================================================\n",
    "# Define the 60-day lookback period\n",
    "lookback_period_df = df_merged[(df_merged['date'] >= lookback_start_date) & (df_merged['date'] < train_start_date)]\n",
    "\n",
    "# Find columns where all values in this period are NaN\n",
    "cols_to_drop = [col for col in lookback_period_df.columns if lookback_period_df[col].isnull().all() and not col.startswith(\"lido\")]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping columns with all NaN values during the lookback period ({lookback_start_date.date()} to {train_start_date.date()}):\")\n",
    "    print(cols_to_drop)\n",
    "    df_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "else:\n",
    "    print(\"No columns to drop; all columns have at least one value in the lookback period.\")\n",
    "\n",
    "# lookback_start_date 이후만 사용\n",
    "df_merged = df_merged[df_merged['date'] >= lookback_start_date].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 최종 shape 및 날짜 범위 확인\n",
    "print(\"최종 데이터 shape:\", df_merged.shape)\n",
    "print(\"날짜 범위:\", df_merged['date'].min(), \"~\", df_merged['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aedee444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BTC_Open</th>\n",
       "      <th>BTC_High</th>\n",
       "      <th>BTC_Low</th>\n",
       "      <th>BTC_Close</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>AVAX_Open</th>\n",
       "      <th>AVAX_High</th>\n",
       "      <th>AVAX_Low</th>\n",
       "      <th>AVAX_Close</th>\n",
       "      <th>AVAX_Volume</th>\n",
       "      <th>DOT_Open</th>\n",
       "      <th>DOT_High</th>\n",
       "      <th>DOT_Low</th>\n",
       "      <th>DOT_Close</th>\n",
       "      <th>DOT_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>109681.945312</td>\n",
       "      <td>112375.484375</td>\n",
       "      <td>109236.945312</td>\n",
       "      <td>112122.640625</td>\n",
       "      <td>33371048505</td>\n",
       "      <td>4018.659668</td>\n",
       "      <td>4143.003906</td>\n",
       "      <td>3969.792969</td>\n",
       "      <td>4141.476562</td>\n",
       "      <td>...</td>\n",
       "      <td>28.790525</td>\n",
       "      <td>30.125532</td>\n",
       "      <td>27.963934</td>\n",
       "      <td>30.007946</td>\n",
       "      <td>8.394608e+08</td>\n",
       "      <td>3.892598</td>\n",
       "      <td>4.014097</td>\n",
       "      <td>3.799078</td>\n",
       "      <td>3.996965</td>\n",
       "      <td>197272471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>112117.875000</td>\n",
       "      <td>114473.570312</td>\n",
       "      <td>111589.953125</td>\n",
       "      <td>114400.382812</td>\n",
       "      <td>60000147466</td>\n",
       "      <td>4141.356445</td>\n",
       "      <td>4234.782715</td>\n",
       "      <td>4087.927246</td>\n",
       "      <td>4217.341797</td>\n",
       "      <td>...</td>\n",
       "      <td>30.008083</td>\n",
       "      <td>30.791185</td>\n",
       "      <td>29.210913</td>\n",
       "      <td>30.452417</td>\n",
       "      <td>1.273919e+09</td>\n",
       "      <td>3.997001</td>\n",
       "      <td>4.020795</td>\n",
       "      <td>3.871955</td>\n",
       "      <td>3.983713</td>\n",
       "      <td>274838183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>114396.523438</td>\n",
       "      <td>114836.617188</td>\n",
       "      <td>112740.562500</td>\n",
       "      <td>114056.085938</td>\n",
       "      <td>58986330258</td>\n",
       "      <td>4217.055176</td>\n",
       "      <td>4238.671387</td>\n",
       "      <td>4095.443604</td>\n",
       "      <td>4145.957520</td>\n",
       "      <td>...</td>\n",
       "      <td>30.452972</td>\n",
       "      <td>30.657635</td>\n",
       "      <td>28.846090</td>\n",
       "      <td>30.003811</td>\n",
       "      <td>9.812852e+08</td>\n",
       "      <td>3.983701</td>\n",
       "      <td>3.989437</td>\n",
       "      <td>3.814803</td>\n",
       "      <td>3.907708</td>\n",
       "      <td>206562308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>114057.593750</td>\n",
       "      <td>118648.929688</td>\n",
       "      <td>113981.398438</td>\n",
       "      <td>118648.929688</td>\n",
       "      <td>71328680132</td>\n",
       "      <td>4146.033691</td>\n",
       "      <td>4351.112305</td>\n",
       "      <td>4125.541992</td>\n",
       "      <td>4351.112305</td>\n",
       "      <td>...</td>\n",
       "      <td>30.003811</td>\n",
       "      <td>31.077721</td>\n",
       "      <td>29.648453</td>\n",
       "      <td>30.704655</td>\n",
       "      <td>1.013651e+09</td>\n",
       "      <td>3.907708</td>\n",
       "      <td>4.121581</td>\n",
       "      <td>3.889161</td>\n",
       "      <td>4.121088</td>\n",
       "      <td>280950394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>118652.382812</td>\n",
       "      <td>121086.406250</td>\n",
       "      <td>118383.156250</td>\n",
       "      <td>120681.257812</td>\n",
       "      <td>71415163912</td>\n",
       "      <td>4352.240723</td>\n",
       "      <td>4517.665039</td>\n",
       "      <td>4336.526367</td>\n",
       "      <td>4487.923828</td>\n",
       "      <td>...</td>\n",
       "      <td>30.705080</td>\n",
       "      <td>31.361811</td>\n",
       "      <td>29.621159</td>\n",
       "      <td>31.023582</td>\n",
       "      <td>1.558734e+09</td>\n",
       "      <td>4.121101</td>\n",
       "      <td>4.337359</td>\n",
       "      <td>4.104230</td>\n",
       "      <td>4.308879</td>\n",
       "      <td>367515728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>120656.984375</td>\n",
       "      <td>123944.703125</td>\n",
       "      <td>119344.312500</td>\n",
       "      <td>122266.531250</td>\n",
       "      <td>83941392228</td>\n",
       "      <td>4486.934570</td>\n",
       "      <td>4591.443848</td>\n",
       "      <td>4431.479004</td>\n",
       "      <td>4514.870605</td>\n",
       "      <td>...</td>\n",
       "      <td>31.023441</td>\n",
       "      <td>31.529491</td>\n",
       "      <td>30.045366</td>\n",
       "      <td>31.361156</td>\n",
       "      <td>1.167826e+09</td>\n",
       "      <td>4.308879</td>\n",
       "      <td>4.377396</td>\n",
       "      <td>4.188099</td>\n",
       "      <td>4.320178</td>\n",
       "      <td>327374794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>122267.468750</td>\n",
       "      <td>122857.640625</td>\n",
       "      <td>121577.570312</td>\n",
       "      <td>122425.429688</td>\n",
       "      <td>36769171735</td>\n",
       "      <td>4514.909180</td>\n",
       "      <td>4519.526855</td>\n",
       "      <td>4444.012695</td>\n",
       "      <td>4489.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>31.361156</td>\n",
       "      <td>31.402140</td>\n",
       "      <td>29.905663</td>\n",
       "      <td>30.142843</td>\n",
       "      <td>6.758896e+08</td>\n",
       "      <td>4.320165</td>\n",
       "      <td>4.326745</td>\n",
       "      <td>4.142659</td>\n",
       "      <td>4.201499</td>\n",
       "      <td>226165346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2025-10-05</td>\n",
       "      <td>122419.671875</td>\n",
       "      <td>125559.210938</td>\n",
       "      <td>122191.960938</td>\n",
       "      <td>123513.476562</td>\n",
       "      <td>73689317763</td>\n",
       "      <td>4489.053223</td>\n",
       "      <td>4616.533203</td>\n",
       "      <td>4472.138672</td>\n",
       "      <td>4515.422852</td>\n",
       "      <td>...</td>\n",
       "      <td>30.142824</td>\n",
       "      <td>31.152348</td>\n",
       "      <td>29.699591</td>\n",
       "      <td>30.085981</td>\n",
       "      <td>7.887874e+08</td>\n",
       "      <td>4.201445</td>\n",
       "      <td>4.365485</td>\n",
       "      <td>4.092994</td>\n",
       "      <td>4.135801</td>\n",
       "      <td>292655150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>123510.453125</td>\n",
       "      <td>126198.070312</td>\n",
       "      <td>123196.046875</td>\n",
       "      <td>124752.531250</td>\n",
       "      <td>72568881188</td>\n",
       "      <td>4515.300781</td>\n",
       "      <td>4736.208984</td>\n",
       "      <td>4492.870117</td>\n",
       "      <td>4687.771484</td>\n",
       "      <td>...</td>\n",
       "      <td>30.085981</td>\n",
       "      <td>30.975527</td>\n",
       "      <td>29.979013</td>\n",
       "      <td>30.712078</td>\n",
       "      <td>8.057175e+08</td>\n",
       "      <td>4.135799</td>\n",
       "      <td>4.414820</td>\n",
       "      <td>4.118031</td>\n",
       "      <td>4.387514</td>\n",
       "      <td>378617545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>2025-10-07</td>\n",
       "      <td>124724.656250</td>\n",
       "      <td>125012.789062</td>\n",
       "      <td>123526.539062</td>\n",
       "      <td>123536.101562</td>\n",
       "      <td>70358523904</td>\n",
       "      <td>4686.296387</td>\n",
       "      <td>4726.128906</td>\n",
       "      <td>4643.724121</td>\n",
       "      <td>4643.724121</td>\n",
       "      <td>...</td>\n",
       "      <td>30.710276</td>\n",
       "      <td>30.710276</td>\n",
       "      <td>29.633904</td>\n",
       "      <td>29.633904</td>\n",
       "      <td>8.077894e+08</td>\n",
       "      <td>4.388586</td>\n",
       "      <td>4.426277</td>\n",
       "      <td>4.255881</td>\n",
       "      <td>4.255881</td>\n",
       "      <td>410121056.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       BTC_Open       BTC_High        BTC_Low      BTC_Close  \\\n",
       "3192 2025-09-28  109681.945312  112375.484375  109236.945312  112122.640625   \n",
       "3193 2025-09-29  112117.875000  114473.570312  111589.953125  114400.382812   \n",
       "3194 2025-09-30  114396.523438  114836.617188  112740.562500  114056.085938   \n",
       "3195 2025-10-01  114057.593750  118648.929688  113981.398438  118648.929688   \n",
       "3196 2025-10-02  118652.382812  121086.406250  118383.156250  120681.257812   \n",
       "3197 2025-10-03  120656.984375  123944.703125  119344.312500  122266.531250   \n",
       "3198 2025-10-04  122267.468750  122857.640625  121577.570312  122425.429688   \n",
       "3199 2025-10-05  122419.671875  125559.210938  122191.960938  123513.476562   \n",
       "3200 2025-10-06  123510.453125  126198.070312  123196.046875  124752.531250   \n",
       "3201 2025-10-07  124724.656250  125012.789062  123526.539062  123536.101562   \n",
       "\n",
       "       BTC_Volume     ETH_Open     ETH_High      ETH_Low    ETH_Close  ...  \\\n",
       "3192  33371048505  4018.659668  4143.003906  3969.792969  4141.476562  ...   \n",
       "3193  60000147466  4141.356445  4234.782715  4087.927246  4217.341797  ...   \n",
       "3194  58986330258  4217.055176  4238.671387  4095.443604  4145.957520  ...   \n",
       "3195  71328680132  4146.033691  4351.112305  4125.541992  4351.112305  ...   \n",
       "3196  71415163912  4352.240723  4517.665039  4336.526367  4487.923828  ...   \n",
       "3197  83941392228  4486.934570  4591.443848  4431.479004  4514.870605  ...   \n",
       "3198  36769171735  4514.909180  4519.526855  4444.012695  4489.197266  ...   \n",
       "3199  73689317763  4489.053223  4616.533203  4472.138672  4515.422852  ...   \n",
       "3200  72568881188  4515.300781  4736.208984  4492.870117  4687.771484  ...   \n",
       "3201  70358523904  4686.296387  4726.128906  4643.724121  4643.724121  ...   \n",
       "\n",
       "      AVAX_Open  AVAX_High   AVAX_Low  AVAX_Close   AVAX_Volume  DOT_Open  \\\n",
       "3192  28.790525  30.125532  27.963934   30.007946  8.394608e+08  3.892598   \n",
       "3193  30.008083  30.791185  29.210913   30.452417  1.273919e+09  3.997001   \n",
       "3194  30.452972  30.657635  28.846090   30.003811  9.812852e+08  3.983701   \n",
       "3195  30.003811  31.077721  29.648453   30.704655  1.013651e+09  3.907708   \n",
       "3196  30.705080  31.361811  29.621159   31.023582  1.558734e+09  4.121101   \n",
       "3197  31.023441  31.529491  30.045366   31.361156  1.167826e+09  4.308879   \n",
       "3198  31.361156  31.402140  29.905663   30.142843  6.758896e+08  4.320165   \n",
       "3199  30.142824  31.152348  29.699591   30.085981  7.887874e+08  4.201445   \n",
       "3200  30.085981  30.975527  29.979013   30.712078  8.057175e+08  4.135799   \n",
       "3201  30.710276  30.710276  29.633904   29.633904  8.077894e+08  4.388586   \n",
       "\n",
       "      DOT_High   DOT_Low  DOT_Close   DOT_Volume  \n",
       "3192  4.014097  3.799078   3.996965  197272471.0  \n",
       "3193  4.020795  3.871955   3.983713  274838183.0  \n",
       "3194  3.989437  3.814803   3.907708  206562308.0  \n",
       "3195  4.121581  3.889161   4.121088  280950394.0  \n",
       "3196  4.337359  4.104230   4.308879  367515728.0  \n",
       "3197  4.377396  4.188099   4.320178  327374794.0  \n",
       "3198  4.326745  4.142659   4.201499  226165346.0  \n",
       "3199  4.365485  4.092994   4.135801  292655150.0  \n",
       "3200  4.414820  4.118031   4.387514  378617545.0  \n",
       "3201  4.426277  4.255881   4.255881  410121056.0  \n",
       "\n",
       "[10 rows x 46 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d4ea531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BTC_Open</th>\n",
       "      <th>BTC_High</th>\n",
       "      <th>BTC_Low</th>\n",
       "      <th>BTC_Close</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>usdt_totalUnreleased</th>\n",
       "      <th>aave_aave_eth_tvl</th>\n",
       "      <th>lido_lido_eth_tvl</th>\n",
       "      <th>makerdao_makerdao_eth_tvl</th>\n",
       "      <th>chain_eth_chain_tvl</th>\n",
       "      <th>funding_fundingRate</th>\n",
       "      <th>sp500_SP500</th>\n",
       "      <th>vix_VIX</th>\n",
       "      <th>gold_GOLD</th>\n",
       "      <th>dxy_DXY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>122267.468750</td>\n",
       "      <td>122857.640625</td>\n",
       "      <td>121577.570312</td>\n",
       "      <td>122425.429688</td>\n",
       "      <td>36769171735</td>\n",
       "      <td>4514.909180</td>\n",
       "      <td>4519.526855</td>\n",
       "      <td>4444.012695</td>\n",
       "      <td>4489.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379634e+09</td>\n",
       "      <td>35415262516</td>\n",
       "      <td>3.837152e+10</td>\n",
       "      <td>6117770105</td>\n",
       "      <td>200146271727</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>2025-10-05</td>\n",
       "      <td>122419.671875</td>\n",
       "      <td>125559.210938</td>\n",
       "      <td>122191.960938</td>\n",
       "      <td>123513.476562</td>\n",
       "      <td>73689317763</td>\n",
       "      <td>4489.053223</td>\n",
       "      <td>4616.533203</td>\n",
       "      <td>4472.138672</td>\n",
       "      <td>4515.422852</td>\n",
       "      <td>...</td>\n",
       "      <td>1.121429e+09</td>\n",
       "      <td>35432813372</td>\n",
       "      <td>3.827480e+10</td>\n",
       "      <td>6262956668</td>\n",
       "      <td>199387628316</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>123510.453125</td>\n",
       "      <td>126198.070312</td>\n",
       "      <td>123196.046875</td>\n",
       "      <td>124752.531250</td>\n",
       "      <td>72568881188</td>\n",
       "      <td>4515.300781</td>\n",
       "      <td>4736.208984</td>\n",
       "      <td>4492.870117</td>\n",
       "      <td>4687.771484</td>\n",
       "      <td>...</td>\n",
       "      <td>1.238672e+09</td>\n",
       "      <td>35558310333</td>\n",
       "      <td>3.853484e+10</td>\n",
       "      <td>6258821993</td>\n",
       "      <td>200181178768</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>6740.279785</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>3984.399902</td>\n",
       "      <td>98.170998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       BTC_Open       BTC_High        BTC_Low      BTC_Close  \\\n",
       "1950 2025-10-04  122267.468750  122857.640625  121577.570312  122425.429688   \n",
       "1951 2025-10-05  122419.671875  125559.210938  122191.960938  123513.476562   \n",
       "1952 2025-10-06  123510.453125  126198.070312  123196.046875  124752.531250   \n",
       "\n",
       "       BTC_Volume     ETH_Open     ETH_High      ETH_Low    ETH_Close  ...  \\\n",
       "1950  36769171735  4514.909180  4519.526855  4444.012695  4489.197266  ...   \n",
       "1951  73689317763  4489.053223  4616.533203  4472.138672  4515.422852  ...   \n",
       "1952  72568881188  4515.300781  4736.208984  4492.870117  4687.771484  ...   \n",
       "\n",
       "      usdt_totalUnreleased  aave_aave_eth_tvl  lido_lido_eth_tvl  \\\n",
       "1950          1.379634e+09        35415262516       3.837152e+10   \n",
       "1951          1.121429e+09        35432813372       3.827480e+10   \n",
       "1952          1.238672e+09        35558310333       3.853484e+10   \n",
       "\n",
       "      makerdao_makerdao_eth_tvl  chain_eth_chain_tvl  funding_fundingRate  \\\n",
       "1950                 6117770105         200146271727             0.000037   \n",
       "1951                 6262956668         199387628316             0.000085   \n",
       "1952                 6258821993         200181178768             0.000093   \n",
       "\n",
       "      sp500_SP500    vix_VIX    gold_GOLD    dxy_DXY  \n",
       "1950          NaN        NaN          NaN        NaN  \n",
       "1951          NaN        NaN          NaN        NaN  \n",
       "1952  6740.279785  16.370001  3984.399902  98.170998  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2cae47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 파일별 날짜 범위 ---\n",
      "**macro_crypto_data.csv    **: 시작일=2017-01-01, 종료일=2025-10-07, 행 수=3202\n",
      "**news_data.csv            **: 시작일=2020-01-01, 종료일=2025-10-06, 행 수=26005\n",
      "**eth_onchain.csv          **: 시작일=2015-08-07, 종료일=2025-10-07, 행 수=3715\n",
      "**fear_greed.csv           **: 시작일=2018-02-01, 종료일=2025-10-07, 행 수=2802\n",
      "**usdt_eth_mcap.csv        **: 시작일=2017-11-29, 종료일=2025-10-07, 행 수=2870\n",
      "**aave_eth_tvl.csv         **: 시작일=2020-05-20, 종료일=2025-10-07, 행 수=1968\n",
      "**lido_eth_tvl.csv         **: 시작일=2020-12-19, 종료일=2025-10-07, 행 수=1754\n",
      "**makerdao_eth_tvl.csv     **: 시작일=2019-01-04, 종료일=2025-10-07, 행 수=2470\n",
      "**eth_chain_tvl.csv        **: 시작일=2017-09-27, 종료일=2025-10-07, 행 수=2933\n",
      "**eth_funding_rate.csv     **: 시작일=2019-11-27, 종료일=2025-10-07, 행 수=2142\n",
      "**SP500.csv                **: 시작일=2017-01-03, 종료일=2025-10-06, 행 수=2202\n",
      "**VIX.csv                  **: 시작일=2017-01-03, 종료일=2025-10-06, 행 수=2202\n",
      "**GOLD.csv                 **: 시작일=2017-01-03, 종료일=2025-10-06, 행 수=2203\n",
      "**DXY.csv                  **: 시작일=2017-01-03, 종료일=2025-10-06, 행 수=2204\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================ \n",
    "# 추가: 파일별 날짜 범위 확인\n",
    "# ============================================================================ \n",
    "print(\"\\n--- 파일별 날짜 범위 ---\")\n",
    "\n",
    "dataframes_info = [\n",
    "    ('macro_crypto_data.csv', macro_df),\n",
    "    ('news_data.csv', news_df),\n",
    "    ('eth_onchain.csv', eth_onchain_df),\n",
    "    ('fear_greed.csv', fear_greed_df),\n",
    "    ('usdt_eth_mcap.csv', usdt_eth_mcap_df),\n",
    "    ('aave_eth_tvl.csv', aave_tvl_df),\n",
    "    ('lido_eth_tvl.csv', lido_tvl_df),\n",
    "    ('makerdao_eth_tvl.csv', makerdao_tvl_df),\n",
    "    ('eth_chain_tvl.csv', eth_chain_tvl_df),\n",
    "    ('eth_funding_rate.csv', eth_funding_df),\n",
    "    ('SP500.csv', sp500_df),\n",
    "    ('VIX.csv', vix_df),\n",
    "    ('GOLD.csv', gold_df),\n",
    "    ('DXY.csv', dxy_df)\n",
    "]\n",
    "\n",
    "for name, df in dataframes_info:\n",
    "    if 'date' in df.columns:\n",
    "        start_date = df['date'].min().strftime('%Y-%m-%d')\n",
    "        end_date = df['date'].max().strftime('%Y-%m-%d')\n",
    "        print(f\"**{name.ljust(25)}**: 시작일={start_date}, 종료일={end_date}, 행 수={len(df)}\")\n",
    "    else:\n",
    "        print(f\"**{name.ljust(25)}**: 날짜 컬럼 ('date')을 찾을 수 없습니다.\")\n",
    "\n",
    "print(\"----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f1c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    최적화된 기술적 지표 생성 (논문 기반 2024-2025)\n",
    "    출처: \n",
    "    - \"CryptoPulse: Short-Term Cryptocurrency Forecasting\" (2024)\n",
    "    - \"Enhancing Price Prediction in Cryptocurrency Using Transformer\" (2024)\n",
    "    - \"Bitcoin Trend Prediction with Attention-Based Deep Learning\" (2024)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 기술적 지표 생성 중 ===\")\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== [핵심] MOMENTUM INDICATORS =====\n",
    "        print(\"  - Momentum 지표 생성 중...\")\n",
    "        \n",
    "        # RSI (필수! - 92.4% accuracy 달성)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        df_ta['RSI_30'] = ta.rsi(close, length=30)\n",
    "        df_ta['RSI_200'] = ta.rsi(close, length=200)  # 장기 RSI 추가\n",
    "        \n",
    "        # MACD (필수! - top feature importance)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic Oscillator (%K, %D - 논문에서 핵심 지표)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=30, d=3)  # 30일 추가\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=200, d=3)  # 200일 추가\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (Rate of Change)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['ROC_20'] = ta.roc(close, length=20)\n",
    "        \n",
    "        # MOM (Momentum - 다양한 기간)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['MOM_30'] = ta.mom(close, length=30)  # 추가\n",
    "        \n",
    "        # CCI (Commodity Channel Index)\n",
    "        df_ta['CCI_20'] = ta.cci(high, low, close, length=20)\n",
    "      \n",
    "        # TSI (True Strength Index)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "        \n",
    "        # UO (Ultimate Oscillator)\n",
    "        try:\n",
    "            df_ta['UO_7_14_28'] = ta.uo(high, low, close)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # KST Oscillator\n",
    "        safe_add(df_ta, ta.kst, close)\n",
    "        \n",
    "        # =====  Ichimoku Cloud (암호화폐 트렌드 분석에 효과적) =====\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ ICHIMOKU 생성 실패\")\n",
    "\n",
    "        # ===== [핵심] OVERLAP INDICATORS =====\n",
    "        print(\"  - Overlap 지표 생성 중...\")\n",
    "        \n",
    "        # SMA (필수! - Golden/Death Cross)\n",
    "        df_ta['SMA_10'] = ta.sma(close, length=10)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['SMA_200'] = ta.sma(close, length=200)\n",
    "        \n",
    "        # EMA (필수!)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['EMA_50'] = ta.ema(close, length=50)\n",
    "        df_ta['EMA_200'] = ta.ema(close, length=200)  # 추가\n",
    "        \n",
    "        # TEMA (Triple EMA - 논문에서 high importance)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['TEMA_30'] = ta.tema(close, length=30)  # 추가\n",
    "        \n",
    "        # WMA (Weighted Moving Average)\n",
    "        df_ta['WMA_10'] = ta.wma(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)  # 추가\n",
    "        \n",
    "        # HMA (Hull Moving Average)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (Double EMA)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        # TRIMA\n",
    "        df_ta['TRIMA_10'] = ta.trima(close, length=10)\n",
    "        \n",
    "        # VWMA (Volume Weighted)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # ZLMA (Zero Lag MA)\n",
    "        safe_add(df_ta, ta.zlma, close, length=20)\n",
    "        \n",
    "        # 가격 조합\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== [핵심] VOLATILITY INDICATORS =====\n",
    "        print(\"  - Volatility 지표 생성 중...\")\n",
    "        \n",
    "        # Bollinger Bands (필수! - 다양한 기간)\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        safe_add(df_ta, ta.bbands, close, length=50, std=2)  # 50일 추가\n",
    "        \n",
    "        # ATR (필수!)\n",
    "        df_ta['ATR_7'] = ta.atr(high, low, close, length=7)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['ATR_21'] = ta.atr(high, low, close, length=21)  # 추가\n",
    "        \n",
    "        # NATR (Normalized ATR)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel (안전하게 처리)\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # MASSI (Mass Index)\n",
    "        try:\n",
    "            massi = ta.massi(high, low)\n",
    "            if isinstance(massi, pd.Series) and not massi.empty:\n",
    "                df_ta['MASSI_9_25'] = massi\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== [핵심] VOLUME INDICATORS =====\n",
    "        print(\"  - Volume 지표 생성 중...\")\n",
    "        \n",
    "        # OBV (필수! - On-Balance Volume)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD (Accumulation/Distribution)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI (Money Flow Index)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF (Chaikin Money Flow - 논문에서 중요 지표)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Elder Force Index)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM (Ease of Movement)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # NVI/PVI\n",
    "        safe_add(df_ta, ta.nvi, close, volume)\n",
    "        safe_add(df_ta, ta.pvi, close, volume)\n",
    "        \n",
    "        # PVT (Price Volume Trend)\n",
    "        df_ta['PVT'] = ta.pvt(close, volume)\n",
    "        \n",
    "        # VWAP (Volume Weighted Average Price) - 추가\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== [핵심] TREND INDICATORS =====\n",
    "        print(\"  - Trend 지표 생성 중...\")\n",
    "        \n",
    "        # ADX (필수! - Average Directional Index)\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon (안전하게 처리)\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR (Parabolic SAR - 안전하게 처리)\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # QStick\n",
    "        safe_add(df_ta, ta.qstick, open_, close, length=14)\n",
    "        \n",
    "        # DPO (Detrended Price Oscillator) - 추가\n",
    "        try:\n",
    "            df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== [추가] 파생 지표 (논문 검증) =====\n",
    "        print(\"  - 파생 지표 생성 중...\")\n",
    "        \n",
    "        # 가격 변화율 (다양한 기간)\n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['PRICE_CHANGE_2'] = close.pct_change(periods=2)\n",
    "        df_ta['PRICE_CHANGE_5'] = close.pct_change(periods=5)\n",
    "        df_ta['PRICE_CHANGE_10'] = close.pct_change(periods=10)  # 추가\n",
    "        \n",
    "        # 변동성 (Rolling Std)\n",
    "        df_ta['VOLATILITY_5'] = close.pct_change().rolling(window=5).std()\n",
    "        df_ta['VOLATILITY_10'] = close.pct_change().rolling(window=10).std()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['VOLATILITY_30'] = close.pct_change().rolling(window=30).std()  # 추가\n",
    "        \n",
    "        # 모멘텀 (Price Ratio)\n",
    "        df_ta['MOMENTUM_5'] = close / close.shift(5) - 1\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['MOMENTUM_20'] = close / close.shift(20) - 1\n",
    "        df_ta['MOMENTUM_30'] = close / close.shift(30) - 1  # 추가\n",
    "        \n",
    "        # 이동평균 대비 위치 (필수! - high feature importance)\n",
    "        df_ta['PRICE_VS_SMA10'] = close / df_ta['SMA_10'] - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_SMA50'] = close / df_ta['SMA_50'] - 1\n",
    "        df_ta['PRICE_VS_SMA200'] = close / df_ta['SMA_200'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1  # 추가\n",
    "        df_ta['PRICE_VS_EMA26'] = close / df_ta['EMA_26'] - 1  # 추가\n",
    "        \n",
    "        # 크로스 신호 (Golden/Death Cross)\n",
    "        df_ta['SMA_CROSS_SIGNAL'] = (df_ta['SMA_10'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_200']).astype(int)  # 추가\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)  # 추가\n",
    "        \n",
    "        # Range 지표\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치 (Intraday Position)\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)  # 추가\n",
    "        \n",
    "        # Linear Regression Slope\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "            df_ta['SLOPE_10'] = ta.linreg(close, length=10, slope=True)\n",
    "            df_ta['LINREG_14'] = ta.linreg(close, length=14)\n",
    "        except:\n",
    "            # 자체 구현\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "            df_ta['SLOPE_10'] = close.rolling(window=10).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 10 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing/Decreasing 신호\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['DEC_1'] = (close < close.shift(1)).astype(int)\n",
    "        df_ta['INC_3'] = (close > close.shift(3)).astype(int)\n",
    "        df_ta['INC_5'] = (close > close.shift(5)).astype(int)  # 추가\n",
    "        \n",
    "        # BOP (Balance of Power)\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== [추가] 고급 파생 지표 =====\n",
    "        print(\"  - 고급 파생 지표 생성 중...\")\n",
    "        \n",
    "        # Bollinger Bands 관련 파생\n",
    "        if 'BBL_20_2.0' in df_ta.columns and 'BBU_20_2.0' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20_2.0'] - df_ta['BBL_20_2.0']) / df_ta['BBM_20_2.0']\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20_2.0']) / (df_ta['BBU_20_2.0'] - df_ta['BBL_20_2.0'])\n",
    "        \n",
    "        # RSI 파생 (Overbought/Oversold)\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile (상대적 거래량 강도)\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration (2차 미분)\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (시가-전일종가)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        # 추가 통계 지표\n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "        print(f\"\\n✓ 기술적 지표 생성 완료: {added}개 추가\")\n",
    "        print(f\"  총 컬럼 수: {df_ta.shape[1]}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_pairwise_correlation_features(df, window_sizes=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    BTC와 ETH 간의 이동 상관관계 피처 추가.\n",
    "    df에는 ETH_Close, BTC_Close 컬럼이 있어야 함.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    for w in window_sizes:\n",
    "        col = f\"corr_ETH_BTC_{w}\"\n",
    "        df2[col] = df2['ETH_Close'].pct_change().rolling(window=w).corr(\n",
    "            df2['BTC_Close'].pct_change()\n",
    "        )\n",
    "    return df2\n",
    "\n",
    "def add_volatility_correlation(df, window_sizes=[14, 30]):\n",
    "    \"\"\"\n",
    "    BTC와 ETH 간의 변동성 (예: 절대 수익률 또는 제곱 수익률) 상관관계 추가\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    # 예: 절대 수익률 또는 제곱 수익률\n",
    "    ret_eth = df2['ETH_Close'].pct_change()\n",
    "    ret_btc = df2['BTC_Close'].pct_change()\n",
    "    abs_eth = ret_eth.abs()\n",
    "    abs_btc = ret_btc.abs()\n",
    "    for w in window_sizes:\n",
    "        df2[f\"volcorr_abs_{w}\"] = abs_eth.rolling(w).corr(abs_btc)\n",
    "        df2[f\"volcorr_sq_{w}\"] = (ret_eth**2).rolling(w).corr(ret_btc**2)\n",
    "    return df2\n",
    "\n",
    "def add_granger_causality_features(df, maxlag=5):\n",
    "    \"\"\"\n",
    "    BTC → ETH, ETH → BTC 방향의 그레인지 인과성 유의성 p-value 등을 특징으로 추가\n",
    "    (주의: 계산 비용 큼)\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    # 수익률 시리즈\n",
    "    ret_eth = df2['ETH_Close'].pct_change().dropna()\n",
    "    ret_btc = df2['BTC_Close'].pct_change().dropna()\n",
    "\n",
    "    # 합쳐서 DataFrame\n",
    "    tmp = pd.concat([ret_eth, ret_btc], axis=1).dropna()\n",
    "    tmp.columns = ['eth_ret', 'btc_ret']\n",
    "\n",
    "    # granger tests\n",
    "    gc = grangercausalitytests(tmp[['eth_ret', 'btc_ret']], maxlag=maxlag, verbose=False)\n",
    "    # 예: eth_ret이 btc_ret을 설명하는 lag p-value (BTC → ETH 방향)\n",
    "    for lag in range(1, maxlag + 1):\n",
    "        p_val = gc[lag][0]['ssr_ftest'][1]\n",
    "        df2.loc[:, f\"p_btc_to_eth_lag{lag}\"] = p_val\n",
    "    # 반대 방향\n",
    "    gc2 = grangercausalitytests(tmp[['btc_ret', 'eth_ret']], maxlag=maxlag, verbose=False)\n",
    "    for lag in range(1, maxlag + 1):\n",
    "        p_val = gc2[lag][0]['ssr_ftest'][1]\n",
    "        df2.loc[:, f\"p_eth_to_btc_lag{lag}\"] = p_val\n",
    "\n",
    "    return df2\n",
    "\n",
    "def add_var_impulse_response_features(df, lags=5, irf_steps=3):\n",
    "    \"\"\"\n",
    "    VAR 모델을 통해 BTC/ETH 간의 충격 반응 (impulse-response) 계수를 피처로 추가.\n",
    "    (이 기능은 계산 비용이 있고 sample이 충분히 커야 안정됨)\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    # 수익률 시리즈만 쓰기\n",
    "    tmp = df2[['ETH_Close', 'BTC_Close']].pct_change().dropna()\n",
    "    model = VAR(tmp)\n",
    "    try:\n",
    "        res = model.fit(lags)\n",
    "        irf = res.irf(irf_steps)\n",
    "        # ex: 충격이 BTC에 가해졌을 때 ETH의 반응 등\n",
    "        for i in range(1, irf_steps + 1):\n",
    "            # shock to BTC → response in ETH\n",
    "            val = irf.irfs[i][1, 0]  # 이더리움(1번 idx)이 비트코인 충격(0번 idx)에 대해 반응\n",
    "            df2.loc[:, f\"irf_eth_from_btc_step{i}\"] = val\n",
    "            # 반대 방향: shock ETH → response BTC\n",
    "            val2 = irf.irfs[i][0, 1]\n",
    "            df2.loc[:, f\"irf_btc_from_eth_step{i}\"] = val2\n",
    "    except Exception as e:\n",
    "        print(\"VAR/IRF 계산 실패:\", e)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7102ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. Lag 적용\n",
    "# ============================================================================\n",
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    \"\"\"\n",
    "    Lag 피처 적용 (원본 유지 + lag 추가)\n",
    "    \n",
    "    핵심 원칙:\n",
    "    1. 원본(lag0) 피처는 그대로 유지\n",
    "    2. lag1, lag2 피처를 추가로 생성\n",
    "    3. 이동평균/차분은 lag 불필요 (이미 과거 참조)\n",
    "    4. 이벤트는 lag 없음 (당일 반영)\n",
    "    \n",
    "    출처: \"Seeing Beyond Noise\" (2024), scikit-learn\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Lag 피처 적용 중 (원본 유지) ===\")\n",
    "    \n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    # ===== Lag 적용 대상: 원본 감성 지표만 =====\n",
    "    raw_sentiment_cols = [\n",
    "        'sentiment_mean', 'sentiment_std', 'sentiment_sum',\n",
    "        'news_count', 'positive_ratio', 'negative_ratio',\n",
    "        'sentiment_polarity', 'sentiment_intensity', \n",
    "        'sentiment_disagreement', 'bull_bear_ratio',\n",
    "        'weighted_sentiment', 'extremity_index',\n",
    "        'extreme_positive_count', 'extreme_negative_count'\n",
    "    ]\n",
    "    \n",
    "    # ===== Lag 제외: 이동평균, 차분 (이미 과거 참조) =====\n",
    "    no_lag_patterns = [\n",
    "        '_ma', '_volatility_', '_trend', '_acceleration', \n",
    "        '_volume_change', '_volume_ma'\n",
    "    ]\n",
    "    \n",
    "    # ===== 온체인 데이터 =====\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', \n",
    "                                  'eth_large', 'eth_token', 'eth_contract',\n",
    "                                  'eth_avg_gas', 'eth_total_gas', \n",
    "                                  'eth_avg_block'])]\n",
    "    \n",
    "    # ===== 기타 외부 변수 =====\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    # ===== 제외 컬럼 =====\n",
    "    exclude_cols = [\n",
    "        'date', 'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Volume', 'ETH_Open',\n",
    "        'BTC_Close', 'BTC_High', 'BTC_Low', 'BTC_Volume', 'BTC_Open',\n",
    "        'BNB_Close', 'XRP_Close', 'SOL_Close', 'ADA_Close', 'DOGE_Close',\n",
    "        'AVAX_Close', 'DOT_Close'\n",
    "    ]\n",
    "    \n",
    "    # 이벤트는 lag 없음\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col])\n",
    "    \n",
    "    # 이미 lag가 있는 컬럼 제외\n",
    "    exclude_cols.extend([col for col in df.columns if '_lag' in col])\n",
    "    \n",
    "    lag_count = 0\n",
    "    \n",
    "    # ===== 1. 원본 감성 지표에만 lag 적용 (원본 유지!) =====\n",
    "    print(\"  [감성 지표 Lag]\")\n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            # 이동평균/차분이 아닌지 확인\n",
    "            is_derived = any(pattern in col for pattern in no_lag_patterns)\n",
    "            \n",
    "            if not is_derived:\n",
    "                # 원본은 그대로 유지하고, lag만 추가\n",
    "                for lag in range(1, news_lag + 1):\n",
    "                    new_col = f\"{col}_lag{lag}\"\n",
    "                    df_lagged[new_col] = df[col].shift(lag)\n",
    "                    lag_count += 1\n",
    "                print(f\"    {col}: 원본 유지 + lag1~{news_lag} 추가\")\n",
    "    \n",
    "    # ===== 2. 온체인 lag (원본 유지!) =====\n",
    "    print(\"  [온체인 지표 Lag]\")\n",
    "    onchain_lag_count = 0\n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            onchain_lag_count += 1\n",
    "    print(f\"    {onchain_lag_count}개 컬럼: 원본 유지 + lag1 추가\")\n",
    "    \n",
    "    # ===== 3. 기타 외부 변수 lag (원본 유지!) =====\n",
    "    print(\"  [기타 외부 변수 Lag]\")\n",
    "    other_lag_count = 0\n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            other_lag_count += 1\n",
    "    print(f\"    {other_lag_count}개 컬럼: 원본 유지 + lag1 추가\")\n",
    "    \n",
    "    # ===== 4. 이동평균/차분은 lag 없음 (명시) =====\n",
    "    no_lag_cols = [col for col in df.columns if any(p in col for p in no_lag_patterns)]\n",
    "    print(f\"  [Lag 미적용] {len(no_lag_cols)}개 컬럼 (이동평균/차분 - 이미 과거 참조)\")\n",
    "    \n",
    "    total_lag = lag_count + onchain_lag_count + other_lag_count\n",
    "    print(f\"\\n✓ 총 Lag 피처 추가: {total_lag}개\")\n",
    "    print(f\"  총 컬럼 수: {df_lagged.shape[1]} (원본 {df.shape[1]} + lag {total_lag})\")\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    \"\"\"\n",
    "    과거 가격을 피처로 추가 (기술적 지표보다 먼저!)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== [STEP 0] 과거 가격 피처 추가 ===\")\n",
    "    \n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    # 과거 종가 (핵심!)\n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    # 과거 고가/저가\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "    \n",
    "    # 과거 거래량\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "    \n",
    "    # 과거 수익률\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    # 과거 가격 비율\n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    added = df_new.shape[1] - df.shape[1]\n",
    "    print(f\"  ✓ 추가된 과거 가격 피처: {added}개\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 타겟 변수 생성\n",
    "# ============================================================================\n",
    "def create_targets(df):\n",
    "    \"\"\"타겟 변수 생성\"\"\"\n",
    "    print(\"\\n=== 타겟 변수 생성 ===\")\n",
    "    \n",
    "    df_target = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "\n",
    "    # 내일 종가\n",
    "    next_close = close.shift(-1)\n",
    "    \n",
    "    # 오늘 → 내일 로그 수익률\n",
    "    df_target['next_log_return'] = np.log(next_close / close)\n",
    "    \n",
    "    # 오늘 → 내일 방향성\n",
    "    df_target['next_direction'] = (next_close > close).astype(int)\n",
    "    \n",
    "    # 참고: 내일 실제 종가\n",
    "    df_target['next_close'] = next_close\n",
    "    \n",
    "    print(\"✓ 타겟 변수 생성 완료\")\n",
    "    \n",
    "    # 확인\n",
    "    print(f\"\\n  검증:\")\n",
    "    print(f\"  오늘 종가 평균: {close.mean():.2f}\")\n",
    "    print(f\"  내일 종가 평균: {next_close.mean():.2f}\")\n",
    "    print(f\"  로그 수익률 평균: {df_target['next_log_return'].mean():.6f}\")\n",
    "    print(f\"  상승 비율: {df_target['next_direction'].mean():.2%}\")\n",
    "    \n",
    "    return df_target\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Train-Val-Test 분할\n",
    "# ============================================================================\n",
    "def split_train_val_test(df, train_start_date, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"시계열 기반 데이터 분할\"\"\"\n",
    "    print(\"\\n=== Train-Val-Test 분할 ===\")\n",
    "    \n",
    "    df_train_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_clean = df_train_period.dropna(subset=['next_log_return', 'next_direction','next_close'])\n",
    "    \n",
    "    print(f\"결측치 제거 후: {len(df_clean)} samples\")\n",
    "    print(f\"기간: {df_clean['date'].min()} ~ {df_clean['date'].max()}\")\n",
    "    \n",
    "    n = len(df_clean)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df_clean.iloc[:train_end].copy()\n",
    "    val_df = df_clean.iloc[train_end:val_end].copy()\n",
    "    test_df = df_clean.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"Train: {len(train_df)} ({train_df['date'].min()} ~ {train_df['date'].max()})\")\n",
    "    print(f\"Val: {len(val_df)} ({val_df['date'].min()} ~ {val_df['date'].max()})\")\n",
    "    print(f\"Test: {len(test_df)} ({test_df['date'].min()} ~ {test_df['date'].max()})\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. 결측치 처리 \n",
    "# ============================================================================\n",
    "def handle_missing_values_paper_based(df, train_start_date):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리 (논문 기반)\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "       - 암호화폐는 브라운 운동과 유사, 복잡한 전처리 불필요\n",
    "       - 원본 데이터 최대한 보존\n",
    "    \n",
    "    2. \"Time Series Data Forecasting\" - 시계열 결측치 처리 원칙\n",
    "       - Forward fill: 과거→현재만 (데이터 누수 방지)\n",
    "       - 기술적 지표 초기값: 자연스러운 결측 (유지)\n",
    "    \n",
    "    처리 원칙:\n",
    "    - Train 시작 이전 데이터만 lookback으로 사용\n",
    "    - 미래 정보 누수 차단\n",
    "    - 최소한의 개입\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 결측치 처리 ===\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    initial_shape = df_processed.shape\n",
    "    \n",
    "    # ===== 1. 기술적 지표 (자연적 결측 유지) =====\n",
    "    tech_cols = [col for col in df.columns if any(x in col for x in \n",
    "                ['RSI', 'MACD', 'SMA', 'EMA', 'WMA', 'HMA', 'ATR', 'BBL', 'BBM', 'BBU',\n",
    "                 'WILLR', 'ROC', 'MOM', 'CCI', 'STOCH', 'ADX', 'AROON', 'TSI', 'UO',\n",
    "                 'FISHER', 'KST', 'VWMA', 'ZLMA', 'NATR', 'UI', 'MASSI', 'CHOP',\n",
    "                 'DPO', 'ICS', 'LINREG', 'SLOPE'])]\n",
    "    \n",
    "    tech_missing = df_processed[tech_cols].isnull().sum().sum() if tech_cols else 0\n",
    "    print(f\"  기술적 지표: {tech_missing:,}개 (초기 계산 기간, 유지)\")\n",
    "    \n",
    "    # ===== 2. 외부 변수 (Forward Fill) =====\n",
    "    external_cols = [col for col in df.columns if any(x in col for x in \n",
    "                    ['eth_', 'fg_', 'usdt_', 'aave_', 'lido_', 'makerdao_',\n",
    "                     'chain_', 'funding_', 'sp500_', 'vix_', 'gold_', 'dxy_'])]\n",
    "    \n",
    "    if external_cols:\n",
    "        ext_before = df_processed[external_cols].isnull().sum().sum()\n",
    "        df_processed[external_cols] = df_processed[external_cols].fillna(method='ffill')\n",
    "        \n",
    "        # 여전히 NaN이면 0으로 (초기 기간)\n",
    "        df_processed[external_cols] = df_processed[external_cols].fillna(0)\n",
    "        ext_after = df_processed[external_cols].isnull().sum().sum()\n",
    "        print(f\"  외부 변수: {ext_before:,} → {ext_after:,}개\")\n",
    "    \n",
    "    # ===== 3. 감성 지표 (Forward Fill + 초기값 0) =====\n",
    "    sentiment_cols = [col for col in df.columns if any(x in col for x in \n",
    "                     ['sentiment', 'news', 'positive_ratio', 'negative_ratio',\n",
    "                      'bull_bear', 'weighted_sentiment', 'extremity'])]\n",
    "    \n",
    "    if sentiment_cols:\n",
    "        sent_before = df_processed[sentiment_cols].isnull().sum().sum()\n",
    "        df_processed[sentiment_cols] = df_processed[sentiment_cols].fillna(method='ffill')\n",
    "        \n",
    "        # 초기 기간 0으로\n",
    "        df_processed[sentiment_cols] = df_processed[sentiment_cols].fillna(0)\n",
    "        sent_after = df_processed[sentiment_cols].isnull().sum().sum()\n",
    "        print(f\"  감성 지표: {sent_before:,} → {sent_after:,}개\")\n",
    "    \n",
    "    # ===== 4. 이벤트 지표 (결측치 없어야 함) =====\n",
    "    event_cols = [col for col in df.columns if 'event_' in col or 'period_' in col]\n",
    "    \n",
    "    if event_cols:\n",
    "        event_missing = df_processed[event_cols].isnull().sum().sum()\n",
    "        if event_missing > 0:\n",
    "            print(f\"  [경고] 이벤트 지표 결측치: {event_missing}개 → 0으로 채움\")\n",
    "            df_processed[event_cols] = df_processed[event_cols].fillna(0).astype(int)\n",
    "        else:\n",
    "            print(f\"  이벤트 지표: 결측치 없음 ✓\")\n",
    "    \n",
    "    # ===== 5. Lag 피처 (자연적 결측) =====\n",
    "    lag_cols = [col for col in df.columns if '_lag' in col]\n",
    "    lag_missing = df_processed[lag_cols].isnull().sum().sum() if lag_cols else 0\n",
    "    print(f\"  Lag 피처: {lag_missing:,}개 (초기 기간, 유지)\")\n",
    "    \n",
    "    # ===== 6. 가격 데이터 확인 =====\n",
    "    price_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Volume', 'ETH_Open']\n",
    "    price_missing = df_processed[price_cols].isnull().sum().sum()\n",
    "    if price_missing > 0:\n",
    "        print(f\"  [오류] 가격 데이터 결측: {price_missing}개 - 확인 필요!\")\n",
    "    else:\n",
    "        print(f\"  가격 데이터: 결측치 없음 ✓\")\n",
    "    \n",
    "    # ===== 7. Lookback Period 제거 =====\n",
    "    before_count = len(df_processed)\n",
    "    df_processed = df_processed[df_processed['date'] >= train_start_date].reset_index(drop=True)\n",
    "    removed = before_count - len(df_processed)\n",
    "    print(f\"  Lookback 제거: {removed}행\")\n",
    "    \n",
    "    # ===== 8. 최종 통계 =====\n",
    "    total_missing = df_processed.isnull().sum().sum()\n",
    "    total_cells = df_processed.shape[0] * df_processed.shape[1]\n",
    "    missing_pct = (total_missing / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\n✓ 최종 결측치: {total_missing:,}/{total_cells:,} ({missing_pct:.2f}%)\")\n",
    "    print(f\"  Shape: {initial_shape} → {df_processed.shape}\")\n",
    "    print(f\"  기간: {df_processed['date'].min().date()} ~ {df_processed['date'].max().date()}\")\n",
    "    \n",
    "    # ===== 9. 높은 결측치 컬럼 경고 =====\n",
    "    col_missing_pct = (df_processed.isnull().sum() / len(df_processed) * 100).sort_values(ascending=False)\n",
    "    high_missing = col_missing_pct[col_missing_pct > 20]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"\\n  [주의] 결측치 20% 이상: {len(high_missing)}개 컬럼\")\n",
    "        for col, pct in high_missing.head(5).items():\n",
    "            print(f\"    - {col}: {pct:.1f}%\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. 데이터 품질 검증\n",
    "# ============================================================================\n",
    "def validate_data_leakage(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    데이터 누수 검증\n",
    "    - Train < Val < Test 날짜 순서 확인\n",
    "    - 겹치는 날짜 없음 확인\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 데이터 누수 검증 ===\")\n",
    "    \n",
    "    train_dates = set(train_df['date'])\n",
    "    val_dates = set(val_df['date'])\n",
    "    test_dates = set(test_df['date'])\n",
    "    \n",
    "    train_max = train_df['date'].max()\n",
    "    val_min = val_df['date'].min()\n",
    "    val_max = val_df['date'].max()\n",
    "    test_min = test_df['date'].min()\n",
    "    \n",
    "    overlap_train_val = len(train_dates & val_dates)\n",
    "    overlap_val_test = len(val_dates & test_dates)\n",
    "    overlap_train_test = len(train_dates & test_dates)\n",
    "    \n",
    "    print(f\"날짜 순서:\")\n",
    "    print(f\"  Train: ~ {train_max.date()}\")\n",
    "    print(f\"  Val:   {val_min.date()} ~ {val_max.date()}\")\n",
    "    print(f\"  Test:  {test_min.date()} ~\")\n",
    "    \n",
    "    print(f\"\\n날짜 중복:\")\n",
    "    print(f\"  Train-Val: {overlap_train_val}개\")\n",
    "    print(f\"  Val-Test: {overlap_val_test}개\")\n",
    "    print(f\"  Train-Test: {overlap_train_test}개\")\n",
    "    \n",
    "    if overlap_train_val + overlap_val_test + overlap_train_test > 0:\n",
    "        print(\"[경고] 데이터 누수 가능성 발견!\")\n",
    "        return False\n",
    "    \n",
    "    if train_max >= val_min or val_max >= test_min:\n",
    "        print(\"[경고] 날짜 순서 오류!\")\n",
    "        return False\n",
    "    \n",
    "    print(\"[통과] 데이터 누수 없음\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96aa31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ethereum_event_features_complete(df):\n",
    "    \"\"\"\n",
    "    이더리움 완전 타임라인 (2013-2025.10.07)\n",
    "    출처: Ethereum.org 공식 문서 (2025년 8월 업데이트)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 이더리움 이벤트 지표 생성 (2013-2025 완전판) ===\")\n",
    "    \n",
    "    df_event = df.copy()\n",
    "    event_count = 0\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PRE-LAUNCH (2013-2014)\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Whitepaper Released (2013-11-27) - 비탈릭 부테린 백서 발표\n",
    "    whitepaper_date = pd.to_datetime('2013-11-27')\n",
    "    df_event['event_whitepaper'] = (df_event['date'] == whitepaper_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Yellowpaper Released (2014-04-01) - Gavin Wood 기술 명세서\n",
    "    yellowpaper_date = pd.to_datetime('2014-04-01')\n",
    "    df_event['event_yellowpaper'] = (df_event['date'] == yellowpaper_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Ether Sale (2014-07-22 ~ 2014-09-02) - ICO 시작/종료\n",
    "    ether_sale_start = pd.to_datetime('2014-07-22')\n",
    "    ether_sale_end = pd.to_datetime('2014-09-02')\n",
    "    df_event['event_ether_sale_start'] = (df_event['date'] == ether_sale_start).astype(int)\n",
    "    df_event['event_ether_sale_end'] = (df_event['date'] == ether_sale_end).astype(int)\n",
    "    event_count += 2\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2015: GENESIS & FRONTIER\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Frontier Launch (2015-07-30) - 이더리움 메인넷 공식 출시\n",
    "    frontier_date = pd.to_datetime('2015-07-30')\n",
    "    df_event['event_frontier'] = (df_event['date'] == frontier_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Frontier Thawing (2015-09-07) - 가스 제한 해제\n",
    "    frontier_thawing_date = pd.to_datetime('2015-09-07')\n",
    "    df_event['event_frontier_thawing'] = (df_event['date'] == frontier_thawing_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Ice Age (2015-09-08) - Difficulty Bomb 도입\n",
    "    ice_age_date = pd.to_datetime('2015-09-08')\n",
    "    df_event['event_ice_age'] = (df_event['date'] == ice_age_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2016: HOMESTEAD & DAO CRISIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Homestead (2016-03-14) - 첫 메이저 업그레이드\n",
    "    homestead_date = pd.to_datetime('2016-03-14')\n",
    "    df_event['event_homestead'] = (df_event['date'] == homestead_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # DAO Created (2016-04-30) - The DAO 크라우드세일 시작\n",
    "    dao_created = pd.to_datetime('2016-04-30')\n",
    "    df_event['event_dao_created'] = (df_event['date'] == dao_created).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # DAO Hack (2016-06-17) - $50M 해킹 사건\n",
    "    dao_hack_date = pd.to_datetime('2016-06-17')\n",
    "    df_event['event_dao_hack'] = (df_event['date'] == dao_hack_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # DAO Hard Fork (2016-07-20) - ETH/ETC 분리\n",
    "    dao_fork_date = pd.to_datetime('2016-07-20')\n",
    "    df_event['event_dao_fork'] = (df_event['date'] == dao_fork_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Tangerine Whistle (2016-10-18) - DoS 공격 대응\n",
    "    tangerine_date = pd.to_datetime('2016-10-18')\n",
    "    df_event['event_tangerine_whistle'] = (df_event['date'] == tangerine_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Spurious Dragon (2016-11-22) - DoS 공격 추가 대응\n",
    "    spurious_date = pd.to_datetime('2016-11-22')\n",
    "    df_event['event_spurious_dragon'] = (df_event['date'] == spurious_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2017: ICO BOOM & METROPOLIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Enterprise Ethereum Alliance (2017-02-28) - 기업 연합 출범\n",
    "    eea_date = pd.to_datetime('2017-02-28')\n",
    "    df_event['event_eea_launch'] = (df_event['date'] == eea_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Byzantium (2017-10-16) - Metropolis Part 1\n",
    "    byzantium_date = pd.to_datetime('2017-10-16')\n",
    "    df_event['event_byzantium'] = (df_event['date'] == byzantium_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Bitcoin ATH (2017-12-17) - BTC $19,783\n",
    "    btc_peak_2017 = pd.to_datetime('2017-12-17')\n",
    "    df_event['event_btc_peak_2017'] = (df_event['date'] == btc_peak_2017).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2018: BEAR MARKET\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crypto Crash 2018 (2018-01-16) - 대규모 하락 시작\n",
    "    crash_2018 = pd.to_datetime('2018-01-16')\n",
    "    df_event['event_crash_2018'] = (df_event['date'] == crash_2018).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Coincheck Hack (2018-01-26) - $530M NEM 해킹\n",
    "    coincheck_hack = pd.to_datetime('2018-01-26')\n",
    "    df_event['event_coincheck_hack'] = (df_event['date'] == coincheck_hack).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ETH Bottom (2018-12-15) - $83 최저점\n",
    "    eth_bottom_2018 = pd.to_datetime('2018-12-15')\n",
    "    df_event['event_eth_bottom_2018'] = (df_event['date'] == eth_bottom_2018).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2019: RECOVERY & ISTANBUL\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Constantinople (2019-02-28) - Metropolis Part 2\n",
    "    constantinople_date = pd.to_datetime('2019-02-28')\n",
    "    df_event['event_constantinople'] = (df_event['date'] == constantinople_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Istanbul (2019-12-08) - 프라이버시 강화\n",
    "    istanbul_date = pd.to_datetime('2019-12-08')\n",
    "    df_event['event_istanbul'] = (df_event['date'] == istanbul_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2020: DEFI SUMMER & ETH 2.0 BEGIN\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Muir Glacier (2020-01-02) - Difficulty Bomb 연기\n",
    "    muir_glacier = pd.to_datetime('2020-01-02')\n",
    "    df_event['event_muir_glacier'] = (df_event['date'] == muir_glacier).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # COVID-19 Black Thursday (2020-03-12) - 역사적 급락\n",
    "    covid_crash = pd.to_datetime('2020-03-12')\n",
    "    df_event['event_covid_crash'] = (df_event['date'] == covid_crash).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Bitcoin Halving (2020-05-11)\n",
    "    btc_halving_2020 = pd.to_datetime('2020-05-11')\n",
    "    df_event['event_btc_halving_2020'] = (df_event['date'] == btc_halving_2020).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # DeFi Summer (2020-06-15) - COMP 토큰 배포\n",
    "    defi_summer_start = pd.to_datetime('2020-06-15')\n",
    "    df_event['event_defi_summer_start'] = (df_event['date'] == defi_summer_start).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Staking Deposit Contract (2020-11-04) - ETH 2.0 준비\n",
    "    deposit_contract = pd.to_datetime('2020-11-04')\n",
    "    df_event['event_deposit_contract'] = (df_event['date'] == deposit_contract).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Beacon Chain Genesis (2020-12-01) - ETH 2.0 시작!\n",
    "    beacon_genesis = pd.to_datetime('2020-12-01')\n",
    "    df_event['event_beacon_genesis'] = (df_event['date'] == beacon_genesis).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2021: BULL RUN & NFT BOOM\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Berlin (2021-04-15)\n",
    "    berlin_date = pd.to_datetime('2021-04-15')\n",
    "    df_event['event_berlin'] = (df_event['date'] == berlin_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ETH ATH May (2021-05-12) - $4,362\n",
    "    eth_ath_may = pd.to_datetime('2021-05-12')\n",
    "    df_event['event_eth_ath_may2021'] = (df_event['date'] == eth_ath_may).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # China Crypto Ban (2021-05-21)\n",
    "    china_ban = pd.to_datetime('2021-05-21')\n",
    "    df_event['event_china_ban'] = (df_event['date'] == china_ban).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # London (2021-08-05) - EIP-1559!\n",
    "    london_date = pd.to_datetime('2021-08-05')\n",
    "    df_event['event_london'] = (df_event['date'] == london_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Altair (2021-10-27)\n",
    "    altair_date = pd.to_datetime('2021-10-27')\n",
    "    df_event['event_altair'] = (df_event['date'] == altair_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ETH ATH Nov (2021-11-16) - $4,891 역대 최고가\n",
    "    eth_ath_nov = pd.to_datetime('2021-11-16')\n",
    "    df_event['event_eth_ath_nov2021'] = (df_event['date'] == eth_ath_nov).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Arrow Glacier (2021-12-09)\n",
    "    arrow_glacier = pd.to_datetime('2021-12-09')\n",
    "    df_event['event_arrow_glacier'] = (df_event['date'] == arrow_glacier).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2022: CRYPTO WINTER & THE MERGE\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Terra/LUNA Collapse (2022-05-09)\n",
    "    terra_collapse = pd.to_datetime('2022-05-09')\n",
    "    df_event['event_terra_collapse'] = (df_event['date'] == terra_collapse).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Gray Glacier (2022-06-30)\n",
    "    gray_glacier = pd.to_datetime('2022-06-30')\n",
    "    df_event['event_gray_glacier'] = (df_event['date'] == gray_glacier).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Celsius Bankruptcy (2022-07-13)\n",
    "    celsius = pd.to_datetime('2022-07-13')\n",
    "    df_event['event_celsius_bankruptcy'] = (df_event['date'] == celsius).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Bellatrix (2022-09-06) - Merge 준비\n",
    "    bellatrix = pd.to_datetime('2022-09-06')\n",
    "    df_event['event_bellatrix'] = (df_event['date'] == bellatrix).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # THE MERGE (2022-09-15) - PoW→PoS!\n",
    "    merge_date = pd.to_datetime('2022-09-15')\n",
    "    df_event['event_merge'] = (df_event['date'] == merge_date).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # FTX Collapse (2022-11-11)\n",
    "    ftx_collapse = pd.to_datetime('2022-11-11')\n",
    "    df_event['event_ftx_collapse'] = (df_event['date'] == ftx_collapse).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2023: STAKING WITHDRAWALS & RECOVERY\n",
    "    # ============================================================================\n",
    "    \n",
    "    # SVB Collapse (2023-03-10)\n",
    "    svb_collapse = pd.to_datetime('2023-03-10')\n",
    "    df_event['event_svb_collapse'] = (df_event['date'] == svb_collapse).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Shanghai/Capella (2023-04-12) - 스테이킹 출금!\n",
    "    shanghai = pd.to_datetime('2023-04-12')\n",
    "    df_event['event_shanghai'] = (df_event['date'] == shanghai).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2024: PROTO-DANKSHARDING & ETF\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Dencun (2024-03-13) - Blob transactions (EIP-4844)\n",
    "    dencun = pd.to_datetime('2024-03-13')\n",
    "    df_event['event_dencun'] = (df_event['date'] == dencun).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # Bitcoin Halving (2024-04-19)\n",
    "    btc_halving_2024 = pd.to_datetime('2024-04-19')\n",
    "    df_event['event_btc_halving_2024'] = (df_event['date'] == btc_halving_2024).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ETH Spot ETF Approval (2024-05-23) - SEC 승인!\n",
    "    eth_etf_approval = pd.to_datetime('2024-05-23')\n",
    "    df_event['event_eth_etf_approval'] = (df_event['date'] == eth_etf_approval).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ETH Spot ETF Trading (2024-07-23) - 거래 시작\n",
    "    eth_etf_trading = pd.to_datetime('2024-07-23')\n",
    "    df_event['event_eth_etf_trading'] = (df_event['date'] == eth_etf_trading).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # 2025: PECTRA & CURRENT\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Pectra (2025-05-07) - EIP-7251, EIP-7702\n",
    "    pectra = pd.to_datetime('2025-05-07')\n",
    "    df_event['event_pectra'] = (df_event['date'] == pectra).astype(int)\n",
    "    event_count += 1\n",
    "    \n",
    "    # 현재까지 (2025-10-07) - 추가 이벤트 없음\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PERIOD EVENTS (장기 트렌드)\n",
    "    # ============================================================================\n",
    "    \n",
    "    periods = {\n",
    "        # ICO 광풍 (2017)\n",
    "        'ico_boom': (pd.to_datetime('2017-01-01'), pd.to_datetime('2018-01-31')),\n",
    "        \n",
    "        # 2018 베어마켓\n",
    "        'bear_2018': (pd.to_datetime('2018-02-01'), pd.to_datetime('2019-03-31')),\n",
    "        \n",
    "        # DeFi Summer (2020)\n",
    "        'defi_summer': (pd.to_datetime('2020-06-01'), pd.to_datetime('2020-09-30')),\n",
    "        \n",
    "        # NFT Boom (2021)\n",
    "        'nft_boom': (pd.to_datetime('2021-01-01'), pd.to_datetime('2021-12-31')),\n",
    "        \n",
    "        # Crypto Winter (2022-2023)\n",
    "        'crypto_winter': (pd.to_datetime('2022-05-01'), pd.to_datetime('2023-03-31')),\n",
    "        \n",
    "        # L2 Scaling Era (2023~)\n",
    "        'l2_scaling': (pd.to_datetime('2023-04-01'), pd.to_datetime('2025-10-07')),\n",
    "    }\n",
    "    \n",
    "    for period_name, (start, end) in periods.items():\n",
    "        df_event[f'period_{period_name}'] = (\n",
    "            (df_event['date'] >= start) & (df_event['date'] <= end)\n",
    "        ).astype(int)\n",
    "        event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # EVENT WINDOWS (주요 이벤트만)\n",
    "    # ============================================================================\n",
    "    \n",
    "    critical_events = {\n",
    "        'event_dao_fork': dao_fork_date,\n",
    "        'event_london': london_date,\n",
    "        'event_merge': merge_date,\n",
    "        'event_shanghai': shanghai,\n",
    "        'event_dencun': dencun,\n",
    "        'event_pectra': pectra,\n",
    "        'event_eth_etf_approval': eth_etf_approval,\n",
    "    }\n",
    "    \n",
    "    for event_name, event_date in critical_events.items():\n",
    "        # 사전 30일\n",
    "        df_event[f'{event_name}_pre30'] = (\n",
    "            (df_event['date'] > event_date - timedelta(days=30)) &\n",
    "            (df_event['date'] < event_date)\n",
    "        ).astype(int)\n",
    "        event_count += 1\n",
    "        \n",
    "        # 사후 7일\n",
    "        df_event[f'{event_name}_post7'] = (\n",
    "            (df_event['date'] > event_date) &\n",
    "            (df_event['date'] <= event_date + timedelta(days=7))\n",
    "        ).astype(int)\n",
    "        event_count += 1\n",
    "        \n",
    "        # 사후 30일\n",
    "        df_event[f'{event_name}_post30'] = (\n",
    "            (df_event['date'] > event_date + timedelta(days=7)) &\n",
    "            (df_event['date'] <= event_date + timedelta(days=30))\n",
    "        ).astype(int)\n",
    "        event_count += 1\n",
    "    \n",
    "    # ============================================================================\n",
    "    # META INDICATORS\n",
    "    # ============================================================================\n",
    "    \n",
    "    all_event_dates = [\n",
    "        whitepaper_date, yellowpaper_date, ether_sale_start, frontier_date,\n",
    "        homestead_date, dao_hack_date, dao_fork_date, byzantium_date,\n",
    "        constantinople_date, istanbul_date, covid_crash, btc_halving_2020,\n",
    "        defi_summer_start, beacon_genesis, berlin_date, london_date,\n",
    "        eth_ath_nov, terra_collapse, merge_date, ftx_collapse,\n",
    "        shanghai, dencun, btc_halving_2024, eth_etf_approval, pectra\n",
    "    ]\n",
    "    \n",
    "    df_event['days_since_last_event'] = 0\n",
    "    df_event['event_count_90d'] = 0\n",
    "    \n",
    "    for idx, row in df_event.iterrows():\n",
    "        current_date = row['date']\n",
    "        \n",
    "        # 마지막 이벤트 이후 경과\n",
    "        past_events = [d for d in all_event_dates if d < current_date]\n",
    "        if past_events:\n",
    "            df_event.at[idx, 'days_since_last_event'] = (current_date - max(past_events)).days\n",
    "        \n",
    "        # 최근 90일 이벤트 수\n",
    "        recent = [d for d in all_event_dates \n",
    "                 if current_date - timedelta(days=90) <= d < current_date]\n",
    "        df_event.at[idx, 'event_count_90d'] = len(recent)\n",
    "    \n",
    "    event_count += 2\n",
    "    \n",
    "    print(f\"✓ 이벤트 지표 추가: {event_count}개\")\n",
    "    print(f\"  기간: 2013-11-27 ~ 2025-10-07\")\n",
    "    \n",
    "    return df_event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beab953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 전처리 파이프라인 시작 ===\n",
      "\n",
      "\n",
      "=== [STEP 0] 과거 가격 피처 추가 ===\n",
      "  ✓ 추가된 과거 가격 피처: 31개\n",
      "\n",
      "=== 기술적 지표 생성 중 ===\n",
      "  - Momentum 지표 생성 중...\n",
      "  - Overlap 지표 생성 중...\n",
      "  - Volatility 지표 생성 중...\n",
      "  - Volume 지표 생성 중...\n",
      "  - Trend 지표 생성 중...\n",
      "  - 파생 지표 생성 중...\n",
      "  - 고급 파생 지표 생성 중...\n",
      "\n",
      "✓ 기술적 지표 생성 완료: 124개 추가\n",
      "  총 컬럼 수: 250\n",
      "\n",
      "=== Lag 피처 적용 중 (원본 유지) ===\n",
      "  [감성 지표 Lag]\n",
      "    sentiment_mean: 원본 유지 + lag1~2 추가\n",
      "    sentiment_std: 원본 유지 + lag1~2 추가\n",
      "    sentiment_sum: 원본 유지 + lag1~2 추가\n",
      "    news_count: 원본 유지 + lag1~2 추가\n",
      "    positive_ratio: 원본 유지 + lag1~2 추가\n",
      "    negative_ratio: 원본 유지 + lag1~2 추가\n",
      "    sentiment_polarity: 원본 유지 + lag1~2 추가\n",
      "    sentiment_intensity: 원본 유지 + lag1~2 추가\n",
      "    sentiment_disagreement: 원본 유지 + lag1~2 추가\n",
      "    bull_bear_ratio: 원본 유지 + lag1~2 추가\n",
      "    weighted_sentiment: 원본 유지 + lag1~2 추가\n",
      "    extremity_index: 원본 유지 + lag1~2 추가\n",
      "    extreme_positive_count: 원본 유지 + lag1~2 추가\n",
      "    extreme_negative_count: 원본 유지 + lag1~2 추가\n",
      "  [온체인 지표 Lag]\n",
      "    10개 컬럼: 원본 유지 + lag1 추가\n",
      "  [기타 외부 변수 Lag]\n",
      "    14개 컬럼: 원본 유지 + lag1 추가\n",
      "  [Lag 미적용] 12개 컬럼 (이동평균/차분 - 이미 과거 참조)\n",
      "\n",
      "✓ 총 Lag 피처 추가: 52개\n",
      "  총 컬럼 수: 309 (원본 257 + lag 52)\n",
      "\n",
      "=== 타겟 변수 생성 ===\n",
      "✓ 타겟 변수 생성 완료\n",
      "\n",
      "  검증:\n",
      "  오늘 종가 평균: 2264.45\n",
      "  내일 종가 평균: 2265.49\n",
      "  로그 수익률 평균: 0.001529\n",
      "  상승 비율: 51.97%\n",
      "\n",
      "=== 결측치 처리 ===\n",
      "  기술적 지표: 1,809개 (초기 계산 기간, 유지)\n",
      "  외부 변수: 5,292 → 0개\n",
      "  감성 지표: 1,869 → 0개\n",
      "  Lag 피처: 204개 (초기 기간, 유지)\n",
      "  가격 데이터: 결측치 없음 ✓\n",
      "  Lookback 제거: 200행\n",
      "\n",
      "✓ 최종 결측치: 39/546,936 (0.01%)\n",
      "  Shape: (1953, 312) → (1753, 312)\n",
      "  기간: 2020-12-19 ~ 2025-10-06\n",
      "\n",
      "=== Train-Val-Test 분할 ===\n",
      "결측치 제거 후: 1752 samples\n",
      "기간: 2020-12-19 00:00:00 ~ 2025-10-05 00:00:00\n",
      "Train: 1226 (2020-12-19 00:00:00 ~ 2024-04-27 00:00:00)\n",
      "Val: 263 (2024-04-28 00:00:00 ~ 2025-01-15 00:00:00)\n",
      "Test: 263 (2025-01-16 00:00:00 ~ 2025-10-05 00:00:00)\n",
      "\n",
      "=== 데이터 누수 검증 ===\n",
      "날짜 순서:\n",
      "  Train: ~ 2024-04-27\n",
      "  Val:   2024-04-28 ~ 2025-01-15\n",
      "  Test:  2025-01-16 ~\n",
      "\n",
      "날짜 중복:\n",
      "  Train-Val: 0개\n",
      "  Val-Test: 0개\n",
      "  Train-Test: 0개\n",
      "[통과] 데이터 누수 없음\n",
      "\n",
      "=== 최종 결과 ===\n",
      "총 피처: 308개\n",
      "이벤트 피처: 0개\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 실행\n",
    "# ============================================================================\n",
    "print(\"=== 데이터 전처리 파이프라인 시작 ===\\n\")\n",
    "\n",
    "\n",
    "# 4. 이벤트 지표 (2015-2025 완전판!)\n",
    "#df_with_events = create_ethereum_event_features_complete(df_merged)\n",
    "df_with_events=df_merged\n",
    "\n",
    "# 5. 과거 가격 피처\n",
    "df_with_price_lags = add_price_lag_features_first(df_with_events)\n",
    "\n",
    "# 6. 기술적 지표\n",
    "df_with_indicators = calculate_technical_indicators(df_with_price_lags)\n",
    "\n",
    "# 7. 상관관계\n",
    "df = add_pairwise_correlation_features(df_with_indicators, window_sizes=[7,14,30])\n",
    "df = add_volatility_correlation(df, window_sizes=[14,30])\n",
    "\n",
    "# 8. Lag 적용 (원본 유지!)\n",
    "df_with_lags = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "# 9. 타겟 생성\n",
    "df_final = create_targets(df_with_lags)\n",
    "\n",
    "# 10. 결측치 처리 (강화 버전!)\n",
    "df_clean = handle_missing_values_paper_based(df_final, train_start_date)\n",
    "df_clean = df_clean.dropna(subset=['next_log_return', 'next_close', 'next_direction'])\n",
    "\n",
    "# 특정 컬럼 추가 처리\n",
    "if 'ICS_26' in df_clean.columns:\n",
    "    df_clean['ICS_26'] = df_clean['ICS_26'].fillna(method='ffill')\n",
    "if 'DPO_20' in df_clean.columns:\n",
    "    df_clean['DPO_20'] = df_clean['DPO_20'].fillna(method='ffill')\n",
    "\n",
    "# lag 결측치 제거\n",
    "lag_cols = [col for col in df_clean.columns if '_lag' in col]\n",
    "df_clean = df_clean.dropna(subset=lag_cols).reset_index(drop=True)\n",
    "\n",
    "# 11. 분할\n",
    "train_df, val_df, test_df = split_train_val_test(df_clean, train_start_date)\n",
    "\n",
    "# 12. 검증\n",
    "validate_data_leakage(train_df, val_df, test_df)\n",
    "\n",
    "print(f\"\\n=== 최종 결과 ===\")\n",
    "print(f\"총 피처: {df_clean.shape[1] - 4}개\")\n",
    "\n",
    "event_features = [col for col in df_clean.columns if 'event_' in col or 'period_' in col]\n",
    "print(f\"이벤트 피처: {len(event_features)}개\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 위의 내용이 지표 생성 및 결측치 처리 train-val-test 셋 분리 #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5781e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BTC_Open</th>\n",
       "      <th>BTC_High</th>\n",
       "      <th>BTC_Low</th>\n",
       "      <th>BTC_Close</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>makerdao_makerdao_eth_tvl_lag1</th>\n",
       "      <th>chain_eth_chain_tvl_lag1</th>\n",
       "      <th>funding_fundingRate_lag1</th>\n",
       "      <th>sp500_SP500_lag1</th>\n",
       "      <th>vix_VIX_lag1</th>\n",
       "      <th>gold_GOLD_lag1</th>\n",
       "      <th>dxy_DXY_lag1</th>\n",
       "      <th>next_log_return</th>\n",
       "      <th>next_direction</th>\n",
       "      <th>next_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>114057.593750</td>\n",
       "      <td>118648.929688</td>\n",
       "      <td>113981.398438</td>\n",
       "      <td>118648.929688</td>\n",
       "      <td>71328680132</td>\n",
       "      <td>4146.033691</td>\n",
       "      <td>4351.112305</td>\n",
       "      <td>4125.541992</td>\n",
       "      <td>4351.112305</td>\n",
       "      <td>...</td>\n",
       "      <td>5.826659e+09</td>\n",
       "      <td>1.886251e+11</td>\n",
       "      <td>2.700000e-07</td>\n",
       "      <td>6688.459961</td>\n",
       "      <td>16.280001</td>\n",
       "      <td>3840.800049</td>\n",
       "      <td>97.769997</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>1</td>\n",
       "      <td>4487.923828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>118652.382812</td>\n",
       "      <td>121086.406250</td>\n",
       "      <td>118383.156250</td>\n",
       "      <td>120681.257812</td>\n",
       "      <td>71415163912</td>\n",
       "      <td>4352.240723</td>\n",
       "      <td>4517.665039</td>\n",
       "      <td>4336.526367</td>\n",
       "      <td>4487.923828</td>\n",
       "      <td>...</td>\n",
       "      <td>6.071020e+09</td>\n",
       "      <td>1.866071e+11</td>\n",
       "      <td>8.433333e-07</td>\n",
       "      <td>6711.200195</td>\n",
       "      <td>16.290001</td>\n",
       "      <td>3867.500000</td>\n",
       "      <td>97.709999</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>1</td>\n",
       "      <td>4514.870605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>120656.984375</td>\n",
       "      <td>123944.703125</td>\n",
       "      <td>119344.312500</td>\n",
       "      <td>122266.531250</td>\n",
       "      <td>83941392228</td>\n",
       "      <td>4486.934570</td>\n",
       "      <td>4591.443848</td>\n",
       "      <td>4431.479004</td>\n",
       "      <td>4514.870605</td>\n",
       "      <td>...</td>\n",
       "      <td>6.211323e+09</td>\n",
       "      <td>1.937340e+11</td>\n",
       "      <td>2.751000e-05</td>\n",
       "      <td>6715.350098</td>\n",
       "      <td>16.629999</td>\n",
       "      <td>3839.699951</td>\n",
       "      <td>97.849998</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>0</td>\n",
       "      <td>4489.197266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>122267.468750</td>\n",
       "      <td>122857.640625</td>\n",
       "      <td>121577.570312</td>\n",
       "      <td>122425.429688</td>\n",
       "      <td>36769171735</td>\n",
       "      <td>4514.909180</td>\n",
       "      <td>4519.526855</td>\n",
       "      <td>4444.012695</td>\n",
       "      <td>4489.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>6.367490e+09</td>\n",
       "      <td>1.987722e+11</td>\n",
       "      <td>4.946000e-05</td>\n",
       "      <td>6715.790039</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>3880.800049</td>\n",
       "      <td>97.720001</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>1</td>\n",
       "      <td>4515.422852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>2025-10-05</td>\n",
       "      <td>122419.671875</td>\n",
       "      <td>125559.210938</td>\n",
       "      <td>122191.960938</td>\n",
       "      <td>123513.476562</td>\n",
       "      <td>73689317763</td>\n",
       "      <td>4489.053223</td>\n",
       "      <td>4616.533203</td>\n",
       "      <td>4472.138672</td>\n",
       "      <td>4515.422852</td>\n",
       "      <td>...</td>\n",
       "      <td>6.117770e+09</td>\n",
       "      <td>2.001463e+11</td>\n",
       "      <td>3.698333e-05</td>\n",
       "      <td>6715.790039</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>3880.800049</td>\n",
       "      <td>97.720001</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>1</td>\n",
       "      <td>4687.771484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       BTC_Open       BTC_High        BTC_Low      BTC_Close  \\\n",
       "1747 2025-10-01  114057.593750  118648.929688  113981.398438  118648.929688   \n",
       "1748 2025-10-02  118652.382812  121086.406250  118383.156250  120681.257812   \n",
       "1749 2025-10-03  120656.984375  123944.703125  119344.312500  122266.531250   \n",
       "1750 2025-10-04  122267.468750  122857.640625  121577.570312  122425.429688   \n",
       "1751 2025-10-05  122419.671875  125559.210938  122191.960938  123513.476562   \n",
       "\n",
       "       BTC_Volume     ETH_Open     ETH_High      ETH_Low    ETH_Close  ...  \\\n",
       "1747  71328680132  4146.033691  4351.112305  4125.541992  4351.112305  ...   \n",
       "1748  71415163912  4352.240723  4517.665039  4336.526367  4487.923828  ...   \n",
       "1749  83941392228  4486.934570  4591.443848  4431.479004  4514.870605  ...   \n",
       "1750  36769171735  4514.909180  4519.526855  4444.012695  4489.197266  ...   \n",
       "1751  73689317763  4489.053223  4616.533203  4472.138672  4515.422852  ...   \n",
       "\n",
       "      makerdao_makerdao_eth_tvl_lag1  chain_eth_chain_tvl_lag1  \\\n",
       "1747                    5.826659e+09              1.886251e+11   \n",
       "1748                    6.071020e+09              1.866071e+11   \n",
       "1749                    6.211323e+09              1.937340e+11   \n",
       "1750                    6.367490e+09              1.987722e+11   \n",
       "1751                    6.117770e+09              2.001463e+11   \n",
       "\n",
       "      funding_fundingRate_lag1  sp500_SP500_lag1  vix_VIX_lag1  \\\n",
       "1747              2.700000e-07       6688.459961     16.280001   \n",
       "1748              8.433333e-07       6711.200195     16.290001   \n",
       "1749              2.751000e-05       6715.350098     16.629999   \n",
       "1750              4.946000e-05       6715.790039     16.650000   \n",
       "1751              3.698333e-05       6715.790039     16.650000   \n",
       "\n",
       "      gold_GOLD_lag1  dxy_DXY_lag1  next_log_return  next_direction  \\\n",
       "1747     3840.800049     97.769997         0.030959               1   \n",
       "1748     3867.500000     97.709999         0.005986               1   \n",
       "1749     3839.699951     97.849998        -0.005703               0   \n",
       "1750     3880.800049     97.720001         0.005825               1   \n",
       "1751     3880.800049     97.720001         0.037458               1   \n",
       "\n",
       "       next_close  \n",
       "1747  4487.923828  \n",
       "1748  4514.870605  \n",
       "1749  4489.197266  \n",
       "1750  4515.422852  \n",
       "1751  4687.771484  \n",
       "\n",
       "[5 rows x 388 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc5573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "결측치 분석: df_clean\n",
      "================================================================================\n",
      "\n",
      "[전체 통계]\n",
      "  Shape: (1752, 312)\n",
      "  전체 셀: 546,624개\n",
      "  결측치: 0개 (0.00%)\n",
      "  날짜 범위: 2020-12-19 ~ 2025-10-05\n",
      "\n",
      "✓ 결측치가 없습니다!\n",
      "\n",
      "================================================================================\n",
      "특정 컬럼 상세 확인\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# df_clean 결측치 상세 분석\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_missing_values(df, name=\"df_clean\"):\n",
    "    \"\"\"\n",
    "    DataFrame의 결측치를 상세하게 분석하는 함수\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"결측치 분석: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. 전체 통계\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    missing_pct = (total_missing / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\n[전체 통계]\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  전체 셀: {total_cells:,}개\")\n",
    "    print(f\"  결측치: {total_missing:,}개 ({missing_pct:.2f}%)\")\n",
    "    print(f\"  날짜 범위: {df['date'].min().date()} ~ {df['date'].max().date()}\")\n",
    "    \n",
    "    # 2. 컬럼별 결측치 (결측치 있는 것만)\n",
    "    col_missing = df.isnull().sum()\n",
    "    col_missing_pct = (col_missing / len(df) * 100)\n",
    "    \n",
    "    missing_cols = col_missing[col_missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_cols) == 0:\n",
    "        print(f\"\\n✓ 결측치가 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n[컬럼별 결측치] (총 {len(missing_cols)}개 컬럼)\")\n",
    "    print(f\"{'컬럼명':<50} {'결측치 개수':>12} {'비율':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for col, count in missing_cols.items():\n",
    "        pct = col_missing_pct[col]\n",
    "        print(f\"{col:<50} {int(count):>12,}개 {pct:>9.2f}%\")\n",
    "    \n",
    "    # 3. 카테고리별 분류\n",
    "    print(f\"\\n[카테고리별 결측치]\")\n",
    "    \n",
    "    categories = {\n",
    "        '기술적 지표': ['RSI', 'MACD', 'SMA', 'EMA', 'WMA', 'HMA', 'ATR', 'BBL', 'BBM', 'BBU',\n",
    "                     'WILLR', 'ROC', 'MOM', 'CCI', 'STOCH', 'ADX', 'AROON', 'TSI', 'UO',\n",
    "                     'DPO', 'ICS', 'LINREG', 'SLOPE', 'TEMA', 'DEMA', 'VWMA', 'NATR'],\n",
    "        '감성 지표': ['sentiment', 'news', 'positive_ratio', 'negative_ratio', 'bull_bear'],\n",
    "        '온체인': ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', 'eth_contract', 'eth_avg'],\n",
    "        'TVL/펀딩': ['tvl', 'lido', 'aave', 'makerdao', 'funding'],\n",
    "        '거시경제': ['sp500', 'vix', 'gold', 'dxy'],\n",
    "        '이벤트': ['event_', 'period_'],\n",
    "        'Lag 피처': ['_lag'],\n",
    "        '가격': ['Close', 'High', 'Low', 'Volume', 'Open'],\n",
    "        '타겟': ['next_']\n",
    "    }\n",
    "    \n",
    "    for cat_name, keywords in categories.items():\n",
    "        cat_cols = [col for col in missing_cols.index \n",
    "                   if any(kw in col for kw in keywords)]\n",
    "        if cat_cols:\n",
    "            cat_missing = sum(col_missing[col] for col in cat_cols)\n",
    "            print(f\"  {cat_name}: {len(cat_cols)}개 컬럼, {cat_missing:,}개 결측치\")\n",
    "    \n",
    "    # 4. 행별 결측치 분포\n",
    "    row_missing = df.isnull().sum(axis=1)\n",
    "    rows_with_missing = (row_missing > 0).sum()\n",
    "    \n",
    "    print(f\"\\n[행별 결측치 분포]\")\n",
    "    print(f\"  결측치 있는 행: {rows_with_missing:,}개 / {len(df):,}개 ({rows_with_missing/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    if rows_with_missing > 0:\n",
    "        print(f\"\\n  결측치 개수별 행 분포:\")\n",
    "        missing_counts = row_missing.value_counts().sort_index()\n",
    "        for num_missing, count in missing_counts.items():\n",
    "            if num_missing > 0:\n",
    "                print(f\"    {int(num_missing):3}개 결측: {int(count):>6,}행\")\n",
    "    \n",
    "    # 5. 결측치가 가장 많은 행 확인\n",
    "    if rows_with_missing > 0:\n",
    "        print(f\"\\n[결측치가 가장 많은 상위 10개 행]\")\n",
    "        top_missing_rows = row_missing.nlargest(10)\n",
    "        \n",
    "        for idx, num_missing in top_missing_rows.items():\n",
    "            if num_missing > 0:\n",
    "                date = df.loc[idx, 'date'].date()\n",
    "                missing_cols_in_row = df.loc[idx][df.loc[idx].isnull()].index.tolist()\n",
    "                print(f\"  행 {idx} ({date}): {int(num_missing)}개 결측\")\n",
    "                print(f\"    결측 컬럼 (처음 5개): {missing_cols_in_row[:5]}\")\n",
    "    \n",
    "    # 6. 특정 기간의 결측치 패턴\n",
    "    print(f\"\\n[시간대별 결측치 패턴]\")\n",
    "    df_temp = df.copy()\n",
    "    df_temp['year_month'] = df_temp['date'].dt.to_period('M')\n",
    "    df_temp['missing_count'] = df_temp.isnull().sum(axis=1)\n",
    "    \n",
    "    period_missing = df_temp.groupby('year_month')['missing_count'].agg(['mean', 'sum'])\n",
    "    period_missing = period_missing[period_missing['sum'] > 0].sort_values('sum', ascending=False).head(10)\n",
    "    \n",
    "    if len(period_missing) > 0:\n",
    "        print(f\"  결측치가 많은 상위 10개 월:\")\n",
    "        for period, row in period_missing.iterrows():\n",
    "            print(f\"    {period}: 평균 {row['mean']:.1f}개/행, 총 {int(row['sum'])}개\")\n",
    "    \n",
    "    # 7. 권장 사항\n",
    "    print(f\"\\n[권장 처리 방법]\")\n",
    "    \n",
    "    for col in missing_cols.head(10).index:\n",
    "        pct = col_missing_pct[col]\n",
    "        \n",
    "        if 'RSI' in col or 'MACD' in col or 'SMA' in col or 'EMA' in col:\n",
    "            suggestion = \"→ 초기 계산 기간 결측 (자연스러움, 유지 권장)\"\n",
    "        elif '_lag' in col:\n",
    "            suggestion = \"→ Lag로 인한 자연 결측 (유지)\"\n",
    "        elif 'sentiment' in col or 'news' in col:\n",
    "            suggestion = \"→ df_clean['{}'].fillna(method='ffill').fillna(0)\".format(col)\n",
    "        elif 'eth_' in col or 'tvl' in col:\n",
    "            suggestion = \"→ df_clean['{}'].fillna(method='ffill')\".format(col)\n",
    "        elif 'event_' in col:\n",
    "            suggestion = \"→ df_clean['{}'].fillna(0).astype(int)\".format(col)\n",
    "        else:\n",
    "            suggestion = \"→ 원인 확인 필요\"\n",
    "        \n",
    "        print(f\"  {col} ({pct:.1f}%): {suggestion}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 실행\n",
    "# ============================================================================\n",
    "\n",
    "# df_clean 분석\n",
    "analyze_missing_values(df_clean, \"df_clean\")\n",
    "\n",
    "# 추가: 특정 컬럼 상세 확인\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"특정 컬럼 상세 확인\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# 결측치 많은 컬럼 10개 선택\n",
    "top_missing_cols = df_clean.isnull().sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "for col in top_missing_cols.index:\n",
    "    if top_missing_cols[col] > 0:\n",
    "        print(f\"\\n[{col}]\")\n",
    "        print(f\"  결측치: {top_missing_cols[col]}개\")\n",
    "        print(f\"  데이터 타입: {df_clean[col].dtype}\")\n",
    "        print(f\"  비결측치 통계:\")\n",
    "        print(df_clean[col].describe())\n",
    "        \n",
    "        # 결측치가 있는 날짜 확인 (처음 5개)\n",
    "        missing_dates = df_clean[df_clean[col].isnull()]['date'].head()\n",
    "        print(f\"  결측 날짜 (처음 5개): {missing_dates.dt.date.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddad463",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 1006 버전 일단 복사 #################\n",
    "############ 1006 버전 일단 복사 #################\n",
    "############ 1006 버전 일단 복사 #################\n",
    "############ 1006 버전 일단 복사 #################\n",
    "############ 1006 버전 일단 복사 #################\n",
    "############ 1006 버전 일단 복사 #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd24fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier, \n",
    "    BaggingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def prepare_feature_target(df, task='classification'):\n",
    "    exclude_cols = ['date', 'next_log_return', 'next_direction','next_close',\n",
    "                   'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    for f in feature_cols:\n",
    "        if f in exclude_cols:\n",
    "            print(f)\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['next_direction'].copy() if task == 'classification' else df['next_log_return'].copy()\n",
    "    dates = df['date'].copy()\n",
    "    prices = df['ETH_Close'].copy()\n",
    "    \n",
    "    return X, y, dates, prices, feature_cols\n",
    "\n",
    "\n",
    "def feature_selection_before_scaling(X_train, y_train, X_val, X_test, n_features=100):\n",
    "    print(f\"\\n[FEATURE SELECTION] {X_train.shape[1]} -> {n_features}\")\n",
    "    \n",
    "    selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "    print(selected_features)\n",
    "    print(f\"  Selected: {len(selected_features)} features\")\n",
    "    \n",
    "    return X_train_selected, X_val_selected, X_test_selected, selected_features\n",
    "\n",
    "\n",
    "def check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DATA LEAKAGE VERIFICATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    train_idx = set(X_train.index)\n",
    "    val_idx = set(X_val.index)\n",
    "    test_idx = set(X_test.index)\n",
    "    \n",
    "    if len(train_idx & val_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and validation index overlap\")\n",
    "    if len(train_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and test index overlap\")\n",
    "    if len(val_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Validation and test index overlap\")\n",
    "    \n",
    "    if train_dates.max() >= val_dates.min():\n",
    "        issues.append(\"CRITICAL: Train dates overlap with validation dates\")\n",
    "    if val_dates.max() >= test_dates.min():\n",
    "        issues.append(\"CRITICAL: Validation dates overlap with test dates\")\n",
    "    \n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {X_train.isnull().sum().sum()} NaN values\")\n",
    "    if X_val.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {X_val.isnull().sum().sum()} NaN values\")\n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {X_test.isnull().sum().sum()} NaN values\")\n",
    "    \n",
    "    if np.isinf(X_train).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {np.isinf(X_train).sum().sum()} inf values\")\n",
    "    if np.isinf(X_val).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {np.isinf(X_val).sum().sum()} inf values\")\n",
    "    if np.isinf(X_test).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {np.isinf(X_test).sum().sum()} inf values\")\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)} samples, {train_dates.min()} to {train_dates.max()}\")\n",
    "    print(f\"Val:   {len(X_val)} samples, {val_dates.min()} to {val_dates.max()}\")\n",
    "    print(f\"Test:  {len(X_test)} samples, {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"\\nNo data leakage detected\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nData leakage issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_val, X_test):\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def get_all_models():\n",
    "    base_models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingClassifier(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.03,\n",
    "            subsample=0.75, random_state=42\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            n_estimators=100, learning_rate=0.5, random_state=42\n",
    "        ),\n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            max_depth=8, min_samples_split=20, min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=0.1, penalty='l2', max_iter=2000, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'RidgeClassifier': RidgeClassifier(\n",
    "            alpha=1.0, random_state=42\n",
    "        ),\n",
    "        'SVM_RBF': SVC(\n",
    "            kernel='rbf', C=1.0, gamma='scale', \n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'SVM_Linear': SVC(\n",
    "            kernel='linear', C=0.1,\n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Small': MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.01, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Medium': MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.001, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=15, weights='distance', n_jobs=-1\n",
    "        ),\n",
    "        'NaiveBayes': GaussianNB(),\n",
    "        'Bagging_RF': BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=8, random_state=42),\n",
    "            n_estimators=50, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost_GPU': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=3,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            tree_method='gpu_hist',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM_GPU': LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=20,\n",
    "            max_depth=4,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            device='gpu',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    voting_soft = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    voting_hard = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    \n",
    "    stacking = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('et', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(C=0.1, random_state=42),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    base_models['Voting_Soft'] = voting_soft\n",
    "    base_models['Voting_Hard'] = voting_hard\n",
    "    base_models['Stacking'] = stacking\n",
    "    \n",
    "    xgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['XGBoost_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    lgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['LightGBM_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    \n",
    "    base_models['XGBoost_Calibrated'] = xgb_calibrated\n",
    "    base_models['LightGBM_Calibrated'] = lgb_calibrated\n",
    "    \n",
    "    return base_models\n",
    "\n",
    "\n",
    "def optimize_threshold_on_validation(y_val, y_proba_val, val_dates, val_prices):\n",
    "    best_sharpe = -np.inf\n",
    "    best_thresholds = (0.55, 0.45)\n",
    "    \n",
    "    for buy_th in np.arange(0.50, 0.70, 0.05):\n",
    "        for sell_th in np.arange(0.30, 0.50, 0.05):\n",
    "            predictions_temp = (y_proba_val > 0.5).astype(int)\n",
    "            \n",
    "            temp_result = calculate_trading_performance_corrected(\n",
    "                predictions_temp, y_proba_val, val_dates, val_prices, y_val,\n",
    "                initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                buy_threshold=buy_th, sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            if temp_result['sharpe_ratio'] > best_sharpe:\n",
    "                best_sharpe = temp_result['sharpe_ratio']\n",
    "                best_thresholds = (buy_th, sell_th)\n",
    "    \n",
    "    return best_thresholds\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                     val_dates=None, val_prices=None, optimize_thresholds=False):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    models_config = get_all_models()\n",
    "    \n",
    "    results = []\n",
    "    models_trained = {}\n",
    "    predictions = {}\n",
    "    probabilities = {}\n",
    "    thresholds = {}\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12} Status\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name, model in models_config.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_pred = model.predict(X_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                train_proba = model.predict_proba(X_train)[:, 1]\n",
    "                val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                test_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                train_proba = train_pred\n",
    "                val_proba = val_pred\n",
    "                test_proba = test_pred\n",
    "            \n",
    "            if optimize_thresholds and val_dates is not None and val_prices is not None:\n",
    "                buy_th, sell_th = optimize_threshold_on_validation(\n",
    "                    y_val, val_proba, val_dates, val_prices\n",
    "                )\n",
    "                thresholds[name] = (buy_th, sell_th)\n",
    "            else:\n",
    "                thresholds[name] = (0.55, 0.45)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            test_precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "            test_recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "            test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                train_auc = roc_auc_score(y_train, train_proba)\n",
    "                val_auc = roc_auc_score(y_val, val_proba)\n",
    "                test_auc = roc_auc_score(y_test, test_proba)\n",
    "            except:\n",
    "                train_auc = val_auc = test_auc = 0.5\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Train_Acc': train_acc,\n",
    "                'Val_Acc': val_acc,\n",
    "                'Test_Acc': test_acc,\n",
    "                'Train_AUC': train_auc,\n",
    "                'Val_AUC': val_auc,\n",
    "                'Test_AUC': test_auc,\n",
    "                'Test_Precision': test_precision,\n",
    "                'Test_Recall': test_recall,\n",
    "                'Test_F1': test_f1,\n",
    "                'Overfit_Gap': train_acc - test_acc,\n",
    "                'Buy_Threshold': thresholds[name][0],\n",
    "                'Sell_Threshold': thresholds[name][1]\n",
    "            })\n",
    "            \n",
    "            models_trained[name] = model\n",
    "            predictions[name] = test_pred\n",
    "            probabilities[name] = test_proba\n",
    "            \n",
    "            status = \"OK\"\n",
    "            print(f\"{name:<30} {train_acc:<12.4f} {val_acc:<12.4f} {test_acc:<12.4f} {test_auc:<12.4f} {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {str(e)[:20]}\")\n",
    "    \n",
    "    return pd.DataFrame(results), models_trained, predictions, probabilities, thresholds\n",
    "\n",
    "\n",
    "def walk_forward_validation(df, models_config, n_splits=5, n_features=100):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols = prepare_feature_target(df)\n",
    "    all_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train_fold.shape[1]))\n",
    "        X_train_selected = selector.fit_transform(X_train_fold, y_train_fold)\n",
    "        X_test_selected = selector.transform(X_test_fold)\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                test_acc = accuracy_score(y_test_fold, test_pred)\n",
    "                test_precision = precision_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_recall = recall_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_f1 = f1_score(y_test_fold, test_pred, zero_division=0)\n",
    "                \n",
    "                try:\n",
    "                    test_auc = roc_auc_score(y_test_fold, test_proba)\n",
    "                except:\n",
    "                    test_auc = 0.5\n",
    "                \n",
    "                all_results.append({\n",
    "                    'Fold': fold_num,\n",
    "                    'Model': name,\n",
    "                    'Test_Acc': test_acc,\n",
    "                    'Test_Precision': test_precision,\n",
    "                    'Test_Recall': test_recall,\n",
    "                    'Test_F1': test_f1,\n",
    "                    'Test_AUC': test_auc\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = results_df.groupby('Model').agg({\n",
    "        'Test_Acc': ['mean', 'std'],\n",
    "        'Test_Precision': ['mean', 'std'],\n",
    "        'Test_Recall': ['mean', 'std'],\n",
    "        'Test_F1': ['mean', 'std'],\n",
    "        'Test_AUC': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "def calculate_trading_performance_corrected(predictions, probabilities, dates, prices, y_true,\n",
    "                                           initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                                           buy_threshold=0.55, sell_threshold=0.45):\n",
    "    df_backtest = pd.DataFrame({\n",
    "        'date': dates.values,\n",
    "        'price': prices.values,\n",
    "        'prediction': predictions,\n",
    "        'probability': probabilities,\n",
    "        'actual_direction': y_true.values\n",
    "    })\n",
    "    \n",
    "    capital = initial_capital\n",
    "    position = 0\n",
    "    eth_holdings = 0\n",
    "    portfolio_values = [initial_capital]\n",
    "    trades = []\n",
    "    \n",
    "    total_cost = transaction_cost + slippage\n",
    "    \n",
    "    for idx in range(len(df_backtest) - 1):\n",
    "        current_row = df_backtest.iloc[idx]\n",
    "        signal = current_row['prediction']\n",
    "        confidence = current_row['probability']\n",
    "        \n",
    "        trade_price = df_backtest.iloc[idx + 1]['price']\n",
    "        \n",
    "        if signal == 1 and position == 0 and confidence > buy_threshold:\n",
    "            eth_to_buy = (capital * 0.95) / trade_price\n",
    "            cost = eth_to_buy * trade_price * (1 + total_cost)\n",
    "            if cost <= capital:\n",
    "                eth_holdings = eth_to_buy\n",
    "                capital -= cost\n",
    "                position = 1\n",
    "                trades.append({'action': 'BUY', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "\n",
    "        elif (signal == 0 or confidence < sell_threshold) and position == 1:\n",
    "            revenue = eth_holdings * trade_price * (1 - total_cost)\n",
    "            capital += revenue\n",
    "            eth_holdings = 0\n",
    "            position = 0\n",
    "            trades.append({'action': 'SELL', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "            \n",
    "        eod_portfolio_value = capital + (eth_holdings * trade_price)\n",
    "        portfolio_values.append(eod_portfolio_value)\n",
    "\n",
    "    final_value = portfolio_values[-1]\n",
    "    total_return = (final_value - initial_capital) / initial_capital * 100\n",
    "    buy_hold_return = (df_backtest.iloc[-1]['price'] - df_backtest.iloc[0]['price']) / df_backtest.iloc[0]['price'] * 100\n",
    "    \n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    if len(portfolio_values) > 1:\n",
    "        returns = (portfolio_values[1:] / portfolio_values[:-1]) - 1\n",
    "        returns = returns[~np.isnan(returns) & ~np.isinf(returns)]\n",
    "        \n",
    "        sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if len(returns) > 0 and np.std(returns) > 0 else 0\n",
    "        \n",
    "        cummax = np.maximum.accumulate(portfolio_values)\n",
    "        drawdown = (portfolio_values - cummax) / cummax\n",
    "        max_drawdown = np.min(drawdown) * 100 if len(drawdown) > 0 else 0\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        \n",
    "    n_trades = len(trades)\n",
    "    n_buys = len([t for t in trades if t['action'] == 'BUY'])\n",
    "    \n",
    "    return {\n",
    "        'final_value': final_value,\n",
    "        'total_return': total_return,\n",
    "        'buy_hold_return': buy_hold_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'n_trades': n_trades,\n",
    "        'n_buys': n_buys\n",
    "    }\n",
    "\n",
    "\n",
    "def backtest_all_models(models, predictions, probabilities, test_dates, test_prices, y_test, thresholds=None):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BACKTESTING RESULTS (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    backtest_results = []\n",
    "    \n",
    "    buy_hold_return = (test_prices.iloc[-1] - test_prices.iloc[0]) / test_prices.iloc[0] * 100\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Final Value':<15} {'Return %':<12} {'vs B&H':<12} {'Sharpe':<10} {'Max DD %':<12} {'Trades':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name in models.keys():\n",
    "        try:\n",
    "            if thresholds and name in thresholds:\n",
    "                buy_th, sell_th = thresholds[name]\n",
    "            else:\n",
    "                buy_th, sell_th = 0.55, 0.45\n",
    "            \n",
    "            results = calculate_trading_performance_corrected(\n",
    "                predictions[name], \n",
    "                probabilities[name],\n",
    "                test_dates, \n",
    "                test_prices, \n",
    "                y_test,\n",
    "                initial_capital=10000,\n",
    "                transaction_cost=0.002,\n",
    "                slippage=0.001,\n",
    "                buy_threshold=buy_th,\n",
    "                sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            outperformance = results['total_return'] - buy_hold_return\n",
    "            \n",
    "            backtest_results.append({\n",
    "                'Model': name,\n",
    "                'Final_Value': results['final_value'],\n",
    "                'Total_Return': results['total_return'],\n",
    "                'Buy_Hold_Return': buy_hold_return,\n",
    "                'Outperformance': outperformance,\n",
    "                'Sharpe_Ratio': results['sharpe_ratio'],\n",
    "                'Max_Drawdown': results['max_drawdown'],\n",
    "                'N_Trades': results['n_trades'],\n",
    "                'N_Buys': results['n_buys'],\n",
    "                'Buy_Threshold': buy_th,\n",
    "                'Sell_Threshold': sell_th\n",
    "            })\n",
    "            \n",
    "            print(f\"{name:<30} ${results['final_value']:<14,.2f} {results['total_return']:<11.2f}% \"\n",
    "                  f\"{outperformance:<11.2f}% {results['sharpe_ratio']:<9.3f} \"\n",
    "                  f\"{results['max_drawdown']:<11.2f}% {results['n_trades']:<10}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} Error: {str(e)[:50]}\")\n",
    "    \n",
    "    print(\"-\" * 110)\n",
    "    print(f\"{'Buy & Hold Baseline':<30} ${10000 * (1 + buy_hold_return/100):<14,.2f} {buy_hold_return:<11.2f}% \"\n",
    "          f\"{'0.00':<11}% {'N/A':<9} {'N/A':<11} {'0':<10}\")\n",
    "    \n",
    "    return pd.DataFrame(backtest_results)\n",
    "\n",
    "\n",
    "def create_comprehensive_report(results_df):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED PERFORMANCE REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    results_sorted = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Model':<30} {'Acc':<10} {'Prec':<10} {'Recall':<10} {'F1':<10} {'AUC':<10} {'Overfit':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for idx, row in results_sorted.iterrows():\n",
    "        print(f\"{idx+1:<6} {row['Model']:<30} {row['Test_Acc']:<10.4f} {row['Test_Precision']:<10.4f} \"\n",
    "              f\"{row['Test_Recall']:<10.4f} {row['Test_F1']:<10.4f} {row['Test_AUC']:<10.4f} {row['Overfit_Gap']:<10.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Best Test Accuracy:  {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_Acc']:.4f})\")\n",
    "    print(f\"Best Test AUC:       {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_AUC']:.4f})\")\n",
    "    print(f\"Best Test F1:        {results_sorted.nlargest(1, 'Test_F1').iloc[0]['Model']} ({results_sorted['Test_F1'].max():.4f})\")\n",
    "    print(f\"\\nMean Test Accuracy:  {results_df['Test_Acc'].mean():.4f} +/- {results_df['Test_Acc'].std():.4f}\")\n",
    "    print(f\"Mean Test AUC:       {results_df['Test_AUC'].mean():.4f} +/- {results_df['Test_AUC'].std():.4f}\")\n",
    "    print(f\"Mean Overfit Gap:    {results_df['Overfit_Gap'].mean():.4f} +/- {results_df['Overfit_Gap'].std():.4f}\")\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "def walk_forward_backtest(df, models_config, n_splits=5, n_features=100):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTESTING (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols = prepare_feature_target(df)\n",
    "    all_backtest_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} Backtest ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        test_dates_fold = dates.iloc[test_idx]\n",
    "        test_prices_fold = prices.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train_fold.shape[1]))\n",
    "        X_train_selected = selector.fit_transform(X_train_fold, y_train_fold)\n",
    "        X_test_selected = selector.transform(X_test_fold)\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                bt_result = calculate_trading_performance_corrected(\n",
    "                    test_pred, \n",
    "                    test_proba,\n",
    "                    test_dates_fold, \n",
    "                    test_prices_fold, \n",
    "                    y_test_fold,\n",
    "                    initial_capital=10000,\n",
    "                    transaction_cost=0.002,\n",
    "                    slippage=0.001\n",
    "                )\n",
    "                \n",
    "                bt_result['Model'] = name\n",
    "                bt_result['Fold'] = fold_num\n",
    "                all_backtest_results.append(bt_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Backtest Error - {str(e)[:50]}\")\n",
    "    \n",
    "    backtest_df = pd.DataFrame(all_backtest_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTEST SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = backtest_df.groupby('Model').agg({\n",
    "        'total_return': ['mean', 'std'],\n",
    "        'sharpe_ratio': ['mean', 'std'],\n",
    "        'max_drawdown': ['mean', 'std'],\n",
    "        'n_trades': ['mean', 'sum']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.sort_values('total_return_mean', ascending=False)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Avg Return %':<15} {'Avg Sharpe':<12} {'Avg Max DD %':<15} {'Total Trades':<12}\")\n",
    "    print(\"-\" * 110)\n",
    "    for model_name in summary.index:\n",
    "        model_data = backtest_df[backtest_df['Model'] == model_name]\n",
    "        avg_return = model_data['total_return'].mean()\n",
    "        avg_sharpe = model_data['sharpe_ratio'].mean()\n",
    "        avg_dd = model_data['max_drawdown'].mean()\n",
    "        total_trades = model_data['n_trades'].sum()\n",
    "        print(f\"{model_name:<30} {avg_return:<15.2f} {avg_sharpe:<12.3f} {avg_dd:<15.2f} {total_trades:<12.0f}\")\n",
    "    \n",
    "    return backtest_df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfc4b6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Train=1225, Val=262, Test=263\n",
      "\n",
      "====================================================================================================\n",
      "DATA LEAKAGE VERIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Train: 1225 samples, 2020-12-20 00:00:00 to 2024-04-27 00:00:00\n",
      "Val:   262 samples, 2024-04-28 00:00:00 to 2025-01-14 00:00:00\n",
      "Test:  263 samples, 2025-01-15 00:00:00 to 2025-10-04 00:00:00\n",
      "\n",
      "No data leakage detected\n",
      "\n",
      "[FEATURE SELECTION] 250 -> 100\n",
      "['BTC_Volume', 'BNB_Open', 'BNB_High', 'BNB_Low', 'BNB_Close', 'XRP_Volume', 'SOL_Low', 'SOL_Close', 'AVAX_Open', 'AVAX_High', 'AVAX_Low', 'AVAX_Close', 'DOT_Volume', 'ETH_Volume', 'eth_tx_count', 'eth_active_addresses', 'eth_new_addresses', 'eth_large_eth_transfers', 'eth_token_transfers', 'eth_contract_events', 'eth_total_gas_used', 'fg_fear_greed', 'usdt_totalCirculating', 'usdt_totalCirculatingUSD', 'usdt_totalMintedUSD', 'makerdao_makerdao_eth_tvl', 'dxy_DXY', 'close_lag1', 'close_lag2', 'close_lag3', 'close_lag7', 'close_lag14', 'close_lag30', 'high_lag1', 'low_lag1', 'high_lag2', 'low_lag2', 'high_lag3', 'low_lag3', 'high_lag5', 'low_lag5', 'high_lag7', 'low_lag7', 'close_ratio_lag1', 'RSI_200', 'ITS_9', 'ICS_26', 'SMA_10', 'SMA_20', 'SMA_200', 'EMA_12', 'EMA_26', 'EMA_50', 'EMA_200', 'TEMA_10', 'TEMA_30', 'WMA_10', 'WMA_20', 'HMA_9', 'DEMA_10', 'TRIMA_10', 'VWMA_20', 'ZLEMA_20', 'HL2', 'HLC3', 'OHLC4', 'BBL_20', 'BBM_20', 'BBM_50', 'KCL_20', 'KCB_20', 'OBV', 'AD', 'NVI_1', 'VWAP', 'DPO_20', 'PRICE_CHANGE', 'PRICE_CHANGE_2', 'VOLUME_RATIO', 'HIGH_LOW_RANGE', 'INTRADAY_POSITION', 'LINREG_14', 'INC_1', 'DEC_1', 'INC_5', 'BOP', 'VOLUME_STRENGTH', 'PRICE_ACCELERATION', 'ROLLING_MIN_20', 'DISTANCE_FROM_HIGH', 'volcorr_abs_30', 'fg_fear_greed_lag1', 'eth_token_transfers_lag1', 'eth_contract_events_lag1', 'eth_total_gas_used_lag1', 'makerdao_makerdao_eth_tvl_lag1', 'usdt_totalCirculating_lag1', 'usdt_totalCirculatingUSD_lag1', 'usdt_totalMintedUSD_lag1', 'dxy_DXY_lag1']\n",
      "  Selected: 100 features\n",
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING AND EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Model                          Train Acc    Val Acc      Test Acc     Test AUC     Status\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "RandomForest                   0.8571       0.5954       0.5589       0.6256       OK\n",
      "GradientBoosting               0.9020       0.5420       0.5513       0.6215       OK\n",
      "ExtraTrees                     0.7690       0.5496       0.5665       0.5942       OK\n",
      "AdaBoost                       0.6678       0.5763       0.5665       0.6580       OK\n",
      "DecisionTree                   0.7861       0.4962       0.4791       0.5042       OK\n",
      "LogisticRegression             0.6669       0.6374       0.6122       0.6338       OK\n",
      "RidgeClassifier                0.6939       0.5611       0.5551       0.5632       OK\n",
      "SVM_RBF                        0.6751       0.6031       0.5323       0.5837       OK\n",
      "SVM_Linear                     0.6612       0.6069       0.5856       0.6118       OK\n",
      "MLP_Small                      0.7208       0.5840       0.5589       0.6094       OK\n",
      "MLP_Medium                     0.6637       0.6031       0.6274       0.6390       OK\n",
      "KNN                            1.0000       0.5076       0.5057       0.5252       OK\n",
      "NaiveBayes                     0.5527       0.4771       0.5171       0.5258       OK\n",
      "Bagging_RF                     0.9176       0.5267       0.5437       0.5448       OK\n",
      "XGBoost_GPU                    0.7543       0.6260       0.6084       0.6736       OK\n",
      "LightGBM_GPU                   0.8294       0.5954       0.6198       0.6747       OK\n",
      "Voting_Soft                    0.8294       0.5916       0.6008       0.6593       OK\n",
      "Voting_Hard                    0.8433       0.5802       0.6046       0.6071       OK\n",
      "Stacking                       0.4376       0.5191       0.5209       0.5950       OK\n",
      "XGBoost_Calibrated             0.7200       0.6183       0.6312       0.6719       OK\n",
      "LightGBM_Calibrated            0.7731       0.6527       0.6046       0.6704       OK\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED PERFORMANCE REPORT\n",
      "====================================================================================================\n",
      "\n",
      "Rank   Model                          Acc        Prec       Recall     F1         AUC        Overfit   \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1      LightGBM_GPU                   0.6198     0.6378     0.6000     0.6183     0.6747     0.2096    \n",
      "2      XGBoost_GPU                    0.6084     0.6333     0.5630     0.5961     0.6736     0.1459    \n",
      "3      XGBoost_Calibrated             0.6312     0.6484     0.6148     0.6312     0.6719     0.0888    \n",
      "4      LightGBM_Calibrated            0.6046     0.6240     0.5778     0.6000     0.6704     0.1685    \n",
      "5      Voting_Soft                    0.6008     0.6364     0.5185     0.5714     0.6593     0.2286    \n",
      "6      AdaBoost                       0.5665     0.6265     0.3852     0.4771     0.6580     0.1012    \n",
      "7      MLP_Medium                     0.6274     0.6095     0.7630     0.6776     0.6390     0.0363    \n",
      "8      LogisticRegression             0.6122     0.6260     0.6074     0.6165     0.6338     0.0548    \n",
      "9      RandomForest                   0.5589     0.6203     0.3630     0.4579     0.6256     0.2982    \n",
      "10     GradientBoosting               0.5513     0.6232     0.3185     0.4216     0.6215     0.3507    \n",
      "11     SVM_Linear                     0.5856     0.6413     0.4370     0.5198     0.6118     0.0757    \n",
      "12     MLP_Small                      0.5589     0.5941     0.4444     0.5085     0.6094     0.1619    \n",
      "13     Voting_Hard                    0.6046     0.6449     0.5111     0.5702     0.6071     0.2387    \n",
      "14     Stacking                       0.5209     0.5197     0.8815     0.6538     0.5950     -0.0834   \n",
      "15     ExtraTrees                     0.5665     0.5766     0.5852     0.5809     0.5942     0.2024    \n",
      "16     SVM_RBF                        0.5323     0.5234     0.9926     0.6854     0.5837     0.1428    \n",
      "17     RidgeClassifier                0.5551     0.6731     0.2593     0.3743     0.5632     0.1387    \n",
      "18     Bagging_RF                     0.5437     0.6154     0.2963     0.4000     0.5448     0.3738    \n",
      "19     NaiveBayes                     0.5171     0.5183     0.8370     0.6402     0.5258     0.0355    \n",
      "20     KNN                            0.5057     0.5200     0.4815     0.5000     0.5252     0.4943    \n",
      "21     DecisionTree                   0.4791     0.4928     0.5037     0.4982     0.5042     0.3070    \n",
      "\n",
      "====================================================================================================\n",
      "STATISTICAL SUMMARY\n",
      "====================================================================================================\n",
      "Best Test Accuracy:  LightGBM_GPU (0.6198)\n",
      "Best Test AUC:       LightGBM_GPU (0.6747)\n",
      "Best Test F1:        SVM_RBF (0.6854)\n",
      "\n",
      "Mean Test Accuracy:  0.5691 +/- 0.0429\n",
      "Mean Test AUC:       0.6091 +/- 0.0526\n",
      "Mean Overfit Gap:    0.1795 +/- 0.1341\n",
      "\n",
      "====================================================================================================\n",
      "WALK-FORWARD VALIDATION\n",
      "====================================================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Train: 295 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Train: 586 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Train: 877 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Train: 1168 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Train: 1459 samples | Test: 291 samples\n",
      "\n",
      "====================================================================================================\n",
      "WALK-FORWARD VALIDATION SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "                     Test_Acc         Test_Precision         Test_Recall  \\\n",
      "                        mean     std           mean     std        mean   \n",
      "Model                                                                     \n",
      "AdaBoost              0.6179  0.0524         0.6120  0.0735      0.7121   \n",
      "Bagging_RF            0.5684  0.0516         0.5717  0.0685      0.5928   \n",
      "DecisionTree          0.5361  0.0292         0.5362  0.0524      0.5634   \n",
      "ExtraTrees            0.5354  0.0353         0.5347  0.0402      0.6994   \n",
      "GradientBoosting      0.5979  0.0303         0.6048  0.0395      0.5790   \n",
      "KNN                   0.5347  0.0139         0.5271  0.0227      0.7049   \n",
      "LightGBM_Calibrated   0.6069  0.0414         0.6005  0.0594      0.6907   \n",
      "LightGBM_GPU          0.6110  0.0320         0.6161  0.0607      0.6291   \n",
      "LogisticRegression    0.5986  0.0422         0.6135  0.0756      0.6691   \n",
      "MLP_Medium            0.5340  0.0620         0.5602  0.0986      0.5627   \n",
      "MLP_Small             0.5636  0.0513         0.5562  0.0599      0.5284   \n",
      "NaiveBayes            0.5223  0.0323         0.5373  0.0560      0.6322   \n",
      "RandomForest          0.5924  0.0559         0.5904  0.0740      0.6941   \n",
      "RidgeClassifier       0.5430  0.0317         0.6065  0.1413      0.6029   \n",
      "SVM_Linear            0.5814  0.0468         0.6089  0.0763      0.6150   \n",
      "SVM_RBF               0.5801  0.0354         0.5640  0.0455      0.7904   \n",
      "Stacking              0.5079  0.0335         0.5356  0.0683      0.6499   \n",
      "Voting_Hard           0.5890  0.0352         0.6022  0.0623      0.5901   \n",
      "Voting_Soft           0.6048  0.0133         0.6306  0.0580      0.5466   \n",
      "XGBoost_Calibrated    0.6151  0.0325         0.6020  0.0437      0.6993   \n",
      "XGBoost_GPU           0.6110  0.0293         0.6209  0.0586      0.6090   \n",
      "\n",
      "                            Test_F1         Test_AUC          \n",
      "                        std    mean     std     mean     std  \n",
      "Model                                                         \n",
      "AdaBoost             0.1042  0.6504  0.0417   0.6642  0.0402  \n",
      "Bagging_RF           0.1189  0.5755  0.0726   0.5885  0.0522  \n",
      "DecisionTree         0.2137  0.5346  0.1044   0.5789  0.0595  \n",
      "ExtraTrees           0.1759  0.5958  0.0513   0.5923  0.0346  \n",
      "GradientBoosting     0.1426  0.5838  0.0791   0.6313  0.0385  \n",
      "KNN                  0.2053  0.5931  0.0794   0.5582  0.0256  \n",
      "LightGBM_Calibrated  0.1244  0.6349  0.0521   0.6482  0.0474  \n",
      "LightGBM_GPU         0.0930  0.6166  0.0433   0.6422  0.0444  \n",
      "LogisticRegression   0.2036  0.6174  0.0516   0.6710  0.0065  \n",
      "MLP_Medium           0.2890  0.5209  0.1321   0.6092  0.0476  \n",
      "MLP_Small            0.3387  0.4929  0.2380   0.5919  0.0599  \n",
      "NaiveBayes           0.2355  0.5565  0.0735   0.5429  0.0156  \n",
      "RandomForest         0.1063  0.6294  0.0437   0.6323  0.0482  \n",
      "RidgeClassifier      0.3342  0.5186  0.2165   0.5508  0.0258  \n",
      "SVM_Linear           0.2310  0.5837  0.0604   0.6566  0.0094  \n",
      "SVM_RBF              0.1184  0.6519  0.0306   0.6273  0.0323  \n",
      "Stacking             0.3333  0.5408  0.1085   0.5512  0.0641  \n",
      "Voting_Hard          0.1083  0.5874  0.0384   0.5926  0.0349  \n",
      "Voting_Soft          0.0789  0.5793  0.0254   0.6547  0.0292  \n",
      "XGBoost_Calibrated   0.1174  0.6425  0.0551   0.6581  0.0468  \n",
      "XGBoost_GPU          0.1025  0.6083  0.0451   0.6561  0.0298  \n",
      "\n",
      "====================================================================================================\n",
      "BACKTESTING RESULTS (Improved: Slippage 0.1% + Transaction Cost 0.2%)\n",
      "====================================================================================================\n",
      "\n",
      "Model                          Final Value     Return %     vs B&H       Sharpe     Max DD %     Trades    \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "RandomForest                   $19,182.29      91.82      % 61.72      % 1.750     -19.67     % 62        \n",
      "GradientBoosting               $20,134.39      101.34     % 71.24      % 2.276     -13.85     % 24        \n",
      "ExtraTrees                     $10,000.00      0.00       % -30.10     % 0.000     0.00       % 0         \n",
      "AdaBoost                       $27,238.49      172.38     % 142.28     % 3.057     -6.25      % 29        \n",
      "DecisionTree                   $8,159.13       -18.41     % -48.51     % -0.203    -41.69     % 97        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             $37,559.30      275.59     % 245.49     % 3.076     -11.26     % 45        \n",
      "RidgeClassifier                $29,017.06      190.17     % 160.07     % 3.220     -13.49     % 33        \n",
      "SVM_RBF                        $18,597.94      85.98      % 55.88      % 1.339     -45.61     % 3         \n",
      "SVM_Linear                     $35,448.30      254.48     % 224.38     % 3.312     -8.96      % 35        \n",
      "MLP_Small                      $25,106.99      151.07     % 120.97     % 2.516     -14.60     % 42        \n",
      "MLP_Medium                     $30,208.62      202.09     % 171.98     % 2.457     -19.79     % 79        \n",
      "KNN                            $14,130.76      41.31      % 11.21      % 1.290     -11.97     % 16        \n",
      "NaiveBayes                     $17,399.61      74.00      % 43.89      % 1.223     -44.19     % 23        \n",
      "Bagging_RF                     $12,217.91      22.18      % -7.92      % 0.998     -3.64      % 4         \n",
      "XGBoost_GPU                    $36,617.15      266.17     % 236.07     % 3.437     -11.02     % 29        \n",
      "LightGBM_GPU                   $28,603.63      186.04     % 155.93     % 2.432     -27.51     % 69        \n",
      "Voting_Soft                    $34,302.49      243.02     % 212.92     % 3.395     -11.09     % 33        \n",
      "Voting_Hard                    $27,816.57      178.17     % 148.06     % 2.558     -17.50     % 75        \n",
      "Stacking                       $27,672.61      176.73     % 146.62     % 2.376     -19.37     % 17        \n",
      "XGBoost_Calibrated             $39,907.10      299.07     % 268.97     % 3.092     -17.10     % 37        \n",
      "LightGBM_Calibrated            $37,069.17      270.69     % 240.59     % 3.396     -12.88     % 29        \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Buy & Hold Baseline            $13,010.14      30.10      % 0.00       % N/A       N/A         0         \n",
      "\n",
      "====================================================================================================\n",
      "WALK-FORWARD BACKTESTING (Improved: Slippage 0.1% + Transaction Cost 0.2%)\n",
      "====================================================================================================\n",
      "\n",
      "--- Fold 1/5 Backtest ---\n",
      "Train: 295 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 2/5 Backtest ---\n",
      "Train: 586 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 3/5 Backtest ---\n",
      "Train: 877 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 4/5 Backtest ---\n",
      "Train: 1168 samples | Test: 291 samples\n",
      "\n",
      "--- Fold 5/5 Backtest ---\n",
      "Train: 1459 samples | Test: 291 samples\n",
      "\n",
      "====================================================================================================\n",
      "WALK-FORWARD BACKTEST SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "                      total_return_mean  total_return_std  sharpe_ratio_mean  \\\n",
      "Model                                                                         \n",
      "AdaBoost                      230.9578          108.6686             2.7672   \n",
      "LightGBM_GPU                  197.1686          156.1655             2.5066   \n",
      "XGBoost_GPU                   192.6114           95.3165             2.6190   \n",
      "XGBoost_Calibrated            168.4097           63.8598             2.2958   \n",
      "LightGBM_Calibrated           161.6097           55.0234             2.3155   \n",
      "GradientBoosting              158.6614           83.3680             2.2910   \n",
      "Voting_Soft                   157.8995          101.8376             2.3553   \n",
      "LogisticRegression            138.1630          116.6922             2.1318   \n",
      "RandomForest                  137.7583           96.0048             2.1267   \n",
      "Voting_Hard                   133.6343          126.6666             1.8852   \n",
      "SVM_RBF                       122.3875           92.4448             1.8872   \n",
      "MLP_Small                     121.5869          148.5607             1.4913   \n",
      "SVM_Linear                    119.3283          111.1190             1.9659   \n",
      "Bagging_RF                     98.1971          119.8051             1.3540   \n",
      "MLP_Medium                     87.2579           85.9628             1.3398   \n",
      "DecisionTree                   84.5633          104.6336             1.3257   \n",
      "RidgeClassifier                77.0334          127.0723             1.1785   \n",
      "ExtraTrees                     69.5485           84.7050             0.9907   \n",
      "NaiveBayes                     18.4061           40.7460             0.5317   \n",
      "Stacking                       10.1323           42.4226             0.5122   \n",
      "KNN                             5.0228           37.9428             0.2191   \n",
      "\n",
      "                     sharpe_ratio_std  max_drawdown_mean  max_drawdown_std  \\\n",
      "Model                                                                        \n",
      "AdaBoost                       0.6532           -16.8413            9.7167   \n",
      "LightGBM_GPU                   1.1094           -17.0176           12.5998   \n",
      "XGBoost_GPU                    0.7825           -20.0792           14.1930   \n",
      "XGBoost_Calibrated             0.6392           -22.8147           15.6034   \n",
      "LightGBM_Calibrated            0.4391           -22.9591           14.3867   \n",
      "GradientBoosting               0.8072           -16.4758           10.2421   \n",
      "Voting_Soft                    0.8699           -16.5474           11.9736   \n",
      "LogisticRegression             1.0734           -21.5657           22.3419   \n",
      "RandomForest                   1.3394           -25.0491           24.7921   \n",
      "Voting_Hard                    1.1031           -22.3346           19.9454   \n",
      "SVM_RBF                        1.1748           -25.3502           24.5093   \n",
      "MLP_Small                      1.2730           -21.0000           16.3200   \n",
      "SVM_Linear                     1.2443           -23.0427           25.9240   \n",
      "Bagging_RF                     1.4277           -24.2848           23.4821   \n",
      "MLP_Medium                     1.2804           -23.0267           21.9260   \n",
      "DecisionTree                   1.4374           -29.3118           12.5517   \n",
      "RidgeClassifier                1.2010           -23.0899           18.9511   \n",
      "ExtraTrees                     1.2480           -31.9567           19.8206   \n",
      "NaiveBayes                     0.5712           -37.0649           21.6524   \n",
      "Stacking                       0.8485           -32.0490           28.1727   \n",
      "KNN                            0.7345           -40.3185           21.7212   \n",
      "\n",
      "                     n_trades_mean  n_trades_sum  \n",
      "Model                                             \n",
      "AdaBoost                      49.2           246  \n",
      "LightGBM_GPU                  62.0           310  \n",
      "XGBoost_GPU                   61.2           306  \n",
      "XGBoost_Calibrated            61.2           306  \n",
      "LightGBM_Calibrated           67.2           336  \n",
      "GradientBoosting              59.8           299  \n",
      "Voting_Soft                   52.0           260  \n",
      "LogisticRegression            52.0           260  \n",
      "RandomForest                  45.0           225  \n",
      "Voting_Hard                   73.6           368  \n",
      "SVM_RBF                       40.2           201  \n",
      "MLP_Small                     44.6           223  \n",
      "SVM_Linear                    47.6           238  \n",
      "Bagging_RF                    61.6           308  \n",
      "MLP_Medium                    50.6           253  \n",
      "DecisionTree                  86.2           431  \n",
      "RidgeClassifier               35.4           177  \n",
      "ExtraTrees                    49.6           248  \n",
      "NaiveBayes                    31.6           158  \n",
      "Stacking                      13.8            69  \n",
      "KNN                           58.4           292  \n",
      "\n",
      "Model                          Avg Return %    Avg Sharpe   Avg Max DD %    Total Trades\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "AdaBoost                       230.96          2.767        -16.84          246         \n",
      "LightGBM_GPU                   197.17          2.507        -17.02          310         \n",
      "XGBoost_GPU                    192.61          2.619        -20.08          306         \n",
      "XGBoost_Calibrated             168.41          2.296        -22.81          306         \n",
      "LightGBM_Calibrated            161.61          2.316        -22.96          336         \n",
      "GradientBoosting               158.66          2.291        -16.48          299         \n",
      "Voting_Soft                    157.90          2.355        -16.55          260         \n",
      "LogisticRegression             138.16          2.132        -21.57          260         \n",
      "RandomForest                   137.76          2.127        -25.05          225         \n",
      "Voting_Hard                    133.63          1.885        -22.33          368         \n",
      "SVM_RBF                        122.39          1.887        -25.35          201         \n",
      "MLP_Small                      121.59          1.491        -21.00          223         \n",
      "SVM_Linear                     119.33          1.966        -23.04          238         \n",
      "Bagging_RF                     98.20           1.354        -24.28          308         \n",
      "MLP_Medium                     87.26           1.340        -23.03          253         \n",
      "DecisionTree                   84.56           1.326        -29.31          431         \n",
      "RidgeClassifier                77.03           1.178        -23.09          177         \n",
      "ExtraTrees                     69.55           0.991        -31.96          248         \n",
      "NaiveBayes                     18.41           0.532        -37.06          158         \n",
      "Stacking                       10.13           0.512        -32.05          69          \n",
      "KNN                            5.02            0.219        -40.32          292         \n"
     ]
    }
   ],
   "source": [
    "all_models = get_all_models()\n",
    "\n",
    "X_train, y_train, train_dates, train_prices, _ = prepare_feature_target(train_df)\n",
    "X_val, y_val, val_dates, val_prices, _ = prepare_feature_target(val_df)\n",
    "X_test, y_test, test_dates, test_prices, _ = prepare_feature_target(test_df)\n",
    "\n",
    "print(f\"\\nDataset: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates)\n",
    "\n",
    "X_train_sel, X_val_sel, X_test_sel, _ = feature_selection_before_scaling(\n",
    "    X_train, y_train, X_val, X_test, n_features=100\n",
    ")\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, _ = scale_features(\n",
    "    X_train_sel, X_val_sel, X_test_sel\n",
    ")\n",
    "\n",
    "results_df, models, predictions, probabilities, thresholds = train_all_models(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    val_dates=val_dates, val_prices=val_prices, optimize_thresholds=True\n",
    ")\n",
    "\n",
    "results_sorted = create_comprehensive_report(results_df)\n",
    "\n",
    "wf_results, wf_summary = walk_forward_validation(df_clean, all_models, n_splits=5, n_features=100)\n",
    "\n",
    "backtest_fixed = backtest_all_models(models, predictions, probabilities, test_dates, test_prices, y_test, thresholds)\n",
    "\n",
    "wf_backtest, wf_backtest_summary = walk_forward_backtest(df_clean, all_models, n_splits=5, n_features=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5d97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Train=1226, Val=263, Test=263\n",
      "\n",
      "====================================================================================================\n",
      "DATA LEAKAGE VERIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Train: 1226 samples, 2020-12-19 00:00:00 to 2024-04-27 00:00:00\n",
      "Val:   263 samples, 2024-04-28 00:00:00 to 2025-01-15 00:00:00\n",
      "Test:  263 samples, 2025-01-16 00:00:00 to 2025-10-05 00:00:00\n",
      "\n",
      "No data leakage detected\n",
      "\n",
      "[FEATURE SELECTION] 304 -> 100\n",
      "['BTC_Volume', 'ETH_Volume', 'BNB_Open', 'BNB_High', 'BNB_Low', 'BNB_Close', 'SOL_Close', 'AVAX_Open', 'AVAX_High', 'AVAX_Low', 'AVAX_Close', 'DOT_Volume', 'sentiment_mean', 'positive_ratio', 'negative_ratio', 'extreme_negative_count', 'sentiment_polarity', 'bull_bear_ratio', 'weighted_sentiment', 'sentiment_ma3', 'sentiment_volatility_3', 'sentiment_volatility_7', 'sentiment_ma14', 'sentiment_volatility_14', 'news_volume_ma7', 'news_volume_ma14', 'eth_tx_count', 'eth_active_addresses', 'eth_new_addresses', 'eth_large_eth_transfers', 'eth_token_transfers', 'eth_contract_events', 'usdt_totalCirculating', 'usdt_totalCirculatingUSD', 'usdt_totalMintedUSD', 'makerdao_makerdao_eth_tvl', 'dxy_DXY', 'close_lag3', 'low_lag1', 'high_lag2', 'high_lag3', 'low_lag3', 'low_lag5', 'close_ratio_lag1', 'RSI_200', 'ICS_26', 'SMA_200', 'EMA_12', 'EMA_200', 'TEMA_10', 'TEMA_30', 'WMA_10', 'HMA_9', 'DEMA_10', 'ZLEMA_20', 'HL2', 'HLC3', 'OHLC4', 'BBL_20', 'KCL_20', 'OBV', 'AD', 'NVI_1', 'VWAP', 'DPO_20', 'PRICE_CHANGE', 'PRICE_CHANGE_2', 'VOLUME_RATIO', 'HIGH_LOW_RANGE', 'INTRADAY_POSITION', 'LINREG_14', 'INC_1', 'DEC_1', 'INC_5', 'BOP', 'VOLUME_STRENGTH', 'PRICE_ACCELERATION', 'ROLLING_MIN_20', 'DISTANCE_FROM_HIGH', 'corr_ETH_BTC_7', 'corr_ETH_BTC_14', 'corr_ETH_BTC_30', 'volcorr_abs_30', 'sentiment_mean_lag1', 'news_count_lag1', 'news_count_lag2', 'positive_ratio_lag1', 'sentiment_polarity_lag1', 'sentiment_intensity_lag1', 'sentiment_intensity_lag2', 'bull_bear_ratio_lag1', 'extremity_index_lag1', 'extremity_index_lag2', 'extreme_negative_count_lag1', 'eth_token_transfers_lag1', 'fg_fear_greed_lag1', 'usdt_totalCirculating_lag1', 'usdt_totalMintedUSD_lag1', 'makerdao_makerdao_eth_tvl_lag1', 'dxy_DXY_lag1']\n",
      "  Selected: 100 features\n",
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING AND EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Model                          Train Acc    Val Acc      Test Acc     Test AUC     Status\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "RandomForest                   0.8915       0.5856       0.5627       0.6171       OK\n",
      "GradientBoosting               0.9290       0.5703       0.5627       0.5983       OK\n",
      "ExtraTrees                     0.8230       0.5894       0.5361       0.5851       OK\n",
      "AdaBoost                       0.6746       0.6198       0.6122       0.6623       OK\n",
      "DecisionTree                   0.7896       0.4525       0.4943       0.4486       OK\n",
      "LogisticRegression             0.5204       0.5209       0.5019       0.5411       OK\n",
      "RidgeClassifier                0.6786       0.5856       0.5513       0.5633       OK\n",
      "SVM_RBF                        0.5375       0.5057       0.4905       0.4858       OK\n"
     ]
    }
   ],
   "source": [
    "all_models = get_all_models()\n",
    "\n",
    "X_train, y_train, train_dates, train_prices, _ = prepare_feature_target(train_df)\n",
    "X_val, y_val, val_dates, val_prices, _ = prepare_feature_target(val_df)\n",
    "X_test, y_test, test_dates, test_prices, _ = prepare_feature_target(test_df)\n",
    "\n",
    "print(f\"\\nDataset: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates)\n",
    "\n",
    "X_train_sel, X_val_sel, X_test_sel, _ = feature_selection_before_scaling(\n",
    "    X_train, y_train, X_val, X_test, n_features=100\n",
    ")\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, _ = scale_features(\n",
    "    X_train_sel, X_val_sel, X_test_sel\n",
    ")\n",
    "\n",
    "results_df, models, predictions, probabilities, thresholds = train_all_models(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    val_dates=val_dates, val_prices=val_prices, optimize_thresholds=True\n",
    ")\n",
    "\n",
    "results_sorted = create_comprehensive_report(results_df)\n",
    "\n",
    "wf_results, wf_summary = walk_forward_validation(df_clean, all_models, n_splits=5, n_features=100)\n",
    "\n",
    "backtest_fixed = backtest_all_models(models, predictions, probabilities, test_dates, test_prices, y_test, thresholds)\n",
    "\n",
    "wf_backtest, wf_backtest_summary = walk_forward_backtest(df_clean, all_models, n_splits=5, n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9a6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################1007 버전 1 클로드 버전#######################\n",
    "#################1007 버전 1 클로드 버전#######################\n",
    "#################1007 버전 1 클로드 버전#######################\n",
    "#################1007 버전 1 클로드 버전#######################\n",
    "#################1007 버전 1 클로드 버전#######################\n",
    "#################1007 버전 1 클로드 버전#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier, \n",
    "    BaggingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def prepare_feature_target(df, task='direction'):\n",
    "    exclude_cols = ['date', 'next_log_return', 'next_direction', 'next_close', \n",
    "                    'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open']\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    if task == 'direction':\n",
    "        y = df['next_direction'].copy()\n",
    "    elif task == 'log_return':\n",
    "        y = df['next_log_return'].copy()\n",
    "    elif task == 'close':\n",
    "        y = df['next_close'].copy()\n",
    "    elif task == 'multi_direction_close':\n",
    "        y = df[['next_direction', 'next_close']].copy()\n",
    "    elif task == 'multi_direction_log':\n",
    "        y = df[['next_direction', 'next_log_return']].copy()\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 task 종류입니다.\")\n",
    "    \n",
    "    return X, y, df['date'], df['ETH_Close'], feature_cols\n",
    "\n",
    "\n",
    "\n",
    "def check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DATA LEAKAGE VERIFICATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    train_idx = set(X_train.index)\n",
    "    val_idx = set(X_val.index)\n",
    "    test_idx = set(X_test.index)\n",
    "    \n",
    "    if len(train_idx & val_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and validation index overlap\")\n",
    "    if len(train_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and test index overlap\")\n",
    "    if len(val_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Validation and test index overlap\")\n",
    "    \n",
    "    if train_dates.max() >= val_dates.min():\n",
    "        issues.append(\"CRITICAL: Train dates overlap with validation dates\")\n",
    "    if val_dates.max() >= test_dates.min():\n",
    "        issues.append(\"CRITICAL: Validation dates overlap with test dates\")\n",
    "    \n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {X_train.isnull().sum().sum()} NaN values\")\n",
    "    if X_val.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {X_val.isnull().sum().sum()} NaN values\")\n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {X_test.isnull().sum().sum()} NaN values\")\n",
    "    \n",
    "    if np.isinf(X_train).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {np.isinf(X_train).sum().sum()} inf values\")\n",
    "    if np.isinf(X_val).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {np.isinf(X_val).sum().sum()} inf values\")\n",
    "    if np.isinf(X_test).sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {np.isinf(X_test).sum().sum()} inf values\")\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)} samples, {train_dates.min()} to {train_dates.max()}\")\n",
    "    print(f\"Val:   {len(X_val)} samples, {val_dates.min()} to {val_dates.max()}\")\n",
    "    print(f\"Test:  {len(X_test)} samples, {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"\\nNo data leakage detected\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nData leakage issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def feature_selection_before_scaling(X_train, y_train, X_val, X_test, n_features=100):\n",
    "    print(f\"\\n[FEATURE SELECTION] {X_train.shape[1]} -> {n_features}\")\n",
    "    \n",
    "    selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "    print(f\"  Selected: {len(selected_features)} features\")\n",
    "    \n",
    "    return X_train_selected, X_val_selected, X_test_selected, selected_features\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_val, X_test):\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def get_all_models():\n",
    "    base_models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingClassifier(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.03,\n",
    "            subsample=0.75, random_state=42\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            n_estimators=100, learning_rate=0.5, random_state=42\n",
    "        ),\n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            max_depth=8, min_samples_split=20, min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=0.1, penalty='l2', max_iter=2000, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'RidgeClassifier': RidgeClassifier(\n",
    "            alpha=1.0, random_state=42\n",
    "        ),\n",
    "        'SVM_RBF': SVC(\n",
    "            kernel='rbf', C=1.0, gamma='scale', \n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'SVM_Linear': SVC(\n",
    "            kernel='linear', C=0.1,\n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Small': MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.01, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Medium': MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.001, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=15, weights='distance', n_jobs=-1\n",
    "        ),\n",
    "        'NaiveBayes': GaussianNB(),\n",
    "        'Bagging_RF': BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=8, random_state=42),\n",
    "            n_estimators=50, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost_GPU': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=3,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            tree_method='gpu_hist',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM_GPU': LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=20,\n",
    "            max_depth=4,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            device='gpu',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    voting_soft = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    voting_hard = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    \n",
    "    stacking = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('et', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(C=0.1, random_state=42),\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    \n",
    "    base_models['Voting_Soft'] = voting_soft\n",
    "    base_models['Voting_Hard'] = voting_hard\n",
    "    base_models['Stacking'] = stacking\n",
    "    \n",
    "    xgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['XGBoost_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    lgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['LightGBM_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    \n",
    "    base_models['XGBoost_Calibrated'] = xgb_calibrated\n",
    "    base_models['LightGBM_Calibrated'] = lgb_calibrated\n",
    "    \n",
    "    return base_models\n",
    "\n",
    "\n",
    "def optimize_threshold_on_validation(y_val, y_proba_val, val_dates, val_prices):\n",
    "    best_sharpe = -np.inf\n",
    "    best_thresholds = (0.55, 0.45)\n",
    "    \n",
    "    for buy_th in np.arange(0.50, 0.70, 0.05):\n",
    "        for sell_th in np.arange(0.30, 0.50, 0.05):\n",
    "            predictions_temp = (y_proba_val > 0.5).astype(int)\n",
    "            \n",
    "            temp_result = calculate_trading_performance_corrected(\n",
    "                predictions_temp, y_proba_val, val_dates, val_prices, y_val,\n",
    "                initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                buy_threshold=buy_th, sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            if temp_result['sharpe_ratio'] > best_sharpe:\n",
    "                best_sharpe = temp_result['sharpe_ratio']\n",
    "                best_thresholds = (buy_th, sell_th)\n",
    "    \n",
    "    return best_thresholds\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                     val_dates=None, val_prices=None, optimize_thresholds=False):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    models_config = get_all_models()\n",
    "    \n",
    "    results = []\n",
    "    models_trained = {}\n",
    "    predictions = {}\n",
    "    probabilities = {}\n",
    "    thresholds = {}\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12} Status\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name, model in models_config.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_pred = model.predict(X_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                train_proba = model.predict_proba(X_train)[:, 1]\n",
    "                val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                test_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                train_proba = train_pred\n",
    "                val_proba = val_pred\n",
    "                test_proba = test_pred\n",
    "            \n",
    "            if optimize_thresholds and val_dates is not None and val_prices is not None:\n",
    "                buy_th, sell_th = optimize_threshold_on_validation(\n",
    "                    y_val, val_proba, val_dates, val_prices\n",
    "                )\n",
    "                thresholds[name] = (buy_th, sell_th)\n",
    "            else:\n",
    "                thresholds[name] = (0.55, 0.45)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            test_precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "            test_recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "            test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                train_auc = roc_auc_score(y_train, train_proba)\n",
    "                val_auc = roc_auc_score(y_val, val_proba)\n",
    "                test_auc = roc_auc_score(y_test, test_proba)\n",
    "            except:\n",
    "                train_auc = val_auc = test_auc = 0.5\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Train_Acc': train_acc,\n",
    "                'Val_Acc': val_acc,\n",
    "                'Test_Acc': test_acc,\n",
    "                'Train_AUC': train_auc,\n",
    "                'Val_AUC': val_auc,\n",
    "                'Test_AUC': test_auc,\n",
    "                'Test_Precision': test_precision,\n",
    "                'Test_Recall': test_recall,\n",
    "                'Test_F1': test_f1,\n",
    "                'Overfit_Gap': train_acc - test_acc,\n",
    "                'Buy_Threshold': thresholds[name][0],\n",
    "                'Sell_Threshold': thresholds[name][1]\n",
    "            })\n",
    "            \n",
    "            models_trained[name] = model\n",
    "            predictions[name] = test_pred\n",
    "            probabilities[name] = test_proba\n",
    "            \n",
    "            status = \"OK\"\n",
    "            print(f\"{name:<30} {train_acc:<12.4f} {val_acc:<12.4f} {test_acc:<12.4f} {test_auc:<12.4f} {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {str(e)[:20]}\")\n",
    "    \n",
    "    return pd.DataFrame(results), models_trained, predictions, probabilities, thresholds\n",
    "\n",
    "\n",
    "\n",
    "def calculate_trading_performance_corrected(predictions, probabilities, dates, prices, y_true,\n",
    "                                           initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                                           buy_threshold=0.55, sell_threshold=0.45):\n",
    "    df_backtest = pd.DataFrame({\n",
    "        'date': dates.values,\n",
    "        'price': prices.values,\n",
    "        'prediction': predictions,\n",
    "        'probability': probabilities,\n",
    "        'actual_direction': y_true.values\n",
    "    })\n",
    "    \n",
    "    capital = initial_capital\n",
    "    position = 0\n",
    "    eth_holdings = 0\n",
    "    portfolio_values = [initial_capital]\n",
    "    trades = []\n",
    "    \n",
    "    total_cost = transaction_cost + slippage\n",
    "    \n",
    "    for idx in range(len(df_backtest) - 1):\n",
    "        current_row = df_backtest.iloc[idx]\n",
    "        signal = current_row['prediction']\n",
    "        confidence = current_row['probability']\n",
    "        \n",
    "        trade_price = df_backtest.iloc[idx + 1]['price']\n",
    "        \n",
    "        if signal == 1 and position == 0 and confidence > buy_threshold:\n",
    "            eth_to_buy = (capital * 0.95) / trade_price\n",
    "            cost = eth_to_buy * trade_price * (1 + total_cost)\n",
    "            if cost <= capital:\n",
    "                eth_holdings = eth_to_buy\n",
    "                capital -= cost\n",
    "                position = 1\n",
    "                trades.append({'action': 'BUY', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "\n",
    "        elif (signal == 0 or confidence < sell_threshold) and position == 1:\n",
    "            revenue = eth_holdings * trade_price * (1 - total_cost)\n",
    "            capital += revenue\n",
    "            eth_holdings = 0\n",
    "            position = 0\n",
    "            trades.append({'action': 'SELL', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "            \n",
    "        eod_portfolio_value = capital + (eth_holdings * trade_price)\n",
    "        portfolio_values.append(eod_portfolio_value)\n",
    "\n",
    "    final_value = portfolio_values[-1]\n",
    "    total_return = (final_value - initial_capital) / initial_capital * 100\n",
    "    buy_hold_return = (df_backtest.iloc[-1]['price'] - df_backtest.iloc[0]['price']) / df_backtest.iloc[0]['price'] * 100\n",
    "    \n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    if len(portfolio_values) > 1:\n",
    "        returns = (portfolio_values[1:] / portfolio_values[:-1]) - 1\n",
    "        returns = returns[~np.isnan(returns) & ~np.isinf(returns)]\n",
    "        \n",
    "        sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if len(returns) > 0 and np.std(returns) > 0 else 0\n",
    "        \n",
    "        cummax = np.maximum.accumulate(portfolio_values)\n",
    "        drawdown = (portfolio_values - cummax) / cummax\n",
    "        max_drawdown = np.min(drawdown) * 100 if len(drawdown) > 0 else 0\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        \n",
    "    n_trades = len(trades)\n",
    "    n_buys = len([t for t in trades if t['action'] == 'BUY'])\n",
    "    \n",
    "    return {\n",
    "        'final_value': final_value,\n",
    "        'total_return': total_return,\n",
    "        'buy_hold_return': buy_hold_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'n_trades': n_trades,\n",
    "        'n_buys': n_buys\n",
    "    }\n",
    "\n",
    "\n",
    "def backtest_all_models(models, predictions, probabilities, test_dates, test_prices, y_test, thresholds=None):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BACKTESTING RESULTS (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    backtest_results = []\n",
    "    \n",
    "    buy_hold_return = (test_prices.iloc[-1] - test_prices.iloc[0]) / test_prices.iloc[0] * 100\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Final Value':<15} {'Return %':<12} {'vs B&H':<12} {'Sharpe':<10} {'Max DD %':<12} {'Trades':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name in models.keys():\n",
    "        try:\n",
    "            if thresholds and name in thresholds:\n",
    "                buy_th, sell_th = thresholds[name]\n",
    "            else:\n",
    "                buy_th, sell_th = 0.55, 0.45\n",
    "            \n",
    "            results = calculate_trading_performance_corrected(\n",
    "                predictions[name], \n",
    "                probabilities[name],\n",
    "                test_dates, \n",
    "                test_prices, \n",
    "                y_test,\n",
    "                initial_capital=10000,\n",
    "                transaction_cost=0.002,\n",
    "                slippage=0.001,\n",
    "                buy_threshold=buy_th,\n",
    "                sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            outperformance = results['total_return'] - buy_hold_return\n",
    "            \n",
    "            backtest_results.append({\n",
    "                'Model': name,\n",
    "                'Final_Value': results['final_value'],\n",
    "                'Total_Return': results['total_return'],\n",
    "                'Buy_Hold_Return': buy_hold_return,\n",
    "                'Outperformance': outperformance,\n",
    "                'Sharpe_Ratio': results['sharpe_ratio'],\n",
    "                'Max_Drawdown': results['max_drawdown'],\n",
    "                'N_Trades': results['n_trades'],\n",
    "                'N_Buys': results['n_buys'],\n",
    "                'Buy_Threshold': buy_th,\n",
    "                'Sell_Threshold': sell_th\n",
    "            })\n",
    "            \n",
    "            print(f\"{name:<30} ${results['final_value']:<14,.2f} {results['total_return']:<11.2f}% \"\n",
    "                  f\"{outperformance:<11.2f}% {results['sharpe_ratio']:<9.3f} \"\n",
    "                  f\"{results['max_drawdown']:<11.2f}% {results['n_trades']:<10}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} Error: {str(e)[:50]}\")\n",
    "    \n",
    "    print(\"-\" * 110)\n",
    "    print(f\"{'Buy & Hold Baseline':<30} ${10000 * (1 + buy_hold_return/100):<14,.2f} {buy_hold_return:<11.2f}% \"\n",
    "          f\"{'0.00':<11}% {'N/A':<9} {'N/A':<11} {'0':<10}\")\n",
    "    \n",
    "    return pd.DataFrame(backtest_results)\n",
    "\n",
    "\n",
    "def create_comprehensive_report(results_df):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED PERFORMANCE REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    results_sorted = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Model':<30} {'Acc':<10} {'Prec':<10} {'Recall':<10} {'F1':<10} {'AUC':<10} {'Overfit':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for idx, row in results_sorted.iterrows():\n",
    "        print(f\"{idx+1:<6} {row['Model']:<30} {row['Test_Acc']:<10.4f} {row['Test_Precision']:<10.4f} \"\n",
    "              f\"{row['Test_Recall']:<10.4f} {row['Test_F1']:<10.4f} {row['Test_AUC']:<10.4f} {row['Overfit_Gap']:<10.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Best Test Accuracy:  {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_Acc']:.4f})\")\n",
    "    print(f\"Best Test AUC:       {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_AUC']:.4f})\")\n",
    "    print(f\"Best Test F1:        {results_sorted.nlargest(1, 'Test_F1').iloc[0]['Model']} ({results_sorted['Test_F1'].max():.4f})\")\n",
    "    print(f\"\\nMean Test Accuracy:  {results_df['Test_Acc'].mean():.4f} +/- {results_df['Test_Acc'].std():.4f}\")\n",
    "    print(f\"Mean Test AUC:       {results_df['Test_AUC'].mean():.4f} +/- {results_df['Test_AUC'].std():.4f}\")\n",
    "    print(f\"Mean Overfit Gap:    {results_df['Overfit_Gap'].mean():.4f} +/- {results_df['Overfit_Gap'].std():.4f}\")\n",
    "    \n",
    "    return results_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee76739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def walk_forward_backtest(df, models_config, n_splits=5, n_features=100):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTESTING (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols = prepare_feature_target(df)\n",
    "    all_backtest_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} Backtest ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        test_dates_fold = dates.iloc[test_idx]\n",
    "        test_prices_fold = prices.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train_fold.shape[1]))\n",
    "        X_train_selected = selector.fit_transform(X_train_fold, y_train_fold)\n",
    "        X_test_selected = selector.transform(X_test_fold)\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                bt_result = calculate_trading_performance_corrected(\n",
    "                    test_pred, \n",
    "                    test_proba,\n",
    "                    test_dates_fold, \n",
    "                    test_prices_fold, \n",
    "                    y_test_fold,\n",
    "                    initial_capital=10000,\n",
    "                    transaction_cost=0.002,\n",
    "                    slippage=0.001\n",
    "                )\n",
    "                \n",
    "                bt_result['Model'] = name\n",
    "                bt_result['Fold'] = fold_num\n",
    "                all_backtest_results.append(bt_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Backtest Error - {str(e)[:50]}\")\n",
    "    \n",
    "    backtest_df = pd.DataFrame(all_backtest_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTEST SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = backtest_df.groupby('Model').agg({\n",
    "        'total_return': ['mean', 'std'],\n",
    "        'sharpe_ratio': ['mean', 'std'],\n",
    "        'max_drawdown': ['mean', 'std'],\n",
    "        'n_trades': ['mean', 'sum']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.sort_values('total_return_mean', ascending=False)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Avg Return %':<15} {'Avg Sharpe':<12} {'Avg Max DD %':<15} {'Total Trades':<12}\")\n",
    "    print(\"-\" * 110)\n",
    "    for model_name in summary.index:\n",
    "        model_data = backtest_df[backtest_df['Model'] == model_name]\n",
    "        avg_return = model_data['total_return'].mean()\n",
    "        avg_sharpe = model_data['sharpe_ratio'].mean()\n",
    "        avg_dd = model_data['max_drawdown'].mean()\n",
    "        total_trades = model_data['n_trades'].sum()\n",
    "        print(f\"{model_name:<30} {avg_return:<15.2f} {avg_sharpe:<12.3f} {avg_dd:<15.2f} {total_trades:<12.0f}\")\n",
    "    \n",
    "    return backtest_df, summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def walk_forward_validation(df, models_config, n_splits=5, n_features=100):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols = prepare_feature_target(df)\n",
    "    all_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train_fold.shape[1]))\n",
    "        X_train_selected = selector.fit_transform(X_train_fold, y_train_fold)\n",
    "        X_test_selected = selector.transform(X_test_fold)\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                test_acc = accuracy_score(y_test_fold, test_pred)\n",
    "                test_precision = precision_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_recall = recall_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_f1 = f1_score(y_test_fold, test_pred, zero_division=0)\n",
    "                \n",
    "                try:\n",
    "                    test_auc = roc_auc_score(y_test_fold, test_proba)\n",
    "                except:\n",
    "                    test_auc = 0.5\n",
    "                \n",
    "                all_results.append({\n",
    "                    'Fold': fold_num,\n",
    "                    'Model': name,\n",
    "                    'Test_Acc': test_acc,\n",
    "                    'Test_Precision': test_precision,\n",
    "                    'Test_Recall': test_recall,\n",
    "                    'Test_F1': test_f1,\n",
    "                    'Test_AUC': test_auc\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = results_df.groupby('Model').agg({\n",
    "        'Test_Acc': ['mean', 'std'],\n",
    "        'Test_Precision': ['mean', 'std'],\n",
    "        'Test_Recall': ['mean', 'std'],\n",
    "        'Test_F1': ['mean', 'std'],\n",
    "        'Test_AUC': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    return results_df, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################1007 버전 2 퍼플렉시티 버전#######################\n",
    "#################1007 버전 2 퍼플렉시티 버전#######################\n",
    "#################1007 버전 2 퍼플렉시티 버전#######################\n",
    "#################1007 버전 2 퍼플렉시티 버전#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e45f9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier, \n",
    "    BaggingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. prepare_feature_target (수정: 이벤트 분리)\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_feature_target(df, task='classification'):\n",
    "    \"\"\"\n",
    "    피처와 타겟 준비 (이벤트 피처 식별 추가)\n",
    "    \"\"\"\n",
    "    exclude_cols = ['date', 'next_log_return', 'next_direction', 'next_close',\n",
    "                   'ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open']\n",
    "    \n",
    "    # 이벤트 피처 식별\n",
    "    event_patterns = ['event_', 'period_', 'in_upgrade', 'in_crisis', \n",
    "                     'days_since_last_event', 'event_count_90d']\n",
    "    event_cols = [col for col in df.columns \n",
    "                  if any(pattern in col for pattern in event_patterns)]\n",
    "    \n",
    "    # 전체 피처\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['next_direction'].copy() if task == 'classification' else df['next_log_return'].copy()\n",
    "    dates = df['date'].copy()\n",
    "    prices = df['ETH_Close'].copy()\n",
    "    \n",
    "    print(f\"  총 피처: {len(feature_cols)}개 (이벤트: {len(event_cols)}개)\")\n",
    "    \n",
    "    return X, y, dates, prices, feature_cols, event_cols\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. feature_selection_before_scaling (수정: 이벤트 강제 포함)\n",
    "# ============================================================================\n",
    "\n",
    "def feature_selection_before_scaling(X_train, y_train, X_val, X_test, \n",
    "                                     event_cols=None, n_features=100):\n",
    "    \"\"\"\n",
    "    피처 선택 (이벤트는 무조건 포함)\n",
    "    \n",
    "    핵심:\n",
    "    - 연속 피처만 SelectKBest로 선택\n",
    "    - 이벤트는 무조건 포함\n",
    "    \"\"\"\n",
    "    print(f\"\\n[FEATURE SELECTION]\")\n",
    "    \n",
    "    if event_cols is None:\n",
    "        event_cols = []\n",
    "    \n",
    "    # 이벤트와 연속 피처 분리\n",
    "    event_mask = X_train.columns.isin(event_cols)\n",
    "    continuous_cols = X_train.columns[~event_mask].tolist()\n",
    "    actual_event_cols = X_train.columns[event_mask].tolist()\n",
    "    \n",
    "    print(f\"  연속 피처: {len(continuous_cols)}개 -> {min(n_features, len(continuous_cols))}개\")\n",
    "    print(f\"  이벤트 피처: {len(actual_event_cols)}개 (전부 유지)\")\n",
    "    \n",
    "    # 연속 피처만 선택\n",
    "    if len(continuous_cols) > 0:\n",
    "        selector = SelectKBest(score_func=f_classif, k=min(n_features, len(continuous_cols)))\n",
    "        X_train_continuous_sel = selector.fit_transform(X_train[continuous_cols], y_train)\n",
    "        X_val_continuous_sel = selector.transform(X_val[continuous_cols])\n",
    "        X_test_continuous_sel = selector.transform(X_test[continuous_cols])\n",
    "        \n",
    "        selected_continuous = [continuous_cols[i] for i in range(len(continuous_cols)) \n",
    "                              if selector.get_support()[i]]\n",
    "    else:\n",
    "        X_train_continuous_sel = np.array([]).reshape(len(X_train), 0)\n",
    "        X_val_continuous_sel = np.array([]).reshape(len(X_val), 0)\n",
    "        X_test_continuous_sel = np.array([]).reshape(len(X_test), 0)\n",
    "        selected_continuous = []\n",
    "    \n",
    "    # 이벤트 피처 추가\n",
    "    if len(actual_event_cols) > 0:\n",
    "        X_train_final = np.hstack([X_train_continuous_sel, X_train[actual_event_cols].values])\n",
    "        X_val_final = np.hstack([X_val_continuous_sel, X_val[actual_event_cols].values])\n",
    "        X_test_final = np.hstack([X_test_continuous_sel, X_test[actual_event_cols].values])\n",
    "    else:\n",
    "        X_train_final = X_train_continuous_sel\n",
    "        X_val_final = X_val_continuous_sel\n",
    "        X_test_final = X_test_continuous_sel\n",
    "    \n",
    "    selected_features = selected_continuous + actual_event_cols\n",
    "    \n",
    "    print(f\"  최종: {len(selected_features)}개\")\n",
    "    \n",
    "    return X_train_final, X_val_final, X_test_final, selected_features, len(actual_event_cols)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. scale_features (수정: 이벤트 스케일링 제외)\n",
    "# ============================================================================\n",
    "\n",
    "def scale_features(X_train, X_val, X_test, n_event_features=0):\n",
    "    \"\"\"\n",
    "    스케일링 (이벤트 제외)\n",
    "    \n",
    "    핵심:\n",
    "    - 마지막 n_event_features개는 0/1 그대로 유지\n",
    "    - 나머지만 RobustScaler\n",
    "    \"\"\"\n",
    "    print(f\"\\n[SCALING]\")\n",
    "    \n",
    "    n_continuous = X_train.shape[1] - n_event_features\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # 연속 피처만 스케일링\n",
    "    if n_continuous > 0:\n",
    "        X_train_scaled[:, :n_continuous] = scaler.fit_transform(X_train[:, :n_continuous])\n",
    "        X_val_scaled[:, :n_continuous] = scaler.transform(X_val[:, :n_continuous])\n",
    "        X_test_scaled[:, :n_continuous] = scaler.transform(X_test[:, :n_continuous])\n",
    "    \n",
    "    print(f\"  연속 피처: {n_continuous}개 스케일링\")\n",
    "    print(f\"  이벤트 피처: {n_event_features}개 유지 (0/1)\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. create_sample_weights (새로 추가: 이벤트 가중치)\n",
    "# ============================================================================\n",
    "\n",
    "def create_sample_weights(train_df, event_cols):\n",
    "    \"\"\"\n",
    "    이벤트 기반 샘플 가중치\n",
    "    \n",
    "    핵심:\n",
    "    - 이벤트 당일: 5배\n",
    "    - 이벤트 윈도우 (_post7, _post30): 2배\n",
    "    - 위기 기간: 1.5배\n",
    "    \"\"\"\n",
    "    weights = np.ones(len(train_df))\n",
    "    \n",
    "    # 주요 이벤트 (당일)\n",
    "    major_events = [col for col in event_cols \n",
    "                   if any(e in col for e in ['event_merge', 'event_london', 'event_shanghai',\n",
    "                                             'event_eth_etf_approval', 'event_ftx_collapse',\n",
    "                                             'event_terra_collapse', 'event_dencun'])]\n",
    "    \n",
    "    # 이벤트 윈도우\n",
    "    event_windows = [col for col in event_cols if '_post7' in col or '_post30' in col]\n",
    "    \n",
    "    for idx, row in train_df.iterrows():\n",
    "        # 이벤트 당일: 5배\n",
    "        if any(col in train_df.columns and row[col] == 1 for col in major_events):\n",
    "            weights[idx] = 5.0\n",
    "        # 이벤트 윈도우: 2배\n",
    "        elif any(col in train_df.columns and row[col] == 1 for col in event_windows):\n",
    "            weights[idx] = 2.0\n",
    "        # 위기 기간: 1.5배\n",
    "        elif 'in_crisis_window' in train_df.columns and row['in_crisis_window'] == 1:\n",
    "            weights[idx] = 1.5\n",
    "    \n",
    "    n_weighted = (weights > 1.0).sum()\n",
    "    print(f\"\\n[SAMPLE WEIGHTS]\")\n",
    "    print(f\"  가중 샘플: {n_weighted}개 / {len(weights)}개 ({n_weighted/len(weights)*100:.1f}%)\")\n",
    "    print(f\"  가중치 5.0: {(weights == 5.0).sum()}개\")\n",
    "    print(f\"  가중치 2.0: {(weights == 2.0).sum()}개\")\n",
    "    print(f\"  가중치 1.5: {(weights == 1.5).sum()}개\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. check_data_leakage (기존 그대로)\n",
    "# ============================================================================\n",
    "\n",
    "def check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DATA LEAKAGE VERIFICATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    train_idx = set(X_train.index)\n",
    "    val_idx = set(X_val.index)\n",
    "    test_idx = set(X_test.index)\n",
    "    \n",
    "    if len(train_idx & val_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and validation index overlap\")\n",
    "    if len(train_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Train and test index overlap\")\n",
    "    if len(val_idx & test_idx) > 0:\n",
    "        issues.append(\"CRITICAL: Validation and test index overlap\")\n",
    "    \n",
    "    if train_dates.max() >= val_dates.min():\n",
    "        issues.append(\"CRITICAL: Train dates overlap with validation dates\")\n",
    "    if val_dates.max() >= test_dates.min():\n",
    "        issues.append(\"CRITICAL: Validation dates overlap with test dates\")\n",
    "    \n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {X_train.isnull().sum().sum()} NaN values\")\n",
    "    if X_val.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {X_val.isnull().sum().sum()} NaN values\")\n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {X_test.isnull().sum().sum()} NaN values\")\n",
    "    \n",
    "    if np.isinf(X_train.values).sum() > 0:\n",
    "        issues.append(f\"WARNING: Train has {np.isinf(X_train.values).sum()} inf values\")\n",
    "    if np.isinf(X_val.values).sum() > 0:\n",
    "        issues.append(f\"WARNING: Validation has {np.isinf(X_val.values).sum()} inf values\")\n",
    "    if np.isinf(X_test.values).sum() > 0:\n",
    "        issues.append(f\"WARNING: Test has {np.isinf(X_test.values).sum()} inf values\")\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)} samples, {train_dates.min()} to {train_dates.max()}\")\n",
    "    print(f\"Val:   {len(X_val)} samples, {val_dates.min()} to {val_dates.max()}\")\n",
    "    print(f\"Test:  {len(X_test)} samples, {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"\\n✓ No data leakage detected\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n⚠ Data leakage issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. get_all_models (기존 그대로)\n",
    "# ============================================================================\n",
    "\n",
    "def get_all_models():\n",
    "    base_models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingClassifier(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.03,\n",
    "            subsample=0.75, random_state=42\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=20,\n",
    "            min_samples_leaf=10, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            n_estimators=100, learning_rate=0.5, random_state=42\n",
    "        ),\n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            max_depth=8, min_samples_split=20, min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=0.1, penalty='l2', max_iter=2000, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'RidgeClassifier': RidgeClassifier(\n",
    "            alpha=1.0, random_state=42\n",
    "        ),\n",
    "        'SVM_RBF': SVC(\n",
    "            kernel='rbf', C=1.0, gamma='scale', \n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'SVM_Linear': SVC(\n",
    "            kernel='linear', C=0.1,\n",
    "            probability=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Small': MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.01, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'MLP_Medium': MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.001, batch_size=64,\n",
    "            learning_rate='adaptive', max_iter=300,\n",
    "            early_stopping=True, random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=15, weights='distance', n_jobs=-1\n",
    "        ),\n",
    "        'NaiveBayes': GaussianNB(),\n",
    "        'Bagging_RF': BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=8, random_state=42),\n",
    "            n_estimators=50, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost_GPU': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=3,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            tree_method='gpu_hist',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM_GPU': LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=20,\n",
    "            max_depth=4,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=5,\n",
    "            device='gpu',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    voting_soft = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    voting_hard = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('lr', LogisticRegression(C=0.1, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    \n",
    "    stacking = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "            ('et', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(C=0.1, random_state=42),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    base_models['Voting_Soft'] = voting_soft\n",
    "    base_models['Voting_Hard'] = voting_hard\n",
    "    base_models['Stacking'] = stacking\n",
    "    \n",
    "    xgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['XGBoost_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    lgb_calibrated = CalibratedClassifierCV(\n",
    "        base_models['LightGBM_GPU'],\n",
    "        method='isotonic',\n",
    "        cv=TimeSeriesSplit(n_splits=3)\n",
    "    )\n",
    "    \n",
    "    base_models['XGBoost_Calibrated'] = xgb_calibrated\n",
    "    base_models['LightGBM_Calibrated'] = lgb_calibrated\n",
    "    \n",
    "    return base_models\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. optimize_threshold_on_validation (기존 그대로)\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_threshold_on_validation(y_val, y_proba_val, val_dates, val_prices):\n",
    "    best_sharpe = -np.inf\n",
    "    best_thresholds = (0.55, 0.45)\n",
    "    \n",
    "    for buy_th in np.arange(0.50, 0.70, 0.05):\n",
    "        for sell_th in np.arange(0.30, 0.50, 0.05):\n",
    "            predictions_temp = (y_proba_val > 0.5).astype(int)\n",
    "            \n",
    "            temp_result = calculate_trading_performance_corrected(\n",
    "                predictions_temp, y_proba_val, val_dates, val_prices, y_val,\n",
    "                initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                buy_threshold=buy_th, sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            if temp_result['sharpe_ratio'] > best_sharpe:\n",
    "                best_sharpe = temp_result['sharpe_ratio']\n",
    "                best_thresholds = (buy_th, sell_th)\n",
    "    \n",
    "    return best_thresholds\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. train_all_models (수정: 샘플 가중치 추가)\n",
    "# ============================================================================\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                     val_dates=None, val_prices=None, optimize_thresholds=False,\n",
    "                     sample_weights=None):\n",
    "    \"\"\"\n",
    "    모든 모델 학습 (샘플 가중치 지원)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    models_config = get_all_models()\n",
    "    \n",
    "    results = []\n",
    "    models_trained = {}\n",
    "    predictions = {}\n",
    "    probabilities = {}\n",
    "    thresholds = {}\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12} Status\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name, model in models_config.items():\n",
    "        try:\n",
    "            # 샘플 가중치 지원 모델 확인\n",
    "            if sample_weights is not None and hasattr(model, 'fit'):\n",
    "                try:\n",
    "                    # XGBoost, LightGBM, tree-based 모델은 sample_weight 지원\n",
    "                    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "                except TypeError:\n",
    "                    # 지원 안 하는 모델은 일반 fit\n",
    "                    model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            train_pred = model.predict(X_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                train_proba = model.predict_proba(X_train)[:, 1]\n",
    "                val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                test_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                train_proba = train_pred\n",
    "                val_proba = val_pred\n",
    "                test_proba = test_pred\n",
    "            \n",
    "            if optimize_thresholds and val_dates is not None and val_prices is not None:\n",
    "                buy_th, sell_th = optimize_threshold_on_validation(\n",
    "                    y_val, val_proba, val_dates, val_prices\n",
    "                )\n",
    "                thresholds[name] = (buy_th, sell_th)\n",
    "            else:\n",
    "                thresholds[name] = (0.55, 0.45)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            test_precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "            test_recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "            test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                train_auc = roc_auc_score(y_train, train_proba)\n",
    "                val_auc = roc_auc_score(y_val, val_proba)\n",
    "                test_auc = roc_auc_score(y_test, test_proba)\n",
    "            except:\n",
    "                train_auc = val_auc = test_auc = 0.5\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Train_Acc': train_acc,\n",
    "                'Val_Acc': val_acc,\n",
    "                'Test_Acc': test_acc,\n",
    "                'Train_AUC': train_auc,\n",
    "                'Val_AUC': val_auc,\n",
    "                'Test_AUC': test_auc,\n",
    "                'Test_Precision': test_precision,\n",
    "                'Test_Recall': test_recall,\n",
    "                'Test_F1': test_f1,\n",
    "                'Overfit_Gap': train_acc - test_acc,\n",
    "                'Buy_Threshold': thresholds[name][0],\n",
    "                'Sell_Threshold': thresholds[name][1]\n",
    "            })\n",
    "            \n",
    "            models_trained[name] = model\n",
    "            predictions[name] = test_pred\n",
    "            probabilities[name] = test_proba\n",
    "            \n",
    "            status = \"OK\"\n",
    "            print(f\"{name:<30} {train_acc:<12.4f} {val_acc:<12.4f} {test_acc:<12.4f} {test_auc:<12.4f} {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {str(e)[:20]}\")\n",
    "    \n",
    "    return pd.DataFrame(results), models_trained, predictions, probabilities, thresholds\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. 나머지 함수들 (기존 그대로)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_trading_performance_corrected(predictions, probabilities, dates, prices, y_true,\n",
    "                                           initial_capital=10000, transaction_cost=0.002, slippage=0.001,\n",
    "                                           buy_threshold=0.55, sell_threshold=0.45):\n",
    "    df_backtest = pd.DataFrame({\n",
    "        'date': dates.values,\n",
    "        'price': prices.values,\n",
    "        'prediction': predictions,\n",
    "        'probability': probabilities,\n",
    "        'actual_direction': y_true.values\n",
    "    })\n",
    "    \n",
    "    capital = initial_capital\n",
    "    position = 0\n",
    "    eth_holdings = 0\n",
    "    portfolio_values = [initial_capital]\n",
    "    trades = []\n",
    "    \n",
    "    total_cost = transaction_cost + slippage\n",
    "    \n",
    "    for idx in range(len(df_backtest) - 1):\n",
    "        current_row = df_backtest.iloc[idx]\n",
    "        signal = current_row['prediction']\n",
    "        confidence = current_row['probability']\n",
    "        \n",
    "        trade_price = df_backtest.iloc[idx + 1]['price']\n",
    "        \n",
    "        if signal == 1 and position == 0 and confidence > buy_threshold:\n",
    "            eth_to_buy = (capital * 0.95) / trade_price\n",
    "            cost = eth_to_buy * trade_price * (1 + total_cost)\n",
    "            if cost <= capital:\n",
    "                eth_holdings = eth_to_buy\n",
    "                capital -= cost\n",
    "                position = 1\n",
    "                trades.append({'action': 'BUY', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "\n",
    "        elif (signal == 0 or confidence < sell_threshold) and position == 1:\n",
    "            revenue = eth_holdings * trade_price * (1 - total_cost)\n",
    "            capital += revenue\n",
    "            eth_holdings = 0\n",
    "            position = 0\n",
    "            trades.append({'action': 'SELL', 'price': trade_price, 'date': df_backtest.iloc[idx + 1]['date']})\n",
    "            \n",
    "        eod_portfolio_value = capital + (eth_holdings * trade_price)\n",
    "        portfolio_values.append(eod_portfolio_value)\n",
    "\n",
    "    final_value = portfolio_values[-1]\n",
    "    total_return = (final_value - initial_capital) / initial_capital * 100\n",
    "    buy_hold_return = (df_backtest.iloc[-1]['price'] - df_backtest.iloc[0]['price']) / df_backtest.iloc[0]['price'] * 100\n",
    "    \n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    if len(portfolio_values) > 1:\n",
    "        returns = (portfolio_values[1:] / portfolio_values[:-1]) - 1\n",
    "        returns = returns[~np.isnan(returns) & ~np.isinf(returns)]\n",
    "        \n",
    "        sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if len(returns) > 0 and np.std(returns) > 0 else 0\n",
    "        \n",
    "        cummax = np.maximum.accumulate(portfolio_values)\n",
    "        drawdown = (portfolio_values - cummax) / cummax\n",
    "        max_drawdown = np.min(drawdown) * 100 if len(drawdown) > 0 else 0\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        \n",
    "    n_trades = len(trades)\n",
    "    n_buys = len([t for t in trades if t['action'] == 'BUY'])\n",
    "    \n",
    "    return {\n",
    "        'final_value': final_value,\n",
    "        'total_return': total_return,\n",
    "        'buy_hold_return': buy_hold_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'n_trades': n_trades,\n",
    "        'n_buys': n_buys\n",
    "    }\n",
    "\n",
    "\n",
    "def backtest_all_models(models, predictions, probabilities, test_dates, test_prices, y_test, thresholds=None):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BACKTESTING RESULTS (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    backtest_results = []\n",
    "    \n",
    "    buy_hold_return = (test_prices.iloc[-1] - test_prices.iloc[0]) / test_prices.iloc[0] * 100\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Final Value':<15} {'Return %':<12} {'vs B&H':<12} {'Sharpe':<10} {'Max DD %':<12} {'Trades':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for name in models.keys():\n",
    "        try:\n",
    "            if thresholds and name in thresholds:\n",
    "                buy_th, sell_th = thresholds[name]\n",
    "            else:\n",
    "                buy_th, sell_th = 0.55, 0.45\n",
    "            \n",
    "            results = calculate_trading_performance_corrected(\n",
    "                predictions[name], \n",
    "                probabilities[name],\n",
    "                test_dates, \n",
    "                test_prices, \n",
    "                y_test,\n",
    "                initial_capital=10000,\n",
    "                transaction_cost=0.002,\n",
    "                slippage=0.001,\n",
    "                buy_threshold=buy_th,\n",
    "                sell_threshold=sell_th\n",
    "            )\n",
    "            \n",
    "            outperformance = results['total_return'] - buy_hold_return\n",
    "            \n",
    "            backtest_results.append({\n",
    "                'Model': name,\n",
    "                'Final_Value': results['final_value'],\n",
    "                'Total_Return': results['total_return'],\n",
    "                'Buy_Hold_Return': buy_hold_return,\n",
    "                'Outperformance': outperformance,\n",
    "                'Sharpe_Ratio': results['sharpe_ratio'],\n",
    "                'Max_Drawdown': results['max_drawdown'],\n",
    "                'N_Trades': results['n_trades'],\n",
    "                'N_Buys': results['n_buys'],\n",
    "                'Buy_Threshold': buy_th,\n",
    "                'Sell_Threshold': sell_th\n",
    "            })\n",
    "            \n",
    "            print(f\"{name:<30} ${results['final_value']:<14,.2f} {results['total_return']:<11.2f}% \"\n",
    "                  f\"{outperformance:<11.2f}% {results['sharpe_ratio']:<9.3f} \"\n",
    "                  f\"{results['max_drawdown']:<11.2f}% {results['n_trades']:<10}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<30} Error: {str(e)[:50]}\")\n",
    "    \n",
    "    print(\"-\" * 110)\n",
    "    print(f\"{'Buy & Hold Baseline':<30} ${10000 * (1 + buy_hold_return/100):<14,.2f} {buy_hold_return:<11.2f}% \"\n",
    "          f\"{'0.00':<11}% {'N/A':<9} {'N/A':<11} {'0':<10}\")\n",
    "    \n",
    "    return pd.DataFrame(backtest_results)\n",
    "\n",
    "\n",
    "def create_comprehensive_report(results_df):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED PERFORMANCE REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    results_sorted = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Model':<30} {'Acc':<10} {'Prec':<10} {'Recall':<10} {'F1':<10} {'AUC':<10} {'Overfit':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    for idx, row in results_sorted.iterrows():\n",
    "        print(f\"{idx+1:<6} {row['Model']:<30} {row['Test_Acc']:<10.4f} {row['Test_Precision']:<10.4f} \"\n",
    "              f\"{row['Test_Recall']:<10.4f} {row['Test_F1']:<10.4f} {row['Test_AUC']:<10.4f} {row['Overfit_Gap']:<10.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Best Test Accuracy:  {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_Acc']:.4f})\")\n",
    "    print(f\"Best Test AUC:       {results_sorted.iloc[0]['Model']} ({results_sorted.iloc[0]['Test_AUC']:.4f})\")\n",
    "    print(f\"Best Test F1:        {results_sorted.nlargest(1, 'Test_F1').iloc[0]['Model']} ({results_sorted['Test_F1'].max():.4f})\")\n",
    "    print(f\"\\nMean Test Accuracy:  {results_df['Test_Acc'].mean():.4f} +/- {results_df['Test_Acc'].std():.4f}\")\n",
    "    print(f\"Mean Test AUC:       {results_df['Test_AUC'].mean():.4f} +/- {results_df['Test_AUC'].std():.4f}\")\n",
    "    print(f\"Mean Overfit Gap:    {results_df['Overfit_Gap'].mean():.4f} +/- {results_df['Overfit_Gap'].std():.4f}\")\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 10. 공통 전처리 함수 (새로 추가: 중복 제거)\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_fold(X_train, y_train, X_test, event_cols, n_features=100):\n",
    "    \"\"\"\n",
    "    Walk-forward용 공통 전처리 함수\n",
    "    \n",
    "    중복 코드를 하나로 통합:\n",
    "    - 피처 선택\n",
    "    - 스케일링\n",
    "    \"\"\"\n",
    "    # 피처 선택\n",
    "    X_train_sel, X_test_sel, _, selected_features, n_events = \\\n",
    "        feature_selection_before_scaling(\n",
    "            X_train, y_train, X_train, X_test,  # val도 train으로 (fold에서는 val 없음)\n",
    "            event_cols=event_cols, \n",
    "            n_features=n_features\n",
    "        )\n",
    "    \n",
    "    # 스케일링\n",
    "    X_train_scaled, _, X_test_scaled, scaler = scale_features(\n",
    "        X_train_sel, X_train_sel, X_test_sel,\n",
    "        n_event_features=n_events\n",
    "    )\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 11. walk_forward_validation (리팩토링: 중복 제거)\n",
    "# ============================================================================\n",
    "\n",
    "def walk_forward_validation(df, models_config, n_splits=5, n_features=100):\n",
    "    \"\"\"\n",
    "    Walk-forward 검증 (리팩토링)\n",
    "    \n",
    "    ✅ 검증: 맞게 구현됨\n",
    "    - 각 fold마다 독립적으로 scaler fit\n",
    "    - 미래 정보 누수 없음\n",
    "    \n",
    "    🔧 개선: 중복 코드 제거\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols, event_cols = prepare_feature_target(df)\n",
    "    all_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        # 공통 전처리 사용 (중복 제거!)\n",
    "        X_train_scaled, X_test_scaled = preprocess_fold(\n",
    "            X_train_fold, y_train_fold, X_test_fold, \n",
    "            event_cols, n_features\n",
    "        )\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                test_acc = accuracy_score(y_test_fold, test_pred)\n",
    "                test_precision = precision_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_recall = recall_score(y_test_fold, test_pred, zero_division=0)\n",
    "                test_f1 = f1_score(y_test_fold, test_pred, zero_division=0)\n",
    "                \n",
    "                try:\n",
    "                    test_auc = roc_auc_score(y_test_fold, test_proba)\n",
    "                except:\n",
    "                    test_auc = 0.5\n",
    "                \n",
    "                all_results.append({\n",
    "                    'Fold': fold_num,\n",
    "                    'Model': name,\n",
    "                    'Test_Acc': test_acc,\n",
    "                    'Test_Precision': test_precision,\n",
    "                    'Test_Recall': test_recall,\n",
    "                    'Test_F1': test_f1,\n",
    "                    'Test_AUC': test_auc\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = results_df.groupby('Model').agg({\n",
    "        'Test_Acc': ['mean', 'std'],\n",
    "        'Test_Precision': ['mean', 'std'],\n",
    "        'Test_Recall': ['mean', 'std'],\n",
    "        'Test_F1': ['mean', 'std'],\n",
    "        'Test_AUC': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 12. walk_forward_backtest (리팩토링: 중복 제거)\n",
    "# ============================================================================\n",
    "\n",
    "def walk_forward_backtest(df, models_config, n_splits=5, n_features=100):\n",
    "    \"\"\"\n",
    "    Walk-forward 백테스팅 (리팩토링)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTESTING (Improved: Slippage 0.1% + Transaction Cost 0.2%)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y, dates, prices, feature_cols, event_cols = prepare_feature_target(df)\n",
    "    all_backtest_results = []\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        fold_num += 1\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits} Backtest ---\")\n",
    "        \n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "        test_dates_fold = dates.iloc[test_idx]\n",
    "        test_prices_fold = prices.iloc[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)} samples | Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        # 공통 전처리 사용\n",
    "        X_train_scaled, X_test_scaled = preprocess_fold(\n",
    "            X_train_fold, y_train_fold, X_test_fold, \n",
    "            event_cols, n_features\n",
    "        )\n",
    "        \n",
    "        for name, model in models_config.items():\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                model_copy = clone(model)\n",
    "                \n",
    "                model_copy.fit(X_train_scaled, y_train_fold)\n",
    "                test_pred = model_copy.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model_copy, 'predict_proba'):\n",
    "                    test_proba = model_copy.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    test_proba = test_pred\n",
    "                \n",
    "                bt_result = calculate_trading_performance_corrected(\n",
    "                    test_pred, \n",
    "                    test_proba,\n",
    "                    test_dates_fold, \n",
    "                    test_prices_fold, \n",
    "                    y_test_fold,\n",
    "                    initial_capital=10000,\n",
    "                    transaction_cost=0.002,\n",
    "                    slippage=0.001\n",
    "                )\n",
    "                \n",
    "                bt_result['Model'] = name\n",
    "                bt_result['Fold'] = fold_num\n",
    "                all_backtest_results.append(bt_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Backtest Error - {str(e)[:50]}\")\n",
    "    \n",
    "    backtest_df = pd.DataFrame(all_backtest_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WALK-FORWARD BACKTEST SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary = backtest_df.groupby('Model').agg({\n",
    "        'total_return': ['mean', 'std'],\n",
    "        'sharpe_ratio': ['mean', 'std'],\n",
    "        'max_drawdown': ['mean', 'std'],\n",
    "        'n_trades': ['mean', 'sum']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.sort_values('total_return_mean', ascending=False)\n",
    "    \n",
    "    print(\"\\n\", summary)\n",
    "    \n",
    "    print(f\"\\n{'Model':<30} {'Avg Return %':<15} {'Avg Sharpe':<12} {'Avg Max DD %':<15} {'Total Trades':<12}\")\n",
    "    print(\"-\" * 110)\n",
    "    for model_name in summary.index:\n",
    "        model_data = backtest_df[backtest_df['Model'] == model_name]\n",
    "        avg_return = model_data['total_return'].mean()\n",
    "        avg_sharpe = model_data['sharpe_ratio'].mean()\n",
    "        avg_dd = model_data['max_drawdown'].mean()\n",
    "        total_trades = model_data['n_trades'].sum()\n",
    "        print(f\"{model_name:<30} {avg_return:<15.2f} {avg_sharpe:<12.3f} {avg_dd:<15.2f} {total_trades:<12.0f}\")\n",
    "    \n",
    "    return backtest_df, summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f270eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  총 피처: 380개 (이벤트: 76개)\n",
      "  총 피처: 380개 (이벤트: 76개)\n",
      "  총 피처: 380개 (이벤트: 76개)\n",
      "\n",
      "Dataset: Train=1226, Val=263, Test=263\n",
      "\n",
      "====================================================================================================\n",
      "DATA LEAKAGE VERIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Train: 1226 samples, 2020-12-19 00:00:00 to 2024-04-27 00:00:00\n",
      "Val:   263 samples, 2024-04-28 00:00:00 to 2025-01-15 00:00:00\n",
      "Test:  263 samples, 2025-01-16 00:00:00 to 2025-10-05 00:00:00\n",
      "\n",
      "✓ No data leakage detected\n",
      "\n",
      "[FEATURE SELECTION]\n",
      "  연속 피처: 304개 -> 100개\n",
      "  이벤트 피처: 76개 (전부 유지)\n",
      "  최종: 176개\n",
      "\n",
      "[SCALING]\n",
      "  연속 피처: 100개 스케일링\n",
      "  이벤트 피처: 76개 유지 (0/1)\n",
      "\n",
      "[SAMPLE WEIGHTS]\n",
      "  가중 샘플: 246개 / 1226개 (20.1%)\n",
      "  가중치 5.0: 246개\n",
      "  가중치 2.0: 0개\n",
      "  가중치 1.5: 0개\n",
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING AND EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Model                          Train Acc    Val Acc      Test Acc     Test AUC     Status\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "RandomForest                   0.8728       0.5741       0.5475       0.5862       OK\n",
      "GradientBoosting               0.8891       0.5627       0.5361       0.5655       OK\n",
      "ExtraTrees                     0.7602       0.5627       0.5399       0.5570       OK\n",
      "AdaBoost                       0.6582       0.5551       0.4829       0.6350       OK\n",
      "DecisionTree                   0.7439       0.5285       0.5932       0.5703       OK\n",
      "LogisticRegression             0.5179       0.5133       0.5209       0.5364       OK\n",
      "RidgeClassifier                0.6737       0.6122       0.5894       0.5936       OK\n",
      "SVM_RBF                        0.5416       0.4905       0.4905       0.5060       OK\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 【실행 예시】\n",
    "# ============================================================================\n",
    "\n",
    "# 데이터 준비 (이벤트 포함!)\n",
    "X_train, y_train, train_dates, train_prices, train_features, train_event_cols = \\\n",
    "    prepare_feature_target(train_df)\n",
    "X_val, y_val, val_dates, val_prices, _, _ = prepare_feature_target(val_df)\n",
    "X_test, y_test, test_dates, test_prices, _, _ = prepare_feature_target(test_df)\n",
    "\n",
    "print(f\"\\nDataset: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "check_data_leakage(X_train, X_val, X_test, train_dates, val_dates, test_dates)\n",
    "\n",
    "# 피처 선택 (이벤트 강제 포함!)\n",
    "X_train_sel, X_val_sel, X_test_sel, selected_features, n_events = \\\n",
    "    feature_selection_before_scaling(\n",
    "        X_train, y_train, X_val, X_test, \n",
    "        event_cols=train_event_cols, \n",
    "        n_features=100\n",
    "    )\n",
    "\n",
    "# 스케일링 (이벤트 제외!)\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_features(\n",
    "    X_train_sel, X_val_sel, X_test_sel, \n",
    "    n_event_features=n_events\n",
    ")\n",
    "\n",
    "# 샘플 가중치 생성\n",
    "sample_weights = create_sample_weights(train_df, train_event_cols)\n",
    "\n",
    "# 모델 학습 (가중치 적용!)\n",
    "results, models, preds, probs, thresholds = train_all_models(\n",
    "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    val_dates=val_dates, val_prices=val_prices,\n",
    "    optimize_thresholds=False,\n",
    "    sample_weights=sample_weights  # 핵심!\n",
    ")\n",
    "\n",
    "# 평가\n",
    "report = create_comprehensive_report(results)\n",
    "\n",
    "# 백테스팅\n",
    "backtest_results = backtest_all_models(\n",
    "    models, preds, probs, test_dates, test_prices, y_test, thresholds\n",
    ")\n",
    "\n",
    "# Walk-forward 검증\n",
    "wf_results, wf_summary = walk_forward_validation(\n",
    "    df_clean, get_all_models(), n_splits=5, n_features=100\n",
    ")\n",
    "\n",
    "# Walk-forward 백테스팅\n",
    "wf_backtest, wf_bt_summary = walk_forward_backtest(\n",
    "    df_clean, get_all_models(), n_splits=5, n_features=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
