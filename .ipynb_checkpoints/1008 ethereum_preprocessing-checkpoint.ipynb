{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf351ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 23:53:25.645324: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-20 23:53:25.645365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-20 23:53:25.646532: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-20 23:53:25.652949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 23:53:26.419448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 감성 지표 생성 완료: 25개 (date 제외)\n",
      "\n",
      "감성 지표 결측 처리:\n",
      "  sentiment_mean: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_std: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  positive_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  negative_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_positive_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extreme_negative_count: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_sum: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_polarity: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_intensity: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_disagreement: 39개 → 0 (데이터 없음 = 중립)\n",
      "  bull_bear_ratio: 39개 → 0 (데이터 없음 = 중립)\n",
      "  weighted_sentiment: 39개 → 0 (데이터 없음 = 중립)\n",
      "  extremity_index: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma3: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_3: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_ma14: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_volatility_14: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_trend: 39개 → 0 (데이터 없음 = 중립)\n",
      "  sentiment_acceleration: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_change: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma7: 39개 → 0 (데이터 없음 = 중립)\n",
      "  news_volume_ma14: 39개 → 0 (데이터 없음 = 중립)\n",
      "\n",
      "외부 변수 FFill 처리:\n",
      "  3,107 → 673개 (FFill)\n",
      "\n",
      "Lookback 기간 제거:\n",
      "  1953 → 1953행\n",
      "\n",
      "초기 결측치 처리:\n",
      "  남은 결측: 673개 → 0\n",
      "\n",
      "✓ 최종 데이터: (1953, 96)\n",
      "  날짜: 2020-06-02 ~ 2025-10-06\n",
      "  결측: 945개\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_ta as ta    \n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # 분류 모델에 roc_auc_score 추가\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier, \n",
    "                              BaggingClassifier, GradientBoostingClassifier,\n",
    "                              StackingClassifier, VotingClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tensorflow.keras.layers import SimpleRNN, Add, Activation\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from tensorflow.keras.layers import Lambda \n",
    "from tensorflow.keras.layers import Permute\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================ \n",
    "# 1. 날짜 파싱 및 CSV 로드 함수\n",
    "# ============================================================================ \n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    #print(df.shape)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    #print(df.shape)\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_data(filepath):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = standardize_date_column(df,filepath)\n",
    "    return df\n",
    "# ============================================================================ \n",
    "# 2. 데이터 로딩\n",
    "# ============================================================================ \n",
    "DATA_DIR = './macro_data'\n",
    "\n",
    "def load_from_macro_data(filename):\n",
    "    return load_and_standardize_data(os.path.join(DATA_DIR, filename))\n",
    "\n",
    "macro_df = load_from_macro_data('macro_crypto_data.csv')\n",
    "news_df = load_from_macro_data('news_data.csv')\n",
    "eth_onchain_df = load_from_macro_data('eth_onchain.csv')\n",
    "fear_greed_df = load_from_macro_data('fear_greed.csv')\n",
    "usdt_eth_mcap_df = load_from_macro_data('usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_from_macro_data('aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_from_macro_data('lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_from_macro_data('makerdao_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_from_macro_data('eth_chain_tvl.csv')\n",
    "eth_funding_df = load_from_macro_data('eth_funding_rate.csv')\n",
    "sp500_df = load_from_macro_data('SP500.csv')\n",
    "vix_df = load_from_macro_data('VIX.csv')\n",
    "gold_df = load_from_macro_data('GOLD.csv')\n",
    "dxy_df = load_from_macro_data('DXY.csv')\n",
    "\n",
    "# ============================================================================ \n",
    "# 3. 기준 날짜 설정 (Lido TVL 시작일 기준)\n",
    "# ============================================================================ \n",
    "train_start_date = pd.to_datetime('2020-12-19')\n",
    "lookback_start_date = train_start_date - timedelta(days=200)\n",
    "end_date= pd.to_datetime('2025-10-06')\n",
    "\n",
    "# ============================================================================ \n",
    "# 4. 뉴스 감성 피처 생성 \n",
    "# ============================================================================ \n",
    "def create_sentiment_features(news_df):\n",
    "    \"\"\"\n",
    "    한국어 뉴스 감성 지표 생성\n",
    "    출처: \"Cryptocurrency Price Prediction Model Based on Sentiment Analysis\" (2024)\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_agg = news_df.groupby('date').agg(\n",
    "        # ===== 기본 통계 =====\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        \n",
    "        # ===== 추가 지표 =====\n",
    "        # 1. 극단 감성 카운트\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        \n",
    "        # 2. 총 감성 점수\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    # ===== 파생 지표 계산 =====\n",
    "    \n",
    "    # 1. Sentiment Polarity \n",
    "    sentiment_agg['sentiment_polarity'] = (\n",
    "        sentiment_agg['positive_ratio'] - sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 2. Sentiment Intensity (감성 강도) \n",
    "    sentiment_agg['sentiment_intensity'] = (\n",
    "        sentiment_agg['positive_ratio'] + sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 3. Sentiment Disagreement \n",
    "    sentiment_agg['sentiment_disagreement'] = (\n",
    "        sentiment_agg['positive_ratio'] * sentiment_agg['negative_ratio']\n",
    "    )\n",
    "    \n",
    "    # 4. Bull/Bear Ratio \n",
    "    sentiment_agg['bull_bear_ratio'] = (\n",
    "        sentiment_agg['positive_ratio'] / (sentiment_agg['negative_ratio'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # 5. Weighted Sentiment \n",
    "    sentiment_agg['weighted_sentiment'] = (\n",
    "        sentiment_agg['sentiment_mean'] * np.log1p(sentiment_agg['news_count'])\n",
    "    )\n",
    "    \n",
    "    # 6. Extremity Index \n",
    "    sentiment_agg['extremity_index'] = (\n",
    "        (sentiment_agg['extreme_positive_count'] + sentiment_agg['extreme_negative_count']) / \n",
    "        (sentiment_agg['news_count'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # ===== 시계열 파생 지표 (이동 평균) =====\n",
    "    \n",
    "    for window in [3, 7, 14]:\n",
    "        # 감성 이동 평균\n",
    "        sentiment_agg[f'sentiment_ma{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # 감성 변동성 (이동 표준편차)\n",
    "        sentiment_agg[f'sentiment_volatility_{window}'] = (\n",
    "            sentiment_agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # 7. Sentiment Trend \n",
    "    sentiment_agg['sentiment_trend'] = sentiment_agg['sentiment_mean'].diff()\n",
    "    \n",
    "    # 8. Sentiment Acceleration\n",
    "    sentiment_agg['sentiment_acceleration'] = sentiment_agg['sentiment_trend'].diff()\n",
    "    \n",
    "    # 9. News Volume Change\n",
    "    sentiment_agg['news_volume_change'] = sentiment_agg['news_count'].pct_change()\n",
    "    \n",
    "    # 10. News Volume MA \n",
    "    for window in [7, 14]:\n",
    "        sentiment_agg[f'news_volume_ma{window}'] = (\n",
    "            sentiment_agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ 감성 지표 생성 완료: {sentiment_agg.shape[1] - 1}개 (date 제외)\")\n",
    "    sentiment_agg = sentiment_agg.fillna(0)\n",
    "    \n",
    "    return sentiment_agg\n",
    "\n",
    "\n",
    "sentiment_features = create_sentiment_features(news_df)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================ \n",
    "# 5. 데이터 병합\n",
    "# ============================================================================ \n",
    "def add_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + col if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=lookback_start_date, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df, sentiment_features, eth_onchain_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, eth_chain_tvl_df,\n",
    "    eth_funding_df, sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "# 1. 외부 데이터 Merge 후\n",
    "for df_to_merge in dataframes_to_merge:\n",
    "    df_merged = pd.merge(df_merged, df_to_merge, on='date', how='left')\n",
    "\n",
    "# 2. 감성 지표 결측 처리 (0)\n",
    "sentiment_cols = [col for col in df_merged.columns \n",
    "                 if any(x in col for x in ['sentiment', 'news', 'ext', 'bull_bear','positive','negative','extreme'])]\n",
    "\n",
    "print(f\"\\n감성 지표 결측 처리:\")\n",
    "for col in sentiment_cols:\n",
    "    missing_before = df_merged[col].isnull().sum()\n",
    "    if missing_before > 0:\n",
    "        df_merged[col] = df_merged[col].fillna(0)\n",
    "        print(f\"  {col}: {missing_before}개 → 0 (데이터 없음 = 중립)\")\n",
    "\n",
    "# 3. 외부 변수 FFill (bfill 절대 금지!)\n",
    "external_cols = [col for col in df_merged.columns \n",
    "                if any(x in col for x in ['eth_', 'fg_', 'usdt_', 'aave_', 'lido_', \n",
    "                                         'makerdao_', 'chain_', 'funding_',\n",
    "                                         'sp500_', 'vix_', 'gold_', 'dxy_'])]\n",
    "\n",
    "print(f\"\\n외부 변수 FFill 처리:\")\n",
    "missing_before = df_merged[external_cols].isnull().sum().sum()\n",
    "df_merged[external_cols] = df_merged[external_cols].fillna(method='ffill')\n",
    "missing_after = df_merged[external_cols].isnull().sum().sum()\n",
    "print(f\"  {missing_before:,} → {missing_after:,}개 (FFill)\")\n",
    "\n",
    "# 4. Lookback 기간 제거\n",
    "print(f\"\\nLookback 기간 제거:\")\n",
    "before = len(df_merged)\n",
    "df_merged = df_merged[df_merged['date'] >= lookback_start_date].reset_index(drop=True)\n",
    "print(f\"  {before} → {len(df_merged)}행\")\n",
    "\n",
    "remaining_missing = df_merged[external_cols].isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\n초기 결측치 처리:\")\n",
    "    print(f\"  남은 결측: {remaining_missing}개 → 0\")\n",
    "    df_merged[external_cols] = df_merged[external_cols].fillna(0)\n",
    "\n",
    "# 6. Lookback 기간 동안 모두 NaN인 컬럼 제거\n",
    "lookback_df = df_merged[df_merged['date'] < train_start_date]\n",
    "cols_to_drop = [col for col in lookback_df.columns \n",
    "               if lookback_df[col].isnull().all() and col != 'date']\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nLookback 기간 완전 결측 컬럼 제거:\")\n",
    "    print(f\"  {cols_to_drop}\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\n✓ 최종 데이터: {df_merged.shape}\")\n",
    "print(f\"  날짜: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"  결측: {df_merged.isnull().sum().sum()}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faec289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 전일대비 1.54% 하락한...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>업비트 암호화폐(가상화폐) 거래소 오전 9시 25분(한국시간) 기준으로 비트코인은 ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 단기 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>업비트 암호화폐(가상화폐) 거래소 오전 9시 50분(한국시간) 기준으로 비트코인은 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>이더리움(Ethereum) 네트워크가 '빙하기'를 늦추기 위한 긴급 하드포크 ‘뮤어...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 곰(b...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>업비트 암호화폐(가상화폐) 거래소 오전 9시 30분(한국시간) 기준으로 비트코인은 ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>패스트푸드 대기업인 버거킹이 베네수엘라 지점에서 비트코인(BTC) 결제를 지원하고 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 일시적...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>업비트 암호화폐(가상화폐) 거래소 1월 4일(한국시간) 오전 10시 20분 기준으로...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                               news  label\n",
       "0 2020-01-01  세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 전일대비 1.54% 하락한...     -1\n",
       "1 2020-01-01  업비트 암호화폐(가상화폐) 거래소 오전 9시 25분(한국시간) 기준으로 비트코인은 ...     -1\n",
       "2 2020-01-02  지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 단기 ...      0\n",
       "3 2020-01-02  업비트 암호화폐(가상화폐) 거래소 오전 9시 50분(한국시간) 기준으로 비트코인은 ...      0\n",
       "4 2020-01-03  이더리움(Ethereum) 네트워크가 '빙하기'를 늦추기 위한 긴급 하드포크 ‘뮤어...      1\n",
       "5 2020-01-03  지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 곰(b...     -1\n",
       "6 2020-01-03  업비트 암호화폐(가상화폐) 거래소 오전 9시 30분(한국시간) 기준으로 비트코인은 ...     -1\n",
       "7 2020-01-04  패스트푸드 대기업인 버거킹이 베네수엘라 지점에서 비트코인(BTC) 결제를 지원하고 ...      1\n",
       "8 2020-01-04  지난 24시간 동안 세계 최대 암호화폐인 비트코인(Bitcoin, BTC)은 일시적...      1\n",
       "9 2020-01-04  업비트 암호화폐(가상화폐) 거래소 1월 4일(한국시간) 오전 10시 20분 기준으로...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04459e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "결측치 상세 분석\n",
      "================================================================================\n",
      "\n",
      "[1] 결측치가 있는 컬럼 (10개):\n",
      "  AVAX_Open: 110개 (5.63%)\n",
      "  AVAX_High: 110개 (5.63%)\n",
      "  AVAX_Low: 110개 (5.63%)\n",
      "  AVAX_Close: 110개 (5.63%)\n",
      "  AVAX_Volume: 110개 (5.63%)\n",
      "  DOT_Open: 79개 (4.05%)\n",
      "  DOT_High: 79개 (4.05%)\n",
      "  DOT_Low: 79개 (4.05%)\n",
      "  DOT_Close: 79개 (4.05%)\n",
      "  DOT_Volume: 79개 (4.05%)\n",
      "\n",
      "[2] 결측치 발생 날짜 범위:\n",
      "\n",
      "  AVAX_Open:\n",
      "    첫 결측: 2020-06-02\n",
      "    마지막 결측: 2020-09-21\n",
      "    Lookback 기간 내: 110개\n",
      "    Train 기간 내: 0개\n",
      "\n",
      "  AVAX_High:\n",
      "    첫 결측: 2020-06-02\n",
      "    마지막 결측: 2020-09-21\n",
      "    Lookback 기간 내: 110개\n",
      "    Train 기간 내: 0개\n",
      "\n",
      "  AVAX_Low:\n",
      "    첫 결측: 2020-06-02\n",
      "    마지막 결측: 2020-09-21\n",
      "    Lookback 기간 내: 110개\n",
      "    Train 기간 내: 0개\n",
      "\n",
      "  AVAX_Close:\n",
      "    첫 결측: 2020-06-02\n",
      "    마지막 결측: 2020-09-21\n",
      "    Lookback 기간 내: 110개\n",
      "    Train 기간 내: 0개\n",
      "\n",
      "  AVAX_Volume:\n",
      "    첫 결측: 2020-06-02\n",
      "    마지막 결측: 2020-09-21\n",
      "    Lookback 기간 내: 110개\n",
      "    Train 기간 내: 0개\n",
      "\n",
      "[3] 행별 결측치 분포:\n",
      "  최대 결측 컬럼 수: 10개/행\n",
      "  결측이 있는 행: 112개\n",
      "\n",
      "  결측이 많은 상위 5개 행:\n",
      "    날짜 2020-06-02: 10개 컬럼 결측\n",
      "    날짜 2020-06-03: 10개 컬럼 결측\n",
      "    날짜 2020-06-04: 10개 컬럼 결측\n",
      "    날짜 2020-06-05: 10개 컬럼 결측\n",
      "    날짜 2020-06-06: 10개 컬럼 결측\n",
      "\n",
      "[4] Train 기간 이후 (2020-12-19~) 결측:\n",
      "  ✓ Train 기간에는 결측 없음!\n",
      "\n",
      "[5] Lookback 기간만 (2020-06-02~2020-12-19) 결측:\n",
      "  Lookback에만 945개 결측\n",
      "    AVAX_Open: 110개\n",
      "    AVAX_High: 110개\n",
      "    AVAX_Low: 110개\n",
      "    AVAX_Close: 110개\n",
      "    AVAX_Volume: 110개\n",
      "    DOT_Open: 79개\n",
      "    DOT_High: 79개\n",
      "    DOT_Low: 79개\n",
      "    DOT_Close: 79개\n",
      "    DOT_Volume: 79개\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 결측치 상세 분석\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"결측치 상세 분석\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. 컬럼별 결측치 확인\n",
    "missing_summary = df_merged.isnull().sum()\n",
    "missing_cols = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n[1] 결측치가 있는 컬럼 ({len(missing_cols)}개):\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = (count / len(df_merged)) * 100\n",
    "        print(f\"  {col}: {count}개 ({pct:.2f}%)\")\n",
    "    \n",
    "    # 2. 결측치 발생 위치 확인 (날짜별)\n",
    "    print(f\"\\n[2] 결측치 발생 날짜 범위:\")\n",
    "    \n",
    "    for col in missing_cols.index[:5]:  # 상위 5개만\n",
    "        null_dates = df_merged[df_merged[col].isnull()]['date']\n",
    "        if len(null_dates) > 0:\n",
    "            print(f\"\\n  {col}:\")\n",
    "            print(f\"    첫 결측: {null_dates.min().date()}\")\n",
    "            print(f\"    마지막 결측: {null_dates.max().date()}\")\n",
    "            \n",
    "            # Lookback 기간 내 결측인지 확인\n",
    "            lookback_nulls = null_dates[null_dates < train_start_date]\n",
    "            train_nulls = null_dates[null_dates >= train_start_date]\n",
    "            \n",
    "            print(f\"    Lookback 기간 내: {len(lookback_nulls)}개\")\n",
    "            print(f\"    Train 기간 내: {len(train_nulls)}개\")\n",
    "            \n",
    "            if len(train_nulls) > 0:\n",
    "                print(f\"    ⚠ Train 기간에 결측 존재!\")\n",
    "    \n",
    "    # 3. 행별 결측치 분포\n",
    "    print(f\"\\n[3] 행별 결측치 분포:\")\n",
    "    missing_per_row = df_merged.isnull().sum(axis=1)\n",
    "    print(f\"  최대 결측 컬럼 수: {missing_per_row.max()}개/행\")\n",
    "    print(f\"  결측이 있는 행: {(missing_per_row > 0).sum()}개\")\n",
    "    \n",
    "    # 결측이 많은 행 확인\n",
    "    if missing_per_row.max() > 0:\n",
    "        worst_rows = missing_per_row.nlargest(5)\n",
    "        print(f\"\\n  결측이 많은 상위 5개 행:\")\n",
    "        for idx, count in worst_rows.items():\n",
    "            date_val = df_merged.loc[idx, 'date']\n",
    "            print(f\"    날짜 {date_val.date()}: {count}개 컬럼 결측\")\n",
    "    \n",
    "    # 4. Train 시작 날짜 이후 결측치만 추출\n",
    "    train_period_df = df_merged[df_merged['date'] >= train_start_date]\n",
    "    train_missing = train_period_df.isnull().sum()\n",
    "    train_missing_cols = train_missing[train_missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n[4] Train 기간 이후 ({train_start_date.date()}~) 결측:\")\n",
    "    if len(train_missing_cols) > 0:\n",
    "        print(f\"  ⚠ Train 기간에 {train_missing_cols.sum()}개 결측 발견!\")\n",
    "        for col, count in train_missing_cols.items():\n",
    "            print(f\"    {col}: {count}개\")\n",
    "    else:\n",
    "        print(f\"  ✓ Train 기간에는 결측 없음!\")\n",
    "    \n",
    "    # 5. Lookback 기간만의 결측치\n",
    "    lookback_period_df = df_merged[df_merged['date'] < train_start_date]\n",
    "    lookback_missing = lookback_period_df.isnull().sum()\n",
    "    lookback_missing_cols = lookback_missing[lookback_missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n[5] Lookback 기간만 ({lookback_start_date.date()}~{train_start_date.date()}) 결측:\")\n",
    "    if len(lookback_missing_cols) > 0:\n",
    "        print(f\"  Lookback에만 {lookback_missing_cols.sum()}개 결측\")\n",
    "        for col, count in lookback_missing_cols.head(10).items():\n",
    "            print(f\"    {col}: {count}개\")\n",
    "    else:\n",
    "        print(f\"  ✓ Lookback 기간에도 결측 없음!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n✓ 결측치 없음!\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705fa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    출처: \n",
    "    - \"CryptoPulse: Short-Term Cryptocurrency Forecasting\" (2024)\n",
    "    - \"Enhancing Price Prediction in Cryptocurrency Using Transformer\" (2024)\n",
    "    - \"Bitcoin Trend Prediction with Attention-Based Deep Learning\" (2024)\n",
    "    \"\"\"\n",
    "    #print(\"\\n=== 기술적 지표 생성 중 ===\")\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== [핵심] MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (필수)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        df_ta['RSI_30'] = ta.rsi(close, length=30)\n",
    "        df_ta['RSI_200'] = ta.rsi(close, length=200)  # 장기 RSI 추가\n",
    "        \n",
    "        # MACD (필수 - top feature importance)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic Oscillator (%K, %D - 논문에서 핵심 지표)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=30, d=3)  # 30일 추가\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=200, d=3)  # 200일 추가\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (Rate of Change)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        df_ta['ROC_20'] = ta.roc(close, length=20)\n",
    "        \n",
    "        # MOM (Momentum - 다양한 기간)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        df_ta['MOM_30'] = ta.mom(close, length=30) \n",
    "        \n",
    "        # CCI (Commodity Channel Index)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_20'] = ta.cci(high, low, close, length=20)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_20'] > 100).astype(int)\n",
    "      \n",
    "        # TSI (True Strength Index)\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "\n",
    "        \n",
    "        # =====  Ichimoku Cloud (암호화폐 트렌드 분석에 효과적) =====\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ ICHIMOKU 생성 실패\")\n",
    "\n",
    "        # ===== [핵심] OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (필수! - Golden/Death Cross)\n",
    "        df_ta['SMA_10'] = ta.sma(close, length=10)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        df_ta['SMA_200'] = ta.sma(close, length=200)\n",
    "        \n",
    "        # EMA (필수!)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        df_ta['EMA_50'] = ta.ema(close, length=50)\n",
    "        df_ta['EMA_200'] = ta.ema(close, length=200) \n",
    "        \n",
    "        # TEMA (Triple EMA - 논문에서 high importance)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        df_ta['TEMA_30'] = ta.tema(close, length=30) \n",
    "        \n",
    "        # WMA (Weighted Moving Average)\n",
    "        df_ta['WMA_10'] = ta.wma(close, length=10)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)  \n",
    "        \n",
    "        # HMA (Hull Moving Average)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (Double EMA)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        \n",
    "        # VWMA (Volume Weighted)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== [핵심] VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands (필수 )\n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        safe_add(df_ta, ta.bbands, close, length=50, std=2)  \n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_7'] = ta.atr(high, low, close, length=7)\n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        df_ta['ATR_21'] = ta.atr(high, low, close, length=21) \n",
    "        \n",
    "        # NATR (Normalized ATR)\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel \n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        # ===== [핵심] VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD (Accumulation/Distribution)\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI (Money Flow Index)\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF (Chaikin Money Flow - 논문에서 중요 지표)\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Elder Force Index)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM (Ease of Movement)\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP (Volume Weighted Average Price) \n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX \n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon \n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex\n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO (Detrended Price Oscillator)\n",
    "        try:\n",
    "            df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        df_ta['PRICE_CHANGE_2'] = close.pct_change(periods=2)\n",
    "        df_ta['PRICE_CHANGE_5'] = close.pct_change(periods=5)\n",
    "        df_ta['PRICE_CHANGE_10'] = close.pct_change(periods=10) \n",
    "        \n",
    "        # 변동성 (Rolling Std)\n",
    "        df_ta['VOLATILITY_5'] = close.pct_change().rolling(window=5).std()\n",
    "        df_ta['VOLATILITY_10'] = close.pct_change().rolling(window=10).std()\n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        df_ta['VOLATILITY_30'] = close.pct_change().rolling(window=30).std() \n",
    "        \n",
    "        # 모멘텀 (Price Ratio)\n",
    "        df_ta['MOMENTUM_5'] = close / close.shift(5) - 1\n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        df_ta['MOMENTUM_20'] = close / close.shift(20) - 1\n",
    "        df_ta['MOMENTUM_30'] = close / close.shift(30) - 1  \n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA10'] = close / df_ta['SMA_10'] - 1\n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_SMA50'] = close / df_ta['SMA_50'] - 1\n",
    "        df_ta['PRICE_VS_SMA200'] = close / df_ta['SMA_200'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1 \n",
    "        df_ta['PRICE_VS_EMA26'] = close / df_ta['EMA_26'] - 1  \n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_CROSS_SIGNAL'] = (df_ta['SMA_10'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_200']).astype(int) \n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)  \n",
    "        \n",
    "        # Range 지표\n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치 \n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)  \n",
    "        \n",
    "        # Linear Regression Slope\n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "            df_ta['SLOPE_10'] = ta.linreg(close, length=10, slope=True)\n",
    "            df_ta['LINREG_14'] = ta.linreg(close, length=14)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "            df_ta['SLOPE_10'] = close.rolling(window=10).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 10 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing/Decreasing 신호\n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        df_ta['DEC_1'] = (close < close.shift(1)).astype(int)\n",
    "        df_ta['INC_3'] = (close > close.shift(3)).astype(int)\n",
    "        df_ta['INC_5'] = (close > close.shift(5)).astype(int)  \n",
    "        \n",
    "        # BOP \n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 관련 파생\n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        else:\n",
    "            print(f\"    ⚠ Bollinger Bands 컬럼 미발견\")\n",
    "        \n",
    "        # RSI 파생 (Overbought/Oversold)\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile (상대적 거래량 강도)\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration (2차 미분)\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (시가-전일종가)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        # Fibonacci Pivots \n",
    "        high_20 = high.rolling(20).max()\n",
    "        low_20 = low.rolling(20).min()\n",
    "        diff = high_20 - low_20\n",
    "        \n",
    "        df_ta['FIB_0'] = high_20\n",
    "        df_ta['FIB_236'] = high_20 - 0.236 * diff\n",
    "        df_ta['FIB_382'] = high_20 - 0.382 * diff\n",
    "        df_ta['FIB_500'] = high_20 - 0.500 * diff\n",
    "        df_ta['FIB_618'] = high_20 - 0.618 * diff\n",
    "        df_ta['FIB_1'] = low_20\n",
    "        \n",
    "        #Directional Change Events \n",
    "        df_ta['DC_EVENT'] = 0\n",
    "        df_ta['DC_TYPE'] = 0\n",
    "        \n",
    "        threshold = 0.05\n",
    "        last_extreme = close.iloc[0]\n",
    "        last_type = 0\n",
    "        \n",
    "        for i in range(1, len(df_ta)):\n",
    "            price = close.iloc[i]\n",
    "            change = (price - last_extreme) / last_extreme\n",
    "            \n",
    "            if last_type <= 0 and change >= threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = 1\n",
    "                last_extreme = price\n",
    "                last_type = 1\n",
    "            elif last_type >= 0 and change <= -threshold:\n",
    "                df_ta.loc[df_ta.index[i], 'DC_EVENT'] = 1\n",
    "                df_ta.loc[df_ta.index[i], 'DC_TYPE'] = -1\n",
    "                last_extreme = price\n",
    "                last_type = -1\n",
    "        \n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = (\n",
    "        df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "    )\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        if 'btc_log_return' not in df_transformed.columns:\n",
    "            df_transformed['btc_log_return'] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(1))\n",
    "        for period in [5, 10, 20, 30]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['btc_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['btc_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = (\n",
    "                    df_transformed[col_name].rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                    btc_volume.pct_change()\n",
    "                )\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (\n",
    "                eth_volume / (btc_volume + 1e-8)\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [\n",
    "        col for col in df_transformed.columns\n",
    "        if 'return' in col.lower() and 'next' not in col\n",
    "    ]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36c7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. Lag 적용\n",
    "# ============================================================================\n",
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    \"\"\"\n",
    "    Lag 피처 적용 (원본 유지 + lag 추가)\n",
    "    \n",
    "    핵심 원칙:\n",
    "    1. 원본(lag0) 피처는 그대로 유지\n",
    "    2. lag1, lag2 피처를 추가로 생성\n",
    "    3. 이동평균/차분은 lag 불필요 (이미 과거 참조)\n",
    "    4. 이벤트는 lag 없음 (당일 반영)\n",
    "    \n",
    "    출처: \"Seeing Beyond Noise\" (2024), scikit-learn\n",
    "    \"\"\"\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    # ===== Lag 적용 대상: 원본 감성 지표만 =====\n",
    "    raw_sentiment_cols = [\n",
    "        'sentiment_mean', 'sentiment_std', 'sentiment_sum',\n",
    "        'news_count', 'positive_ratio', 'negative_ratio',\n",
    "        'sentiment_polarity', 'sentiment_intensity', \n",
    "        'sentiment_disagreement', 'bull_bear_ratio',\n",
    "        'weighted_sentiment', 'extremity_index',\n",
    "        'extreme_positive_count', 'extreme_negative_count'\n",
    "    ]\n",
    "    \n",
    "    # ===== Lag 제외: 이동평균, 차분 (이미 과거 참조) =====\n",
    "    no_lag_patterns = [\n",
    "        '_ma', '_volatility_', '_trend', '_acceleration', \n",
    "        '_volume_change', '_volume_ma'\n",
    "    ]\n",
    "    \n",
    "    # ===== 온체인 데이터 =====\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', \n",
    "                                  'eth_large', 'eth_token', 'eth_contract',\n",
    "                                  'eth_avg_gas', 'eth_total_gas', \n",
    "                                  'eth_avg_block'])]\n",
    "    \n",
    "    # ===== 기타 외부 변수 =====\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    # ===== 제외 컬럼 =====\n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open','date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col])\n",
    "    exclude_cols.extend([col for col in df.columns if '_lag' in col])\n",
    "    \n",
    "    lag_count = 0\n",
    "    \n",
    "    # ===== 1. 원본 감성 지표에만 lag 적용 =====\n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            is_derived = any(pattern in col for pattern in no_lag_patterns)\n",
    "            \n",
    "            if not is_derived:\n",
    "                for lag in range(1, news_lag):\n",
    "                    new_col = f\"{col}_lag{lag}\"\n",
    "                    df_lagged[new_col] = df[col].shift(lag)\n",
    "                    lag_count += 1\n",
    "    \n",
    "    # ===== 2. 온체인 lag =====\n",
    "    onchain_lag_count = 0\n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            onchain_lag_count += 1\n",
    "    \n",
    "    # ===== 3. 기타 외부 변수 lag  =====\n",
    "    other_lag_count = 0\n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            other_lag_count += 1\n",
    "    \n",
    "    total_lag = lag_count + onchain_lag_count + other_lag_count\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    \"\"\"\n",
    "    과거 가격을 피처로 추가 \n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    # 과거 종가 \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    # 과거 고가/저가\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "    \n",
    "    # 과거 거래량\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "    \n",
    "    # 과거 수익률\n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    # 과거 가격 비율\n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    added = df_new.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 타겟 변수 생성\n",
    "# ============================================================================\n",
    "\n",
    "def create_targets(df):\n",
    "    \"\"\"타겟 변수 생성\"\"\"\n",
    "    df_target = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "\n",
    "    # 내일 종가\n",
    "    next_close = close.shift(-1)\n",
    "    \n",
    "    # 오늘 → 내일 로그 수익률\n",
    "    df_target['next_log_return'] = np.log(next_close / close)\n",
    "    \n",
    "    # 오늘 → 내일 방향성\n",
    "    df_target['next_direction'] = (next_close > close).astype(int)\n",
    "    \n",
    "    # 내일 실제 종가\n",
    "    df_target['next_close'] = next_close   \n",
    "    \n",
    "    return df_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ed9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_cyclic_features(df):\n",
    "    \"\"\"\n",
    "    시간 주기성 특징 추가 \n",
    "    \n",
    "    Reference:\n",
    "    - \"The Importance of Time-Based Cyclic Features\" (2025)\n",
    "    - \"Feature engineering for time-series data\" (Statsig, 2025)\n",
    "    \"\"\"\n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    # 기본 시간 특징\n",
    "    df_temporal['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df_temporal['day_of_month'] = df['date'].dt.day\n",
    "    df_temporal['month'] = df['date'].dt.month\n",
    "    df_temporal['quarter'] = df['date'].dt.quarter\n",
    "    df_temporal['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # 월말/월초 효과 \n",
    "    df_temporal['is_month_start'] = (df['date'].dt.is_month_start).astype(int)\n",
    "    df_temporal['is_month_end'] = (df['date'].dt.is_month_end).astype(int)\n",
    "    df_temporal['is_quarter_start'] = (df['date'].dt.is_quarter_start).astype(int)\n",
    "    df_temporal['is_quarter_end'] = (df['date'].dt.is_quarter_end).astype(int)\n",
    "    \n",
    "    # 주말 효과 \n",
    "    df_temporal['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical Encoding (Sine/Cosine for periodicity)\n",
    "    df_temporal['day_of_week_sin'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['day_of_week_cos'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['month_sin'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['month_cos'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['day_of_month_sin'] = np.sin(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    df_temporal['day_of_month_cos'] = np.cos(2 * np.pi * df_temporal['day_of_month'] / 31)\n",
    "    \n",
    "    added = df_temporal.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    \"\"\"\n",
    "    고차원 상호작용 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): Momentum × Volatility 상호작용 중요\n",
    "    - \"Causal Feature Engineering\" (2023): 특징 조합이 단일 특징보다 예측력 높음\n",
    "    \"\"\"\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    # 1. RSI × Volume\n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 2. Bollinger Band Position × Sentiment\n",
    "    if 'BB_POSITION' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['BB_Sentiment_Consensus'] = df['BB_POSITION'] * df['sentiment_polarity']\n",
    "    \n",
    "    # 3. VIX × ETH Volatility\n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    # 4. MACD × Volume\n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    # 5. BTC Return × ETH-BTC Correlation\n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    # 6. Sentiment × News Volume\n",
    "    if 'sentiment_polarity' in df.columns and 'news_count' in df.columns:\n",
    "        df_interact['Sentiment_Volume_Intensity'] = df['sentiment_polarity'] * np.log1p(df['news_count'])\n",
    "    \n",
    "    # 7. ATR × Volume Ratio\n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    # 8. RSI Overbought × High Volume\n",
    "    if 'RSI_OVERBOUGHT' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Overbought_High_Volume'] = df['RSI_OVERBOUGHT'] * (df['VOLUME_RATIO'] > 1.5).astype(int)\n",
    "    \n",
    "    # 9. Golden Cross × Positive Sentiment\n",
    "    if 'SMA_GOLDEN_CROSS' in df.columns and 'sentiment_polarity' in df.columns:\n",
    "        df_interact['Golden_Sentiment_Align'] = df['SMA_GOLDEN_CROSS'] * (df['sentiment_polarity'] > 0).astype(int)\n",
    "    \n",
    "    # 10. Price Acceleration × Momentum\n",
    "    if 'PRICE_ACCELERATION' in df.columns and 'MOMENTUM_10' in df.columns:\n",
    "        df_interact['Acceleration_Momentum'] = df['PRICE_ACCELERATION'] * df['MOMENTUM_10']\n",
    "    \n",
    "    added = df_interact.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    \"\"\"\n",
    "    변동성 체제 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Intraday trading of cryptocurrencies\" (2023): 변동성 체제별 예측 정확도 차이 존재\n",
    "\n",
    "    \"\"\"\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        # 1. 고변동성 vs 저변동성 \n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        # 2. 변동성 급증 이벤트\n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        # 3. 변동성 백분위수\n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        # 4. 변동성 추세\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        \n",
    "        # 5. 변동성 체제 지속기간\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    added = df_regime.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    \"\"\"\n",
    "    정규화된 가격 Lag 특징 추가 (분류 모델용)\n",
    "    \n",
    "    Reference:\n",
    "    - \"Financial Forecasting with ML: Price vs Return\" (2021)\n",
    "    - 분류 문제에서 절대 가격보다 비율이 2-3배 더 예측력 높음\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        current_close = df['ETH_Close']\n",
    "    else:\n",
    "        return df_norm\n",
    "    \n",
    "    # 1. 가격 Lag를 현재 가격 대비 비율로 변환\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    # 2. High/Low Lag를 Close 대비 비율\n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        \n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    added = df_norm.shape[1] - df.shape[1]\n",
    "\n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_cumulative_streak_features(df):\n",
    "    \"\"\"\n",
    "    누적 및 연속 패턴 특징 추가\n",
    "    \n",
    "    Reference:\n",
    "    - \"Feature engineering for time-series\" (2025): 연속 패턴은 모멘텀 지속성 예측에 핵심\n",
    "    \"\"\"\n",
    "    df_cum = df.copy()\n",
    "    \n",
    "    if 'eth_log_return' in df.columns:\n",
    "        returns = df['eth_log_return']\n",
    "        \n",
    "        # 1. 연속 상승 일수\n",
    "        df_cum['consecutive_up_days'] = (returns > 0).astype(int).groupby(\n",
    "            (returns <= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 2. 연속 하락 일수\n",
    "        df_cum['consecutive_down_days'] = (returns < 0).astype(int).groupby(\n",
    "            (returns >= 0).cumsum()\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 3. 최근 20일 내 최대 연속 상승\n",
    "        df_cum['max_consecutive_up_20d'] = df_cum['consecutive_up_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 4. 최근 20일 내 최대 연속 하락\n",
    "        df_cum['max_consecutive_down_20d'] = df_cum['consecutive_down_days'].rolling(20, min_periods=5).max()\n",
    "        \n",
    "        # 5. 누적 수익률 (20일)\n",
    "        df_cum['cumulative_return_20d'] = returns.rolling(20, min_periods=5).sum()\n",
    "        \n",
    "        # 6. 상승/하락 비율 (20일 내)\n",
    "        df_cum['up_down_ratio_20d'] = (\n",
    "            (returns > 0).rolling(20, min_periods=5).sum() / \n",
    "            ((returns < 0).rolling(20, min_periods=5).sum() + 1e-8)\n",
    "        )\n",
    "\n",
    "    added = df_cum.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_cum\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    \"\"\"\n",
    "\n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy\" (2025): 백분위수 특징이 상대적 위치 파악에 효과적\n",
    "    \"\"\"\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    # 1. 가격 백분위수 (250일)\n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 2. 거래량 백분위수 (90일)\n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    # 3. RSI 백분위수 (60일)\n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    added = df_pct.shape[1] - df.shape[1]\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65936a28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=40):\n",
    "    \"\"\"\n",
    "    Multi-Target Feature Selection\n",
    "    \n",
    "    5가지 케이스별 최적화된 feature selection:\n",
    "    1. direction (분류)\n",
    "    2. return (회귀)  \n",
    "    3. price (회귀)\n",
    "    4. direction_return (혼합)\n",
    "    5. direction_price (혼합)\n",
    "    \n",
    "    Reference:\n",
    "    - \"Multi-target HSIC-Lasso\" (2024)\n",
    "    - \"Feature selection for multi-target regression\" (2021)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if target_type == 'direction':\n",
    "        # 순수 분류\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'return':\n",
    "        # 순수 회귀 (수익률)\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'price':\n",
    "        # 순수 회귀 (가격)\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "    elif target_type == 'direction_return':\n",
    "        # 혼합: 분류 + 회귀 (방향 + 수익률)\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Return (50%)\")\n",
    "        \n",
    "        # 각각 절반씩 선택\n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        ret_features, ret_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_log_return'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 합집합으로 결합 (중복 제거)\n",
    "        selected = list(dict.fromkeys(dir_features + ret_features))\n",
    "        \n",
    "        # 부족하면 MI 스코어 높은 순으로 추가\n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **ret_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        # 너무 많으면 자르기\n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'ret_stats': ret_stats,\n",
    "            'overlap': len(set(dir_features) & set(ret_features))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    elif target_type == 'direction_price':\n",
    "        # 혼합: 분류 + 회귀 (방향 + 가격)\n",
    "        print(\"\\n[Hybrid] Direction (50%) + Price (50%)\")\n",
    "        \n",
    "        dir_features, dir_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        price_features, price_stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_close'], \n",
    "            task='reg', \n",
    "            top_n=top_n // 2,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        selected = list(dict.fromkeys(dir_features + price_features))\n",
    "        \n",
    "        if len(selected) < top_n:\n",
    "            all_mi_scores = {**dir_stats['mi_scores'], **price_stats['mi_scores']}\n",
    "            sorted_features = sorted(all_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for feat, _ in sorted_features:\n",
    "                if feat not in selected:\n",
    "                    selected.append(feat)\n",
    "                    if len(selected) >= top_n:\n",
    "                        break\n",
    "        \n",
    "        selected = selected[:top_n]\n",
    "        \n",
    "        stats = {\n",
    "            'dir_stats': dir_stats,\n",
    "            'price_stats': price_stats,\n",
    "            'overlap': len(set(dir_features) & set(price_features))\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target_type: {target_type}\")\n",
    "    \n",
    "    print(\"선택된 지표들\")\n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=40, verbose=True):\n",
    "    \"\"\"\n",
    "    검증된 Feature Selection 방법 (2025 연구 기반)\n",
    "    \n",
    "    핵심 원칙:\n",
    "    1. 하이퍼파라미터 튜닝 없이 기본 파라미터 사용\n",
    "    2. MI + RFE + RF Importance 앙상블\n",
    "    3. 빠른 실행 속도\n",
    "    \n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy in Cryptocurrency Markets\" (2025)\n",
    "    - \"Feature Selection After Split\" (Reddit, 2022)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    \n",
    "    # 기본 파라미터만 사용 \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,  # 10%씩 제거\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    \n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 전체 파이프라인 \n",
    "# ============================================================================\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                     method='tvt', target_type='direction', **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    1. Feature Engineering (전체 데이터)\n",
    "    2. Target 생성 (전체 데이터)  \n",
    "    3. Train/Val/Test Split\n",
    "    4. Missing Value Handling \n",
    "    5. Feature Selection \n",
    "    6. Scaling (Train에서만 Fit)\n",
    "    \n",
    "    Reference:\n",
    "    - \"Feature Selection After Split\" (Stack Overflow, 2019)\n",
    "    - \"Scaling After Feature Selection\" (Reddit, 2023)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Target 생성 \n",
    "    df = create_targets(df)\n",
    "    \n",
    "    # 과거 가격 Lag\n",
    "    df = add_price_lag_features_first(df)\n",
    "    \n",
    "    # 기술적 지표\n",
    "    df = calculate_technical_indicators(df)\n",
    "    \n",
    "    # 시간 주기성\n",
    "    df = add_temporal_cyclic_features(df)\n",
    "    \n",
    "    # BTC-ETH 교차 특징\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    \n",
    "    # 변동성 체제\n",
    "    df = add_volatility_regime_features(df)\n",
    "    \n",
    "    # 상호작용 특징\n",
    "    df = add_interaction_features(df)\n",
    "    \n",
    "    # 누적/연속 특징\n",
    "    df = add_cumulative_streak_features(df)\n",
    "    \n",
    "    # 백분위수 특징\n",
    "    df = add_percentile_features(df)\n",
    "    \n",
    "    # 정규화 가격 Lag\n",
    "    df = add_normalized_price_lags(df)\n",
    "    \n",
    "    # Raw 가격 제거\n",
    "    df = remove_raw_prices_and_transform(df)\n",
    "    \n",
    "    # Lag 적용 (감성, 온체인)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "\n",
    "\n",
    "    # 1. 원본 VIX 확인\n",
    "    if 'vix_VIX' in df.columns:\n",
    "        vix_missing = df['vix_VIX'].isnull().sum()\n",
    "\n",
    "    # 2. VOLATILITY_20 확인\n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_missing = df['VOLATILITY_20'].isnull().sum()\n",
    "\n",
    "    # 3. 상호작용 특징 확인\n",
    "    if 'VIX_ETH_Vol_Cross' in df.columns:\n",
    "        cross_missing = df['VIX_ETH_Vol_Cross'].isnull().sum()\n",
    "\n",
    "\n",
    "    # 4. Lag 적용 후 확인\n",
    "    if 'VIX_ETH_Vol_Cross_lag1' in df.columns:\n",
    "        cross_lag_missing = df['VIX_ETH_Vol_Cross_lag1'].isnull().sum()\n",
    "\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    column_list = df.columns.tolist()\n",
    "    df = df.iloc[:-1]  \n",
    "    split_kwargs = {}\n",
    "    if method == 'tvt':\n",
    "        if 'train_ratio' in kwargs:\n",
    "            split_kwargs['train_ratio'] = kwargs['train_ratio']\n",
    "        if 'val_ratio' in kwargs:\n",
    "            split_kwargs['val_ratio'] = kwargs['val_ratio']\n",
    "        splits = split_tvt_method(df, train_start_date, **split_kwargs)\n",
    "    elif method == 'walk_forward':\n",
    "        if 'n_splits' in kwargs:\n",
    "            split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "        if 'initial_train_size' in kwargs:\n",
    "            split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "        if 'test_size' in kwargs:\n",
    "            split_kwargs['test_size'] = kwargs['test_size']\n",
    "        splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # PHASE 3: 각 Split에 대해 Missing/Selection/Scaling 수행\n",
    "    # ===================================================================\n",
    "\n",
    "    \n",
    "    if method == 'tvt':\n",
    "            result = process_single_split(\n",
    "        splits, \n",
    "        target_type=target_type,  \n",
    "        top_n=40                 \n",
    "        )\n",
    "    else:\n",
    "            result = [\n",
    "        process_single_split(\n",
    "            fold, \n",
    "            target_type=target_type,  \n",
    "            top_n=40,\n",
    "            fold_idx=i+1\n",
    "        ) \n",
    "        for i, fold in enumerate(splits)\n",
    "        ]\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Split 함수들 \n",
    "# ============================================================================\n",
    "\n",
    "def split_tvt_method(df, train_start_date, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"TVT 분할 (결측치 처리 X, 단순 분할만)\"\"\"\n",
    "    \n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    \n",
    "    n = len(df_period)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df_period.iloc[:train_end].copy()\n",
    "    val_df = df_period.iloc[train_end:val_end].copy()\n",
    "    test_df = df_period.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)} ({train_df['date'].min().date()} ~ {train_df['date'].max().date()})\")\n",
    "    print(f\"  Val:   {len(val_df)} ({val_df['date'].min().date()} ~ {val_df['date'].max().date()})\")\n",
    "    print(f\"  Test:  {len(test_df)} ({test_df['date'].min().date()} ~ {test_df['date'].max().date()})\")\n",
    "    \n",
    "    return {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, \n",
    "                              n_splits=None,\n",
    "                              initial_train_size=600, \n",
    "                              val_size=60,      \n",
    "                              test_size=60,\n",
    "                              lookback=30):     \n",
    "    \"\"\"\n",
    "    Walk-Forward 분할 (Anchored/Expanding Window)\n",
    "    \n",
    "    설정:\n",
    "    - Initial Train: 600일\n",
    "    - Val: 60일\n",
    "    - Test: 60일\n",
    "    - Step: 60일\n",
    "    - n_splits: None이면 데이터 최대 활용하여 자동 계산\n",
    "    \n",
    "    Reference:\n",
    "    - \"Optimizing Forecast Accuracy in Cryptocurrency\" (2025)\n",
    "    - Anchored Window: Train이 매 Fold마다 확장\n",
    "    \"\"\"\n",
    "    \n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    step = 60\n",
    "    \n",
    "    if n_splits is None:\n",
    "        total_data = len(df_period)\n",
    "        min_required = initial_train_size + val_size + test_size\n",
    "        remaining = total_data - min_required\n",
    "        n_splits = (remaining // step) + 1\n",
    "        print(f\"Auto-calculated n_splits: {n_splits} (from {total_data} days)\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Walk-Forward Configuration\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total data: {len(df_period)} days\")\n",
    "    print(f\"Train={initial_train_size}d, Val={val_size}d, Test={test_size}d, Step={step}d\")\n",
    "    print(f\"Lookback={lookback}d, Val sequences: {val_size - lookback}\")\n",
    "    print(f\"Target folds: {n_splits}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        train_end_idx = initial_train_size + (fold_idx * step)\n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + val_size\n",
    "        test_start_idx = val_end_idx\n",
    "        test_end_idx = test_start_idx + test_size\n",
    "        \n",
    "        if test_end_idx > len(df_period):\n",
    "            print(f\"Insufficient data: Fold {fold_idx+1} stopped (need {test_end_idx}, have {len(df_period)})\")\n",
    "            break\n",
    "        \n",
    "        train_fold = df_period.iloc[:train_end_idx].copy()\n",
    "        val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        assert train_fold['date'].max() < val_fold['date'].min(), \"Train/Val overlap detected!\"\n",
    "        assert val_fold['date'].max() < test_fold['date'].min(), \"Val/Test overlap detected!\"\n",
    "        \n",
    "        print(f\"Fold {fold_idx + 1:2d}:\")\n",
    "        print(f\"  Train: {len(train_fold):4d}d  ({train_fold['date'].min().date()} ~ {train_fold['date'].max().date()})\")\n",
    "        print(f\"  Val:   {len(val_fold):4d}d  ({val_fold['date'].min().date()} ~ {val_fold['date'].max().date()})\")\n",
    "        print(f\"  Test:  {len(test_fold):4d}d  ({test_fold['date'].min().date()} ~ {test_fold['date'].max().date()})\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Summary: {len(folds)} folds generated\")\n",
    "    print(f\"Total test days: {len(folds) * test_size}\")\n",
    "    print(f\"Test coverage: {folds[0]['test']['date'].min().date()} ~ {folds[-1]['test']['date'].max().date()}\")\n",
    "    print(f\"Data utilization: {(test_end_idx/len(df_period)*100):.1f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 핵심: 각 Split 처리 \n",
    "# ============================================================================\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "    \"\"\"\n",
    "    개선된 전처리 파이프라인\n",
    "    \n",
    "    변경사항:\n",
    "    1. GridSearchCV 제거 (feature selection 단계에서)\n",
    "    2. 검증된 MI+RFE+RF 앙상블 사용\n",
    "    3. Multi-target 지원\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    \n",
    "    # ===== 1. 결측치 처리 =====\n",
    "    \n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(),\n",
    "        train_start_date=train_df['date'].min(),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(),\n",
    "        train_start_date=val_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(),\n",
    "        train_start_date=test_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close']\n",
    "    \n",
    "    train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    \n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        target_type=target_type, \n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    # ===== 6. 결과 패키징 =====\n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_df['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),  \n",
    "            'dates': test_df['date'].reset_index(drop=True)  \n",
    "        },\n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7d2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7c4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 논문 기반 암호화폐 예측 모델 \n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional, Conv1D, MaxPooling1D, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, GRU, Dense, Dropout, Bidirectional, Conv1D, MaxPooling1D, \n",
    "    Flatten, Input, Concatenate, BatchNormalization, Attention, Add,\n",
    "    MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D,\n",
    "    ConvLSTM2D, Reshape, TimeDistributed, RepeatVector\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DirectionModels (15개 모델)\n",
    "# ============================================================================\n",
    "\n",
    "class DirectionModels:\n",
    "    \"\"\"방향 예측 모델 (15개 논문 기반)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train):\n",
    "        \"\"\"1. Random Forest - 과적합 방지\"\"\"\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=4, max_features='sqrt', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        2. LightGBM\n",
    "        Reference: \"Cryptocurrency Price Prediction\" (IEEE 2024)\n",
    "        \"\"\"\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            min_child_samples=20, random_state=42, verbose=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                 callbacks=[early_stopping(50, verbose=False)])\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        3. XGBoost\n",
    "        Reference: \"Cryptocurrency Value Prediction with Boosting\" (2022)\n",
    "        \"\"\"\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "            min_child_weight=3, gamma=0.1, random_state=42, eval_metric='logloss'\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def svm(X_train, y_train):\n",
    "        \"\"\"\n",
    "        4. Support Vector Classifier\n",
    "        Reference: \"Support Vector Regression to Improve ETH Prediction\" (2025)\n",
    "        R²: 0.9985\n",
    "        \"\"\"\n",
    "        model = SVC(\n",
    "            kernel='rbf', C=100, gamma='scale', random_state=42, probability=True\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        5. LSTM\n",
    "        Reference: \"AI-based model for cryptocurrency prediction\" (2025)\n",
    "        R²: 97.44% (Ethereum)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, activation='tanh', return_sequences=True, \n",
    "                 input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        6. Bidirectional LSTM\n",
    "        Reference: \"Predicting Bitcoin Prices Using Deep Learning\" (2025)\n",
    "        R²: 0.98, MSE: 0.001183\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)), \n",
    "                         input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        7. GRU\n",
    "        Reference: \"Comparative Analysis of LSTM and GRU for ETH\" (2025)\n",
    "        RMSE: 0.0234, R²: 0.9442\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            GRU(128, activation='tanh', return_sequences=True, \n",
    "                input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacked_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        8. Stacked LSTM (3 layers)\n",
    "        Reference: \"LSTM-Driven Cryptocurrency Forecasting\" (2024)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        9. CNN-LSTM\n",
    "        Reference: \"Application of CNN-BiLSTM for ETH\" (2025)\n",
    "        MAPE: 2.8546%, R²: 0.9415\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        10. CNN-GRU\n",
    "        Reference: \"Deep Learning Algorithms for Crypto\" (ACM 2024)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            GRU(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        11. CNN-BiLSTM\n",
    "        Reference: \"Application of CNN-BiLSTM for ETH\" (2025)\n",
    "        MAPE: 2.8546%, R²: 0.9415\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm_attention(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        12. LSTM with Self-Attention\n",
    "        Reference: \"Optimized EWT-Seq2Seq-LSTM with Attention\" (2023)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "        lstm_out = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "        attention = Attention()([lstm_out, lstm_out])\n",
    "        combined = Add()([lstm_out, attention])\n",
    "        pooled = GlobalAveragePooling1D()(combined)\n",
    "        dense = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(pooled)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        13. Transformer\n",
    "        Reference: \"Transformer-based approach for ETH\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        attn_output = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "        ff = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        ff = Dropout(0.1)(ff)\n",
    "        ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tcn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        14. Temporal Convolutional Network (TCN)\n",
    "        Reference: \"Utilising TCN for Cryptocurrency Forecasting\" (2024)\n",
    "        Superior to LSTM with lower complexity\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        # TCN with dilated causal convolutions\n",
    "        x = inputs\n",
    "        for dilation_rate in [1, 2, 4, 8]:\n",
    "            conv = Conv1D(64, 3, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "            x = Add()([x, conv]) if x.shape[-1] == 64 else conv\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def dtw_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        15. DTW-LSTM (Dynamic Time Warping + LSTM)\n",
    "        Reference: \"Application of DTW on ETH Prediction\" (2024)\n",
    "        23.4% better than baseline LSTM\n",
    "        \"\"\"\n",
    "        # DTW-enhanced LSTM (simplified)\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, \n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def tabnet(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        16. TabNet Classifier\n",
    "        Reference: \"TabNet: Attentive Interpretable Tabular Learning\" (2019)\n",
    "        암호화폐 예측에서 특징 중요도 해석 가능 (2024)\n",
    "        \"\"\"\n",
    "        model = TabNetClassifier(\n",
    "            n_d=64, n_a=64, n_steps=5,\n",
    "            gamma=1.5, n_independent=2, n_shared=2,\n",
    "            lambda_sparse=1e-4, momentum=0.3,\n",
    "            mask_type='entmax', optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            verbose=0, seed=42\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            max_epochs=100, patience=20,\n",
    "            batch_size=256, virtual_batch_size=128\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def informer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        17. Informer (Efficient Transformer)\n",
    "        Reference: \"Informer in Algorithmic Investment on Bitcoin\" (2025)\n",
    "        비트코인 고빈도 데이터에서 GMADL loss로 우수한 성과\n",
    "        R²: 0.98+, 5분봉 데이터에서 buy-hold 전략 능가\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # ProbSparse Self-Attention (simplified)\n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            # Multi-head attention with reduced complexity\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            # Distilling layer (halving)\n",
    "            x = Conv1D(input_shape[1], 1, activation='relu', \n",
    "                      kernel_regularizer=l2(0.01))(x)\n",
    "            x = MaxPooling1D(2, padding='same')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def nbeats(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        18. N-BEATS (Neural Basis Expansion Analysis)\n",
    "        Reference: \"Bitcoin Price Prediction Using N-BEATS\" (2025)\n",
    "        R²: 0.9998, MAE: 0.00240 - 비트코인 예측에서 LSTM 능가\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Stack 1: Trend\n",
    "        x1 = Flatten()(inputs)\n",
    "        for _ in range(4):\n",
    "            x1 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x1)\n",
    "            x1 = BatchNormalization()(x1)\n",
    "            x1 = Dropout(0.2)(x1)\n",
    "        trend_forecast = Dense(64, activation='linear', kernel_regularizer=l2(0.01))(x1)\n",
    "        \n",
    "        # Stack 2: Seasonality\n",
    "        x2 = Flatten()(inputs)\n",
    "        for _ in range(4):\n",
    "            x2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x2)\n",
    "            x2 = BatchNormalization()(x2)\n",
    "            x2 = Dropout(0.2)(x2)\n",
    "        season_forecast = Dense(64, activation='linear', kernel_regularizer=l2(0.01))(x2)\n",
    "        \n",
    "        # Ensemble forecasts\n",
    "        combined = Add()([trend_forecast, season_forecast])\n",
    "        combined = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "        combined = Dropout(0.3)(combined)\n",
    "        outputs = Dense(1, activation='sigmoid')(combined)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def temporal_fusion_transformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        19. Temporal Fusion Transformer (TFT)\n",
    "        Reference: \"Adaptive TFT for Cryptocurrency Prediction\" (2025)\n",
    "        ETH-USDT 10분봉에서 LSTM 대비 큰 성능 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Variable Selection Network\n",
    "        x = Flatten()(inputs)\n",
    "        var_weights = Dense(input_shape[0] * input_shape[1], activation='softmax',\n",
    "                           kernel_regularizer=l2(0.01))(x)\n",
    "        var_weights = Reshape(input_shape)(var_weights)\n",
    "        selected = Multiply()([inputs, var_weights])\n",
    "        \n",
    "        # LSTM processing\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(selected)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Output\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def performer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Projection layer to match dimensions\n",
    "        x = Dense(128, kernel_regularizer=l2(0.01))(inputs)  \n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        # Performer blocks\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn) \n",
    "\n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def patchtst(X_train, y_train, X_val, y_val, input_shape, patch_len=16, stride=8):\n",
    "        \"\"\"\n",
    "        21. PatchTST (Patch Time Series Transformer)\n",
    "        Reference: \"Neural Foundations of Crypto Predictions\" (2024)\n",
    "\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Patching: 시계열을 패치로 분할\n",
    "        x = inputs\n",
    "        num_patches = (input_shape[0] - patch_len) // stride + 1\n",
    "        \n",
    "        # Patch embedding\n",
    "        patches = []\n",
    "        for i in range(0, input_shape[0] - patch_len + 1, stride):\n",
    "            patch = Lambda(lambda z: z[:, i:i+patch_len, :])(x)\n",
    "            patch = Flatten()(patch)\n",
    "            patch = Dense(128, kernel_regularizer=l2(0.01))(patch)\n",
    "            patches.append(patch)\n",
    "        \n",
    "        if len(patches) > 1:\n",
    "            x = tf.stack(patches, axis=1)\n",
    "        else:\n",
    "            x = tf.expand_dims(patches[0], axis=1)\n",
    "        \n",
    "        # Transformer encoder with channel independence\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        # Prediction head\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def autoformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        22. Autoformer (Decomposition Transformer with Auto-Correlation)\n",
    "        Reference: \"Autoformer: Decomposition Transformers\" (2021)\n",
    "\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Series Decomposition\n",
    "        x = inputs\n",
    "        # Moving average decomposition (trend)\n",
    "        trend = tf.keras.layers.AveragePooling1D(pool_size=25, strides=1, \n",
    "                                                  padding='same')(x)\n",
    "        seasonal = tf.subtract(x, trend)\n",
    "        \n",
    "        # Auto-Correlation mechanism on seasonal component\n",
    "        x = seasonal\n",
    "        for _ in range(2):\n",
    "            # Auto-correlation (simplified with correlation-based attention)\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            # Feed-forward with decomposition\n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        # Combine seasonal and trend\n",
    "        seasonal_out = GlobalAveragePooling1D()(x)\n",
    "        trend_out = GlobalAveragePooling1D()(trend)\n",
    "        combined = Concatenate()([seasonal_out, trend_out])\n",
    "        \n",
    "        combined = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "        combined = BatchNormalization()(combined)\n",
    "        combined = Dropout(0.3)(combined)\n",
    "        outputs = Dense(1, activation='sigmoid')(combined)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def itransformer(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        23. iTransformer (Inverted Transformer)\n",
    "        Reference: \"Neural Foundations of Crypto Predictions\" (2024)\n",
    "\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Inverted: 변수를 토큰으로, 시간을 임베딩 차원으로\n",
    "        # (batch, time, features) -> (batch, features, time)\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        \n",
    "        # Embed each variate (now treated as tokens)\n",
    "        x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Transformer blocks on variate dimension\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=16, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(64, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        # Aggregate\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def ethervoyant(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        24. EtherVoyant (Specialized Ethereum Forecasting Model)\n",
    "        Reference: \"Empowering Global Ethereum Price Prediction\" (2024)\n",
    "\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Multi-scale CNN feature extraction\n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "        \n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        x = Concatenate()([conv1, conv2])\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        \n",
    "        # Bidirectional LSTM for temporal dependencies\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, \n",
    "                              kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Final LSTM\n",
    "        x = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Output\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def vmd_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        25. VMD-PatchTST Hybrid (Variational Mode Decomposition)\n",
    "        Reference: \"Enhanced Forecasting with VMD-PatchTST\" (2024)\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. 입력을 32차원으로 먼저 투영 \n",
    "        x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        # 2. 주파수 대역 분해\n",
    "        low_freq = AveragePooling1D(pool_size=5, strides=1, padding='same')(x)\n",
    "        low_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(low_freq)\n",
    "\n",
    "        # Medium frequency \n",
    "        mid_freq = x - low_freq\n",
    "        mid_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(mid_freq)\n",
    "\n",
    "        # High frequency \n",
    "        high_freq = x - low_freq - mid_freq\n",
    "        high_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                          kernel_regularizer=l2(0.01))(high_freq)\n",
    "\n",
    "        # 3. 모든 주파수 대역 결합\n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])  # (batch, time, 96)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        # 4. PatchTST-style Transformer 처리\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(96, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        # 5. 출력 레이어\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, \n",
    "                                  restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train):\n",
    "        \"\"\"\n",
    "        26. Logistic Regression\n",
    "        Reference: \"Forecasting mid-price movement of Bitcoin futures\" (2021)\n",
    "        \"\"\"\n",
    "        model = LogisticRegression(\n",
    "            C=1.0, penalty='l2', solver='lbfgs', \n",
    "            max_iter=1000, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def naive_bayes(X_train, y_train):\n",
    "        \"\"\"\n",
    "        27. Gaussian Naive Bayes\n",
    "        Reference: \"Forecasting mid-price movement of Bitcoin futures\" (2021)\n",
    "        \"\"\"\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def knn(X_train, y_train):\n",
    "        \"\"\"\n",
    "        28. K-Nearest Neighbors\n",
    "        Reference: \"Forecasting mid-price movement of Bitcoin futures\" (2021)\n",
    "        \"\"\"\n",
    "        model = KNeighborsClassifier(\n",
    "            n_neighbors=7, weights='distance', \n",
    "            metric='minkowski', p=2, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train):\n",
    "        \"\"\"\n",
    "        29. AdaBoost Classifier\n",
    "        Reference: \"Comparative Analysis of Ensemble-Based Models\" (2025)\n",
    "        \"\"\"\n",
    "        model = AdaBoostClassifier(\n",
    "            n_estimators=200, learning_rate=0.5, \n",
    "            algorithm='SAMME', random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        30. CatBoost Classifier\n",
    "        Reference: \"TRX Cryptocurrency Prediction\" (2023)\n",
    "        \"\"\"\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=200, learning_rate=0.1, depth=7,\n",
    "            l2_leaf_reg=3, border_count=128, \n",
    "            random_seed=42, verbose=False\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), \n",
    "                 early_stopping_rounds=50, verbose=False)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def decision_tree(X_train, y_train):\n",
    "        \"\"\"\n",
    "        31. Decision Tree Classifier\n",
    "        Reference: \"Comparative Analysis of Ensemble-Based Models\" (2025)\n",
    "        \"\"\"\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=15, min_samples_split=10, \n",
    "            min_samples_leaf=4, random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def extra_trees(X_train, y_train):\n",
    "        \"\"\"\n",
    "        32. Extra Trees Classifier\n",
    "        Reference: \"Enhancing financial product forecasting\" (2025)\n",
    "        \"\"\"\n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators=200, max_depth=15, \n",
    "            min_samples_split=10, min_samples_leaf=4,\n",
    "            max_features='sqrt', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def bagging(X_train, y_train):\n",
    "        \"\"\"\n",
    "        33. Bagging Classifier\n",
    "        Reference: \"Enhancing financial product forecasting\" (2025)\n",
    "        \"\"\"\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        model = BaggingClassifier(\n",
    "            estimator=base_estimator, n_estimators=100, \n",
    "            max_samples=0.8, max_features=0.8, \n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train):\n",
    "        \"\"\"\n",
    "        34. Gradient Boosting Classifier\n",
    "        Reference: \"Comparative Analysis of Ensemble-Based Models\" (2025)\n",
    "        \"\"\"\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=200, learning_rate=0.1, \n",
    "            max_depth=7, subsample=0.8, \n",
    "            min_samples_split=10, random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def simple_rnn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        35. Simple RNN\n",
    "        Reference: \"Utilizing RNN for Real-time Cryptocurrency\" (2024)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            SimpleRNN(128, activation='tanh', return_sequences=True,\n",
    "                     input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            SimpleRNN(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def mlp(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        36. Multi-Layer Perceptron\n",
    "        Reference: \"Deep neural networks for cryptocurrencies\" (2019)\n",
    "        Reference: \"Comparative Analysis Ensemble-Based Models\" (2025)\n",
    "        \"\"\"\n",
    "        input_dim=X_train.shape[1]\n",
    "        model = Sequential([\n",
    "            Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def emd_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        37. EMD-LSTM (Empirical Mode Decomposition + LSTM)\n",
    "        Reference: \"EMD-LSTM for Cryptocurrency Price Forecasting\" (2025)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Simulate EMD by multi-scale feature extraction\n",
    "        # Low frequency (trend)\n",
    "        low_freq = tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        low_freq = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(low_freq)\n",
    "\n",
    "        # High frequency (detail)\n",
    "        high_freq = inputs - tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        high_freq = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(high_freq)\n",
    "\n",
    "        # Combine decomposed features\n",
    "        x = Concatenate()([low_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def hybrid_lstm_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        38. Hybrid LSTM-GRU\n",
    "        Reference: \"Cryptocurrency price prediction through integrated forecasting\" (2024)\n",
    "        Reference: \"Development of cryptocurrency price prediction model\" (2025)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(32, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def parallel_cnn(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        39. Parallel CNN\n",
    "        Reference: \"Time series prediction for cryptocurrency with transformer and parallel CNN\" (2025)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Branch 1: Small kernel\n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = MaxPooling1D(2)(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "\n",
    "        # Branch 2: Medium kernel\n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = MaxPooling1D(2)(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "\n",
    "        # Branch 3: Large kernel\n",
    "        conv3 = Conv1D(64, 7, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = MaxPooling1D(2)(conv3)\n",
    "        conv3 = Dropout(0.2)(conv3)\n",
    "\n",
    "        # Merge branches\n",
    "        x = Concatenate()([conv1, conv2, conv3])\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        40. Stacking Ensemble Classifier\n",
    "        Reference: \"Stacking Ensemble Deep Learning for Bitcoin\" (2022)\n",
    "        Reference: \"Comparative Analysis of Ensemble-Based Models\" (2025)\n",
    "        In thesis, Accuracy: 81.80%, F1: 81.49%, AUC: 88.43%\n",
    "        \"\"\"\n",
    "        # Base learners\n",
    "        base_learners = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, verbose=-1))\n",
    "        ]\n",
    "\n",
    "        # Meta learner\n",
    "        meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "        model = StackingClassifier(\n",
    "            estimators=base_learners, \n",
    "            final_estimator=meta_learner, \n",
    "            cv=5, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def voting_hard(X_train, y_train):\n",
    "        \"\"\"\n",
    "        41. Voting Classifier (Hard Voting)\n",
    "        Reference: \"Detecting Anomalies in Blockchain\" (2024)\n",
    "        Accuracy: 97%, 다수결 투표 방식\n",
    "        \"\"\"\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)),\n",
    "            ('svm', SVC(kernel='rbf', C=1.0, random_state=42, probability=True))\n",
    "        ]\n",
    "\n",
    "        model = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def voting_soft(X_train, y_train):\n",
    "        \"\"\"\n",
    "        42. Voting Classifier (Soft Voting)\n",
    "        Reference: \"Enhancing blockchain transaction classification\" (2025)\n",
    "        확률 기반 소프트 투표 방식\n",
    "        \"\"\"\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "            ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)),\n",
    "            ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]\n",
    "\n",
    "        model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm_xgboost_hybrid(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        43. LSTM + XGBoost Hybrid\n",
    "        Reference: \"CRYPTO PRICE PREDICTION USING LSTM+XGBOOST\" (2025)\n",
    "        \"\"\"\n",
    "        # Stage 1: LSTM feature extraction\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        lstm_features = Dropout(0.3)(x)\n",
    "\n",
    "        # Dense layers for feature transformation\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(lstm_features)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def residual_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        44. Residual LSTM\n",
    "        Reference: \"Deep Learning for Cryptocurrency\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # First LSTM block with residual\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Second LSTM block with residual\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        x = Add()([x, lstm_out])  # Residual connection\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Final LSTM\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def wavenet(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        45. WaveNet-style Dilated Causal CNN\n",
    "        Reference: \"Advanced Time Series Models for Crypto\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        x = inputs\n",
    "        skip_connections = []\n",
    "\n",
    "        # Dilated causal convolutions\n",
    "        for dilation_rate in [1, 2, 4, 8, 16, 32]:\n",
    "            # Dilated conv\n",
    "            conv = Conv1D(64, 2, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "\n",
    "            # Skip connection\n",
    "            skip = Conv1D(64, 1, kernel_regularizer=l2(0.01))(conv)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "            # Residual connection\n",
    "            res = Conv1D(64, 1, kernel_regularizer=l2(0.01))(conv)\n",
    "            if x.shape[-1] != 64:\n",
    "                x = Conv1D(64, 1, kernel_regularizer=l2(0.01))(x)\n",
    "            x = Add()([x, res])\n",
    "\n",
    "        # Aggregate skip connections\n",
    "        x = Add()(skip_connections)\n",
    "        x = Activation('relu')(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    \n",
    "# ============================================================================\n",
    "# RegressionModels (15개 모델)\n",
    "# ============================================================================\n",
    "\n",
    "class RegressionModels:\n",
    "    \"\"\"회귀 모델 (15개 논문 기반)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest_reg(X_train, y_train):\n",
    "        \"\"\"1. Random Forest Regressor\"\"\"\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=4, max_features='sqrt', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm_reg(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        2. LightGBM Regressor\n",
    "        Reference: \"Beyond Conventional Methods\" (2025)\n",
    "        \"\"\"\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            min_child_samples=20, random_state=42, verbose=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                 callbacks=[early_stopping(50, verbose=False)])\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost_reg(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        3. XGBoost Regressor\n",
    "        Reference: \"Beyond Conventional Methods\" (2025)\n",
    "        \"\"\"\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "            min_child_weight=3, gamma=0.1, random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def svr(X_train, y_train):\n",
    "        \"\"\"\n",
    "        4. Support Vector Regressor\n",
    "        Reference: \"SVR to Improve ETH Prediction\" (2025)\n",
    "        R²: 0.9985, MSE: 2137.97\n",
    "        \"\"\"\n",
    "        model = SVR(kernel='linear', C=100, epsilon=1, gamma='scale')\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        5. LSTM Regressor\n",
    "        Reference: \"AI-based model\" (2025) - R²: 97.44%\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, activation='tanh', return_sequences=True, \n",
    "                 input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bilstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        6. BiLSTM Regressor\n",
    "        Reference: \"Predicting Bitcoin\" (2025) - R²: 0.98\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)), \n",
    "                         input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        7. GRU Regressor\n",
    "        Reference: \"Comparative Analysis\" (2025) - RMSE: 0.0234\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            GRU(128, activation='tanh', return_sequences=True, \n",
    "                input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacked_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"8. Stacked LSTM (3 layers)\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        9. CNN-LSTM\n",
    "        Reference: \"Application of CNN-BiLSTM\" (2025)\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Conv1D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"10. CNN-GRU\"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            GRU(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            GRU(64, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        11. CNN-BiLSTM\n",
    "        Reference: \"Application of CNN-BiLSTM\" (2025)\n",
    "        MAPE: 2.8546%, R²: 0.9415\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', padding='same', \n",
    "                   input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def seq2seq(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        12. Seq2Seq (Encoder-Decoder)\n",
    "        Reference: \"Bitcoin price prediction using LSTM autoencoder\" (2024)\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=input_shape)\n",
    "        encoder = LSTM(128, return_state=True, kernel_regularizer=l2(0.01))\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_inputs = RepeatVector(1)(encoder_outputs)\n",
    "        decoder_lstm = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))\n",
    "        decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_outputs = Dropout(0.3)(decoder_outputs)\n",
    "        decoder_dense = TimeDistributed(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        decoder_outputs = Flatten()(decoder_outputs)\n",
    "        outputs = Dense(1)(decoder_outputs)\n",
    "        \n",
    "        model = Model(encoder_inputs, outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def wavenet(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        13. WaveNet\n",
    "        Reference: \"Bitcoin price prediction using WaveNets\" (2019)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        # WaveNet-style dilated convolutions\n",
    "        for dilation_rate in [1, 2, 4, 8, 16]:\n",
    "            tanh_out = Conv1D(32, 2, padding='causal', dilation_rate=dilation_rate,\n",
    "                             activation='tanh', kernel_regularizer=l2(0.01))(x)\n",
    "            sigmoid_out = Conv1D(32, 2, padding='causal', dilation_rate=dilation_rate,\n",
    "                                activation='sigmoid', kernel_regularizer=l2(0.01))(x)\n",
    "            z = tf.keras.layers.Multiply()([tanh_out, sigmoid_out])\n",
    "            z = Conv1D(32, 1, kernel_regularizer=l2(0.01))(z)\n",
    "            x = Add()([x, z]) if x.shape[-1] == 32 else z\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tcn_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        14. Temporal Convolutional Network\n",
    "        Reference: \"Utilising TCN for Cryptocurrency\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for dilation_rate in [1, 2, 4, 8]:\n",
    "            conv = Conv1D(64, 3, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "            x = Add()([x, conv]) if x.shape[-1] == 64 else conv\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformer_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        15. Transformer Regressor\n",
    "        Reference: \"Transformer-based approach for ETH\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        attn_output = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "        ff = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        ff = Dropout(0.1)(ff)\n",
    "        ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def tabnet_reg(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        16. TabNet Regressor\n",
    "        Reference: \"TabNet for Cryptocurrency Forecasting\" (2024)\n",
    "        해석 가능한 특징 선택으로 암호화폐 가격 예측\n",
    "        \"\"\"\n",
    "        model = TabNetRegressor(\n",
    "            n_d=64, n_a=64, n_steps=5,\n",
    "            gamma=1.5, n_independent=2, n_shared=2,\n",
    "            lambda_sparse=1e-4, momentum=0.3,\n",
    "            mask_type='entmax', optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            verbose=0, seed=42\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            max_epochs=100, patience=20,\n",
    "            batch_size=256, virtual_batch_size=128\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def informer_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        17. Informer Regressor\n",
    "        Reference: \"Informer on High Frequency Bitcoin Data\" (2025)\n",
    "        5분봉 비트코인 데이터에서 Buy-and-Hold 전략 능가\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            x = Conv1D(input_shape[1], 1, activation='relu', \n",
    "                      kernel_regularizer=l2(0.01))(x)\n",
    "            x = MaxPooling1D(2, padding='same')(x)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def nbeats_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        18. N-BEATS Regressor\n",
    "        Reference: \"N-BEATS Perceiver for Crypto Portfolio\" (2024)\n",
    "        암호화폐 포트폴리오 예측에서 정확도와 견고성 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Trend stack\n",
    "        x1 = Flatten()(inputs)\n",
    "        for _ in range(4):\n",
    "            x1 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x1)\n",
    "            x1 = BatchNormalization()(x1)\n",
    "            x1 = Dropout(0.2)(x1)\n",
    "        trend = Dense(64, activation='linear', kernel_regularizer=l2(0.01))(x1)\n",
    "        \n",
    "        # Seasonality stack\n",
    "        x2 = Flatten()(inputs)\n",
    "        for _ in range(4):\n",
    "            x2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x2)\n",
    "            x2 = BatchNormalization()(x2)\n",
    "            x2 = Dropout(0.2)(x2)\n",
    "        season = Dense(64, activation='linear', kernel_regularizer=l2(0.01))(x2)\n",
    "        \n",
    "        combined = Add()([trend, season])\n",
    "        combined = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "        combined = Dropout(0.3)(combined)\n",
    "        outputs = Dense(1)(combined)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tft_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        19. Temporal Fusion Transformer Regressor\n",
    "        Reference: \"Adaptive TFT for Cryptocurrency\" (2025)\n",
    "        동적 subseries와 패턴 기반 분류로 단기 예측 강화\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = Flatten()(inputs)\n",
    "        var_weights = Dense(input_shape[0] * input_shape[1], activation='softmax',\n",
    "                           kernel_regularizer=l2(0.01))(x)\n",
    "        var_weights = Reshape(input_shape)(var_weights)\n",
    "        selected = Multiply()([inputs, var_weights])\n",
    "        \n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(selected)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def performer_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        20. Performer Regressor\n",
    "        Reference: \"Performer with BiLSTM for Crypto\" (2024)\n",
    "        시간별·일별 주요 암호화폐 예측에서 기존 방법 능가\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Bidirectional(LSTM(64, return_sequences=True, \n",
    "                                   kernel_regularizer=l2(0.01)))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def patchtst_reg(X_train, y_train, X_val, y_val, input_shape, patch_len=16, stride=8):\n",
    "        \"\"\"\n",
    "        21. PatchTST Regressor\n",
    "        Reference: \"Crypto Predictions with PatchTST\" (2024)\n",
    "        이더리움 LLM 예측에서 GPT-2, Llama 다음으로 우수\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        num_patches = (input_shape[0] - patch_len) // stride + 1\n",
    "        \n",
    "        patches = []\n",
    "        for i in range(0, input_shape[0] - patch_len + 1, stride):\n",
    "            patch = Lambda(lambda z: z[:, i:i+patch_len, :])(x)\n",
    "            patch = Flatten()(patch)\n",
    "            patch = Dense(128, kernel_regularizer=l2(0.01))(patch)\n",
    "            patches.append(patch)\n",
    "        \n",
    "        if len(patches) > 1:\n",
    "            x = tf.stack(patches, axis=1)\n",
    "        else:\n",
    "            x = tf.expand_dims(patches[0], axis=1)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def autoformer_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        22. Autoformer Regressor\n",
    "        Reference: \"Autoformer for Long-term Forecasting\" (2021)\n",
    "        시계열 분해로 트렌드와 계절성 동시 포착\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        trend = tf.keras.layers.AveragePooling1D(pool_size=25, strides=1, \n",
    "                                                  padding='same')(x)\n",
    "        seasonal = tf.subtract(x, trend)\n",
    "        \n",
    "        x = seasonal\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        seasonal_out = GlobalAveragePooling1D()(x)\n",
    "        trend_out = GlobalAveragePooling1D()(trend)\n",
    "        combined = Concatenate()([seasonal_out, trend_out])\n",
    "        \n",
    "        combined = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "        combined = Dropout(0.3)(combined)\n",
    "        outputs = Dense(1)(combined)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def itransformer_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        23. iTransformer Regressor\n",
    "        Reference: \"iTransformer Crypto Forecasting\" (2024)\n",
    "        변수 간 관계를 시간보다 우선시하여 처리\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=16, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(64, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def ethervoyant_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        24. EtherVoyant Regressor\n",
    "        Reference: \"EtherVoyant: State-of-the-art ETH Forecasting\" (2024)\n",
    "        이더리움 가격 예측 전용 최신 모델\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "        \n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "        \n",
    "        x = Concatenate()([conv1, conv2])\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        \n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, \n",
    "                              kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        x = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def vmd_hybrid_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        25. VMD-PatchTST Hybrid Regressor\n",
    "        Reference: \"VMD-PatchTST for Stock/Crypto Forecasting\" (2024)\n",
    "        변동성 분해 + Transformer로 정확도 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        low_freq = tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, \n",
    "                                                     padding='same')(inputs)\n",
    "        low_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(low_freq)\n",
    "        \n",
    "        mid_freq = inputs - low_freq\n",
    "        mid_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(mid_freq)\n",
    "        \n",
    "        high_freq = inputs - low_freq - mid_freq\n",
    "        high_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                          kernel_regularizer=l2(0.01))(high_freq)\n",
    "        \n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(96, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def dtw_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        26. DTW-LSTM Regressor\n",
    "        Reference: \"Application of Dynamic Time Warping on ETH Prediction\" (2024)\n",
    "        DTW로 유사 패턴 식별 후 LSTM 예측 - 기존 LSTM 대비 23.4% 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # DTW-inspired feature extraction using Conv1D with multiple dilation rates\n",
    "        dtw_features = []\n",
    "        for dilation in [1, 2, 4, 8]:\n",
    "            conv = Conv1D(32, 3, dilation_rate=dilation, padding='causal',\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(inputs)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            dtw_features.append(conv)\n",
    "\n",
    "        x = Concatenate()(dtw_features)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        # LSTM layers\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def attention_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        27. Attention-LSTM Regressor\n",
    "        Reference: \"ETH Gas Price Prediction with Attention\" (2024)\n",
    "        어텐션 메커니즘으로 중요 시점 강조\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = Dense(1, activation='tanh')(lstm_out)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(128)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        # Apply attention\n",
    "        attended = Multiply()([lstm_out, attention])\n",
    "        attended = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n",
    "\n",
    "        attended = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(attended)\n",
    "        attended = BatchNormalization()(attended)\n",
    "        attended = Dropout(0.3)(attended)\n",
    "\n",
    "        attended = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(attended)\n",
    "        attended = Dropout(0.2)(attended)\n",
    "        outputs = Dense(1)(attended)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def dual_attention_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        28. Dual Attention Mechanism Regressor\n",
    "        Reference: \"Dual Attention Mechanism for Crypto Trend\" (2024)\n",
    "        시계열 어텐션 + 특징 어텐션 결합으로 20% 성능 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Feature attention\n",
    "        feature_attention = Dense(input_shape[1], activation='sigmoid',\n",
    "                                 kernel_regularizer=l2(0.01))(inputs)\n",
    "        feature_attended = Multiply()([inputs, feature_attention])\n",
    "\n",
    "        # Temporal processing\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(feature_attended)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Temporal attention\n",
    "        temporal_attention = Dense(1, activation='tanh')(x)\n",
    "        temporal_attention = Flatten()(temporal_attention)\n",
    "        temporal_attention = Activation('softmax')(temporal_attention)\n",
    "        temporal_attention = RepeatVector(128)(temporal_attention)\n",
    "        temporal_attention = Permute([2, 1])(temporal_attention)\n",
    "\n",
    "        x = Multiply()([x, temporal_attention])\n",
    "        x = Lambda(lambda z: tf.reduce_sum(z, axis=1))(x)\n",
    "\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_correlation_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        29. Cross-Correlation LSTM Regressor\n",
    "        Reference: \"Crypto Volatility Prediction via Cross-Correlation\" (2024)\n",
    "        가격·수익률·변동성 간 상호상관으로 특징 선택\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Feature correlation learning\n",
    "        x = Conv1D(64, 1, activation='relu', kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        # Multi-scale temporal processing\n",
    "        branch1 = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        branch2 = GRU(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "        merged = Concatenate()([branch1, branch2])\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(0.3)(merged)\n",
    "\n",
    "        merged = LSTM(64, kernel_regularizer=l2(0.01))(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(0.3)(merged)\n",
    "\n",
    "        merged = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "        merged = Dropout(0.2)(merged)\n",
    "        outputs = Dense(1)(merged)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_optimized_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        30. Gradient-Optimized LSTM Regressor\n",
    "        Reference: \"Gradient-Specific Optimization for Bitcoin\" (2024)\n",
    "        그래디언트 특화 최적화로 예측 성능 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Gradient-specific optimizer with custom learning rate schedule\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, \n",
    "                                     min_lr=1e-6, verbose=0)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, \n",
    "                 callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def ensemble_stacking_reg(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        31. Ensemble Stacking Regressor\n",
    "        Reference: \"Ensemble Bitcoin Price Prediction\" (2025)\n",
    "        스태킹 앙상블 - 81.8% 정확도, 다양한 모델 결합\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import StackingRegressor\n",
    "        from sklearn.linear_model import Ridge\n",
    "\n",
    "        # Base models\n",
    "        base_models = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, \n",
    "                                        min_samples_split=10, random_state=42, n_jobs=-1)),\n",
    "            ('lgbm', LGBMRegressor(n_estimators=100, max_depth=5, learning_rate=0.05,\n",
    "                                  num_leaves=31, random_state=42, verbose=-1)),\n",
    "            ('xgb', XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.05,\n",
    "                                random_state=42))\n",
    "        ]\n",
    "\n",
    "        # Meta-learner\n",
    "        meta_model = Ridge(alpha=1.0)\n",
    "\n",
    "        # Stacking ensemble\n",
    "        model = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def ensemble_voting_reg(X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        32. Ensemble Voting Regressor\n",
    "        Reference: \"Crypto Trading with Ensemble Methods\" (2025)\n",
    "        보팅 앙상블로 리스크 완화, Sharpe ratio 0.28 달성\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "        # Diverse base models\n",
    "        estimators = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, max_depth=10,\n",
    "                                        random_state=42, n_jobs=-1)),\n",
    "            ('lgbm', LGBMRegressor(n_estimators=100, max_depth=5,\n",
    "                                  learning_rate=0.05, random_state=42, verbose=-1)),\n",
    "            ('xgb', XGBRegressor(n_estimators=100, max_depth=5,\n",
    "                                learning_rate=0.05, random_state=42)),\n",
    "            ('svr', SVR(kernel='rbf', C=100, epsilon=0.1))\n",
    "        ]\n",
    "\n",
    "        model = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm_xgboost_hybrid_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        33. LSTM-XGBoost Hybrid Regressor\n",
    "        Reference: \"LSTM+XGBoost Crypto Price Prediction\" (2025)\n",
    "        LSTM 특징 추출 + XGBoost 예측\n",
    "        \"\"\"\n",
    "        # LSTM feature extraction\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        lstm_out = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01))(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "\n",
    "        feature_model = Model(inputs=inputs, outputs=lstm_out)\n",
    "        feature_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        # Extract features\n",
    "        X_train_features = feature_model.predict(X_train, verbose=0)\n",
    "        X_val_features = feature_model.predict(X_val, verbose=0)\n",
    "\n",
    "        # XGBoost on extracted features\n",
    "        xgb_model = XGBRegressor(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            reg_alpha=0.1, reg_lambda=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "        xgb_model.fit(X_train_features, y_train,\n",
    "                     eval_set=[(X_val_features, y_val)],\n",
    "                     verbose=False)\n",
    "\n",
    "        # Return both models (need custom wrapper for prediction)\n",
    "        class HybridModel:\n",
    "            def __init__(self, feature_extractor, predictor):\n",
    "                self.feature_extractor = feature_extractor\n",
    "                self.predictor = predictor\n",
    "\n",
    "            def predict(self, X):\n",
    "                features = self.feature_extractor.predict(X, verbose=0)\n",
    "                return self.predictor.predict(features)\n",
    "\n",
    "        return HybridModel(feature_model, xgb_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def residual_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        34. Residual LSTM Regressor\n",
    "        Reference: \"Deep Residual Networks for Time Series\" (2024)\n",
    "        잔차 연결로 깊은 네트워크 학습 안정화\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # First LSTM block\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Residual block 1\n",
    "        residual1 = x\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([x, residual1])\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Residual block 2\n",
    "        residual2 = x\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([x, residual2])\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Final layers\n",
    "        x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def multiscale_cnn_lstm_reg(X_train, y_train, X_val, y_val, input_shape):\n",
    "        \"\"\"\n",
    "        35. Multi-Scale CNN-LSTM Regressor\n",
    "        Reference: \"Multi-Scale Feature Extraction for Crypto\" (2024)\n",
    "        다중 스케일 특징 추출로 다양한 시간 패턴 포착\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Multi-scale CNN branches\n",
    "        branch1 = Conv1D(32, 3, padding='same', activation='relu',\n",
    "                        kernel_regularizer=l2(0.01))(inputs)\n",
    "        branch1 = BatchNormalization()(branch1)\n",
    "        branch1 = MaxPooling1D(2)(branch1)\n",
    "\n",
    "        branch2 = Conv1D(32, 5, padding='same', activation='relu',\n",
    "                        kernel_regularizer=l2(0.01))(inputs)\n",
    "        branch2 = BatchNormalization()(branch2)\n",
    "        branch2 = MaxPooling1D(2)(branch2)\n",
    "\n",
    "        branch3 = Conv1D(32, 7, padding='same', activation='relu',\n",
    "                        kernel_regularizer=l2(0.01))(inputs)\n",
    "        branch3 = BatchNormalization()(branch3)\n",
    "        branch3 = MaxPooling1D(2)(branch3)\n",
    "\n",
    "        # Merge branches\n",
    "        merged = Concatenate()([branch1, branch2, branch3])\n",
    "        merged = Dropout(0.2)(merged)\n",
    "\n",
    "        # LSTM processing\n",
    "        merged = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(0.3)(merged)\n",
    "\n",
    "        merged = LSTM(64, kernel_regularizer=l2(0.01))(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(0.3)(merged)\n",
    "\n",
    "        merged = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "        merged = Dropout(0.2)(merged)\n",
    "        outputs = Dense(1)(merged)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "        return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MultiTaskModels (15개 모델 - 논문 기반)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiTaskModels:\n",
    "    \"\"\"멀티태스크 학습 모델 15개 (논문 기반)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def hard_sharing_lstm(X_train, y_train_dir, y_train_ret, \n",
    "                          X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        1. Hard Parameter Sharing LSTM\n",
    "        Reference: \"Cryptocurrency price prediction and portfolio optimization\" (2025)\n",
    "        Most common MTL approach\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Shared layers (Hard sharing)\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = BatchNormalization()(dir_head)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = BatchNormalization()(ret_head)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def bilstm_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                   X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        2. BiLSTM Multi-Task (Hard Sharing)\n",
    "        Reference: \"Multi-iTR\" (2025) - R²: 0.98\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = BatchNormalization()(dir_head)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = BatchNormalization()(ret_head)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def gru_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        3. GRU Multi-Task\n",
    "        Reference: \"Comparative Analysis of LSTM and GRU for ETH\" (2025)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = GRU(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = GRU(64, kernel_regularizer=l2(0.01))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def soft_sharing_lstm(X_train, y_train_dir, y_train_ret, \n",
    "                         X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        4. Soft Parameter Sharing LSTM\n",
    "        Reference: \"Task's Choice: Pruning-Based Feature Sharing\" (2022)\n",
    "        Each task has its own network, regularized to be similar\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Task 1 network (Direction)\n",
    "        dir_lstm1 = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01), name='dir_lstm1')(inputs)\n",
    "        dir_lstm1 = BatchNormalization()(dir_lstm1)\n",
    "        dir_lstm1 = Dropout(0.3)(dir_lstm1)\n",
    "        dir_lstm2 = LSTM(64, kernel_regularizer=l2(0.01), name='dir_lstm2')(dir_lstm1)\n",
    "        dir_lstm2 = BatchNormalization()(dir_lstm2)\n",
    "        \n",
    "        # Task 2 network (Return)\n",
    "        ret_lstm1 = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01), name='ret_lstm1')(inputs)\n",
    "        ret_lstm1 = BatchNormalization()(ret_lstm1)\n",
    "        ret_lstm1 = Dropout(0.3)(ret_lstm1)\n",
    "        ret_lstm2 = LSTM(64, kernel_regularizer=l2(0.01), name='ret_lstm2')(ret_lstm1)\n",
    "        ret_lstm2 = BatchNormalization()(ret_lstm2)\n",
    "        \n",
    "        # Task-specific outputs\n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dir_lstm2)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(ret_lstm2)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_stitch_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                        X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        5. Cross-Stitch Networks\n",
    "        Reference: \"Cross-stitch Networks for Multi-task Learning\" (CVPR 2016)\n",
    "        Learns optimal combination of shared and task-specific representations\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Task 1 branch\n",
    "        dir_lstm1 = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        \n",
    "        # Task 2 branch\n",
    "        ret_lstm1 = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        \n",
    "        # Cross-stitch unit (linear combination)\n",
    "        # α_dir * dir_lstm1 + β_dir * ret_lstm1 -> new_dir\n",
    "        # α_ret * ret_lstm1 + β_ret * dir_lstm1 -> new_ret\n",
    "        concat = Concatenate()([dir_lstm1, ret_lstm1])\n",
    "        \n",
    "        # Learn cross-stitch weights\n",
    "        dir_cross = Dense(64, kernel_regularizer=l2(0.01))(concat)\n",
    "        dir_cross = BatchNormalization()(dir_cross)\n",
    "        dir_cross = Dropout(0.3)(dir_cross)\n",
    "        \n",
    "        ret_cross = Dense(64, kernel_regularizer=l2(0.01))(concat)\n",
    "        ret_cross = BatchNormalization()(ret_cross)\n",
    "        ret_cross = Dropout(0.3)(ret_cross)\n",
    "        \n",
    "        # Second layer\n",
    "        dir_lstm2 = LSTM(64, kernel_regularizer=l2(0.01))(dir_cross)\n",
    "        ret_lstm2 = LSTM(64, kernel_regularizer=l2(0.01))(ret_cross)\n",
    "        \n",
    "        # Outputs\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_lstm2)\n",
    "        ret_output = Dense(1, name='return')(ret_lstm2)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def mmoe_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                X_val, y_val_dir, y_val_ret, input_shape, num_experts=3):\n",
    "        \"\"\"\n",
    "        6. Multi-gate Mixture-of-Experts (MMoE)\n",
    "        Reference: \"Modeling Task Relationships in Multi-task Learning\" (Google, KDD 2018)\n",
    "        Explicitly learns task relationships from data\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Expert networks\n",
    "        experts = []\n",
    "        for i in range(num_experts):\n",
    "            expert = LSTM(64, return_sequences=False, \n",
    "                         kernel_regularizer=l2(0.01), \n",
    "                         name=f'expert_{i}')(inputs)\n",
    "            expert = BatchNormalization()(expert)\n",
    "            experts.append(expert)\n",
    "        \n",
    "        # Stack experts\n",
    "        experts_stacked = tf.stack(experts, axis=1)  # (batch, num_experts, 64)\n",
    "        \n",
    "        # Gate for direction task\n",
    "        dir_gate_input = LSTM(32, kernel_regularizer=l2(0.01))(inputs)\n",
    "        dir_gate = Dense(num_experts, activation='softmax', \n",
    "                        kernel_regularizer=l2(0.01), \n",
    "                        name='dir_gate')(dir_gate_input)\n",
    "        dir_gate = tf.expand_dims(dir_gate, -1)  # (batch, num_experts, 1)\n",
    "        \n",
    "        # Weighted sum for direction\n",
    "        dir_weighted = tf.reduce_sum(experts_stacked * dir_gate, axis=1)\n",
    "        dir_weighted = Dropout(0.3)(dir_weighted)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_weighted)\n",
    "        \n",
    "        # Gate for return task\n",
    "        ret_gate_input = LSTM(32, kernel_regularizer=l2(0.01))(inputs)\n",
    "        ret_gate = Dense(num_experts, activation='softmax', \n",
    "                        kernel_regularizer=l2(0.01), \n",
    "                        name='ret_gate')(ret_gate_input)\n",
    "        ret_gate = tf.expand_dims(ret_gate, -1)\n",
    "        \n",
    "        # Weighted sum for return\n",
    "        ret_weighted = tf.reduce_sum(experts_stacked * ret_gate, axis=1)\n",
    "        ret_weighted = Dropout(0.3)(ret_weighted)\n",
    "        ret_output = Dense(1, name='return')(ret_weighted)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_lstm_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                    X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        7. CNN-LSTM Multi-Task\n",
    "        Reference: \"Bitcoin Price Direction Forecasting\" (2024)\n",
    "        CNN-LSTM for direction + price prediction\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Shared CNN layers\n",
    "        x = Conv1D(64, 3, activation='relu', padding='same', \n",
    "                  kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # Shared LSTM layers\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def cnn_gru_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                   X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        8. CNN-GRU Multi-Task\n",
    "        Faster than CNN-LSTM while maintaining performance\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        x = GRU(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = GRU(64, kernel_regularizer=l2(0.01))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                       X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        9. Transformer Multi-Task\n",
    "        Reference: \"Multi-iTR\" (2025) - R²: 0.98\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        attn_output = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "        ff = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "        ff = Dropout(0.1)(ff)\n",
    "        ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = BatchNormalization()(dir_head)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = BatchNormalization()(ret_head)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=50, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def stacked_lstm_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                        X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        10. Stacked LSTM Multi-Task (3 layers)\n",
    "        Deep shared representation learning\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = LSTM(96, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                     X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        11. LSTM-Attention Multi-Task\n",
    "        Reference: \"Representation Learning for Financial Time Series\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        lstm_out = Dropout(0.3)(lstm_out)\n",
    "        lstm_out = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)\n",
    "        lstm_out = BatchNormalization()(lstm_out)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention = Attention()([lstm_out, lstm_out])\n",
    "        combined = Add()([lstm_out, attention])\n",
    "        shared = GlobalAveragePooling1D()(combined)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tcn_mtl(X_train, y_train_dir, y_train_ret, \n",
    "               X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        12. Temporal Convolutional Network Multi-Task\n",
    "        Reference: \"Utilising TCN for Cryptocurrency Forecasting\" (2024)\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for dilation_rate in [1, 2, 4, 8]:\n",
    "            conv = Conv1D(64, 3, padding='causal', dilation_rate=dilation_rate,\n",
    "                         activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "            conv = BatchNormalization()(conv)\n",
    "            conv = Dropout(0.2)(conv)\n",
    "            x = Add()([x, conv]) if x.shape[-1] == 64 else conv\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def hierarchical_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                        X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        13. Hierarchical Multi-Task Learning\n",
    "        Reference: \"Deep Multitask Learning with Progressive Parameter Sharing\" (2023)\n",
    "        Progressive sharing from low to high layers\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Low-level shared features\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Mid-level: partially shared\n",
    "        dir_mid = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        ret_mid = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        \n",
    "        # High-level: task-specific\n",
    "        dir_high = LSTM(32, kernel_regularizer=l2(0.01))(dir_mid)\n",
    "        ret_high = LSTM(32, kernel_regularizer=l2(0.01))(ret_mid)\n",
    "        \n",
    "        dir_head = Dense(16, activation='relu', kernel_regularizer=l2(0.01))(dir_high)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(16, activation='relu', kernel_regularizer=l2(0.01))(ret_high)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def weighted_loss_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                         X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        14. Adaptive Loss Weighting Multi-Task\n",
    "        Reference: \"Multi-Task Transformer with Adaptive Cross-Entropy Loss\" (2022)\n",
    "        Automatically balances task learning\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        shared = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        \n",
    "        # Adaptive loss weights (learned during training)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.5, 'return': 1.0},  # Give more weight to direction\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensemble_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                    X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        15. Ensemble Multi-Task Learning\n",
    "        Reference: \"Ensemble-based models for cryptocurrency trading\" (2024)\n",
    "        Multiple shared layers with ensemble output\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Multiple shared branches\n",
    "        branch1 = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        branch1 = BatchNormalization()(branch1)\n",
    "        branch1 = Dropout(0.3)(branch1)\n",
    "        branch1 = LSTM(32, kernel_regularizer=l2(0.01))(branch1)\n",
    "        \n",
    "        branch2 = GRU(64, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)\n",
    "        branch2 = BatchNormalization()(branch2)\n",
    "        branch2 = Dropout(0.3)(branch2)\n",
    "        branch2 = GRU(32, kernel_regularizer=l2(0.01))(branch2)\n",
    "        \n",
    "        # Ensemble\n",
    "        shared = Concatenate()([branch1, branch2])\n",
    "        shared = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def tabnet_mtl(X_train, y_train_dir, y_train_ret, \n",
    "                   X_val, y_val_dir, y_val_ret):\n",
    "        \"\"\"\n",
    "        16. TabNet Multi-Task (Pseudo MTL)\n",
    "        Reference: \"TabNet for Multi-Task Learning\" (2024)\n",
    "        해석 가능한 특징 중요도와 멀티태스크 학습 결합\n",
    "        Note: TabNet은 native MTL 미지원, 두 모델 앙상블로 구현\n",
    "        \"\"\"\n",
    "        # Direction model\n",
    "        dir_model = TabNetClassifier(\n",
    "            n_d=64, n_a=64, n_steps=5, gamma=1.5,\n",
    "            lambda_sparse=1e-4, optimizer_params=dict(lr=2e-2),\n",
    "            verbose=0, seed=42\n",
    "        )\n",
    "        dir_model.fit(X_train, y_train_dir, eval_set=[(X_val, y_val_dir)],\n",
    "                     max_epochs=100, patience=20, batch_size=256)\n",
    "        \n",
    "        # Return model\n",
    "        ret_model = TabNetRegressor(\n",
    "            n_d=64, n_a=64, n_steps=5, gamma=1.5,\n",
    "            lambda_sparse=1e-4, optimizer_params=dict(lr=2e-2),\n",
    "            verbose=0, seed=42\n",
    "        )\n",
    "        ret_model.fit(X_train, y_train_ret, eval_set=[(X_val, y_val_ret)],\n",
    "                     max_epochs=100, patience=20, batch_size=256)\n",
    "        \n",
    "        return {'direction': dir_model, 'return': ret_model}\n",
    "    \n",
    "    @staticmethod\n",
    "    def informer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                    X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        17. Informer Multi-Task\n",
    "        Reference: \"Informer for Bitcoin Trading\" (2025)\n",
    "        고빈도 데이터에서 방향과 수익률 동시 예측\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Shared Informer layers\n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            x = Conv1D(input_shape[1], 1, activation='relu', \n",
    "                      kernel_regularizer=l2(0.01))(x)\n",
    "            x = MaxPooling1D(2, padding='same')(x)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def nbeats_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                  X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        18. N-BEATS Multi-Task\n",
    "        Reference: \"N-BEATS Perceiver\" (2024)\n",
    "        트렌드와 계절성 분해로 암호화폐 포트폴리오 예측\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        flat = Flatten()(inputs)\n",
    "        \n",
    "        # Shared trend analysis\n",
    "        trend = flat\n",
    "        for _ in range(3):\n",
    "            trend = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(trend)\n",
    "            trend = BatchNormalization()(trend)\n",
    "            trend = Dropout(0.2)(trend)\n",
    "        \n",
    "        # Shared seasonality analysis\n",
    "        season = flat\n",
    "        for _ in range(3):\n",
    "            season = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(season)\n",
    "            season = BatchNormalization()(season)\n",
    "            season = Dropout(0.2)(season)\n",
    "        \n",
    "        # Combine\n",
    "        shared = Concatenate()([trend, season])\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        # Task heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def tft_mtl(X_train, y_train_dir, y_train_ret,\n",
    "               X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        19. Temporal Fusion Transformer MTL\n",
    "        Reference: \"Adaptive TFT for Cryptocurrency\" (2025)\n",
    "        변수 선택 네트워크로 중요 특징 자동 선택\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Variable selection\n",
    "        x = Flatten()(inputs)\n",
    "        var_weights = Dense(input_shape[0] * input_shape[1], activation='softmax',\n",
    "                           kernel_regularizer=l2(0.01))(x)\n",
    "        var_weights = Reshape(input_shape)(var_weights)\n",
    "        selected = Multiply()([inputs, var_weights])\n",
    "        \n",
    "        # Shared processing\n",
    "        x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(selected)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        # Task heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def performer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                     X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        20. Performer Multi-Task\n",
    "        Reference: \"Performer with BiLSTM\" (2024)\n",
    "        효율적 어텐션 메커니즘으로 계산 비용 절감\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Shared Performer layers\n",
    "        x = inputs\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Bidirectional(LSTM(64, return_sequences=True, \n",
    "                                   kernel_regularizer=l2(0.01)))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        # Task heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def patchtst_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                    X_val, y_val_dir, y_val_ret, input_shape, patch_len=16, stride=8):\n",
    "        \"\"\"\n",
    "        21. PatchTST Multi-Task\n",
    "        Reference: \"PatchTST for Multi-output Forecasting\" (2024)\n",
    "        채널 독립성으로 멀티태스크 효율성 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        patches = []\n",
    "        for i in range(0, input_shape[0] - patch_len + 1, stride):\n",
    "            patch = Lambda(lambda z: z[:, i:i+patch_len, :])(x)\n",
    "            patch = Flatten()(patch)\n",
    "            patch = Dense(128, kernel_regularizer=l2(0.01))(patch)\n",
    "            patches.append(patch)\n",
    "        \n",
    "        if len(patches) > 1:\n",
    "            x = tf.stack(patches, axis=1)\n",
    "        else:\n",
    "            x = tf.expand_dims(patches[0], axis=1)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def autoformer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                      X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        22. Autoformer Multi-Task\n",
    "        Reference: \"Autoformer Decomposition\" (2021)\n",
    "        시계열 분해를 멀티태스크에 적용\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = inputs\n",
    "        trend = tf.keras.layers.AveragePooling1D(pool_size=25, strides=1, \n",
    "                                                  padding='same')(x)\n",
    "        seasonal = tf.subtract(x, trend)\n",
    "        \n",
    "        x = seasonal\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        seasonal_out = GlobalAveragePooling1D()(x)\n",
    "        trend_out = GlobalAveragePooling1D()(trend)\n",
    "        shared = Concatenate()([seasonal_out, trend_out])\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def itransformer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                        X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        23. iTransformer Multi-Task\n",
    "        Reference: \"iTransformer Multivariate Forecasting\" (2024)\n",
    "        변수 간 관계 우선 처리로 멀티태스크 향상\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=16, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(64, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def ethervoyant_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                       X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        24. EtherVoyant Multi-Task\n",
    "        Reference: \"EtherVoyant ETH Prediction\" (2024)\n",
    "        이더리움 전용 멀티태스크 아키텍처\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        conv1 = Conv1D(64, 3, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Dropout(0.2)(conv1)\n",
    "        \n",
    "        conv2 = Conv1D(64, 5, activation='relu', padding='same', \n",
    "                      kernel_regularizer=l2(0.01))(inputs)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Dropout(0.2)(conv2)\n",
    "        \n",
    "        x = Concatenate()([conv1, conv2])\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        \n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, \n",
    "                              kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        shared = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def vmd_hybrid_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                      X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        25. VMD-Hybrid Multi-Task\n",
    "        Reference: \"VMD Decomposition for Volatility\" (2024)\n",
    "        변동성 분해로 방향과 수익률 동시 예측\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        low_freq = tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, \n",
    "                                                     padding='same')(inputs)\n",
    "        low_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(low_freq)\n",
    "        \n",
    "        mid_freq = inputs - low_freq\n",
    "        mid_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                         kernel_regularizer=l2(0.01))(mid_freq)\n",
    "        \n",
    "        high_freq = inputs - low_freq - mid_freq\n",
    "        high_freq = Conv1D(32, 3, activation='relu', padding='same',\n",
    "                          kernel_regularizer=l2(0.01))(high_freq)\n",
    "        \n",
    "        x = Concatenate()([low_freq, mid_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "            \n",
    "            ff = Dense(128, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(96, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "        \n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "        \n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "        \n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def snas_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        26. SNAS (Stochastic Neural Architecture Search) Multi-Task\n",
    "        Reference: \"Multi-Task Time Series Forecasting With Shared Attention\" (2021)\n",
    "        태스크 간 공유 어텐션으로 아키텍처 자동 탐색\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Shared Attention Module\n",
    "        shared_attn = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
    "        shared_attn = Dropout(0.1)(shared_attn)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs + shared_attn)\n",
    "\n",
    "        # Parallel Task-Specific Encoders\n",
    "        dir_encoder = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        dir_encoder = BatchNormalization()(dir_encoder)\n",
    "        dir_encoder = Dropout(0.3)(dir_encoder)\n",
    "        dir_features = LSTM(64, kernel_regularizer=l2(0.01))(dir_encoder)\n",
    "\n",
    "        ret_encoder = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "        ret_encoder = BatchNormalization()(ret_encoder)\n",
    "        ret_encoder = Dropout(0.3)(ret_encoder)\n",
    "        ret_features = LSTM(64, kernel_regularizer=l2(0.01))(ret_encoder)\n",
    "\n",
    "        # Task outputs\n",
    "        dir_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dir_features)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "\n",
    "        ret_head = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(ret_features)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def dlinear_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                   X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        27. DLinear Multi-Task\n",
    "        Reference: \"Are Transformers Effective for Time Series Forecasting?\" (2023)\n",
    "        단순 선형 레이어로 트렌드/계절성 분해 후 예측\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Decomposition using moving average\n",
    "        trend = tf.keras.layers.AveragePooling1D(pool_size=25, strides=1, padding='same')(inputs)\n",
    "        seasonal = tf.subtract(inputs, trend)\n",
    "\n",
    "        # Flatten for linear layers\n",
    "        trend_flat = Flatten()(trend)\n",
    "        seasonal_flat = Flatten()(seasonal)\n",
    "\n",
    "        # Shared feature extraction\n",
    "        trend_features = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(trend_flat)\n",
    "        trend_features = BatchNormalization()(trend_features)\n",
    "        trend_features = Dropout(0.3)(trend_features)\n",
    "\n",
    "        seasonal_features = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(seasonal_flat)\n",
    "        seasonal_features = BatchNormalization()(seasonal_features)\n",
    "        seasonal_features = Dropout(0.3)(seasonal_features)\n",
    "\n",
    "        shared = Concatenate()([trend_features, seasonal_features])\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "\n",
    "        # Task-specific heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "\n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def fedformer_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                     X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        28. FEDformer Multi-Task\n",
    "        Reference: \"FEDformer: Frequency Enhanced Decomposed Transformer\" (2022)\n",
    "        주파수 도메인에서 시계열 분석 수행\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Frequency domain processing (approximation using FFT concepts)\n",
    "        # Low frequency components\n",
    "        low_freq = tf.keras.layers.AveragePooling1D(pool_size=5, strides=1, padding='same')(inputs)\n",
    "        low_freq = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(low_freq)\n",
    "\n",
    "        # High frequency components\n",
    "        high_freq = inputs - low_freq\n",
    "        high_freq = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(high_freq)\n",
    "\n",
    "        # Combine frequency components\n",
    "        x = Concatenate()([low_freq, high_freq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for _ in range(2):\n",
    "            attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(128, kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "        shared = GlobalAveragePooling1D()(x)\n",
    "        shared = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "\n",
    "        # Task heads\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "\n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def units_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                 X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        29. UniTS Multi-Task\n",
    "        Reference: \"UniTS: A Unified Multi-Task Time Series Model\" (NeurIPS 2024)\n",
    "        Task tokenization으로 예측/생성 태스크 통합\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Task tokens (learnable embeddings)\n",
    "        dir_task_token = tf.Variable(tf.random.normal([1, 1, input_shape[1]]), trainable=True)\n",
    "        ret_task_token = tf.Variable(tf.random.normal([1, 1, input_shape[1]]), trainable=True)\n",
    "\n",
    "        # Expand task tokens to batch size\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        dir_token_expanded = tf.tile(dir_task_token, [batch_size, 1, 1])\n",
    "        ret_token_expanded = tf.tile(ret_task_token, [batch_size, 1, 1])\n",
    "\n",
    "        # Concatenate task tokens with input\n",
    "        dir_input = Concatenate(axis=1)([dir_token_expanded, inputs])\n",
    "        ret_input = Concatenate(axis=1)([ret_token_expanded, inputs])\n",
    "\n",
    "        # Shared transformer encoder\n",
    "        def transformer_block(x):\n",
    "            attn = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(x, x)\n",
    "            attn = Dropout(0.1)(attn)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "\n",
    "            ff = Dense(256, activation='gelu', kernel_regularizer=l2(0.01))(x)\n",
    "            ff = Dropout(0.1)(ff)\n",
    "            ff = Dense(input_shape[1], kernel_regularizer=l2(0.01))(ff)\n",
    "            x = LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "            return x\n",
    "\n",
    "        dir_features = transformer_block(dir_input)\n",
    "        dir_features = GlobalAveragePooling1D()(dir_features)\n",
    "\n",
    "        ret_features = transformer_block(ret_input)\n",
    "        ret_features = GlobalAveragePooling1D()(ret_features)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dir_features)\n",
    "        dir_head = BatchNormalization()(dir_head)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "\n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(ret_features)\n",
    "        ret_head = BatchNormalization()(ret_head)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.0, 'return': 1.0},\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def metarl_crypto_mtl(X_train, y_train_dir, y_train_ret,\n",
    "                         X_val, y_val_dir, y_val_ret, input_shape):\n",
    "        \"\"\"\n",
    "        30. Meta-RL Crypto Multi-Task\n",
    "        Reference: \"Meta-Learning Reinforcement Learning for Crypto-Return Prediction\" (2025)\n",
    "        메타학습과 강화학습을 결합한 암호화폐 예측\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Meta-learning feature extractor\n",
    "        x = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        # Multi-modal processing\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Attention mechanism for meta-learning\n",
    "        attn = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        shared = Bidirectional(LSTM(64, kernel_regularizer=l2(0.01)))(x)\n",
    "        shared = BatchNormalization()(shared)\n",
    "        shared = Dropout(0.3)(shared)\n",
    "\n",
    "        # Multi-objective reward design\n",
    "        dir_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        dir_head = BatchNormalization()(dir_head)\n",
    "        dir_head = Dropout(0.3)(dir_head)\n",
    "        dir_output = Dense(1, activation='sigmoid', name='direction')(dir_head)\n",
    "\n",
    "        ret_head = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(shared)\n",
    "        ret_head = BatchNormalization()(ret_head)\n",
    "        ret_head = Dropout(0.3)(ret_head)\n",
    "        ret_output = Dense(1, name='return')(ret_head)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[dir_output, ret_output])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            loss={'direction': 'binary_crossentropy', 'return': 'mse'},\n",
    "            loss_weights={'direction': 1.2, 'return': 0.8},  # Adaptive weighting\n",
    "            metrics={'direction': 'accuracy', 'return': 'mae'}\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "        model.fit(\n",
    "            X_train, {'direction': y_train_dir, 'return': y_train_ret},\n",
    "            validation_data=(X_val, {'direction': y_val_dir, 'return': y_val_ret}),\n",
    "            epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0\n",
    "        )\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee90b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"모델 평가 및 백테스팅 (Task별 전략 구현)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        if isinstance(pred, list):\n",
    "            # 멀티태스크: 각 output별로 반환\n",
    "            return [p.squeeze() for p in pred] \n",
    "        else:\n",
    "            if len(pred.shape) > 1 and pred.shape[1] == 1:\n",
    "                pred = pred.squeeze()\n",
    "            return pred\n",
    "\n",
    "    \n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                     X_test, y_test, test_returns, test_dates, model_name,\n",
    "                                     is_deep_learning=False):\n",
    "        \"\"\"분류 모델 평가 - Binary Signal 전략\"\"\"\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        # 예측\n",
    "        train_pred = self._predict_model(model, X_train)\n",
    "        val_pred = self._predict_model(model, X_val)\n",
    "        test_pred = self._predict_model(model, X_test)\n",
    "        if is_deep_learning:\n",
    "            test_pred_proba = test_pred\n",
    "            # 멀티태스크: 분류 output만 선택\n",
    "            if isinstance(train_pred, list):\n",
    "                train_pred = train_pred[0]\n",
    "                val_pred = val_pred[0]\n",
    "                test_pred = test_pred[0]\n",
    "            train_pred = (train_pred > 0.5).astype(int).ravel()\n",
    "            val_pred = (val_pred > 0.5).astype(int).ravel()\n",
    "            test_pred = (test_pred > 0.5).astype(int).ravel()\n",
    "        else:\n",
    "            test_pred_proba = None\n",
    "        \n",
    "        # 분류 지표\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        \n",
    "        test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "        test_rec = recall_score(y_test, test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, test_pred)\n",
    "        \n",
    "        # 백테스팅 - Binary Signal 전략\n",
    "        backtest = self._backtest_classification(\n",
    "            test_pred, test_pred_proba, test_returns, test_dates\n",
    "        )\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_acc,\n",
    "            'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc,\n",
    "            'Test_Precision': test_prec,\n",
    "            'Test_Recall': test_rec,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_AUC_ROC': test_roc_auc,\n",
    "            **backtest\n",
    "        })\n",
    "        \n",
    "        return self.results[-1]\n",
    "    \n",
    "    def evaluate_regression_model(self, model, X_train, y_train, X_val, y_val,\n",
    "                                  X_test, y_test, test_returns, test_dates, model_name,\n",
    "                                  is_deep_learning=False, task_output_index=None): \n",
    "        \"\"\"회귀 모델 평가 - Proportional Position Sizing 전략\"\"\"\n",
    "\n",
    "        # 예측\n",
    "        if is_deep_learning and task_output_index is not None:\n",
    "            train_pred = self._predict_model(model, X_train)[task_output_index]\n",
    "            val_pred = self._predict_model(model, X_val)[task_output_index]\n",
    "            test_pred = self._predict_model(model, X_test)[task_output_index]\n",
    "        else:\n",
    "            train_pred = self._predict_model(model, X_train)\n",
    "            val_pred = self._predict_model(model, X_val)\n",
    "            test_pred = self._predict_model(model, X_test)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        \n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        # MAPE 계산 (0으로 나누기 방지)\n",
    "        mask = y_test != 0\n",
    "        if mask.sum() > 0:\n",
    "            test_mape = np.mean(np.abs((y_test[mask] - test_pred[mask]) / y_test[mask])) * 100\n",
    "        else:\n",
    "            test_mape = np.nan\n",
    "        \n",
    "        # 방향 예측 정확도\n",
    "        test_pred_direction = (test_pred > 0).astype(int)\n",
    "        test_true_direction = (y_test > 0).astype(int)\n",
    "        direction_acc = accuracy_score(test_true_direction, test_pred_direction)\n",
    "        \n",
    "        # 백테스팅 - Proportional Position Sizing 전략\n",
    "        backtest = self._backtest_regression(\n",
    "            test_pred, test_returns, test_dates\n",
    "        )\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_RMSE': train_rmse,\n",
    "            'Val_RMSE': val_rmse,\n",
    "            'Test_RMSE': test_rmse,\n",
    "            'Test_MAE': test_mae,\n",
    "            'Test_R2': test_r2,\n",
    "            'Test_MAPE': test_mape,\n",
    "            'Direction_Accuracy': direction_acc,\n",
    "            **backtest\n",
    "        })\n",
    "        \n",
    "        return self.results[-1]\n",
    "    \n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        #print(f\"[DEBUG] _predict_model - Raw prediction shape: {np.shape(pred)}\")\n",
    "\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for i, p in enumerate(pred):\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    #print(f\"[DEBUG] _predict_model - pred[{i}] shape: {p.shape}\")\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "\n",
    "    def evaluate_multitask_model(self, model, X_test, y_test_list, test_returns, test_dates, \n",
    "                                  model_name, task_types=['classification', 'regression']):\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        if isinstance(preds, list):\n",
    "            preds = [p.squeeze() if p.shape[-1] == 1 else p for p in preds]\n",
    "        else:\n",
    "            preds = [preds.squeeze()]\n",
    "\n",
    "        for i, (pred, y_true, task_type) in enumerate(zip(preds, y_test_list, task_types)):\n",
    "\n",
    "            if pred.ndim == 1 or (pred.ndim == 2 and pred.shape[1] == 1):\n",
    "                single_pred = pred.squeeze()\n",
    "                single_true = y_true.squeeze()\n",
    "                suffix = f\"{model_name}_task{i+1}\"\n",
    "\n",
    "                if task_type == 'classification':\n",
    "                    self.evaluate_classification_model(\n",
    "                        model, X_test, single_true, X_test, single_true, X_test, single_true,\n",
    "                        test_returns, test_dates, f\"{suffix}_cls\", is_deep_learning=True\n",
    "                    )\n",
    "                else:\n",
    "                    self.evaluate_regression_model(\n",
    "                        model, X_test, single_true, X_test, single_true, X_test, single_true,\n",
    "                        test_returns, test_dates, f\"{suffix}_reg\", is_deep_learning=True,task_output_index=i\n",
    "                    )\n",
    "\n",
    "            elif pred.ndim == 2 and pred.shape[1] > 1:\n",
    "                for j in range(pred.shape[1]):\n",
    "                    single_pred = pred[:, j]\n",
    "                    if y_true.ndim == 2:\n",
    "                        single_true = y_true[:, j]\n",
    "                    else:\n",
    "                        single_true = y_true  \n",
    "\n",
    "                    suffix = f\"{model_name}_task{i+1}_col{j+1}\"\n",
    "\n",
    "                    if task_type == 'classification':\n",
    "                        self.evaluate_classification_model(\n",
    "                            model, X_test, single_true, X_test, single_true, X_test, single_true,\n",
    "                            test_returns, test_dates, f\"{suffix}_cls\", is_deep_learning=True\n",
    "                        )\n",
    "                    else:\n",
    "                        self.evaluate_regression_model(\n",
    "                            model, X_test, single_true, X_test, single_true, X_test, single_true,\n",
    "                            test_returns, test_dates, f\"{suffix}_reg\", is_deep_learning=True\n",
    "                        )\n",
    "\n",
    "        backtest = self._backtest_multitask(\n",
    "            preds, y_test_list, test_returns, test_dates, task_types\n",
    "        )\n",
    "\n",
    "        self.results.append({\n",
    "            'Model': f\"{model_name}_combined\",\n",
    "            **backtest\n",
    "        })\n",
    "\n",
    "        return self.results[-1]\n",
    "\n",
    "    \n",
    "    def _backtest_classification(self, predictions, probabilities, returns, dates, \n",
    "                                 trading_cost=0.0004, slippage_func=None, confidence_threshold=0.6):\n",
    "        \"\"\"\n",
    "        분류 모델 백테스팅 전략\n",
    "\n",
    "        Reference:\n",
    "        - Piparo (2025): \"Backtesting Expected Shortfall in Cryptocurrencies\" - SSRN 5296678\n",
    "        - Mettalex (2025): \"CEX vs DEX Trading\" - CEX 평균 거래비용 0.04%\n",
    "\n",
    "        Fixes:\n",
    "        - Look-ahead bias prevention with np.roll\n",
    "        - Adaptive slippage based on volatility\n",
    "        \"\"\"\n",
    "        if slippage_func is None:\n",
    "            # 동적 슬리피지: 변동성 고려\n",
    "            realized_vol = pd.Series(returns).rolling(30, min_periods=1).std().values\n",
    "            slippage_func = lambda pos_change, vol: 0.0001 * np.sqrt(abs(pos_change)) * np.clip(vol / 0.02, 0.5, 3.0)\n",
    "\n",
    "        if isinstance(predictions, list):\n",
    "            pred_bin = predictions[0]  \n",
    "        else:\n",
    "            pred_bin = predictions\n",
    "        if isinstance(probabilities, list):\n",
    "            proba = probabilities[0]\n",
    "        else:\n",
    "            proba = probabilities\n",
    "\n",
    "        positions = np.where(pred_bin == 1, 1, 0)\n",
    "        positions = np.roll(positions, 1)  # Look-ahead bias 방지\n",
    "        positions[0] = 0\n",
    "\n",
    "        position_changes = np.abs(np.diff(positions, prepend=0))\n",
    "\n",
    "        # 변동성 기반 슬리피지 계산\n",
    "        if callable(slippage_func):\n",
    "            try:\n",
    "                slippage_costs = slippage_func(position_changes, realized_vol)\n",
    "            except:\n",
    "                slippage_costs = slippage_func(position_changes)\n",
    "        else:\n",
    "            slippage_costs = 0.0001 * np.sqrt(position_changes)\n",
    "\n",
    "        trading_costs = position_changes * trading_cost + slippage_costs\n",
    "        strategy_returns = positions * returns - trading_costs\n",
    "\n",
    "        basic_metrics = self._calculate_metrics(strategy_returns, positions, position_changes, returns)\n",
    "\n",
    "        if proba is not None:\n",
    "            high_confidence = (proba > confidence_threshold) | (proba < (1 - confidence_threshold))\n",
    "            confident_positions = np.where(\n",
    "                high_confidence & (proba > 0.5), 1, 0\n",
    "            )\n",
    "            confident_positions = np.roll(confident_positions, 1)\n",
    "            confident_positions[0] = 0\n",
    "\n",
    "            position_changes_conf = np.abs(np.diff(confident_positions, prepend=0))\n",
    "\n",
    "            try:\n",
    "                slippage_costs_conf = slippage_func(position_changes_conf, realized_vol)\n",
    "            except:\n",
    "                slippage_costs_conf = slippage_func(position_changes_conf)\n",
    "\n",
    "            trading_costs_conf = position_changes_conf * trading_cost + slippage_costs_conf\n",
    "            confident_returns = confident_positions * returns - trading_costs_conf\n",
    "\n",
    "            conf_metrics = self._calculate_metrics(confident_returns, confident_positions, position_changes_conf, returns)\n",
    "\n",
    "            return {\n",
    "                'Total_Return(%)': basic_metrics['Total_Return(%)'],\n",
    "                'Sharpe': basic_metrics['Sharpe'],\n",
    "                'Sortino': basic_metrics['Sortino'],\n",
    "                'Calmar': basic_metrics['Calmar'],\n",
    "                'Max_Drawdown(%)': basic_metrics['Max_Drawdown(%)'],\n",
    "                'CVaR_95(%)': basic_metrics['CVaR_95(%)'],\n",
    "                'Win_Rate(%)': basic_metrics['Win_Rate(%)'],\n",
    "                'Total_Trades': basic_metrics['Total_Trades'],\n",
    "                'Confident_Return(%)': conf_metrics['Total_Return(%)'],\n",
    "                'Confident_Sharpe': conf_metrics['Sharpe'],\n",
    "                'Profit_Factor': basic_metrics.get('Profit_Factor', 0),\n",
    "                'Confident_Trades': conf_metrics['Total_Trades']\n",
    "            }\n",
    "        else:\n",
    "            return basic_metrics\n",
    "\n",
    "\n",
    "    def _backtest_regression(self, predictions, returns, dates, trading_cost=0.0004,\n",
    "                            slippage_func=None, volatility_target=0.30):\n",
    "        \"\"\"\n",
    "        회귀 모델 백테스팅 전략\n",
    "\n",
    "        Reference:\n",
    "        - Mettalex (2025): \"Perpetual Futures Trading CEX vs DEX\" - 거래비용 및 슬리피지\n",
    "        - Piparo (2025): Expected Shortfall 백테스팅 - 암호화폐 극단 손실 특성\n",
    "        - Dudek (2024): \"Forecasting Crypto Volatility\" - 변동성 특성\n",
    "        - SpeedBot (2025): \"Position Sizing Techniques\" - Kelly Criterion 적용\n",
    "\n",
    "        Fixes:\n",
    "        - Corrected Kelly Criterion formula\n",
    "        - Reduced rolling window from 60 to 30 days\n",
    "        - More conservative Kelly fraction clipping (0.10 instead of 0.25)\n",
    "        - Adaptive slippage model\n",
    "        \"\"\"\n",
    "        # 변동성 계산 (30일 윈도우로 단축)\n",
    "        realized_vol = pd.Series(returns).rolling(30, min_periods=1).std().values\n",
    "\n",
    "        if slippage_func is None:\n",
    "            # 동적 슬리피지: 변동성이 높을수록 슬리피지 증가\n",
    "            slippage_func = lambda pos_change, vol: 0.0001 * np.sqrt(abs(pos_change)) * np.clip(vol / 0.02, 0.5, 3.0)\n",
    "\n",
    "        # === 1. Directional Strategy (기본) ===\n",
    "        directional_positions = np.where(predictions > 0, 1, 0)\n",
    "        directional_positions = np.roll(directional_positions, 1)\n",
    "        directional_positions[0] = 0\n",
    "\n",
    "        position_changes_dir = np.abs(np.diff(directional_positions, prepend=0))\n",
    "        try:\n",
    "            slippage_costs_dir = slippage_func(position_changes_dir, realized_vol)\n",
    "        except:\n",
    "            slippage_costs_dir = slippage_func(position_changes_dir)\n",
    "\n",
    "        trading_costs_dir = position_changes_dir * trading_cost + slippage_costs_dir\n",
    "        directional_returns = directional_positions * returns - trading_costs_dir\n",
    "\n",
    "        dir_metrics = self._calculate_metrics(directional_returns, directional_positions, position_changes_dir, returns)\n",
    "\n",
    "        # === 2. Proportional Strategy with Kelly Criterion ===\n",
    "        pred_mean = np.mean(predictions)\n",
    "        pred_std = np.std(predictions)\n",
    "\n",
    "        # Kelly Criterion 계산 (수정된 공식)\n",
    "        win_rate = np.mean((predictions > 0) == (returns > 0))\n",
    "        avg_win = np.mean(returns[returns > 0]) if np.sum(returns > 0) > 0 else 0.01\n",
    "        avg_loss = abs(np.mean(returns[returns < 0])) if np.sum(returns < 0) > 0 else 0.01\n",
    "        win_loss_ratio = avg_win / avg_loss\n",
    "\n",
    "        # 올바른 Kelly Criterion 공식: f = (p * b - q) / b\n",
    "        # p = win_rate, q = 1 - win_rate, b = win_loss_ratio\n",
    "        kelly_fraction = (win_rate * win_loss_ratio - (1 - win_rate)) / win_loss_ratio if win_loss_ratio > 0 else 0\n",
    "        kelly_fraction = np.clip(kelly_fraction, 0, 0.10)  # 더 보수적으로 10% 제한\n",
    "\n",
    "        # 신호 강도 계산\n",
    "        if pred_std > 0:\n",
    "            normalized_pred = (predictions - pred_mean) / pred_std\n",
    "            signal_strength = 1 / (1 + np.exp(-normalized_pred))\n",
    "        else:\n",
    "            signal_strength = np.where(predictions > 0, 0.5, 0)\n",
    "\n",
    "        # Kelly Criterion 기반 포지션 사이징 (스케일링 조정)\n",
    "        proportional_positions = signal_strength * kelly_fraction * 10  # 10배 스케일링\n",
    "        proportional_positions = np.clip(proportional_positions, 0, 1)\n",
    "        proportional_positions = np.roll(proportional_positions, 1)\n",
    "        proportional_positions[0] = 0\n",
    "\n",
    "        position_changes_prop = np.abs(np.diff(proportional_positions, prepend=0))\n",
    "        try:\n",
    "            slippage_costs_prop = slippage_func(position_changes_prop, realized_vol)\n",
    "        except:\n",
    "            slippage_costs_prop = slippage_func(position_changes_prop)\n",
    "\n",
    "        trading_costs_prop = position_changes_prop * trading_cost + slippage_costs_prop\n",
    "        proportional_returns = proportional_positions * returns - trading_costs_prop\n",
    "\n",
    "        prop_metrics = self._calculate_metrics(proportional_returns, proportional_positions, position_changes_prop, returns)\n",
    "\n",
    "        # === 3. Volatility-Scaled Strategy ===\n",
    "        rolling_window = 30  # 60일 -> 30일로 단축\n",
    "\n",
    "        # EWMA 방식으로 변경 (더 반응성 있게)\n",
    "        ewma_vol = pd.Series(returns).ewm(span=20, min_periods=rolling_window).std().values\n",
    "\n",
    "        annualized_vol = ewma_vol * np.sqrt(365)\n",
    "\n",
    "        vol_scalar = np.where(\n",
    "            (annualized_vol > 0) & (~np.isnan(annualized_vol)), \n",
    "            volatility_target / annualized_vol, \n",
    "            1\n",
    "        )\n",
    "        vol_scalar = np.clip(vol_scalar, 0.3, 1.5)\n",
    "\n",
    "        vol_scaled_positions = proportional_positions * vol_scalar\n",
    "        vol_scaled_positions = np.clip(vol_scaled_positions, 0, 1.0)\n",
    "\n",
    "        vol_scaled_positions[:rolling_window] = 0\n",
    "\n",
    "        position_changes_vol = np.abs(np.diff(vol_scaled_positions, prepend=0))\n",
    "        try:\n",
    "            slippage_costs_vol = slippage_func(position_changes_vol, realized_vol)\n",
    "        except:\n",
    "            slippage_costs_vol = slippage_func(position_changes_vol)\n",
    "\n",
    "        trading_costs_vol = position_changes_vol * trading_cost + slippage_costs_vol\n",
    "        vol_scaled_returns = vol_scaled_positions * returns - trading_costs_vol\n",
    "\n",
    "        vol_metrics = self._calculate_metrics(vol_scaled_returns, vol_scaled_positions, position_changes_vol, returns)\n",
    "\n",
    "        return {\n",
    "            'Directional_Return(%)': dir_metrics['Total_Return(%)'],\n",
    "            'Directional_Sharpe': dir_metrics['Sharpe'],\n",
    "\n",
    "            'Total_Return(%)': prop_metrics['Total_Return(%)'],\n",
    "            'Sharpe': prop_metrics['Sharpe'],\n",
    "            'Sortino': prop_metrics['Sortino'],\n",
    "            'Calmar': prop_metrics['Calmar'],\n",
    "            'Max_Drawdown(%)': prop_metrics['Max_Drawdown(%)'],\n",
    "            'CVaR_95(%)': prop_metrics['CVaR_95(%)'],\n",
    "            'Win_Rate(%)': prop_metrics['Win_Rate(%)'],\n",
    "            'Total_Trades': prop_metrics['Total_Trades'],\n",
    "\n",
    "            'VolScaled_Return(%)': vol_metrics['Total_Return(%)'],\n",
    "            'VolScaled_Sharpe': vol_metrics['Sharpe'],\n",
    "            'Profit_Factor': prop_metrics.get('Profit_Factor', 0),\n",
    "            'Kelly_Fraction': kelly_fraction\n",
    "        }\n",
    "\n",
    "\n",
    "    def _backtest_multitask(self, predictions, y_test_list, returns, dates, task_types, \n",
    "                           trading_cost=0.0004, slippage_func=None):\n",
    "        \"\"\"\n",
    "        멀티태스크 모델 백테스팅\n",
    "\n",
    "        Reference:\n",
    "        - Mettalex (2025): CEX 평균 거래비용 0.04%\n",
    "\n",
    "        Fixes:\n",
    "        - Adaptive slippage model\n",
    "        \"\"\"\n",
    "        realized_vol = pd.Series(returns).rolling(30, min_periods=1).std().values\n",
    "\n",
    "        if slippage_func is None:\n",
    "            slippage_func = lambda pos_change, vol: 0.0001 * np.sqrt(abs(pos_change)) * np.clip(vol / 0.02, 0.5, 3.0)\n",
    "\n",
    "        classification_pred = None\n",
    "        regression_pred = None\n",
    "        for pred, task_type in zip(predictions, task_types):\n",
    "            if task_type == 'classification':\n",
    "                classification_pred = (pred > 0.5).astype(float).ravel()\n",
    "            elif task_type == 'regression':\n",
    "                regression_pred = pred.ravel()\n",
    "\n",
    "        if regression_pred is not None:\n",
    "            reg_mean = np.mean(regression_pred)\n",
    "            reg_std = np.std(regression_pred)\n",
    "            if reg_std > 0:\n",
    "                normalized_reg = (regression_pred - reg_mean) / reg_std\n",
    "                signal_strength = 1 / (1 + np.exp(-normalized_reg))\n",
    "            else:\n",
    "                signal_strength = np.where(regression_pred > 0, 0.5, 0)\n",
    "            regression_positions = signal_strength\n",
    "        else:\n",
    "            regression_positions = None\n",
    "\n",
    "        if classification_pred is not None and regression_positions is not None:\n",
    "            and_positions = classification_pred * regression_positions\n",
    "            or_positions = np.clip(classification_pred + regression_positions, 0, 1)\n",
    "            mul_positions = classification_pred * signal_strength\n",
    "        else:\n",
    "            and_positions = or_positions = mul_positions = None\n",
    "\n",
    "        results = {}\n",
    "        if classification_pred is not None:\n",
    "            results['direction'] = self._backtest_strategy(classification_pred, returns, dates, trading_cost, slippage_func, realized_vol)\n",
    "        if regression_positions is not None:\n",
    "            results['regression'] = self._backtest_strategy(regression_positions, returns, dates, trading_cost, slippage_func, realized_vol)\n",
    "        if and_positions is not None:\n",
    "            results['and'] = self._backtest_strategy(and_positions, returns, dates, trading_cost, slippage_func, realized_vol)\n",
    "        if or_positions is not None:\n",
    "            results['or'] = self._backtest_strategy(or_positions, returns, dates, trading_cost, slippage_func, realized_vol)\n",
    "        if mul_positions is not None:\n",
    "            results['mul'] = self._backtest_strategy(mul_positions, returns, dates, trading_cost, slippage_func, realized_vol)\n",
    "\n",
    "        best_key = max(results, key=lambda k: results[k]['Sharpe'])\n",
    "        best_result = results[best_key]\n",
    "        best_result['Best_Strategy'] = best_key\n",
    "\n",
    "        return best_result\n",
    "\n",
    "\n",
    "    def _backtest_strategy(self, positions, returns, dates, trading_cost, slippage_func, realized_vol):\n",
    "        \"\"\"\n",
    "        멀티태스크용 헬퍼 함수\n",
    "\n",
    "        Fixes:\n",
    "        - Added volatility-aware slippage\n",
    "        \"\"\"\n",
    "        positions = np.roll(positions, 1)\n",
    "        positions[0] = 0\n",
    "\n",
    "        position_changes = np.abs(np.diff(positions, prepend=0))\n",
    "\n",
    "        try:\n",
    "            slippage_costs = slippage_func(position_changes, realized_vol)\n",
    "        except:\n",
    "            slippage_costs = slippage_func(position_changes)\n",
    "\n",
    "        trading_costs = position_changes * trading_cost + slippage_costs\n",
    "        strategy_returns = positions * returns - trading_costs\n",
    "\n",
    "        return self._calculate_metrics(strategy_returns, positions, position_changes, returns)\n",
    "\n",
    "\n",
    "    def _calculate_metrics(self, strategy_returns, positions, position_changes, returns=None):\n",
    "        \"\"\"\n",
    "        공통 지표 계산\n",
    "\n",
    "        Reference:\n",
    "        - Piparo (2025): \"Backtesting Expected Shortfall\" - CVaR 계산 필수\n",
    "        - Mettalex (2025): 암호화폐 극단 손실 특성\n",
    "\n",
    "        Fixes:\n",
    "        - CVaR annualization corrected: sqrt(365) -> 365\n",
    "        - Sharpe ratio uses 365 days (crypto trades 24/7)\n",
    "        \"\"\"\n",
    "\n",
    "        cumulative_returns = (1 + strategy_returns).cumprod()\n",
    "        total_return = (cumulative_returns[-1] - 1) * 100\n",
    "\n",
    "        # Sharpe Ratio (암호화폐는 365일 기준)\n",
    "        if len(strategy_returns) > 0 and np.std(strategy_returns) > 0:\n",
    "            sharpe = np.mean(strategy_returns) / np.std(strategy_returns) * np.sqrt(365)\n",
    "        else:\n",
    "            sharpe = 0\n",
    "\n",
    "        # Sortino Ratio\n",
    "        downside_returns = strategy_returns[strategy_returns < 0]\n",
    "        if len(downside_returns) > 0 and np.std(downside_returns) > 0:\n",
    "            sortino = np.mean(strategy_returns) / np.std(downside_returns) * np.sqrt(365)\n",
    "        else:\n",
    "            sortino = 0\n",
    "\n",
    "        # Maximum Drawdown\n",
    "        cummax = np.maximum.accumulate(cumulative_returns)\n",
    "        drawdown = (cumulative_returns - cummax) / cummax\n",
    "        max_dd = drawdown.min() * 100 if len(drawdown) > 0 else 0\n",
    "\n",
    "        # Calmar Ratio\n",
    "        days = len(strategy_returns)\n",
    "        if days > 0 and max_dd != 0:\n",
    "            annualized_return = ((cumulative_returns[-1]) ** (365 / days) - 1) * 100\n",
    "            calmar = annualized_return / abs(max_dd)\n",
    "        else:\n",
    "            calmar = 0\n",
    "\n",
    "        # CVaR (Conditional Value at Risk) - 수정된 연율화\n",
    "        if len(strategy_returns) > 0:\n",
    "            var_95 = np.percentile(strategy_returns, 5)\n",
    "            # CVaR는 평균값이므로 365를 곱함 (sqrt 아님!)\n",
    "            cvar_95 = np.mean(strategy_returns[strategy_returns <= var_95]) * 365 * 100\n",
    "        else:\n",
    "            cvar_95 = 0\n",
    "\n",
    "        # Trade-based Win Rate and Profit Factor\n",
    "        trade_returns = []\n",
    "        in_position = False\n",
    "        entry_idx = 0\n",
    "        entry_value = 1.0\n",
    "        position_threshold = 0.01\n",
    "\n",
    "        for i in range(len(positions)):\n",
    "            if positions[i] > position_threshold and not in_position:\n",
    "                in_position = True\n",
    "                entry_idx = i\n",
    "                entry_value = cumulative_returns[i] if i > 0 else 1.0\n",
    "\n",
    "            elif positions[i] <= position_threshold and in_position:\n",
    "                exit_value = cumulative_returns[i]\n",
    "                trade_return = (exit_value / entry_value - 1) if entry_value > 0 else 0\n",
    "                trade_returns.append(trade_return)\n",
    "                in_position = False\n",
    "\n",
    "        if in_position and len(cumulative_returns) > 0:\n",
    "            exit_value = cumulative_returns[-1]\n",
    "            trade_return = (exit_value / entry_value - 1) if entry_value > 0 else 0\n",
    "            trade_returns.append(trade_return)\n",
    "\n",
    "        # Win Rate 계산\n",
    "        if len(trade_returns) > 0:\n",
    "            winning_trades = sum(1 for r in trade_returns if r > 0)\n",
    "            total_trades = len(trade_returns)\n",
    "            win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0\n",
    "        else:\n",
    "            # Fallback: 일별 수익 기반\n",
    "            winning_days = np.sum(strategy_returns > 0)\n",
    "            losing_days = np.sum(strategy_returns < 0)\n",
    "            total_trades = int(np.sum(position_changes > 0))\n",
    "            win_rate = (winning_days / (winning_days + losing_days) * 100) if (winning_days + losing_days) > 0 else 0\n",
    "            total_trades = max(total_trades, 1)\n",
    "\n",
    "        # Profit Factor\n",
    "        gross_profit = np.sum(strategy_returns[strategy_returns > 0])\n",
    "        gross_loss = abs(np.sum(strategy_returns[strategy_returns < 0]))\n",
    "        profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'Total_Return(%)': total_return,\n",
    "            'Sharpe': sharpe,\n",
    "            'Sortino': sortino,\n",
    "            'Calmar': calmar,\n",
    "            'Max_Drawdown(%)': max_dd,\n",
    "            'CVaR_95(%)': cvar_95,\n",
    "            'Win_Rate(%)': win_rate,\n",
    "            'Total_Trades': len(trade_returns) if len(trade_returns) > 0 else total_trades,\n",
    "            'Profit_Factor': profit_factor\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97065780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 모델 설정 정의\n",
    "# ============================================================================\n",
    "\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': False},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'SVM', 'func': DirectionModels.svm, 'needs_val': False},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': False},\n",
    "    {'index': 6, 'name': 'NaiveBayes', 'func': DirectionModels.naive_bayes, 'needs_val': False},\n",
    "    {'index': 7, 'name': 'KNN', 'func': DirectionModels.knn, 'needs_val': False},\n",
    "    {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': False},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 10, 'name': 'DecisionTree', 'func': DirectionModels.decision_tree, 'needs_val': False},\n",
    "    {'index': 11, 'name': 'ExtraTrees', 'func': DirectionModels.extra_trees, 'needs_val': False},\n",
    "    {'index': 12, 'name': 'Bagging', 'func': DirectionModels.bagging, 'needs_val': False},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': False},\n",
    "    {'index': 14, 'name': 'TabNet', 'func': DirectionModels.tabnet, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "    {'index': 16, 'name': 'VotingHard', 'func': DirectionModels.voting_hard, 'needs_val': False},\n",
    "    {'index': 17, 'name': 'VotingSoft', 'func': DirectionModels.voting_soft, 'needs_val': False},\n",
    "    {'index': 18, 'name': 'MLP', 'func': DirectionModels.mlp, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# ======================== 시계열/딥러닝/트랜스포머/하이브리드 ===========================\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru},\n",
    "    {'index': 22, 'name': 'Stacked_LSTM', 'func': DirectionModels.stacked_lstm},\n",
    "    {'index': 23, 'name': 'CNN_LSTM', 'func': DirectionModels.cnn_lstm},\n",
    "    {'index': 24, 'name': 'CNN_GRU', 'func': DirectionModels.cnn_gru},\n",
    "    {'index': 25, 'name': 'CNN_BiLSTM', 'func': DirectionModels.cnn_bilstm},\n",
    "    {'index': 26, 'name': 'LSTM_Attention', 'func': DirectionModels.lstm_attention},\n",
    "    {'index': 27, 'name': 'Transformer', 'func': DirectionModels.transformer},\n",
    "    {'index': 28, 'name': 'TCN', 'func': DirectionModels.tcn},\n",
    "    {'index': 29, 'name': 'DTW_LSTM', 'func': DirectionModels.dtw_lstm},\n",
    "    {'index': 30, 'name': 'Informer', 'func': DirectionModels.informer},\n",
    "    {'index': 31, 'name': 'NBEATS', 'func': DirectionModels.nbeats},\n",
    "    {'index': 32, 'name': 'TFT', 'func': DirectionModels.temporal_fusion_transformer},\n",
    "    {'index': 33, 'name': 'Performer', 'func': DirectionModels.performer},\n",
    "    {'index': 34, 'name': 'PatchTST', 'func': DirectionModels.patchtst},\n",
    "    {'index': 35, 'name': 'Autoformer', 'func': DirectionModels.autoformer},\n",
    "    {'index': 36, 'name': 'iTransformer', 'func': DirectionModels.itransformer},\n",
    "    {'index': 37, 'name': 'EtherVoyant', 'func': DirectionModels.ethervoyant},\n",
    "    {'index': 38, 'name': 'VMD_Hybrid', 'func': DirectionModels.vmd_hybrid},\n",
    "    {'index': 39, 'name': 'SimpleRNN', 'func': DirectionModels.simple_rnn},\n",
    "    {'index': 40, 'name': 'EMD_LSTM', 'func': DirectionModels.emd_lstm},\n",
    "    {'index': 41, 'name': 'Hybrid_LSTM_GRU', 'func': DirectionModels.hybrid_lstm_gru},\n",
    "    {'index': 42, 'name': 'Parallel_CNN', 'func': DirectionModels.parallel_cnn},\n",
    "    {'index': 43, 'name': 'LSTM_XGBoost_Hybrid', 'func': DirectionModels.lstm_xgboost_hybrid},\n",
    "    {'index': 44, 'name': 'Residual_LSTM', 'func': DirectionModels.residual_lstm},\n",
    "    {'index': 45, 'name': 'WaveNet', 'func': DirectionModels.wavenet},\n",
    "]\n",
    "\n",
    "\n",
    "####################################### 멀티 테스크 모델 #####################\n",
    "\n",
    "DL_MODELS_MULTITASK_ENSEMBLE = [\n",
    "    {'index': 1, 'name': 'HardSharing_LSTM_MTL', 'func': MultiTaskModels.hard_sharing_lstm, 'outputs': ['direction', 'return']},\n",
    "    {'index': 2, 'name': 'BiLSTM_MTL', 'func': MultiTaskModels.bilstm_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 3, 'name': 'GRU_MTL', 'func': MultiTaskModels.gru_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 4, 'name': 'SoftSharing_LSTM_MTL', 'func': MultiTaskModels.soft_sharing_lstm, 'outputs': ['direction', 'return']},\n",
    "    {'index': 5, 'name': 'CrossStitch_MTL', 'func': MultiTaskModels.cross_stitch_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 6, 'name': 'MMoE_MTL', 'func': MultiTaskModels.mmoe_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 7, 'name': 'CNN_LSTM_MTL', 'func': MultiTaskModels.cnn_lstm_mtl, 'outputs': ['direction', 'price']},\n",
    "    {'index': 8, 'name': 'CNN_GRU_MTL', 'func': MultiTaskModels.cnn_gru_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 9, 'name': 'Transformer_MTL', 'func': MultiTaskModels.transformer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 10, 'name': 'Stacked_LSTM_MTL', 'func': MultiTaskModels.stacked_lstm_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 11, 'name': 'Attention_MTL', 'func': MultiTaskModels.attention_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 12, 'name': 'TCN_MTL', 'func': MultiTaskModels.tcn_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 13, 'name': 'Hierarchical_MTL', 'func': MultiTaskModels.hierarchical_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 14, 'name': 'WeightedLoss_MTL', 'func': MultiTaskModels.weighted_loss_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 15, 'name': 'Ensemble_MTL', 'func': MultiTaskModels.ensemble_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 16, 'name': 'TabNet_MTL', 'func': MultiTaskModels.tabnet_mtl, 'outputs': ['direction', 'return'],'needs_val':True},\n",
    "    {'index': 17, 'name': 'Informer_MTL', 'func': MultiTaskModels.informer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 18, 'name': 'NBEATS_MTL', 'func': MultiTaskModels.nbeats_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 19, 'name': 'TFT_MTL', 'func': MultiTaskModels.tft_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 20, 'name': 'Performer_MTL', 'func': MultiTaskModels.performer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 21, 'name': 'PatchTST_MTL', 'func': MultiTaskModels.patchtst_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 22, 'name': 'Autoformer_MTL', 'func': MultiTaskModels.autoformer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 23, 'name': 'iTransformer_MTL', 'func': MultiTaskModels.itransformer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 24, 'name': 'EtherVoyant_MTL', 'func': MultiTaskModels.ethervoyant_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 25, 'name': 'VMD_Hybrid_MTL', 'func': MultiTaskModels.vmd_hybrid_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 26, 'name': 'SNAS_MTL', 'func': MultiTaskModels.snas_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 27, 'name': 'DLinear_MTL', 'func': MultiTaskModels.dlinear_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 28, 'name': 'FEDformer_MTL', 'func': MultiTaskModels.fedformer_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 29, 'name': 'UniTS_MTL', 'func': MultiTaskModels.units_mtl, 'outputs': ['direction', 'return']},\n",
    "    {'index': 30, 'name': 'MetaRL_Crypto_MTL', 'func': MultiTaskModels.metarl_crypto_mtl, 'outputs': ['direction', 'return']}\n",
    "]\n",
    "\n",
    "##############################   회귀 모델 ################################\n",
    "\n",
    "\n",
    "# ML 모델 설정 (1-4번)\n",
    "ML_MODELS_REGRESSION = [\n",
    "    {'index': 1, 'name': 'RandomForest_Reg', 'func': RegressionModels.random_forest_reg, 'needs_val': False},\n",
    "    {'index': 2, 'name': 'LightGBM_Reg', 'func': RegressionModels.lightgbm_reg, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost_Reg', 'func': RegressionModels.xgboost_reg, 'needs_val': True},\n",
    "    {'index': 4, 'name': 'SVR', 'func': RegressionModels.svr, 'needs_val': False}\n",
    "]\n",
    "\n",
    "# 딥러닝 모델 설정 (5-35번)\n",
    "DL_MODELS_REGRESSION = [\n",
    "    # 기본 RNN 모델 (5-8)\n",
    "    {'index': 5, 'name': 'LSTM_Reg', 'func': RegressionModels.lstm_reg},\n",
    "    {'index': 6, 'name': 'BiLSTM_Reg', 'func': RegressionModels.bilstm_reg},\n",
    "    {'index': 7, 'name': 'GRU', 'func': RegressionModels.gru},\n",
    "    {'index': 8, 'name': 'Stacked_LSTM_Reg', 'func': RegressionModels.stacked_lstm_reg},\n",
    "    \n",
    "    # CNN-RNN 하이브리드 (9-11)\n",
    "    {'index': 9, 'name': 'CNN-LSTM', 'func': RegressionModels.cnn_lstm},\n",
    "    {'index': 10, 'name': 'CNN-GRU', 'func': RegressionModels.cnn_gru},\n",
    "    {'index': 11, 'name': 'CNN-BiLSTM', 'func': RegressionModels.cnn_bilstm},\n",
    "    \n",
    "    # 고급 시퀀스 모델 (12-14)\n",
    "    {'index': 12, 'name': 'Seq2Seq', 'func': RegressionModels.seq2seq},\n",
    "    {'index': 13, 'name': 'WaveNet', 'func': RegressionModels.wavenet},\n",
    "    {'index': 14, 'name': 'TCN_Reg', 'func': RegressionModels.tcn_reg},\n",
    "    \n",
    "    # Transformer 기반 (15-25)\n",
    "    {'index': 15, 'name': 'Transformer_Reg', 'func': RegressionModels.transformer_reg},\n",
    "    {'index': 16, 'name': 'TabNet_Reg', 'func': RegressionModels.tabnet_reg,'needs_val':True},\n",
    "    {'index': 17, 'name': 'Informer_Reg', 'func': RegressionModels.informer_reg},\n",
    "    {'index': 18, 'name': 'NBEATS_Reg', 'func': RegressionModels.nbeats_reg},\n",
    "    {'index': 19, 'name': 'TFT_Reg', 'func': RegressionModels.tft_reg},\n",
    "    {'index': 20, 'name': 'Performer_Reg', 'func': RegressionModels.performer_reg},\n",
    "    {'index': 21, 'name': 'PatchTST_Reg', 'func': RegressionModels.patchtst_reg},\n",
    "    {'index': 22, 'name': 'Autoformer_Reg', 'func': RegressionModels.autoformer_reg},\n",
    "    {'index': 23, 'name': 'iTransformer_Reg', 'func': RegressionModels.itransformer_reg},\n",
    "    {'index': 24, 'name': 'EtherVoyant_Reg', 'func': RegressionModels.ethervoyant_reg},\n",
    "    {'index': 25, 'name': 'VMD_Hybrid_Reg', 'func': RegressionModels.vmd_hybrid_reg},\n",
    "    \n",
    "    # 신규 추가 모델 (26-35)\n",
    "    {'index': 26, 'name': 'DTW_LSTM_Reg', 'func': RegressionModels.dtw_lstm_reg},\n",
    "    {'index': 27, 'name': 'Attention_LSTM_Reg', 'func': RegressionModels.attention_lstm_reg},\n",
    "    {'index': 28, 'name': 'Dual_Attention_Reg', 'func': RegressionModels.dual_attention_reg},\n",
    "    {'index': 29, 'name': 'Cross_Correlation_LSTM', 'func': RegressionModels.cross_correlation_lstm_reg},\n",
    "    {'index': 30, 'name': 'Gradient_Optimized_LSTM', 'func': RegressionModels.gradient_optimized_lstm_reg},\n",
    "    {'index': 31, 'name': 'Ensemble_Stacking_Reg', 'func': RegressionModels.ensemble_stacking_reg},\n",
    "    {'index': 32, 'name': 'Ensemble_Voting_Reg', 'func': RegressionModels.ensemble_voting_reg},\n",
    "    {'index': 33, 'name': 'LSTM_XGBoost_Hybrid', 'func': RegressionModels.lstm_xgboost_hybrid_reg},\n",
    "    {'index': 34, 'name': 'Residual_LSTM_Reg', 'func': RegressionModels.residual_lstm_reg},\n",
    "    {'index': 35, 'name': 'MultiScale_CNN_LSTM', 'func': RegressionModels.multiscale_cnn_lstm_reg}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a58630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator, lookback=30,\n",
    "                    ml_models=None, dl_models=None, task='classification'):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{task.capitalize()} 모델 학습 시작 (총 {len(ml_models) + len(dl_models)}개 모델)\")\n",
    "    print(\"=\"*80)\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    # ML 모델\n",
    "    print(f\"\\n[Part 1/2] Machine Learning 모델 ({len(ml_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    ml_success_count = 0\n",
    "    for model_config in ml_models:\n",
    "        success = trainer.train_ml_model(\n",
    "            model_config, X_train, y_train, X_val, y_val,\n",
    "            X_test, y_test, test_returns, test_dates, task=task\n",
    "        )\n",
    "        if success:\n",
    "            ml_success_count += 1\n",
    "    print(f\"\\n✓ ML 모델 완료: {ml_success_count}/{len(ml_models)}개 성공\")\n",
    "\n",
    "    # DL 모델\n",
    "    print(f\"\\n[Part 2/2] Deep Learning/시계열 모델 ({len(dl_models)}개)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\n시퀀스 데이터 생성 중 (lookback={lookback})...\")\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test, lookback)\n",
    "    test_returns_seq = test_returns[lookback:]\n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    print(f\"  ✓ Train shape: {X_train_seq.shape}\")\n",
    "    print(f\"  ✓ Val shape: {X_val_seq.shape}\")\n",
    "    print(f\"  ✓ Test shape: {X_test_seq.shape}\")\n",
    "    print(f\"  ✓ Input shape: {input_shape}\\n\")\n",
    "    dl_success_count = 0\n",
    "    for model_config in dl_models:\n",
    "        if model_config['name'] in ['TabNet', 'TabNet_Reg', 'Ensemble_Stacking', 'Ensemble_Voting']:\n",
    "            success = trainer.train_ml_model(\n",
    "                model_config, X_train, y_train, X_val, y_val,\n",
    "                X_test, y_test, test_returns, test_dates, task=task\n",
    "            )\n",
    "        else:\n",
    "            if 'outputs' in model_config and len(model_config['outputs']) > 1:\n",
    "                y_train_list = [y_train_seq[:, i] for i in range(y_train_seq.shape[1])]\n",
    "                y_val_list = [y_val_seq[:, i] for i in range(y_val_seq.shape[1])]\n",
    "                y_test_list = [y_test_seq[:, i] for i in range(y_test_seq.shape[1])]\n",
    "                success = trainer.train_dl_multitask_model(\n",
    "                    model_config, X_train_seq, y_train_list, X_val_seq, y_val_list,\n",
    "                    X_test_seq, y_test_list, test_returns_seq, test_dates_seq, input_shape\n",
    "                )\n",
    "            else:\n",
    "                success = trainer.train_dl_model(\n",
    "                    model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq, input_shape, task=task\n",
    "                )\n",
    "        if success:\n",
    "            dl_success_count += 1\n",
    "    print(f\"\\n✓ DL 모델 완료: {dl_success_count}/{len(dl_models)}개 성공\")\n",
    "    total_success = ml_success_count + dl_success_count\n",
    "    total_models = len(ml_models) + len(dl_models)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"전체 학습 완료: {total_success}/{total_models}개 모델 성공\")\n",
    "    print(\"=\"*80)\n",
    "    return total_success\n",
    "\n",
    "def train_models_for_fold(fold_idx, X_train, y_train, X_val, y_val,\n",
    "                          X_test, y_test, test_returns, test_dates,\n",
    "                          evaluator, all_fold_results, lookback=30,\n",
    "                          ml_models=None, dl_models=None, task='classification'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Fold {fold_idx + 1} - {task.capitalize()} 모델 학습\")\n",
    "    print(f\"{'='*80}\")\n",
    "    success_count = train_all_models(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        test_returns, test_dates, evaluator, lookback,\n",
    "        ml_models=ml_models, dl_models=dl_models, task=task\n",
    "    )\n",
    "    fold_summary = evaluator.get_summary_dataframe()\n",
    "    fold_summary['Fold'] = fold_idx + 1\n",
    "    all_fold_results.append(fold_summary)\n",
    "    print(f\"\\n✓ Fold {fold_idx + 1} 완료 ({success_count}개 모델)\")\n",
    "    return fold_summary\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"모델 학습 및 평가를 위한 통합 클래스 (분류/회귀 공통)\"\"\"\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            # DataFrame이면 .iloc, array면 직접 인덱싱\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val,\n",
    "                       X_test, y_test, test_returns, test_dates, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name']\n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, model_config['name']\n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "\n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq,\n",
    "                       X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "                       test_returns_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']}...\")\n",
    "            model = model_config['func'](\n",
    "                X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape\n",
    "            )\n",
    "            if task == 'classification':\n",
    "                self.evaluator.evaluate_classification_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            else:\n",
    "                self.evaluator.evaluate_regression_model(\n",
    "                    model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                    X_test_seq, y_test_seq, test_returns_seq, test_dates_seq,\n",
    "                    model_config['name'], is_deep_learning=True\n",
    "                )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "        \n",
    "    def train_dl_multitask_model(self, model_config, X_train_seq, y_train_list,\n",
    "                                 X_val_seq, y_val_list, X_test_seq, y_test_list,\n",
    "                                 test_returns_seq, test_dates_seq, input_shape):\n",
    "        try:\n",
    "            print(f\"  [{model_config['index']}] {model_config['name']} (멀티태스크)...\")\n",
    "            # unpack y_train_list 등은 [y1, y2] 형태\n",
    "            model = model_config['func'](\n",
    "                X_train_seq, *y_train_list, X_val_seq, *y_val_list, input_shape\n",
    "            )\n",
    "            # 평가: outputs에 따라 자동 분기\n",
    "            self.evaluator.evaluate_multitask_model(\n",
    "                model, X_test_seq, y_test_list, test_returns_seq, test_dates_seq, model_config['name']\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"    ⚠ {model_config['name']} 스킵: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"    상세: {traceback.format_exc()}\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ad0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cases = [\n",
    "    {'name': 'direction', 'target_type': 'direction', 'outputs': ['next_direction']}\n",
    "#     {'name': 'return', 'target_type': 'return', 'outputs': ['next_log_return']},\n",
    "#     {'name': 'direction_return', 'target_type': 'direction_return', 'outputs': ['next_direction', 'next_log_return']},  \n",
    "#     {'name': 'price', 'target_type': 'price', 'outputs': ['next_close']},\n",
    "#     {'name': 'direction_price', 'target_type': 'direction_price', 'outputs': ['next_direction', 'next_close']}\n",
    "]\n",
    "\n",
    "split_methods = [\n",
    "    {'name': 'walk_forward', 'method': 'walk_forward'},\n",
    "    {'name': 'tvt', 'method': 'tvt'}\n",
    "]\n",
    "\n",
    "\n",
    "RESULT_DIR = \"model_results\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "def save_walk_forward_results(all_fold_results, target_name, task):\n",
    "    \n",
    "    detailed_results = []\n",
    "    for fold_idx, fold_df in enumerate(all_fold_results, start=1):\n",
    "        fold_df_copy = fold_df.copy()\n",
    "        fold_df_copy.insert(0, 'Fold', fold_idx)\n",
    "        detailed_results.append(fold_df_copy)\n",
    "    \n",
    "    detailed_df = pd.concat(detailed_results, ignore_index=True)\n",
    "    \n",
    "    if 'Test_Accuracy' in detailed_df.columns:\n",
    "        detailed_df = detailed_df.sort_values(\n",
    "            by=['Fold', 'Test_Accuracy'], \n",
    "            ascending=[True, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    detailed_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward_detailed.csv\")\n",
    "    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {detailed_path}\")\n",
    "    \n",
    "    numeric_cols = detailed_df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'Fold']\n",
    "    \n",
    "    avg_results = []\n",
    "    for model in detailed_df['Model'].unique():\n",
    "        model_data = detailed_df[detailed_df['Model'] == model]\n",
    "        avg_row = {'Model': model}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in model_data.columns:\n",
    "                avg_row[col] = model_data[col].mean()\n",
    "                avg_row[f'{col}_Std'] = model_data[col].std()\n",
    "        \n",
    "        avg_results.append(avg_row)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_results)\n",
    "    \n",
    "    # Average: Test_Accuracy 기준 정렬\n",
    "    if 'Test_Accuracy' in avg_df.columns:\n",
    "        avg_df = avg_df.sort_values(by='Test_Accuracy', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    avg_path = os.path.join(RESULT_DIR, f\"{target_name}_walk_forward.csv\")\n",
    "    avg_df.to_csv(avg_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {avg_path}\")\n",
    "    \n",
    "    return detailed_df, avg_df\n",
    "\n",
    "\n",
    "def save_summary_csv(summary_df, target_name, split_name, task):\n",
    "    \n",
    "    if task == 'classification':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', \n",
    "                       'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_AUC_ROC']\n",
    "        backtest_cols = ['Model', 'Total_Return(%)', 'Sharpe', 'Sortino', 'Calmar',\n",
    "                         'Max_Drawdown(%)', 'Win_Rate(%)', 'Total_Trades', 'Profit_Factor']\n",
    "        if 'Confident_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['Confident_Return(%)', 'Confident_Sharpe', 'Confident_Trades']\n",
    "            \n",
    "    elif task == 'regression':\n",
    "        metric_cols = ['Model', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Train_MAE', 'Val_MAE', 'Test_MAE', 'Test_R2', 'Test_MAPE', 'Direction_Accuracy']\n",
    "        backtest_cols = ['Model', 'Directional_Return(%)', 'Directional_Sharpe',\n",
    "                         'Total_Return(%)', 'Sharpe', 'Sortino', 'Calmar',\n",
    "                         'Max_Drawdown(%)', 'Win_Rate(%)', 'Total_Trades', 'Profit_Factor',\n",
    "                         'VolScaled_Return(%)', 'VolScaled_Sharpe']\n",
    "                         \n",
    "    elif task == 'multitask':\n",
    "        metric_cols = ['Model', 'Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', 'Test_Precision', \n",
    "                       'Test_Recall', 'Test_F1', 'Train_RMSE', 'Val_RMSE', 'Test_RMSE', \n",
    "                       'Test_MAE', 'Test_R2', 'Direction_Accuracy']\n",
    "        backtest_cols = ['Model', 'Total_Return(%)', 'Sharpe', 'Sortino', 'Calmar',\n",
    "                         'Max_Drawdown(%)', 'Win_Rate(%)', 'Total_Trades', 'Profit_Factor']\n",
    "        if 'Directional_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['Directional_Return(%)', 'Directional_Sharpe']\n",
    "        if 'VolScaled_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['VolScaled_Return(%)', 'VolScaled_Sharpe']\n",
    "        if 'Confident_Return(%)' in summary_df.columns:\n",
    "            backtest_cols += ['Confident_Return(%)', 'Confident_Sharpe', 'Confident_Trades']\n",
    "    \n",
    "    available_cols = [col for col in metric_cols + backtest_cols if col in summary_df.columns]\n",
    "    save_df = summary_df[[\"Model\"] + [col for col in available_cols if col != \"Model\"]]\n",
    "    \n",
    "    # Test_Accuracy 기준 정렬 (분류/멀티태스크) 또는 Test_RMSE 기준 정렬 (회귀)\n",
    "    if 'Test_Accuracy' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_Accuracy', ascending=False).reset_index(drop=True)\n",
    "    elif 'Test_RMSE' in save_df.columns:\n",
    "        save_df = save_df.sort_values(by='Test_RMSE', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    filename = f\"{target_name}_{split_name}.csv\"\n",
    "    file_path = os.path.join(RESULT_DIR, filename)\n",
    "    save_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b9c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Experiment: direction x walk_forward\n",
      "================================================================================\n",
      "Auto-calculated n_splits: 18 (from 1752 days)\n",
      "\n",
      "================================================================================\n",
      "Walk-Forward Configuration\n",
      "================================================================================\n",
      "Total data: 1752 days\n",
      "Train=600d, Val=60d, Test=60d, Step=60d\n",
      "Lookback=30d, Val sequences: 30\n",
      "Target folds: 18\n",
      "================================================================================\n",
      "\n",
      "Fold  1:\n",
      "  Train:  600d  (2020-12-19 ~ 2022-08-10)\n",
      "  Val:     60d  (2022-08-11 ~ 2022-10-09)\n",
      "  Test:    60d  (2022-10-10 ~ 2022-12-08)\n",
      "Fold  2:\n",
      "  Train:  660d  (2020-12-19 ~ 2022-10-09)\n",
      "  Val:     60d  (2022-10-10 ~ 2022-12-08)\n",
      "  Test:    60d  (2022-12-09 ~ 2023-02-06)\n",
      "Fold  3:\n",
      "  Train:  720d  (2020-12-19 ~ 2022-12-08)\n",
      "  Val:     60d  (2022-12-09 ~ 2023-02-06)\n",
      "  Test:    60d  (2023-02-07 ~ 2023-04-07)\n",
      "Fold  4:\n",
      "  Train:  780d  (2020-12-19 ~ 2023-02-06)\n",
      "  Val:     60d  (2023-02-07 ~ 2023-04-07)\n",
      "  Test:    60d  (2023-04-08 ~ 2023-06-06)\n",
      "Fold  5:\n",
      "  Train:  840d  (2020-12-19 ~ 2023-04-07)\n",
      "  Val:     60d  (2023-04-08 ~ 2023-06-06)\n",
      "  Test:    60d  (2023-06-07 ~ 2023-08-05)\n",
      "Fold  6:\n",
      "  Train:  900d  (2020-12-19 ~ 2023-06-06)\n",
      "  Val:     60d  (2023-06-07 ~ 2023-08-05)\n",
      "  Test:    60d  (2023-08-06 ~ 2023-10-04)\n",
      "Fold  7:\n",
      "  Train:  960d  (2020-12-19 ~ 2023-08-05)\n",
      "  Val:     60d  (2023-08-06 ~ 2023-10-04)\n",
      "  Test:    60d  (2023-10-05 ~ 2023-12-03)\n",
      "Fold  8:\n",
      "  Train: 1020d  (2020-12-19 ~ 2023-10-04)\n",
      "  Val:     60d  (2023-10-05 ~ 2023-12-03)\n",
      "  Test:    60d  (2023-12-04 ~ 2024-02-01)\n",
      "Fold  9:\n",
      "  Train: 1080d  (2020-12-19 ~ 2023-12-03)\n",
      "  Val:     60d  (2023-12-04 ~ 2024-02-01)\n",
      "  Test:    60d  (2024-02-02 ~ 2024-04-01)\n",
      "Fold 10:\n",
      "  Train: 1140d  (2020-12-19 ~ 2024-02-01)\n",
      "  Val:     60d  (2024-02-02 ~ 2024-04-01)\n",
      "  Test:    60d  (2024-04-02 ~ 2024-05-31)\n",
      "Fold 11:\n",
      "  Train: 1200d  (2020-12-19 ~ 2024-04-01)\n",
      "  Val:     60d  (2024-04-02 ~ 2024-05-31)\n",
      "  Test:    60d  (2024-06-01 ~ 2024-07-30)\n",
      "Fold 12:\n",
      "  Train: 1260d  (2020-12-19 ~ 2024-05-31)\n",
      "  Val:     60d  (2024-06-01 ~ 2024-07-30)\n",
      "  Test:    60d  (2024-07-31 ~ 2024-09-28)\n",
      "Fold 13:\n",
      "  Train: 1320d  (2020-12-19 ~ 2024-07-30)\n",
      "  Val:     60d  (2024-07-31 ~ 2024-09-28)\n",
      "  Test:    60d  (2024-09-29 ~ 2024-11-27)\n",
      "Fold 14:\n",
      "  Train: 1380d  (2020-12-19 ~ 2024-09-28)\n",
      "  Val:     60d  (2024-09-29 ~ 2024-11-27)\n",
      "  Test:    60d  (2024-11-28 ~ 2025-01-26)\n",
      "Fold 15:\n",
      "  Train: 1440d  (2020-12-19 ~ 2024-11-27)\n",
      "  Val:     60d  (2024-11-28 ~ 2025-01-26)\n",
      "  Test:    60d  (2025-01-27 ~ 2025-03-27)\n",
      "Fold 16:\n",
      "  Train: 1500d  (2020-12-19 ~ 2025-01-26)\n",
      "  Val:     60d  (2025-01-27 ~ 2025-03-27)\n",
      "  Test:    60d  (2025-03-28 ~ 2025-05-26)\n",
      "Fold 17:\n",
      "  Train: 1560d  (2020-12-19 ~ 2025-03-27)\n",
      "  Val:     60d  (2025-03-28 ~ 2025-05-26)\n",
      "  Test:    60d  (2025-05-27 ~ 2025-07-25)\n",
      "Fold 18:\n",
      "  Train: 1620d  (2020-12-19 ~ 2025-05-26)\n",
      "  Val:     60d  (2025-05-27 ~ 2025-07-25)\n",
      "  Test:    60d  (2025-07-26 ~ 2025-09-23)\n",
      "\n",
      "================================================================================\n",
      "Summary: 18 folds generated\n",
      "Total test days: 1080\n",
      "Test coverage: 2022-10-10 ~ 2025-09-23\n",
      "Data utilization: 99.3%\n",
      "================================================================================\n",
      "\n",
      "선택된 지표들\n",
      "DPO_20, VOLUME_CHANGE, HIGH_CLOSE_RANGE, btc_return_lag10, VOLUME_STRENGTH, extremity_index_lag1, AD, eth_btc_volume_ratio_ma30, high_lag3_ratio, MOM_30, GAP, btc_return_lag5, BB_Sentiment_Consensus, low_lag2_ratio, btc_volatility_7d, bnb_return, xrp_volume_ratio_20d, sol_return, sol_volume_change, ada_volume_change, dot_volume_ratio_20d, eth_large_eth_transfers_lag1, eth_total_gas_used_lag1, low_lag5_ratio, PRICE_VS_SMA10, eth_btc_corr_7d, vol_regime_duration, ADX_14, INC_5, low_lag1, btc_intraday_range, low_lag7, VIX_ETH_Vol_Cross_lag1, ada_volume_ratio_20d, sp500_SP500_lag1, sentiment_sum, low_lag1_ratio, news_count_lag1, return_lag2, RSI_OVERBOUGHT\n",
      "선택된 지표들\n",
      "DPO_20, sol_return, HIGH_CLOSE_RANGE, VOLUME_STRENGTH, btc_return_lag2, extremity_index_lag1, eth_avg_block_difficulty, HIGH_LOW_RANGE, eth_intraday_range, bnb_volume_change, eth_large_eth_transfers_lag1, low_lag2_ratio, eth_avg_block_size, MOM_10, TRUERANGE, GAP, eth_btc_corr_3d, bnb_return, xrp_volume_change, sol_volume_change, avax_volume_change, dot_volume_change, dot_volume_ratio_20d, eth_active_addresses_lag1, PRICE_VS_SMA10, vol_regime_duration, eth_btc_corr_7d, HLC3, ADX_14, low_lag5_ratio, news_volume_ma14, btc_intraday_range, VOLUME_CHANGE_5, RSI_OVERBOUGHT, sentiment_sum, low_lag1, RSI_percentile_60d, PRICE_VS_SMA200, btc_return_lag10, ada_volume_ratio_20d\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag10, extremity_index_lag1, eth_intraday_range, HIGH_LOW_RANGE, btc_return_lag5, VOLUME_STRENGTH, btc_return_lag2, news_volume_ma7, eth_active_addresses, eth_btc_corr_3d, btc_eth_strength_ratio, btc_dominance, Acceleration_Momentum, low_lag1_ratio, bnb_return, bnb_volume_change, sol_return, doge_volume_change, eth_active_addresses_lag1, vol_regime_duration, eth_btc_corr_7d, news_volume_ma14, PRICE_VS_SMA10, ADX_14, HLC3, low_lag2_ratio, volume_percentile_90d, HIGH_CLOSE_RANGE, close_lag2_logret, VOLUME_CHANGE_5, btc_intraday_range, bull_bear_ratio, RSI_percentile_60d, low_lag1, bnb_volume_ratio_20d, RSI_200, high_lag1_ratio, eth_avg_block_difficulty, low_lag5_ratio\n",
      "선택된 지표들\n",
      "DPO_20, volume_percentile_90d, eth_btc_corr_7d, VOLUME_CHANGE, VOLUME_CHANGE_5, bnb_volume_ratio_20d, btc_return_lag10, btc_return_lag5, high_lag1_ratio, sentiment_acceleration, btc_return_lag1, btc_return_lag2, eth_btc_corr_3d, Acceleration_Momentum, bnb_return, xrp_volume_change, sol_return, eth_large_eth_transfers_lag1, vol_regime_duration, bull_bear_ratio, HLC3, low_lag1, RSI_percentile_60d, eth_avg_block_difficulty, PRICE_VS_SMA10, btc_intraday_range, Liquidity_Risk, AD, ADX_14, low_lag2_ratio, day_of_month, EMA_CROSS_SIGNAL, news_volume_ma14, HIGH_LOW_RANGE, eth_intraday_range, extremity_index_lag1, btc_body_ratio, btc_return_20d, VTXM_14, HIGH_CLOSE_RANGE\n",
      "선택된 지표들\n",
      "DPO_20, VOLUME_CHANGE_5, low_lag2_ratio, btc_return_lag5, eth_btc_corr_7d, bnb_volume_ratio_20d, volume_lag3, VOLUME_CHANGE, btc_return_lag10, btc_body_ratio, volume_lag5, sentiment_ma3, news_volume_change, eth_active_addresses, return_lag1, GAP, eth_btc_corr_3d, Acceleration_Momentum, bnb_return, bnb_volume_change, sol_return, ada_volume_change, dot_volume_ratio_20d, eth_btc_volume_corr_30d, eth_large_eth_transfers_lag1, eth_contract_events_lag1, bull_bear_ratio, volume_percentile_90d, low_lag1, eth_avg_block_difficulty, PRICE_VS_SMA10, RSI_30, news_volume_ma14, vol_regime_duration, Liquidity_Risk, RSI_percentile_60d, HIGH_CLOSE_RANGE, eth_btc_spread, sentiment_sum, sp500_SP500_lag1\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag5, bull_bear_ratio, eth_btc_spread, bnb_volume_ratio_20d, volume_lag5, btc_return_20d, VOLUME_CHANGE_5, sentiment_volatility_14, eth_active_addresses, btc_return_lag1, btc_return_lag2, eth_btc_corr_3d, btc_dominance, Acceleration_Momentum, high_lag1_ratio, bnb_return, bnb_volume_change, sol_return, dot_volume_ratio_20d, volume_percentile_90d, eth_intraday_range, HIGH_LOW_RANGE, eth_avg_block_difficulty, RSI_percentile_60d, low_lag2_ratio, low_lag1, SUPERTREND, HLC3, VTXM_14, vol_regime_duration, news_volume_ma14, HIGH_CLOSE_RANGE, sp500_SP500_lag1, eth_btc_corr_7d, PRICE_VS_SMA10, RSI_30, return_lag5, close_lag2_logret, high_lag5\n",
      "선택된 지표들\n",
      "DPO_20, bnb_volume_ratio_20d, eth_btc_spread, dot_volume_ratio_20d, doge_volume_ratio_20d, btc_eth_strength_ratio, btc_return_lag5, HIGH_CLOSE_RANGE, sentiment_acceleration, close_lag2_logret, volume_lag5, low_lag1_ratio, VOLUME_CHANGE, GAP, btc_return_lag1, btc_return_lag10, eth_btc_corr_3d, eth_btc_beta_30d, Acceleration_Momentum, btc_volume_ratio_20d, bnb_return, bnb_volume_change, sol_return, ada_volume_change, sentiment_std_lag1, RSI_percentile_60d, news_volume_ma14, volume_percentile_90d, eth_avg_block_difficulty, btc_intraday_range, HLC3, low_lag2_ratio, CCI_50, vol_regime_duration, RSI_30, sp500_SP500_lag1, eth_btc_volcorr_30d, SUPERTREND, eth_intraday_range, HIGH_LOW_RANGE\n",
      "선택된 지표들\n",
      "DPO_20, dot_volume_ratio_20d, low_lag1_ratio, eth_btc_corr_3d, VTXM_14, btc_return_lag5, volume_lag5, btc_intraday_range, bnb_volume_ratio_20d, close_lag2_logret, news_volume_ma14, AD, VOLUME_CHANGE, eth_active_addresses, GAP, Acceleration_Momentum, high_lag1_ratio, bnb_return, bnb_volume_change, xrp_return, sol_return, ada_volume_change, avax_volume_change, eth_large_eth_transfers_lag1, eth_avg_block_difficulty, high_lag5_ratio, HLC3, RSI_percentile_60d, price_percentile_250d, vol_regime_duration, HIGH_CLOSE_RANGE, sentiment_sum, MACDH_12_26_9, volume_percentile_90d, sp500_SP500_lag1, HIGH_LOW_RANGE, eth_intraday_range, extremity_index_lag1, eth_large_eth_transfers, PRICE_VS_SMA10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 지표들\n",
      "DPO_20, eth_btc_corr_3d, btc_return_lag5, high_lag5_ratio, eth_btc_corr_7d, MACDH_12_26_9, volume_lag5, sentiment_acceleration, eth_avg_gas_price, GAP, btc_return_lag1, btc_dominance, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, sol_return, doge_volume_ratio_20d, eth_btc_volume_ratio, eth_large_eth_transfers_lag1, price_percentile_250d, HIGH_CLOSE_RANGE, EMA_CROSS_SIGNAL, btc_intraday_range, bull_bear_ratio, RSI_percentile_60d, eth_avg_block_difficulty, bull_bear_ratio_lag1, PRICE_VS_SMA10, sentiment_sum, eth_intraday_range, HIGH_LOW_RANGE, DISTANCE_FROM_LOW, VTXM_14, eth_btc_spread, eth_btc_volcorr_30d, vol_regime_duration, funding_fundingRate_lag1, BTC_Weighted_Impact, VOLUME_CHANGE, CCI_14\n",
      "선택된 지표들\n",
      "DPO_20, VOLUME_CHANGE, eth_btc_corr_3d, btc_return_lag5, HIGH_CLOSE_RANGE, eth_btc_corr_7d, CCI_14, HIGH_LOW_RANGE, vol_trend, close_lag2_logret, xrp_volume_change, eth_active_addresses, eth_contract_events, btc_return_lag1, btc_return_lag2, btc_dominance, eth_btc_beta_30d, Acceleration_Momentum, bnb_return, bnb_volume_change, sol_return, avax_volume_change, dot_volume_ratio_20d, price_percentile_250d, PRICE_VS_SMA10, EMA_CROSS_SIGNAL, vol_spike, high_lag5_ratio, eth_token_transfers, DISTANCE_FROM_LOW, vol_regime_duration, BTC_Weighted_Impact, news_volume_ma14, VTXM_14, eth_avg_block_difficulty, SUPERTREND, day_of_month, eth_intraday_range, fg_fear_greed_lag1, btc_return\n",
      "선택된 지표들\n",
      "DPO_20, eth_btc_corr_3d, eth_token_transfers, bnb_return, btc_return_lag5, GAP, eth_btc_corr_7d, CCI_14, eth_btc_spread, VTXM_14, sentiment_ma3, eth_active_addresses, eth_contract_events, VOLUME_CHANGE_5, btc_return_lag1, Acceleration_Momentum, bnb_volume_change, bnb_volume_ratio_20d, xrp_return, sol_return, ada_volume_change, avax_volume_change, dot_volume_ratio_20d, eth_large_eth_transfers_lag1, PRICE_VS_SMA10, vol_regime_duration, HIGH_CLOSE_RANGE, DISTANCE_FROM_LOW, day_of_month, price_percentile_250d, eth_avg_block_difficulty, fg_fear_greed_lag1, funding_fundingRate_lag1, SUPERTREND, BTC_Weighted_Impact, EMA_CROSS_SIGNAL, bull_bear_ratio_lag1, eth_intraday_range, HIGH_LOW_RANGE, close_lag2\n",
      "선택된 지표들\n",
      "DPO_20, eth_btc_corr_3d, bnb_return, eth_btc_spread, VOLUME_STRENGTH, GAP, day_of_month_sin, eth_tx_count, btc_eth_strength_ratio, Acceleration_Momentum, xrp_return, sol_return, ada_volume_change, dot_volume_change, dot_volume_ratio_20d, eth_large_eth_transfers_lag1, btc_return_lag5, DISTANCE_FROM_LOW, PRICE_VS_SMA10, vol_regime_duration, eth_avg_block_difficulty, eth_btc_corr_7d, BTC_Weighted_Impact, high_lag5_ratio, HIGH_CLOSE_RANGE, eth_large_eth_transfers, btc_intraday_range, sentiment_sum, price_percentile_250d, eth_token_transfers, day_of_month, return_lag3, btc_return_20d, vol_spike, funding_fundingRate_lag1, STOCHF_14, RSI_percentile_60d, close_lag14_ratio, fg_fear_greed_lag1, close_lag14\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag5, eth_btc_corr_3d, dot_volume_ratio_20d, day_of_month_sin, BTC_Weighted_Impact, btc_intraday_range, GAP, eth_token_transfers, return_lag3, sentiment_trend, news_volume_change, eth_active_addresses, return_lag1, VOLUME_CHANGE, VOLUME_CHANGE_5, btc_dominance, Acceleration_Momentum, bnb_return, xrp_return, xrp_volume_change, sol_return, ada_volume_change, eth_large_eth_transfers_lag1, day_of_month, PRICE_VS_SMA10, DISTANCE_FROM_LOW, STOCHF_3, eth_avg_block_difficulty, price_percentile_250d, bull_bear_ratio, vol_regime_duration, close_lag14_ratio, eth_btc_corr_7d, high_lag5_ratio, fg_fear_greed_lag1, btc_return_20d, extremity_index_lag1, eth_btc_spread, STOCHF_14\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag5, eth_btc_corr_3d, eth_btc_volcorr_sq_7d, GAP, eth_btc_volcorr_7d, eth_btc_corr_7d, dot_volume_ratio_20d, eth_active_addresses, btc_return_lag3, bnb_return, bnb_volume_change, xrp_return, xrp_volume_change, sol_return, ada_volume_change, doge_volume_change, avax_volume_change, eth_btc_volume_ratio, eth_avg_block_difficulty, price_percentile_250d, high_lag5_ratio, bull_bear_ratio, STOCHF_3, DISTANCE_FROM_LOW, DEMA_10, day_of_month_sin, BTC_Weighted_Impact, vol_regime_duration, low_lag3, btc_intraday_range, PRICE_VS_SMA20, eth_btc_volcorr_30d, btc_return_20d, RV_RATIO, HIGH_CLOSE_RANGE, day_of_month, PRICE_VS_SMA10, close_lag14_ratio, FIB_1\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag5, eth_btc_corr_3d, dot_volume_ratio_20d, bull_bear_ratio, GAP, HIGH_CLOSE_RANGE, VOLUME_CHANGE, eth_btc_corr_7d, eth_btc_spread, dot_volume_change, sentiment_ma3, return_lag5, VOLUME_CHANGE_5, btc_return_lag1, btc_return_lag3, eth_btc_volcorr_sq_14d, BB_Sentiment_Consensus, Acceleration_Momentum, bnb_return, xrp_return, sol_return, doge_volume_change, avax_volume_change, eth_btc_volume_ratio, BTC_Weighted_Impact, eth_avg_block_difficulty, eth_btc_volcorr_30d, DEMA_10, STOCHF_3, price_percentile_250d, PRICE_VS_SMA20, high_lag5_ratio, vol_regime_duration, DISTANCE_FROM_LOW, day_of_month, btc_intraday_range, STOCHF_14, HIGH_LOW_RANGE, eth_intraday_range\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag5, eth_btc_corr_3d, dot_volume_ratio_20d, GAP, bull_bear_ratio, VOLUME_CHANGE, eth_tx_count, eth_btc_volcorr_7d, dot_volume_change, sentiment_acceleration, volume_lag5, VOLUME_STRENGTH, btc_return_lag1, btc_return_lag10, btc_dominance, Acceleration_Momentum, xrp_return, sol_return, doge_volume_change, BTC_Weighted_Impact, HIGH_CLOSE_RANGE, eth_btc_volcorr_30d, eth_avg_block_difficulty, day_of_month, eth_avg_block_size, vol_regime_duration, btc_intraday_range, DISTANCE_FROM_LOW, DEMA_10, HIGH_LOW_RANGE, eth_intraday_range, bnb_volatility_30d, btc_return_20d, eth_total_gas_used_lag1, STOCHF_14, high_lag5_ratio, WILLR_14, eth_btc_corr_7d, EMA_CROSS_SIGNAL\n",
      "선택된 지표들\n",
      "DPO_20, GAP, HIGH_CLOSE_RANGE, eth_btc_corr_3d, eth_tx_count, bull_bear_ratio, low_lag2_ratio, BB_POSITION, VOLUME_CHANGE, sentiment_ma3, volume_lag5, btc_return_lag10, btc_eth_strength_ratio, BB_Sentiment_Consensus, close_lag2_ratio, close_lag2_logret, bnb_volume_change, xrp_return, sol_return, avax_volume_change, avax_volume_ratio_20d, dot_volume_change, btc_return_lag5, eth_avg_block_difficulty, BTC_Weighted_Impact, day_of_month, HIGH_LOW_RANGE, eth_intraday_range, eth_avg_block_size, DISTANCE_FROM_LOW, bnb_volatility_30d, btc_intraday_range, eth_btc_volcorr_7d, vol_regime_duration, price_percentile_250d, eth_btc_corr_7d, eth_btc_volume_ratio_ma30, DEMA_10, eth_btc_volcorr_30d, EMA_CROSS_SIGNAL\n",
      "선택된 지표들\n",
      "DPO_20, dot_volume_ratio_20d, GAP, bull_bear_ratio, volume_lag5, high_lag2_ratio, xrp_return, eth_tx_count, VOLUME_CHANGE_5, btc_return_lag1, btc_return_lag2, btc_return_lag10, eth_btc_corr_3d, eth_btc_volcorr_sq_7d, btc_volume_price_corr_30d, bnb_return, xrp_volume_ratio_20d, sol_return, doge_volume_change, avax_volume_change, positive_ratio_lag1, BTC_Weighted_Impact, day_of_month, HIGH_CLOSE_RANGE, eth_avg_block_size, btc_return_lag5, eth_intraday_range, HIGH_LOW_RANGE, DISTANCE_FROM_LOW, eth_avg_block_difficulty, return_lag3, EMA_CROSS_SIGNAL, eth_btc_volcorr_7d, bnb_volatility_30d, vol_regime_duration, eth_btc_volume_ratio_ma30, eth_btc_corr_7d, RV_20, VIX_ETH_Vol_Cross_lag1, btc_intraday_range\n",
      "\n",
      "  Processing Fold 1/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.65045\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 23:56:24.280949: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (570, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "18/18 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [20] BiLSTM...\n",
      "18/18 [==============================] - 1s 19ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "18/18 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "18/18 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [23] CNN_LSTM...\n",
      "18/18 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [24] CNN_GRU...\n",
      "18/18 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "18/18 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1760972634.976916 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760972636.518351 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/18 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760972695.710796 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [27] Transformer...\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [28] TCN...\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "18/18 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [30] Informer...\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [31] NBEATS...\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [32] TFT...\n",
      "18/18 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [33] Performer...\n",
      "18/18 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [34] PatchTST...\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [35] Autoformer...\n",
      "18/18 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [36] iTransformer...\n",
      "18/18 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [37] EtherVoyant...\n",
      "18/18 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "18/18 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "18/18 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "18/18 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [42] Parallel_CNN...\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "18/18 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [44] Residual_LSTM...\n",
      "18/18 [==============================] - 1s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [45] WaveNet...\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 1 completed\n",
      "\n",
      "  Processing Fold 2/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.68187\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (630, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "20/20 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "20/20 [==============================] - 1s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "20/20 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "20/20 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [23] CNN_LSTM...\n",
      "20/20 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [24] CNN_GRU...\n",
      "20/20 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "20/20 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760974050.026256 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760974051.597230 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/20 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760974104.844309 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [27] Transformer...\n",
      "20/20 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [28] TCN...\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "20/20 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [30] Informer...\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [32] TFT...\n",
      "20/20 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [33] Performer...\n",
      "20/20 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [34] PatchTST...\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "20/20 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [36] iTransformer...\n",
      "20/20 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [37] EtherVoyant...\n",
      "20/20 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "20/20 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "20/20 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [42] Parallel_CNN...\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "20/20 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [44] Residual_LSTM...\n",
      "20/20 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [45] WaveNet...\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 2 completed\n",
      "\n",
      "  Processing Fold 3/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_0_auc = 0.71886\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (690, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "22/22 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "22/22 [==============================] - 1s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [21] GRU...\n",
      "22/22 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "22/22 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [23] CNN_LSTM...\n",
      "22/22 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [24] CNN_GRU...\n",
      "22/22 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "22/22 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760975657.958003 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760975659.615596 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/22 [=======>......................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760975740.574761 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [27] Transformer...\n",
      "22/22 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [28] TCN...\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "22/22 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [30] Informer...\n",
      "22/22 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [32] TFT...\n",
      "22/22 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [33] Performer...\n",
      "22/22 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [34] PatchTST...\n",
      "22/22 [==============================] - 4s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "22/22 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "22/22 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [37] EtherVoyant...\n",
      "22/22 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "22/22 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "22/22 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [42] Parallel_CNN...\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "22/22 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [44] Residual_LSTM...\n",
      "22/22 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [45] WaveNet...\n",
      "22/22 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 3 completed\n",
      "\n",
      "  Processing Fold 4/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.71314\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "25/25 [==============================] - 0s 986us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (750, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "24/24 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [20] BiLSTM...\n",
      "24/24 [==============================] - 1s 20ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "24/24 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "24/24 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [23] CNN_LSTM...\n",
      "24/24 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [24] CNN_GRU...\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "24/24 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760977343.455063 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760977345.158222 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/24 [======>.......................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760977417.348062 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [27] Transformer...\n",
      "24/24 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [28] TCN...\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [29] DTW_LSTM...\n",
      "24/24 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [30] Informer...\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "24/24 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [33] Performer...\n",
      "24/24 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [34] PatchTST...\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [35] Autoformer...\n",
      "24/24 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [36] iTransformer...\n",
      "24/24 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [37] EtherVoyant...\n",
      "24/24 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "24/24 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "24/24 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "24/24 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [42] Parallel_CNN...\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "24/24 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [44] Residual_LSTM...\n",
      "24/24 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [45] WaveNet...\n",
      "24/24 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 4 completed\n",
      "\n",
      "  Processing Fold 5/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.78889\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (810, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "26/26 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [20] BiLSTM...\n",
      "26/26 [==============================] - 1s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [21] GRU...\n",
      "26/26 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "26/26 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [23] CNN_LSTM...\n",
      "26/26 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [24] CNN_GRU...\n",
      "26/26 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "26/26 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760979124.742638 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760979126.724358 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/26 [=====>........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760979208.548971 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [27] Transformer...\n",
      "26/26 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [28] TCN...\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "26/26 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [30] Informer...\n",
      "26/26 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "26/26 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [32] TFT...\n",
      "26/26 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [33] Performer...\n",
      "26/26 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [34] PatchTST...\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "26/26 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [36] iTransformer...\n",
      "26/26 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [37] EtherVoyant...\n",
      "26/26 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [40] EMD_LSTM...\n",
      "26/26 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "26/26 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [42] Parallel_CNN...\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "26/26 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [44] Residual_LSTM...\n",
      "26/26 [==============================] - 1s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [45] WaveNet...\n",
      "26/26 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 5 completed\n",
      "\n",
      "  Processing Fold 6/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.74074\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (870, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "28/28 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "28/28 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "28/28 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "28/28 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [23] CNN_LSTM...\n",
      "28/28 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [24] CNN_GRU...\n",
      "28/28 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "28/28 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760981269.989101 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760981271.927106 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/28 [=====>........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760981342.258585 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [27] Transformer...\n",
      "28/28 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [28] TCN...\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "28/28 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [30] Informer...\n",
      "28/28 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "  [32] TFT...\n",
      "28/28 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [33] Performer...\n",
      "28/28 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [34] PatchTST...\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "28/28 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [36] iTransformer...\n",
      "28/28 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [37] EtherVoyant...\n",
      "28/28 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [40] EMD_LSTM...\n",
      "28/28 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "28/28 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [42] Parallel_CNN...\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "28/28 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [44] Residual_LSTM...\n",
      "28/28 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [45] WaveNet...\n",
      "28/28 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 6 completed\n",
      "\n",
      "  Processing Fold 7/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.68552\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (930, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "30/30 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [20] BiLSTM...\n",
      "30/30 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "  [21] GRU...\n",
      "30/30 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "30/30 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [23] CNN_LSTM...\n",
      "30/30 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [24] CNN_GRU...\n",
      "30/30 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "30/30 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760983437.679643 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760983439.744354 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/30 [====>.........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760983543.281684 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [27] Transformer...\n",
      "30/30 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [28] TCN...\n",
      "30/30 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "30/30 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [30] Informer...\n",
      "30/30 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [31] NBEATS...\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "30/30 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [33] Performer...\n",
      "30/30 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [34] PatchTST...\n",
      "30/30 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "30/30 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "30/30 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [37] EtherVoyant...\n",
      "30/30 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "30/30 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "30/30 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "30/30 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [42] Parallel_CNN...\n",
      "30/30 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "30/30 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [44] Residual_LSTM...\n",
      "30/30 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [45] WaveNet...\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 7 completed\n",
      "\n",
      "  Processing Fold 8/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.73804\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (990, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "31/31 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "31/31 [==============================] - 2s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "31/31 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "31/31 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [23] CNN_LSTM...\n",
      "31/31 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [24] CNN_GRU...\n",
      "31/31 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "31/31 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760985827.892197 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760985830.055616 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/31 [===>..........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760985910.567429 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [27] Transformer...\n",
      "31/31 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [28] TCN...\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [29] DTW_LSTM...\n",
      "31/31 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [30] Informer...\n",
      "31/31 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [31] NBEATS...\n",
      "31/31 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [33] Performer...\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [34] PatchTST...\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "31/31 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [36] iTransformer...\n",
      "31/31 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [37] EtherVoyant...\n",
      "31/31 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [40] EMD_LSTM...\n",
      "31/31 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "31/31 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [42] Parallel_CNN...\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "31/31 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [44] Residual_LSTM...\n",
      "31/31 [==============================] - 1s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [45] WaveNet...\n",
      "31/31 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 8 completed\n",
      "\n",
      "  Processing Fold 9/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_auc = 0.68889\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "34/34 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1050, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "33/33 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [20] BiLSTM...\n",
      "33/33 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [21] GRU...\n",
      "33/33 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "33/33 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [23] CNN_LSTM...\n",
      "33/33 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [24] CNN_GRU...\n",
      "33/33 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "33/33 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760988167.943317 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760988170.298119 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/33 [===>..........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760988310.697573 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [27] Transformer...\n",
      "33/33 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [28] TCN...\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [29] DTW_LSTM...\n",
      "33/33 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [30] Informer...\n",
      "33/33 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "33/33 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [33] Performer...\n",
      "33/33 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [34] PatchTST...\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [35] Autoformer...\n",
      "33/33 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "33/33 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [37] EtherVoyant...\n",
      "33/33 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [40] EMD_LSTM...\n",
      "33/33 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "33/33 [==============================] - 2s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [42] Parallel_CNN...\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "33/33 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [44] Residual_LSTM...\n",
      "33/33 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [45] WaveNet...\n",
      "33/33 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 9 completed\n",
      "\n",
      "  Processing Fold 10/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.74774\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1110, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [20] BiLSTM...\n",
      "35/35 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "35/35 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "35/35 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [23] CNN_LSTM...\n",
      "35/35 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [24] CNN_GRU...\n",
      "35/35 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "35/35 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760990809.579981 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760990811.786303 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/35 [====>.........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760990925.165851 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [27] Transformer...\n",
      "35/35 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [28] TCN...\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [29] DTW_LSTM...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [30] Informer...\n",
      "35/35 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "35/35 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [32] TFT...\n",
      "35/35 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [33] Performer...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [34] PatchTST...\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "35/35 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [36] iTransformer...\n",
      "35/35 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [37] EtherVoyant...\n",
      "35/35 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [40] EMD_LSTM...\n",
      "35/35 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "35/35 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "  [42] Parallel_CNN...\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [44] Residual_LSTM...\n",
      "35/35 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [45] WaveNet...\n",
      "35/35 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 10 completed\n",
      "\n",
      "  Processing Fold 11/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.68238\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "38/38 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1170, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "37/37 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [20] BiLSTM...\n",
      "37/37 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "  [21] GRU...\n",
      "37/37 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "37/37 [==============================] - 1s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [23] CNN_LSTM...\n",
      "37/37 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [24] CNN_GRU...\n",
      "37/37 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "37/37 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760993692.228298 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760993694.548174 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/37 [===>..........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760993798.170229 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [27] Transformer...\n",
      "37/37 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [28] TCN...\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [29] DTW_LSTM...\n",
      "37/37 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [30] Informer...\n",
      "37/37 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "37/37 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "37/37 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [33] Performer...\n",
      "37/37 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [34] PatchTST...\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "37/37 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [36] iTransformer...\n",
      "37/37 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [37] EtherVoyant...\n",
      "37/37 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [40] EMD_LSTM...\n",
      "37/37 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "37/37 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [42] Parallel_CNN...\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "37/37 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [44] Residual_LSTM...\n",
      "37/37 [==============================] - 1s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [45] WaveNet...\n",
      "37/37 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 11 completed\n",
      "\n",
      "  Processing Fold 12/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.69556\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "40/40 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1230, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "39/39 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "39/39 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [21] GRU...\n",
      "39/39 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "39/39 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [23] CNN_LSTM...\n",
      "39/39 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [24] CNN_GRU...\n",
      "39/39 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "39/39 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760996271.080364 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1760996273.691145 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/39 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760996413.813977 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [27] Transformer...\n",
      "39/39 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [28] TCN...\n",
      "39/39 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [29] DTW_LSTM...\n",
      "39/39 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [30] Informer...\n",
      "39/39 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [31] NBEATS...\n",
      "39/39 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "39/39 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [33] Performer...\n",
      "39/39 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [34] PatchTST...\n",
      "39/39 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [35] Autoformer...\n",
      "39/39 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "39/39 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [37] EtherVoyant...\n",
      "39/39 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "39/39 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [40] EMD_LSTM...\n",
      "39/39 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "39/39 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 2s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "45/45 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [42] Parallel_CNN...\n",
      "45/45 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "45/45 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [44] Residual_LSTM...\n",
      "45/45 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "  [45] WaveNet...\n",
      "45/45 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 15 completed\n",
      "\n",
      "  Processing Fold 16/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.66667\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1470, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "46/46 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [20] BiLSTM...\n",
      "46/46 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [21] GRU...\n",
      "46/46 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "46/46 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [23] CNN_LSTM...\n",
      "46/46 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [24] CNN_GRU...\n",
      "46/46 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "46/46 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761008763.151814 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1761008766.120099 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/46 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761008918.100150 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [27] Transformer...\n",
      "46/46 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [28] TCN...\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [29] DTW_LSTM...\n",
      "46/46 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "  [30] Informer...\n",
      "46/46 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "46/46 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "46/46 [==============================] - 1s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [33] Performer...\n",
      "46/46 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [34] PatchTST...\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [35] Autoformer...\n",
      "46/46 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "46/46 [==============================] - 1s 11ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [37] EtherVoyant...\n",
      "46/46 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [40] EMD_LSTM...\n",
      "46/46 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "46/46 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [42] Parallel_CNN...\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "46/46 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [44] Residual_LSTM...\n",
      "46/46 [==============================] - 2s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [45] WaveNet...\n",
      "46/46 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 16 completed\n",
      "\n",
      "  Processing Fold 17/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.70412\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "49/49 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1530, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "48/48 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [20] BiLSTM...\n",
      "48/48 [==============================] - 2s 22ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "  [21] GRU...\n",
      "48/48 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "48/48 [==============================] - 2s 17ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [23] CNN_LSTM...\n",
      "48/48 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [24] CNN_GRU...\n",
      "48/48 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "48/48 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761012366.423174 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1761012369.263054 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/48 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761012514.338051 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [27] Transformer...\n",
      "48/48 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [28] TCN...\n",
      "48/48 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [29] DTW_LSTM...\n",
      "48/48 [==============================] - 2s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "  [30] Informer...\n",
      "48/48 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [31] NBEATS...\n",
      "48/48 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "48/48 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [33] Performer...\n",
      "48/48 [==============================] - 1s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [34] PatchTST...\n",
      "48/48 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "48/48 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [36] iTransformer...\n",
      "48/48 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [37] EtherVoyant...\n",
      "48/48 [==============================] - 2s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "48/48 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "48/48 [==============================] - 2s 17ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "48/48 [==============================] - 2s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "  [42] Parallel_CNN...\n",
      "48/48 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "48/48 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  [44] Residual_LSTM...\n",
      "48/48 [==============================] - 2s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [45] WaveNet...\n",
      "48/48 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 17 completed\n",
      "\n",
      "  Processing Fold 18/18\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.74057\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "51/51 [==============================] - 0s 991us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1590, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "50/50 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [20] BiLSTM...\n",
      "50/50 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "  [21] GRU...\n",
      "50/50 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "50/50 [==============================] - 2s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [23] CNN_LSTM...\n",
      "50/50 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [24] CNN_GRU...\n",
      "50/50 [==============================] - 1s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "50/50 [==============================] - 2s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761015951.122755 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1761015953.647871 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/50 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761016068.839158 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 9ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [27] Transformer...\n",
      "50/50 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [28] TCN...\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [29] DTW_LSTM...\n",
      "50/50 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [30] Informer...\n",
      "50/50 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  [31] NBEATS...\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [32] TFT...\n",
      "50/50 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [33] Performer...\n",
      "50/50 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [34] PatchTST...\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "  [35] Autoformer...\n",
      "50/50 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [36] iTransformer...\n",
      "50/50 [==============================] - 1s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [37] EtherVoyant...\n",
      "50/50 [==============================] - 2s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "  [40] EMD_LSTM...\n",
      "50/50 [==============================] - 2s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "50/50 [==============================] - 2s 19ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "  [42] Parallel_CNN...\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "50/50 [==============================] - 1s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "  [44] Residual_LSTM...\n",
      "50/50 [==============================] - 2s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "  [45] WaveNet...\n",
      "50/50 [==============================] - 1s 5ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "  Fold 18 completed\n",
      "\n",
      "  Aggregating 18 folds...\n",
      "Saved: model_results/direction_walk_forward_detailed.csv\n",
      "Saved: model_results/direction_walk_forward.csv\n",
      "\n",
      "================================================================================\n",
      "Experiment: direction x tvt\n",
      "================================================================================\n",
      "  Train: 1226 (2020-12-19 ~ 2024-04-27)\n",
      "  Val:   263 (2024-04-28 ~ 2025-01-15)\n",
      "  Test:  263 (2025-01-16 ~ 2025-10-05)\n",
      "선택된 지표들\n",
      "DPO_20, eth_btc_corr_3d, GAP, bnb_return, close_lag2_logret, btc_return_lag5, eth_btc_corr_7d, eth_btc_spread, HIGH_LOW_RANGE, sentiment_ma3, eth_active_addresses, volume_lag5, VOLUME_CHANGE, PRICE_ACCELERATION, btc_return_lag1, btc_dominance, btc_volume_price_corr_30d, bnb_volume_change, bnb_volume_ratio_20d, sol_return, ada_volume_change, avax_volume_change, dot_volume_ratio_20d, DISTANCE_FROM_LOW, vol_regime_duration, PRICE_VS_SMA10, eth_avg_block_difficulty, day_of_month, HIGH_CLOSE_RANGE, SUPERTREND, price_percentile_250d, vol_spike, low_lag3, BTC_Weighted_Impact, RSI_percentile_60d, funding_fundingRate_lag1, high_lag5_ratio, fg_fear_greed_lag1, negative_ratio_lag1, EMA_CROSS_SIGNAL\n",
      "\n",
      "================================================================================\n",
      "Classification 모델 학습 시작 (총 45개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (18개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest...\n",
      "  [2] LightGBM...\n",
      "  [3] XGBoost...\n",
      "  [4] SVM...\n",
      "  [5] LogisticRegression...\n",
      "  [6] NaiveBayes...\n",
      "  [7] KNN...\n",
      "  [8] AdaBoost...\n",
      "  [9] CatBoost...\n",
      "  [10] DecisionTree...\n",
      "  [11] ExtraTrees...\n",
      "  [12] Bagging...\n",
      "  [13] GradientBoosting...\n",
      "  [14] TabNet...\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.6675\n",
      "  [15] StackingEnsemble...\n",
      "  [16] VotingHard...\n",
      "  [17] VotingSoft...\n",
      "  [18] MLP...\n",
      "39/39 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "    ⚠ MLP 스킵: ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 111, in train_ml_model\n",
      "    self.evaluator.evaluate_classification_model(\n",
      "  File \"/tmp/ipykernel_1210809/1057377665.py\", line 42, in evaluate_classification_model\n",
      "    train_acc = accuracy_score(y_train, train_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 359, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/raid/invigoworks/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py\", line 106, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "\n",
      "✓ ML 모델 완료: 17/18개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (27개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (1196, 30, 40)\n",
      "  ✓ Val shape: (233, 30, 40)\n",
      "  ✓ Test shape: (233, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [19] LSTM...\n",
      "38/38 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "  [20] BiLSTM...\n",
      "38/38 [==============================] - 2s 23ms/step\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "  [21] GRU...\n",
      "38/38 [==============================] - 1s 9ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "  [22] Stacked_LSTM...\n",
      "38/38 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "  [23] CNN_LSTM...\n",
      "38/38 [==============================] - 1s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 7ms/step\n",
      "  [24] CNN_GRU...\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "  [25] CNN_BiLSTM...\n",
      "38/38 [==============================] - 2s 13ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "  [26] LSTM_Attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761019168.862825 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1761019171.390841 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/38 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761019289.771805 1210809 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 2 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 31719424 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "  [27] Transformer...\n",
      "38/38 [==============================] - 1s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "  [28] TCN...\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "  [29] DTW_LSTM...\n",
      "38/38 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "  [30] Informer...\n",
      "38/38 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "  [31] NBEATS...\n",
      "38/38 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "  [32] TFT...\n",
      "38/38 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "  [33] Performer...\n",
      "38/38 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "  [34] PatchTST...\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "  [35] Autoformer...\n",
      "38/38 [==============================] - 1s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "  [36] iTransformer...\n",
      "38/38 [==============================] - 1s 11ms/step\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "  [37] EtherVoyant...\n",
      "38/38 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "  [38] VMD_Hybrid...\n",
      "    ⚠ VMD_Hybrid 스킵: NameError: name 'inputs' is not defined\n",
      "    상세: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1210809/987515945.py\", line 132, in train_dl_model\n",
      "    model = model_config['func'](\n",
      "  File \"/tmp/ipykernel_1210809/2720413276.py\", line 780, in vmd_hybrid\n",
      "    x = Conv1D(32, 1, padding='same', kernel_regularizer=l2(0.01))(inputs)\n",
      "NameError: name 'inputs' is not defined\n",
      "\n",
      "  [39] SimpleRNN...\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "  [40] EMD_LSTM...\n",
      "38/38 [==============================] - 2s 19ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "  [41] Hybrid_LSTM_GRU...\n",
      "38/38 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "  [42] Parallel_CNN...\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "  [43] LSTM_XGBoost_Hybrid...\n",
      "38/38 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "  [44] Residual_LSTM...\n",
      "38/38 [==============================] - 2s 22ms/step\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "  [45] WaveNet...\n",
      "38/38 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "\n",
      "✓ DL 모델 완료: 26/27개 성공\n",
      "\n",
      "================================================================================\n",
      "전체 학습 완료: 43/45개 모델 성공\n",
      "================================================================================\n",
      "Saved: model_results/direction_tvt.csv\n",
      "\n",
      "================================================================================\n",
      "Experiment: return x walk_forward\n",
      "================================================================================\n",
      "Auto-calculated n_splits: 18 (from 1752 days)\n",
      "\n",
      "================================================================================\n",
      "Walk-Forward Configuration\n",
      "================================================================================\n",
      "Total data: 1752 days\n",
      "Train=600d, Val=60d, Test=60d, Step=60d\n",
      "Lookback=30d, Val sequences: 30\n",
      "Target folds: 18\n",
      "================================================================================\n",
      "\n",
      "Fold  1:\n",
      "  Train:  600d  (2020-12-19 ~ 2022-08-10)\n",
      "  Val:     60d  (2022-08-11 ~ 2022-10-09)\n",
      "  Test:    60d  (2022-10-10 ~ 2022-12-08)\n",
      "Fold  2:\n",
      "  Train:  660d  (2020-12-19 ~ 2022-10-09)\n",
      "  Val:     60d  (2022-10-10 ~ 2022-12-08)\n",
      "  Test:    60d  (2022-12-09 ~ 2023-02-06)\n",
      "Fold  3:\n",
      "  Train:  720d  (2020-12-19 ~ 2022-12-08)\n",
      "  Val:     60d  (2022-12-09 ~ 2023-02-06)\n",
      "  Test:    60d  (2023-02-07 ~ 2023-04-07)\n",
      "Fold  4:\n",
      "  Train:  780d  (2020-12-19 ~ 2023-02-06)\n",
      "  Val:     60d  (2023-02-07 ~ 2023-04-07)\n",
      "  Test:    60d  (2023-04-08 ~ 2023-06-06)\n",
      "Fold  5:\n",
      "  Train:  840d  (2020-12-19 ~ 2023-04-07)\n",
      "  Val:     60d  (2023-04-08 ~ 2023-06-06)\n",
      "  Test:    60d  (2023-06-07 ~ 2023-08-05)\n",
      "Fold  6:\n",
      "  Train:  900d  (2020-12-19 ~ 2023-06-06)\n",
      "  Val:     60d  (2023-06-07 ~ 2023-08-05)\n",
      "  Test:    60d  (2023-08-06 ~ 2023-10-04)\n",
      "Fold  7:\n",
      "  Train:  960d  (2020-12-19 ~ 2023-08-05)\n",
      "  Val:     60d  (2023-08-06 ~ 2023-10-04)\n",
      "  Test:    60d  (2023-10-05 ~ 2023-12-03)\n",
      "Fold  8:\n",
      "  Train: 1020d  (2020-12-19 ~ 2023-10-04)\n",
      "  Val:     60d  (2023-10-05 ~ 2023-12-03)\n",
      "  Test:    60d  (2023-12-04 ~ 2024-02-01)\n",
      "Fold  9:\n",
      "  Train: 1080d  (2020-12-19 ~ 2023-12-03)\n",
      "  Val:     60d  (2023-12-04 ~ 2024-02-01)\n",
      "  Test:    60d  (2024-02-02 ~ 2024-04-01)\n",
      "Fold 10:\n",
      "  Train: 1140d  (2020-12-19 ~ 2024-02-01)\n",
      "  Val:     60d  (2024-02-02 ~ 2024-04-01)\n",
      "  Test:    60d  (2024-04-02 ~ 2024-05-31)\n",
      "Fold 11:\n",
      "  Train: 1200d  (2020-12-19 ~ 2024-04-01)\n",
      "  Val:     60d  (2024-04-02 ~ 2024-05-31)\n",
      "  Test:    60d  (2024-06-01 ~ 2024-07-30)\n",
      "Fold 12:\n",
      "  Train: 1260d  (2020-12-19 ~ 2024-05-31)\n",
      "  Val:     60d  (2024-06-01 ~ 2024-07-30)\n",
      "  Test:    60d  (2024-07-31 ~ 2024-09-28)\n",
      "Fold 13:\n",
      "  Train: 1320d  (2020-12-19 ~ 2024-07-30)\n",
      "  Val:     60d  (2024-07-31 ~ 2024-09-28)\n",
      "  Test:    60d  (2024-09-29 ~ 2024-11-27)\n",
      "Fold 14:\n",
      "  Train: 1380d  (2020-12-19 ~ 2024-09-28)\n",
      "  Val:     60d  (2024-09-29 ~ 2024-11-27)\n",
      "  Test:    60d  (2024-11-28 ~ 2025-01-26)\n",
      "Fold 15:\n",
      "  Train: 1440d  (2020-12-19 ~ 2024-11-27)\n",
      "  Val:     60d  (2024-11-28 ~ 2025-01-26)\n",
      "  Test:    60d  (2025-01-27 ~ 2025-03-27)\n",
      "Fold 16:\n",
      "  Train: 1500d  (2020-12-19 ~ 2025-01-26)\n",
      "  Val:     60d  (2025-01-27 ~ 2025-03-27)\n",
      "  Test:    60d  (2025-03-28 ~ 2025-05-26)\n",
      "Fold 17:\n",
      "  Train: 1560d  (2020-12-19 ~ 2025-03-27)\n",
      "  Val:     60d  (2025-03-28 ~ 2025-05-26)\n",
      "  Test:    60d  (2025-05-27 ~ 2025-07-25)\n",
      "Fold 18:\n",
      "  Train: 1620d  (2020-12-19 ~ 2025-05-26)\n",
      "  Val:     60d  (2025-05-27 ~ 2025-07-25)\n",
      "  Test:    60d  (2025-07-26 ~ 2025-09-23)\n",
      "\n",
      "================================================================================\n",
      "Summary: 18 folds generated\n",
      "Total test days: 1080\n",
      "Test coverage: 2022-10-10 ~ 2025-09-23\n",
      "Data utilization: 99.3%\n",
      "================================================================================\n",
      "\n",
      "선택된 지표들\n",
      "DPO_20, gold_GOLD_lag1, ADOSC_3_10, volume_lag5, volume_lag1, volume_lag2, return_lag3, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, btc_return_lag2, btc_return_lag10, btc_eth_strength_ratio, close_lag2_logret, btc_return_10d, xrp_volatility_30d, xrp_volume_ratio_20d, doge_volume_ratio_20d, eth_large_eth_transfers_lag1, aave_aave_eth_tvl_lag1, close_lag1, doge_volatility_30d, eth_close_position, INTRADAY_POSITION, KCL_20, ROC_20, MOMENTUM_20, close_lag14_ratio, chain_eth_chain_tvl_lag1, sentiment_volatility_3, eth_body_ratio, high_lag1, chain_eth_chain_tvl, sentiment_intensity, extreme_positive_count_lag1, eth_btc_volcorr_7d, month_cos, eth_log_return, PRICE_CHANGE, eth_return\n",
      "선택된 지표들\n",
      "DPO_20, btc_return_lag1, btc_eth_strength_ratio, ADOSC_3_10, sentiment_ma14, eth_large_eth_transfers, volume_lag5, return_lag2, return_lag3, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, btc_return_lag2, eth_btc_corr_3d, close_lag3_ratio, low_lag3_ratio, low_lag7_ratio, xrp_volatility_30d, xrp_volume_ratio_20d, eth_large_eth_transfers_lag1, aave_aave_eth_tvl_lag1, gold_GOLD_lag1, sentiment_volatility_7, bnb_return, close_lag1, chain_eth_chain_tvl_lag1, usdt_totalCirculating, VOLATILITY_20, eth_close_position, low_lag1, KCL_20, INTRADAY_POSITION, usdt_totalCirculating_lag1, chain_eth_chain_tvl, high_lag2, eth_log_return, RV_20, doge_volatility_30d, eth_return, PRICE_CHANGE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 지표들\n",
      "DPO_20, btc_eth_strength_ratio, ADOSC_3_10, eth_contract_events, btc_return_lag1, btc_return_20d, eth_large_eth_transfers, volume_lag5, return_lag2, return_lag3, CCI_14, PRICE_CHANGE_5, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, high_lag3_ratio, low_lag3_ratio, btc_return_10d, ada_volume_change, doge_return, eth_large_eth_transfers_lag1, bnb_return, ISA_9, aave_aave_eth_tvl_lag1, makerdao_makerdao_eth_tvl, sentiment_volatility_7, gold_GOLD_lag1, chain_eth_chain_tvl_lag1, ISB_26, KCL_20, news_volume_ma7, eth_close_position, INTRADAY_POSITION, month_cos, high_lag2, high_lag1_ratio, close_lag1, makerdao_makerdao_eth_tvl_lag1, extreme_positive_count_lag1, eth_btc_beta_90d\n",
      "선택된 지표들\n",
      "DPO_20, btc_eth_strength_ratio, volume_lag5, Acceleration_Momentum, btc_return_lag1, eth_contract_events, chain_eth_chain_tvl, btc_volatility_7d, ADOSC_3_10, close_lag5_ratio, btc_return_20d, eth_large_eth_transfers, return_lag2, return_lag3, PRICE_CHANGE_5, CLOSE_LOW_RANGE, GAP, btc_return_lag3, btc_dominance, high_lag3_ratio, low_lag3_ratio, low_lag5_ratio, btc_return_10d, btc_intraday_range, eth_large_eth_transfers_lag1, close_lag1, makerdao_makerdao_eth_tvl, aave_aave_eth_tvl_lag1, ISA_9, gold_GOLD_lag1, ISB_26, KCL_20, eth_btc_beta_90d, VOLUME_SMA_20, HL2, TEMA_10, high_lag1, makerdao_makerdao_eth_tvl_lag1, chain_eth_chain_tvl_lag1, eth_total_gas_used\n",
      "선택된 지표들\n",
      "DPO_20, btc_eth_strength_ratio, eth_contract_events, volume_lag5, Acceleration_Momentum, btc_return_20d, btc_return_lag1, bnb_return, return_lag2, ADOSC_3_10, PRICE_CHANGE_5, CLOSE_LOW_RANGE, GAP, btc_return_lag10, btc_dominance, high_lag3_ratio, low_lag3_ratio, eth_large_eth_transfers_lag1, eth_avg_gas_price_lag1, aave_aave_eth_tvl_lag1, close_lag1, chain_eth_chain_tvl, chain_eth_chain_tvl_lag1, makerdao_makerdao_eth_tvl, VOLUME_SMA_20, HL2, KCL_20, low_lag1, makerdao_makerdao_eth_tvl_lag1, BBL_20, LINREG_14, eth_total_gas_used, doge_volatility_30d, gold_GOLD_lag1, TEMA_10, OHLC4, high_lag7_ratio, high_lag1, gold_GOLD, eth_total_gas_used_lag1\n",
      "선택된 지표들\n",
      "DPO_20, btc_eth_strength_ratio, volume_lag5, chain_eth_chain_tvl, bnb_return, eth_contract_events, volume_lag1, volume_lag2, eth_avg_gas_price_lag1, return_lag3, ADOSC_3_10, PRICE_CHANGE_5, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, btc_return_lag3, close_lag3_ratio, low_lag2_ratio, high_lag3_ratio, low_lag3_ratio, doge_return, eth_large_eth_transfers_lag1, chain_eth_chain_tvl_lag1, close_lag1, aave_aave_eth_tvl_lag1, makerdao_makerdao_eth_tvl, VOLUME_SMA_20, HL2, ISA_9, KCL_20, eth_total_gas_used, gold_GOLD_lag1, doge_volatility_30d, high_lag7_ratio, BBL_20, VOLATILITY_10, RV_20, low_lag1, makerdao_makerdao_eth_tvl_lag1, OHLC4\n",
      "선택된 지표들\n",
      "DPO_20, CLOSE_LOW_RANGE, volume_lag5, chain_eth_chain_tvl, eth_contract_events, btc_eth_strength_ratio, bnb_return, INTRADAY_POSITION, eth_large_eth_transfers, return_lag2, return_lag3, ADOSC_3_10, PRICE_VS_SMA10, GAP, btc_return_lag3, btc_dominance, high_lag2_ratio, high_lag3_ratio, low_lag3_ratio, high_lag5_ratio, low_lag7_ratio, btc_return_10d, btc_intraday_range, eth_large_eth_transfers_lag1, aave_aave_eth_tvl_lag1, chain_eth_chain_tvl_lag1, close_lag1, KCL_20, gold_GOLD_lag1, RV_20, VOLUME_SMA_20, ISA_9, VOLATILITY_10, ITS_9, makerdao_makerdao_eth_tvl, eth_total_gas_used_lag1, eth_total_gas_used, volume_lag1, BBM_50, EMA_50\n",
      "선택된 지표들\n",
      "DPO_20, volume_lag5, dot_volatility_30d, eth_contract_events, bnb_return, high_lag7_ratio, volume_lag2, eth_large_eth_transfers, return_lag3, close_ratio_lag7, ADOSC_3_10, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, btc_return_lag3, btc_return_lag10, high_lag3_ratio, low_lag3_ratio, high_lag5_ratio, btc_return_10d, sol_return, eth_large_eth_transfers_lag1, aave_aave_eth_tvl_lag1, chain_eth_chain_tvl_lag1, makerdao_makerdao_eth_tvl, VOLUME_SMA_20, BB_WIDTH, RV_20, usdt_totalCirculating, ISA_9, usdt_totalCirculatingUSD, close_lag1, gold_GOLD_lag1, eth_total_gas_used_lag1, lido_lido_eth_tvl_lag1, VOLATILITY_10, btc_volatility_30d, makerdao_makerdao_eth_tvl_lag1, eth_btc_spread_std7, volume_lag1\n",
      "선택된 지표들\n",
      "DPO_20, volume_lag5, volume_lag1, volume_lag2, high_lag7_ratio, close_lag14_ratio, chain_eth_chain_tvl, VIX_ETH_Vol_Cross, bnb_return, eth_large_eth_transfers, return_lag2, return_lag3, ADOSC_3_10, PRICE_VS_SMA10, CLOSE_LOW_RANGE, GAP, btc_return_lag3, btc_return_lag10, Acceleration_Momentum, high_lag2_ratio, high_lag3_ratio, low_lag3_ratio, high_lag5_ratio, doge_volume_ratio_20d, eth_large_eth_transfers_lag1, chain_eth_chain_tvl_lag1, eth_total_gas_used_lag1, aave_aave_eth_tvl_lag1, eth_contract_events, VOLATILITY_10, RV_20, usdt_totalCirculating, VOLATILITY_20, usdt_totalCirculatingUSD, KCL_20, usdt_totalCirculatingUSD_lag1, PRICE_VS_SMA200, makerdao_makerdao_eth_tvl, lido_lido_eth_tvl_lag1, doge_volatility_30d\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, high_lag7_ratio, volume_lag2, volume_lag5, btc_intraday_range, volume_lag1, return_lag2, return_lag3, return_lag5, ADOSC_3_10, PRICE_CHANGE_5, PRICE_VS_SMA10, VOLUME_STRENGTH, GAP, btc_eth_strength_ratio, Acceleration_Momentum, high_lag3_ratio, low_lag3_ratio, high_lag5_ratio, sol_return, doge_volume_ratio_20d, eth_large_eth_transfers_lag1, chain_eth_chain_tvl_lag1, eth_total_gas_used_lag1, aave_aave_eth_tvl_lag1, VOLATILITY_20, eth_contract_events, usdt_totalCirculatingUSD, usdt_totalCirculating, lido_lido_eth_tvl_lag1, doge_volatility_30d, KCL_20, VOLUME_SMA_20, usdt_totalCirculatingUSD_lag1, RV_20, sol_volatility_30d, usdt_totalCirculating_lag1, chain_eth_chain_tvl, volume_lag3\n",
      "선택된 지표들\n",
      "DPO_20, volume_lag5, eth_large_eth_transfers, eth_large_eth_transfers_lag1, high_lag7_ratio, volume_lag2, btc_intraday_range, return_lag2, return_lag3, return_lag5, PRICE_CHANGE_5, PRICE_VS_SMA10, GAP, close_lag3_ratio, high_lag3_ratio, low_lag5_ratio, sol_return, doge_volume_ratio_20d, eth_contract_events, VOLATILITY_20, chain_eth_chain_tvl_lag1, usdt_totalCirculatingUSD, lido_lido_eth_tvl_lag1, usdt_totalCirculating, doge_volatility_30d, aave_aave_eth_tvl_lag1, btc_volatility_30d, sol_volatility_30d, usdt_totalCirculatingUSD_lag1, eth_total_gas_used_lag1, volume_lag3, PRICE_VS_SMA200, ISB_26, KCL_20, makerdao_makerdao_eth_tvl, usdt_totalCirculating_lag1, VOLUME_SMA_20, xrp_volatility_30d, RV_20, VOLATILITY_10\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, volume_lag5, BB_WIDTH, volume_lag2, sentiment_volatility_7, funding_fundingRate, return_lag2, return_lag3, ADOSC_3_10, PRICE_VS_SMA10, SLOPE_5, GAP, btc_return_lag3, Acceleration_Momentum, close_lag3_ratio, high_lag3_ratio, high_lag5_ratio, low_lag7_ratio, btc_intraday_range, sol_return, usdt_totalCirculatingUSD, usdt_totalCirculating, volume_lag3, eth_total_gas_used_lag1, VOLATILITY_20, btc_volatility_30d, lido_lido_eth_tvl_lag1, chain_eth_chain_tvl_lag1, eth_contract_events, makerdao_makerdao_eth_tvl, VOLUME_SMA_20, KCL_20, usdt_totalCirculating_lag1, doge_volatility_30d, usdt_totalCirculatingUSD_lag1, high_lag7_ratio, aave_aave_eth_tvl_lag1, VWAP, ISB_26\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, volume_lag5, volume_lag2, dxy_DXY_lag1, funding_fundingRate, volume_lag1, return_lag2, return_lag3, return_lag5, ADOSC_3_10, PRICE_CHANGE_5, GAP, Acceleration_Momentum, close_lag3_ratio, low_lag2_ratio, high_lag3_ratio, high_lag5_ratio, low_lag5_ratio, low_lag7_ratio, btc_return_10d, btc_intraday_range, bnb_volume_ratio_20d, sol_return, eth_large_eth_transfers_lag1, usdt_totalCirculatingUSD, eth_total_gas_used_lag1, VOLATILITY_20, usdt_totalCirculating, lido_lido_eth_tvl_lag1, makerdao_makerdao_eth_tvl, RV_20, volume_lag3, VOLATILITY_10, btc_volatility_30d, eth_contract_events, sol_volatility_30d, VWAP, xrp_volatility_30d, chain_eth_chain_tvl_lag1\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, volume_lag5, volume_lag2, dxy_DXY_lag1, chain_eth_chain_tvl, volume_lag1, return_lag2, return_lag3, return_lag5, ADOSC_3_10, PRICE_CHANGE_5, PRICE_VS_SMA10, GAP, Acceleration_Momentum, close_lag3_ratio, high_lag3_ratio, high_lag5_ratio, low_lag5_ratio, low_lag7_ratio, btc_return_10d, eth_large_eth_transfers_lag1, usdt_totalCirculatingUSD, usdt_totalCirculating, eth_total_gas_used_lag1, VOLATILITY_20, volume_lag3, EMA_50, BBM_50, RV_20, lido_lido_eth_tvl_lag1, Liquidity_Risk, BTC_Weighted_Impact, VOLATILITY_10, PRICE_CHANGE_2, usdt_totalCirculating_lag1, usdt_totalCirculatingUSD_lag1, eth_contract_events, high_lag7_ratio, sol_volatility_30d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, volume_lag5, sentiment_ma14, volume_lag1, return_lag3, return_lag5, ADOSC_3_10, PRICE_VS_SMA10, GAP, eth_btc_spread_ma7, close_lag2_logret, close_lag3_ratio, high_lag2_ratio, high_lag3_ratio, low_lag5_ratio, btc_return_10d, doge_return, eth_large_eth_transfers_lag1, VOLATILITY_10, lido_lido_eth_tvl_lag1, VOLATILITY_20, usdt_totalCirculatingUSD, BBM_50, EMA_50, usdt_totalCirculating, RV_20, BTC_Weighted_Impact, btc_volatility_30d, news_volume_ma7, KCL_20, Liquidity_Risk, eth_total_gas_used_lag1, ISB_26, volume_lag3, PRICE_CHANGE_2, PRICE_VS_SMA200, close_lag2_ratio, eth_contract_events, aave_aave_eth_tvl_lag1\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, low_lag7_ratio, dxy_DXY_lag1, volume_lag5, PRICE_VS_SMA20, return_lag3, return_lag5, ADOSC_3_10, PRICE_VS_SMA10, SLOPE_5, BB_WIDTH, GAP, btc_return_lag2, btc_return_lag3, Acceleration_Momentum, close_lag3_ratio, low_lag2_ratio, high_lag3_ratio, low_lag3_ratio, high_lag5_ratio, sol_return, eth_large_eth_transfers_lag1, EMA_50, BBM_50, KCL_20, lido_lido_eth_tvl_lag1, news_volume_ma7, RV_20, VOLATILITY_20, VOLATILITY_10, eth_total_gas_used_lag1, ISB_26, volume_lag3, BTC_Weighted_Impact, PRICE_CHANGE_2, btc_volatility_30d, chain_eth_chain_tvl_lag1, usdt_totalUnreleased, Liquidity_Risk\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, low_lag1_ratio, volume_lag5, volume_lag2, return_lag3, ADOSC_3_10, SLOPE_5, BB_WIDTH, GAP, Acceleration_Momentum, low_lag2_ratio, high_lag3_ratio, high_lag5_ratio, doge_return, avax_volume_change, eth_large_eth_transfers_lag1, EMA_50, BBM_50, RV_20, news_volume_ma7, VOLATILITY_20, eth_total_gas_used_lag1, VOLATILITY_10, chain_eth_chain_tvl_lag1, ISB_26, usdt_totalUnreleased, volume_lag3, BTC_Weighted_Impact, KCL_20, makerdao_makerdao_eth_tvl, lido_lido_eth_tvl_lag1, MACDH_12_26_9, Liquidity_Risk, PRICE_CHANGE_2, dxy_DXY_lag1, close_lag2_ratio, doge_volatility_30d, chain_eth_chain_tvl, ATR_14\n",
      "선택된 지표들\n",
      "DPO_20, eth_large_eth_transfers, PRICE_VS_SMA10, volume_lag5, eth_btc_volume_ratio_ma30, low_lag1_ratio, close_ratio_lag7, return_lag2, return_lag3, return_lag5, ADOSC_3_10, SLOPE_5, GAP, btc_return_lag2, close_lag3_ratio, low_lag2_ratio, high_lag3_ratio, high_lag5_ratio, low_lag5_ratio, btc_return_10d, sol_return, eth_large_eth_transfers_lag1, BBM_50, EMA_50, RV_20, news_volume_ma7, VOLATILITY_20, ISB_26, Liquidity_Risk, usdt_totalUnreleased, eth_total_gas_used_lag1, VOLATILITY_10, lido_lido_eth_tvl_lag1, usdt_totalBridgedToUSD, volume_lag3, BTC_Weighted_Impact, close_lag2_ratio, PRICE_CHANGE_2, MACDH_12_26_9, chain_eth_chain_tvl_lag1\n",
      "\n",
      "  Processing Fold 1/18\n",
      "\n",
      "================================================================================\n",
      "Regression 모델 학습 시작 (총 35개 모델)\n",
      "================================================================================\n",
      "\n",
      "[Part 1/2] Machine Learning 모델 (4개)\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] RandomForest_Reg...\n",
      "  [2] LightGBM_Reg...\n",
      "  [3] XGBoost_Reg...\n",
      "  [4] SVR...\n",
      "\n",
      "✓ ML 모델 완료: 4/4개 성공\n",
      "\n",
      "[Part 2/2] Deep Learning/시계열 모델 (31개)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "시퀀스 데이터 생성 중 (lookback=30)...\n",
      "  ✓ Train shape: (570, 30, 40)\n",
      "  ✓ Val shape: (30, 30, 40)\n",
      "  ✓ Test shape: (30, 30, 40)\n",
      "  ✓ Input shape: (30, 40)\n",
      "\n",
      "  [5] LSTM_Reg...\n",
      "18/18 [==============================] - 1s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "  [6] BiLSTM_Reg...\n",
      "18/18 [==============================] - 2s 23ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "  [7] GRU...\n",
      "18/18 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [8] Stacked_LSTM_Reg...\n",
      "18/18 [==============================] - 1s 19ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "  [9] CNN-LSTM...\n",
      "18/18 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "  [10] CNN-GRU...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Scikit-learn ML 모델\n",
    "# ============================================================================\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Boosting Libraries\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"Warning: pytorch-tabnet not installed. TabNet models will be skipped.\")\n",
    "\n",
    "# ============================================================================\n",
    "# TensorFlow/Keras 딥러닝 레이어\n",
    "# ============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, GRU, Bidirectional, SimpleRNN,\n",
    "    Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    Dropout, BatchNormalization, LayerNormalization,\n",
    "    Flatten, Concatenate, Add, Multiply,\n",
    "    MultiHeadAttention, Attention,\n",
    "    AveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch \n",
    "# ============================================================================\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"Warning: PyTorch not installed. Some models may not work.\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for target_case in target_cases:\n",
    "    for split_method in split_methods:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Experiment: {target_case['name']} x {split_method['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = build_complete_pipeline_corrected(\n",
    "            df_merged, train_start_date,\n",
    "            method=split_method['method'],\n",
    "            target_type=target_case['target_type'],\n",
    "        )\n",
    "        \n",
    "        if split_method['method'] == 'tvt':\n",
    "            X_train = result['train']['X_robust']\n",
    "            X_val = result['val']['X_robust']\n",
    "            X_test = result['test']['X_robust']\n",
    "            test_returns = result['test']['y']['next_log_return'].values  \n",
    "            test_dates = result['test']['dates'].values \n",
    "            \n",
    "            if len(target_case['outputs']) == 1:\n",
    "                y_train = result['train']['y'][target_case['outputs'][0]].values\n",
    "                y_val = result['val']['y'][target_case['outputs'][0]].values\n",
    "                y_test = result['test']['y'][target_case['outputs'][0]].values\n",
    "                ml_models = ML_MODELS_REGRESSION if target_case['target_type'] in ['return', 'price'] else ML_MODELS_CLASSIFICATION\n",
    "                dl_models = DL_MODELS_REGRESSION if target_case['target_type'] in ['return', 'price'] else DL_MODELS_CLASSIFICATION\n",
    "                task = 'regression' if target_case['target_type'] in ['return', 'price'] else 'classification'\n",
    "            else:\n",
    "                y_train = result['train']['y'][target_case['outputs']].values\n",
    "                y_val = result['val']['y'][target_case['outputs']].values\n",
    "                y_test = result['test']['y'][target_case['outputs']].values\n",
    "                ml_models = []\n",
    "                dl_models = DL_MODELS_MULTITASK_ENSEMBLE\n",
    "                task = 'multitask'\n",
    "            \n",
    "            evaluator = ModelEvaluator()\n",
    "            train_all_models(\n",
    "                X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                test_returns, test_dates, evaluator,\n",
    "                ml_models=ml_models, dl_models=dl_models, task=task\n",
    "            )\n",
    "            summary_df = evaluator.get_summary_dataframe()\n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = summary_df\n",
    "            save_summary_csv(summary_df, target_case['name'], split_method['name'], task)\n",
    "            \n",
    "        else:\n",
    "            fold_results = []\n",
    "            \n",
    "            for fold_idx, fold in enumerate(result, start=1):\n",
    "                print(f\"\\n  Processing Fold {fold_idx}/{len(result)}\")\n",
    "                \n",
    "                X_train = fold['train']['X_robust']\n",
    "                X_val = fold['val']['X_robust']\n",
    "                X_test = fold['test']['X_robust']\n",
    "                test_returns = fold['test']['y']['next_log_return'].values  \n",
    "                test_dates = fold['test']['dates'].values  \n",
    "                \n",
    "                if len(target_case['outputs']) == 1:\n",
    "                    y_train = fold['train']['y'][target_case['outputs'][0]].values  \n",
    "                    y_val = fold['val']['y'][target_case['outputs'][0]].values\n",
    "                    y_test = fold['test']['y'][target_case['outputs'][0]].values\n",
    "                    ml_models = ML_MODELS_REGRESSION if target_case['target_type'] in ['return', 'price'] else ML_MODELS_CLASSIFICATION\n",
    "                    dl_models = DL_MODELS_REGRESSION if target_case['target_type'] in ['return', 'price'] else DL_MODELS_CLASSIFICATION\n",
    "                    task = 'regression' if target_case['target_type'] in ['return', 'price'] else 'classification'\n",
    "                else:\n",
    "                    y_train = fold['train']['y'][target_case['outputs']].values\n",
    "                    y_val = fold['val']['y'][target_case['outputs']].values\n",
    "                    y_test = fold['test']['y'][target_case['outputs']].values\n",
    "                    ml_models = []\n",
    "                    dl_models = DL_MODELS_MULTITASK_ENSEMBLE\n",
    "                    task = 'multitask'\n",
    "                \n",
    "                evaluator = ModelEvaluator()\n",
    "                train_all_models(\n",
    "                    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    test_returns, test_dates, evaluator,\n",
    "                    ml_models=ml_models, dl_models=dl_models, task=task\n",
    "                )\n",
    "                fold_summary = evaluator.get_summary_dataframe()\n",
    "                fold_results.append(fold_summary)\n",
    "                print(f\"  Fold {fold_idx} completed\")\n",
    "            \n",
    "            print(f\"\\n  Aggregating {len(fold_results)} folds...\")\n",
    "            detailed_df, avg_df = save_walk_forward_results(\n",
    "                fold_results, target_case['name'], task\n",
    "            )\n",
    "            all_results[f\"{target_case['name']}_{split_method['name']}\"] = avg_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366541b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0595f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772dbc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_train),type(y_val),type(y_test),type(X_train),type(X_val),type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_val,y_test,X_train,X_val,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d83981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
