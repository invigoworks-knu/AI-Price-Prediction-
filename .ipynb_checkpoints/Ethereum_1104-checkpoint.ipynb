{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa057518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 03:16:56.020403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-10 03:16:56.020444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-10 03:16:56.021889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-10 03:16:56.029199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-10 03:16:56.885233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING\n",
      "================================================================================\n",
      "(2968, 41) macro_crypto_data.csv\n",
      "(2226, 2) SP500.csv\n",
      "(2226, 2) VIX.csv\n",
      "(2227, 2) GOLD.csv\n",
      "(2228, 2) DXY.csv\n",
      "(2835, 2) fear_greed.csv\n",
      "(2175, 2) eth_funding_rate.csv\n",
      "(2903, 6) usdt_eth_mcap.csv\n",
      "(2001, 2) aave_eth_tvl.csv\n",
      "(1787, 2) lido_eth_tvl.csv\n",
      "(2503, 2) makerdao_eth_tvl.csv\n",
      "(2565, 2) uniswap_eth_tvl.csv\n",
      "(2095, 2) curve-dex_eth_tvl.csv\n",
      "(2966, 2) eth_chain_tvl.csv\n",
      "(1593, 5) layer2_tvl.csv\n",
      "Loaded 10 files\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT FEATURES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATA MERGING\n",
      "================================================================================\n",
      "Merged shape: (2338, 62)\n",
      "Missing before fill: 23,704\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUE HANDLING\n",
      "================================================================================\n",
      "Missing after fill: 0\n",
      "Shape: (2338, 62)\n",
      "Period: 2019-06-15 ~ 2025-11-07\n",
      "Missing: 0\n",
      "\n",
      "Feature groups:\n",
      "  Crypto prices: 40\n",
      "  On-chain: 0\n",
      "  DeFi TVL: 6\n",
      "  Layer 2: 4\n",
      "  Sentiment: 0\n",
      "  Macro: 4\n",
      "  Fear & Greed: 1\n",
      "  Funding Rate: 1\n",
      "  Stablecoin: 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ethereum Price Prediction - Data Loading & Preprocessing\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# 기본 라이브러리 및 유틸리티\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "# 날짜/시간\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 데이터 처리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from collections import Counter\n",
    "from numba import jit\n",
    "# ============================================================================\n",
    "# ML/DL 라이브러리 및 도구\n",
    "# ============================================================================\n",
    "\n",
    "# 하이퍼파라미터 최적화\n",
    "import optuna\n",
    "\n",
    "# Scikit-learn: 데이터 전처리\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, RFE,\n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# Scikit-learn: 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Scikit-learn: 앙상블 모델\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, AdaBoostRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    HistGradientBoostingClassifier, # 추가된 항목\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "\n",
    "# Scikit-learn: 평가 지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, # 분류 지표\n",
    "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error # 회귀 지표\n",
    ")\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm.callback import early_stopping \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# TensorFlow/Keras 딥러닝\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    # 기본 레이어\n",
    "    Input, Dense, Flatten, Dropout, Activation,\n",
    "    # RNN 레이어\n",
    "    LSTM, GRU, SimpleRNN, Bidirectional,\n",
    "    # CNN 레이어\n",
    "    Conv1D, MaxPooling1D, AveragePooling1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    # 정규화 레이어\n",
    "    BatchNormalization, LayerNormalization,\n",
    "    # Attention 레이어\n",
    "    Attention, MultiHeadAttention,\n",
    "    # 유틸리티 레이어\n",
    "    Concatenate, Add, Multiply, Lambda,\n",
    "    Reshape, Permute, RepeatVector, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 시계열 분석 (Statsmodels)\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "# PyTorch \n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, AdaBoostClassifier, BaggingClassifier,\n",
    "    GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier,\n",
    "    VotingClassifier, HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# ============================================================================\n",
    "# 환경 설정 및 경고 무시\n",
    "# ============================================================================\n",
    "\n",
    "# GPU 메모리 증가 허용 설정\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DATA_DIR_MAIN = './macro_data'\n",
    "DATA_DIR_NEW = './macro_data/macro_data'\n",
    "\n",
    "TRAIN_START_DATE = pd.to_datetime('2020-01-01')\n",
    "LOOKBACK_DAYS = 200\n",
    "LOOKBACK_START_DATE = TRAIN_START_DATE - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "\n",
    "def standardize_date_column(df,file_name):\n",
    "    \"\"\"날짜 컬럼 자동 탐지 + datetime 통일 + tz 제거 + 시각 제거\"\"\"\n",
    "\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        print(\"[Warning] 날짜 컬럼을 찾을 수 없습니다.\")\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    \n",
    "\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    \n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()  \n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    print(df.shape,file_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"[Warning] {filename} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    return standardize_date_column(df, filename)\n",
    "\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [f\"{prefix}_{col}\" if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sentiment_features(news_df):\n",
    "    if news_df.empty:\n",
    "        return pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    agg = news_df.groupby('date').agg(\n",
    "        sentiment_mean=('label', 'mean'),\n",
    "        sentiment_std=('label', 'std'),\n",
    "        news_count=('label', 'count'),\n",
    "        positive_ratio=('label', lambda x: (x == 1).sum() / len(x)),\n",
    "        negative_ratio=('label', lambda x: (x == -1).sum() / len(x)),\n",
    "        extreme_positive_count=('label', lambda x: (x == 1).sum()),\n",
    "        extreme_negative_count=('label', lambda x: (x == -1).sum()),\n",
    "        sentiment_sum=('label', 'sum'),\n",
    "    ).reset_index().fillna(0)\n",
    "    \n",
    "    agg['sentiment_polarity'] = agg['positive_ratio'] - agg['negative_ratio']\n",
    "    agg['sentiment_intensity'] = agg['positive_ratio'] + agg['negative_ratio']\n",
    "    agg['sentiment_disagreement'] = agg['positive_ratio'] * agg['negative_ratio']\n",
    "    agg['bull_bear_ratio'] = agg['positive_ratio'] / (agg['negative_ratio'] + 1e-10)\n",
    "    agg['weighted_sentiment'] = agg['sentiment_mean'] * np.log1p(agg['news_count'])\n",
    "    agg['extremity_index'] = (agg['extreme_positive_count'] + agg['extreme_negative_count']) / (agg['news_count'] + 1e-10)\n",
    "    \n",
    "    for window in [3,7]:\n",
    "        agg[f'sentiment_ma{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).mean()\n",
    "        agg[f'sentiment_volatility_{window}'] = agg['sentiment_mean'].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    agg['sentiment_trend'] = agg['sentiment_mean'].diff()\n",
    "    agg['sentiment_acceleration'] = agg['sentiment_trend'].diff()\n",
    "    agg['news_volume_change'] = agg['news_count'].pct_change()\n",
    "    \n",
    "    for window in [7, 14]:\n",
    "        agg[f'news_volume_ma{window}'] = agg['news_count'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return agg.fillna(0)\n",
    "\n",
    "\n",
    "def smart_fill_missing(df_merged):\n",
    "    REFERENCE_START_DATE = pd.to_datetime('2020-01-01')\n",
    "    \n",
    "    for col in df_merged.columns:\n",
    "        if col == 'date':\n",
    "            continue\n",
    "        \n",
    "        if df_merged[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        non_null_idx = df_merged[col].first_valid_index()\n",
    "        \n",
    "        if non_null_idx is None:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "            continue\n",
    "        \n",
    "        first_date = df_merged.loc[non_null_idx, 'date']\n",
    "        \n",
    "        before_mask = df_merged['date'] < first_date\n",
    "        after_mask = df_merged['date'] >= first_date\n",
    "        \n",
    "        df_merged.loc[before_mask, col] = df_merged.loc[before_mask, col].fillna(0)\n",
    "        df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(method='ffill')\n",
    "        \n",
    "        remaining = df_merged.loc[after_mask, col].isnull().sum()\n",
    "        if remaining > 0:\n",
    "            df_merged.loc[after_mask, col] = df_merged.loc[after_mask, col].fillna(0)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#news_df = load_csv(DATA_DIR_MAIN, 'news_data.csv')\n",
    "#eth_onchain_df = load_csv(DATA_DIR_MAIN, 'eth_onchain.csv')\n",
    "macro_df = load_csv(DATA_DIR_NEW, 'macro_crypto_data.csv')\n",
    "sp500_df = load_csv(DATA_DIR_NEW, 'SP500.csv')\n",
    "vix_df = load_csv(DATA_DIR_NEW, 'VIX.csv')\n",
    "gold_df = load_csv(DATA_DIR_NEW, 'GOLD.csv')\n",
    "dxy_df = load_csv(DATA_DIR_NEW, 'DXY.csv')\n",
    "fear_greed_df = load_csv(DATA_DIR_NEW, 'fear_greed.csv')\n",
    "eth_funding_df = load_csv(DATA_DIR_NEW, 'eth_funding_rate.csv')\n",
    "usdt_eth_mcap_df = load_csv(DATA_DIR_NEW, 'usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_csv(DATA_DIR_NEW, 'aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_csv(DATA_DIR_NEW, 'lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_csv(DATA_DIR_NEW, 'makerdao_eth_tvl.csv')\n",
    "uniswap_tvl_df = load_csv(DATA_DIR_NEW, 'uniswap_eth_tvl.csv')\n",
    "curve_tvl_df = load_csv(DATA_DIR_NEW, 'curve-dex_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_csv(DATA_DIR_NEW, 'eth_chain_tvl.csv')\n",
    "layer2_tvl_df = load_csv(DATA_DIR_NEW, 'layer2_tvl.csv')\n",
    "\n",
    "print(f\"Loaded {len([df for df in [fear_greed_df, eth_funding_df, usdt_eth_mcap_df, aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df, eth_chain_tvl_df, layer2_tvl_df] if not df.empty])} files\")\n",
    "\n",
    "all_dataframes = [\n",
    "    macro_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df, \n",
    "    sp500_df, vix_df, gold_df, dxy_df#,news_df, eth_onchain_df\n",
    "]\n",
    "\n",
    "last_dates = [\n",
    "    pd.to_datetime(df['date']).max() \n",
    "    for df in all_dataframes \n",
    "    if not df.empty and 'date' in df.columns\n",
    "]\n",
    "\n",
    "end_date = min(last_dates) if last_dates else pd.Timestamp.today()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "#sentiment_features = create_sentiment_features(news_df)\n",
    "#print(f\"Generated {sentiment_features.shape[1]-1} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA MERGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#eth_onchain_df = add_prefix(eth_onchain_df, 'eth')\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "uniswap_tvl_df = add_prefix(uniswap_tvl_df, 'uniswap')\n",
    "curve_tvl_df = add_prefix(curve_tvl_df, 'curve')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "layer2_tvl_df = add_prefix(layer2_tvl_df, 'l2')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=LOOKBACK_START_DATE, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df,  fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df,\n",
    "    sp500_df, vix_df, gold_df, dxy_df#,sentiment_features,eth_onchain_df,\n",
    "]\n",
    "\n",
    "for df in dataframes_to_merge:\n",
    "    if not df.empty:\n",
    "        df_merged = pd.merge(df_merged, df, on='date', how='left')\n",
    "\n",
    "print(f\"Merged shape: {df_merged.shape}\")\n",
    "print(f\"Missing before fill: {df_merged.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUE HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_merged = smart_fill_missing(df_merged)\n",
    "\n",
    "missing_after = df_merged.isnull().sum().sum()\n",
    "print(f\"Missing after fill: {missing_after:,}\")\n",
    "\n",
    "if missing_after > 0:\n",
    "    df_merged = df_merged.fillna(0)\n",
    "    print(f\"Remaining filled with 0\")\n",
    "\n",
    "lookback_df = df_merged[df_merged['date'] < TRAIN_START_DATE]\n",
    "cols_to_drop = [\n",
    "    col for col in lookback_df.columns \n",
    "    if lookback_df[col].isnull().all() and col != 'date'\n",
    "]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping {len(cols_to_drop)} fully missing columns\")\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Shape: {df_merged.shape}\")\n",
    "print(f\"Period: {df_merged['date'].min().date()} ~ {df_merged['date'].max().date()}\")\n",
    "print(f\"Missing: {df_merged.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nFeature groups:\")\n",
    "print(f\"  Crypto prices: {len([c for c in df_merged.columns if any(x in c for x in ['BTC_', 'ETH_', 'BNB_', 'XRP_', 'SOL_', 'ADA_', 'DOGE_', 'AVAX_', 'DOT_'])])}\")\n",
    "print(f\"  On-chain: {len([c for c in df_merged.columns if c.startswith('eth_')])}\")\n",
    "print(f\"  DeFi TVL: {len([c for c in df_merged.columns if any(x in c for x in ['aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_'])])}\")\n",
    "print(f\"  Layer 2: {len([c for c in df_merged.columns if c.startswith('l2_')])}\")\n",
    "print(f\"  Sentiment: {len([c for c in df_merged.columns if any(x in c for x in ['sentiment', 'news', 'bull_bear', 'positive', 'negative', 'extreme'])])}\")\n",
    "print(f\"  Macro: {len([c for c in df_merged.columns if any(x in c for x in ['sp500_', 'vix_', 'gold_', 'dxy_'])])}\")\n",
    "print(f\"  Fear & Greed: {len([c for c in df_merged.columns if c.startswith('fg_')])}\")\n",
    "print(f\"  Funding Rate: {len([c for c in df_merged.columns if c.startswith('funding_')])}\")\n",
    "print(f\"  Stablecoin: {len([c for c in df_merged.columns if c.startswith('usdt_')])}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b6834d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BTC_Open</th>\n",
       "      <th>BTC_High</th>\n",
       "      <th>BTC_Low</th>\n",
       "      <th>BTC_Close</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>ETH_Open</th>\n",
       "      <th>ETH_High</th>\n",
       "      <th>ETH_Low</th>\n",
       "      <th>ETH_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>AVAX_Open</th>\n",
       "      <th>AVAX_High</th>\n",
       "      <th>AVAX_Low</th>\n",
       "      <th>AVAX_Close</th>\n",
       "      <th>AVAX_Volume</th>\n",
       "      <th>DOT_Open</th>\n",
       "      <th>DOT_High</th>\n",
       "      <th>DOT_Low</th>\n",
       "      <th>DOT_Close</th>\n",
       "      <th>DOT_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>165911000.0</td>\n",
       "      <td>169336000.0</td>\n",
       "      <td>162011000.0</td>\n",
       "      <td>163400000.0</td>\n",
       "      <td>2420.857207</td>\n",
       "      <td>5974000.0</td>\n",
       "      <td>6100000.0</td>\n",
       "      <td>5801000.0</td>\n",
       "      <td>5837000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30530.0</td>\n",
       "      <td>31070.0</td>\n",
       "      <td>29370.0</td>\n",
       "      <td>29440.0</td>\n",
       "      <td>636935.209634</td>\n",
       "      <td>4623.0</td>\n",
       "      <td>4737.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>1.218881e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>2025-10-22</td>\n",
       "      <td>163400000.0</td>\n",
       "      <td>164406000.0</td>\n",
       "      <td>162500000.0</td>\n",
       "      <td>163153000.0</td>\n",
       "      <td>1291.512957</td>\n",
       "      <td>5838000.0</td>\n",
       "      <td>5862000.0</td>\n",
       "      <td>5691000.0</td>\n",
       "      <td>5768000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29440.0</td>\n",
       "      <td>29760.0</td>\n",
       "      <td>28100.0</td>\n",
       "      <td>28780.0</td>\n",
       "      <td>437173.156699</td>\n",
       "      <td>4528.0</td>\n",
       "      <td>4569.0</td>\n",
       "      <td>4333.0</td>\n",
       "      <td>4423.0</td>\n",
       "      <td>1.053011e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>163153000.0</td>\n",
       "      <td>166400000.0</td>\n",
       "      <td>163035000.0</td>\n",
       "      <td>165546000.0</td>\n",
       "      <td>1119.204121</td>\n",
       "      <td>5768000.0</td>\n",
       "      <td>5895000.0</td>\n",
       "      <td>5746000.0</td>\n",
       "      <td>5804000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28790.0</td>\n",
       "      <td>29600.0</td>\n",
       "      <td>28520.0</td>\n",
       "      <td>28810.0</td>\n",
       "      <td>227109.590202</td>\n",
       "      <td>4421.0</td>\n",
       "      <td>4541.0</td>\n",
       "      <td>4392.0</td>\n",
       "      <td>4507.0</td>\n",
       "      <td>7.905389e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>2025-10-24</td>\n",
       "      <td>165546000.0</td>\n",
       "      <td>167381000.0</td>\n",
       "      <td>164420000.0</td>\n",
       "      <td>165703000.0</td>\n",
       "      <td>1440.094873</td>\n",
       "      <td>5806000.0</td>\n",
       "      <td>6000000.0</td>\n",
       "      <td>5790000.0</td>\n",
       "      <td>5869000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28810.0</td>\n",
       "      <td>29790.0</td>\n",
       "      <td>28650.0</td>\n",
       "      <td>29100.0</td>\n",
       "      <td>267813.105066</td>\n",
       "      <td>4516.0</td>\n",
       "      <td>4665.0</td>\n",
       "      <td>4504.0</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>7.823628e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>2025-10-25</td>\n",
       "      <td>165703000.0</td>\n",
       "      <td>166849000.0</td>\n",
       "      <td>165331000.0</td>\n",
       "      <td>166310000.0</td>\n",
       "      <td>515.289843</td>\n",
       "      <td>5869000.0</td>\n",
       "      <td>5900000.0</td>\n",
       "      <td>5845000.0</td>\n",
       "      <td>5891000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29100.0</td>\n",
       "      <td>29380.0</td>\n",
       "      <td>28760.0</td>\n",
       "      <td>29310.0</td>\n",
       "      <td>195200.063079</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>4605.0</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>4.941984e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>166330000.0</td>\n",
       "      <td>170000000.0</td>\n",
       "      <td>166104000.0</td>\n",
       "      <td>169514000.0</td>\n",
       "      <td>1297.923989</td>\n",
       "      <td>5891000.0</td>\n",
       "      <td>6173000.0</td>\n",
       "      <td>5857000.0</td>\n",
       "      <td>6149000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29330.0</td>\n",
       "      <td>30990.0</td>\n",
       "      <td>28930.0</td>\n",
       "      <td>30820.0</td>\n",
       "      <td>349410.882294</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>4780.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4730.0</td>\n",
       "      <td>5.948005e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>169514000.0</td>\n",
       "      <td>171500000.0</td>\n",
       "      <td>168793000.0</td>\n",
       "      <td>168980000.0</td>\n",
       "      <td>1539.674254</td>\n",
       "      <td>6150000.0</td>\n",
       "      <td>6272000.0</td>\n",
       "      <td>6070000.0</td>\n",
       "      <td>6096000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30820.0</td>\n",
       "      <td>31150.0</td>\n",
       "      <td>29940.0</td>\n",
       "      <td>30080.0</td>\n",
       "      <td>373422.589692</td>\n",
       "      <td>4728.0</td>\n",
       "      <td>4790.0</td>\n",
       "      <td>4610.0</td>\n",
       "      <td>4646.0</td>\n",
       "      <td>8.136853e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>168980000.0</td>\n",
       "      <td>170859000.0</td>\n",
       "      <td>167550000.0</td>\n",
       "      <td>168263000.0</td>\n",
       "      <td>1268.518901</td>\n",
       "      <td>6097000.0</td>\n",
       "      <td>6148000.0</td>\n",
       "      <td>5900000.0</td>\n",
       "      <td>5940000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30080.0</td>\n",
       "      <td>30350.0</td>\n",
       "      <td>28630.0</td>\n",
       "      <td>29040.0</td>\n",
       "      <td>358551.700128</td>\n",
       "      <td>4646.0</td>\n",
       "      <td>4709.0</td>\n",
       "      <td>4473.0</td>\n",
       "      <td>4545.0</td>\n",
       "      <td>7.823002e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>168263000.0</td>\n",
       "      <td>168401000.0</td>\n",
       "      <td>164000000.0</td>\n",
       "      <td>164617000.0</td>\n",
       "      <td>1947.231432</td>\n",
       "      <td>5940000.0</td>\n",
       "      <td>5992000.0</td>\n",
       "      <td>5800000.0</td>\n",
       "      <td>5839000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29040.0</td>\n",
       "      <td>30110.0</td>\n",
       "      <td>28780.0</td>\n",
       "      <td>29360.0</td>\n",
       "      <td>296259.374437</td>\n",
       "      <td>4545.0</td>\n",
       "      <td>4740.0</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>4602.0</td>\n",
       "      <td>9.652192e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>164597000.0</td>\n",
       "      <td>166201000.0</td>\n",
       "      <td>162000000.0</td>\n",
       "      <td>164258000.0</td>\n",
       "      <td>2160.389729</td>\n",
       "      <td>5839000.0</td>\n",
       "      <td>5900000.0</td>\n",
       "      <td>5610000.0</td>\n",
       "      <td>5769000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29400.0</td>\n",
       "      <td>29780.0</td>\n",
       "      <td>26660.0</td>\n",
       "      <td>27490.0</td>\n",
       "      <td>536989.008239</td>\n",
       "      <td>4602.0</td>\n",
       "      <td>4650.0</td>\n",
       "      <td>4235.0</td>\n",
       "      <td>4356.0</td>\n",
       "      <td>1.762021e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>164259000.0</td>\n",
       "      <td>166000000.0</td>\n",
       "      <td>163255000.0</td>\n",
       "      <td>164275000.0</td>\n",
       "      <td>1085.963603</td>\n",
       "      <td>5770000.0</td>\n",
       "      <td>5843000.0</td>\n",
       "      <td>5715000.0</td>\n",
       "      <td>5768000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27480.0</td>\n",
       "      <td>27770.0</td>\n",
       "      <td>26720.0</td>\n",
       "      <td>27250.0</td>\n",
       "      <td>263859.271039</td>\n",
       "      <td>4356.0</td>\n",
       "      <td>4398.0</td>\n",
       "      <td>4251.0</td>\n",
       "      <td>4320.0</td>\n",
       "      <td>7.602354e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>164275000.0</td>\n",
       "      <td>164692000.0</td>\n",
       "      <td>163410000.0</td>\n",
       "      <td>164000000.0</td>\n",
       "      <td>394.934976</td>\n",
       "      <td>5768000.0</td>\n",
       "      <td>5816000.0</td>\n",
       "      <td>5742000.0</td>\n",
       "      <td>5770000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27250.0</td>\n",
       "      <td>27910.0</td>\n",
       "      <td>27060.0</td>\n",
       "      <td>27880.0</td>\n",
       "      <td>173125.086800</td>\n",
       "      <td>4320.0</td>\n",
       "      <td>4429.0</td>\n",
       "      <td>4293.0</td>\n",
       "      <td>4427.0</td>\n",
       "      <td>5.976827e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>2025-11-02</td>\n",
       "      <td>164001000.0</td>\n",
       "      <td>165139000.0</td>\n",
       "      <td>163300000.0</td>\n",
       "      <td>164129000.0</td>\n",
       "      <td>538.341715</td>\n",
       "      <td>5771000.0</td>\n",
       "      <td>5814000.0</td>\n",
       "      <td>5720000.0</td>\n",
       "      <td>5798000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27900.0</td>\n",
       "      <td>28210.0</td>\n",
       "      <td>27010.0</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>172647.115825</td>\n",
       "      <td>4418.0</td>\n",
       "      <td>4479.0</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>4424.0</td>\n",
       "      <td>8.250238e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>164074000.0</td>\n",
       "      <td>164133000.0</td>\n",
       "      <td>158000000.0</td>\n",
       "      <td>158587000.0</td>\n",
       "      <td>3370.616160</td>\n",
       "      <td>5792000.0</td>\n",
       "      <td>5800000.0</td>\n",
       "      <td>5305000.0</td>\n",
       "      <td>5363000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27950.0</td>\n",
       "      <td>28040.0</td>\n",
       "      <td>24500.0</td>\n",
       "      <td>24810.0</td>\n",
       "      <td>754639.267640</td>\n",
       "      <td>4427.0</td>\n",
       "      <td>4436.0</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>3849.0</td>\n",
       "      <td>3.892335e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>158589000.0</td>\n",
       "      <td>161128000.0</td>\n",
       "      <td>147651000.0</td>\n",
       "      <td>152873000.0</td>\n",
       "      <td>6623.281667</td>\n",
       "      <td>5368000.0</td>\n",
       "      <td>5485000.0</td>\n",
       "      <td>4582000.0</td>\n",
       "      <td>4954000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24810.0</td>\n",
       "      <td>25480.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>24070.0</td>\n",
       "      <td>782217.196260</td>\n",
       "      <td>3849.0</td>\n",
       "      <td>3985.0</td>\n",
       "      <td>3510.0</td>\n",
       "      <td>3777.0</td>\n",
       "      <td>3.151653e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>152873000.0</td>\n",
       "      <td>156324000.0</td>\n",
       "      <td>146679000.0</td>\n",
       "      <td>156008000.0</td>\n",
       "      <td>4638.822224</td>\n",
       "      <td>4954000.0</td>\n",
       "      <td>5199000.0</td>\n",
       "      <td>4693000.0</td>\n",
       "      <td>5135000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24060.0</td>\n",
       "      <td>25350.0</td>\n",
       "      <td>22660.0</td>\n",
       "      <td>24900.0</td>\n",
       "      <td>564413.727874</td>\n",
       "      <td>3774.0</td>\n",
       "      <td>4006.0</td>\n",
       "      <td>3539.0</td>\n",
       "      <td>4001.0</td>\n",
       "      <td>1.626101e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>156008000.0</td>\n",
       "      <td>156126000.0</td>\n",
       "      <td>150500000.0</td>\n",
       "      <td>151401000.0</td>\n",
       "      <td>1828.656113</td>\n",
       "      <td>5136000.0</td>\n",
       "      <td>5160000.0</td>\n",
       "      <td>4862000.0</td>\n",
       "      <td>4957000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24930.0</td>\n",
       "      <td>24930.0</td>\n",
       "      <td>23370.0</td>\n",
       "      <td>24070.0</td>\n",
       "      <td>269327.947021</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>4010.0</td>\n",
       "      <td>3797.0</td>\n",
       "      <td>3980.0</td>\n",
       "      <td>7.570426e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>151412000.0</td>\n",
       "      <td>154961000.0</td>\n",
       "      <td>148831000.0</td>\n",
       "      <td>153850000.0</td>\n",
       "      <td>2342.631612</td>\n",
       "      <td>4957000.0</td>\n",
       "      <td>5174000.0</td>\n",
       "      <td>4800000.0</td>\n",
       "      <td>5112000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24100.0</td>\n",
       "      <td>26890.0</td>\n",
       "      <td>24060.0</td>\n",
       "      <td>26420.0</td>\n",
       "      <td>568541.044085</td>\n",
       "      <td>3978.0</td>\n",
       "      <td>4929.0</td>\n",
       "      <td>3977.0</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>3.478051e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>2025-11-08</td>\n",
       "      <td>153850000.0</td>\n",
       "      <td>154400000.0</td>\n",
       "      <td>151353000.0</td>\n",
       "      <td>152541000.0</td>\n",
       "      <td>879.041770</td>\n",
       "      <td>5111000.0</td>\n",
       "      <td>5195000.0</td>\n",
       "      <td>5010000.0</td>\n",
       "      <td>5070000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26420.0</td>\n",
       "      <td>27470.0</td>\n",
       "      <td>25380.0</td>\n",
       "      <td>26080.0</td>\n",
       "      <td>410009.589017</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>4584.0</td>\n",
       "      <td>4799.0</td>\n",
       "      <td>2.822870e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>152542000.0</td>\n",
       "      <td>154763000.0</td>\n",
       "      <td>151279000.0</td>\n",
       "      <td>154483000.0</td>\n",
       "      <td>774.789757</td>\n",
       "      <td>5070000.0</td>\n",
       "      <td>5298000.0</td>\n",
       "      <td>5006000.0</td>\n",
       "      <td>5247000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26100.0</td>\n",
       "      <td>26600.0</td>\n",
       "      <td>25060.0</td>\n",
       "      <td>26440.0</td>\n",
       "      <td>235892.661817</td>\n",
       "      <td>4798.0</td>\n",
       "      <td>4883.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>4740.0</td>\n",
       "      <td>9.782762e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     BTC_Open     BTC_High      BTC_Low    BTC_Close  \\\n",
       "2948 2025-10-21  165911000.0  169336000.0  162011000.0  163400000.0   \n",
       "2949 2025-10-22  163400000.0  164406000.0  162500000.0  163153000.0   \n",
       "2950 2025-10-23  163153000.0  166400000.0  163035000.0  165546000.0   \n",
       "2951 2025-10-24  165546000.0  167381000.0  164420000.0  165703000.0   \n",
       "2952 2025-10-25  165703000.0  166849000.0  165331000.0  166310000.0   \n",
       "2953 2025-10-26  166330000.0  170000000.0  166104000.0  169514000.0   \n",
       "2954 2025-10-27  169514000.0  171500000.0  168793000.0  168980000.0   \n",
       "2955 2025-10-28  168980000.0  170859000.0  167550000.0  168263000.0   \n",
       "2956 2025-10-29  168263000.0  168401000.0  164000000.0  164617000.0   \n",
       "2957 2025-10-30  164597000.0  166201000.0  162000000.0  164258000.0   \n",
       "2958 2025-10-31  164259000.0  166000000.0  163255000.0  164275000.0   \n",
       "2959 2025-11-01  164275000.0  164692000.0  163410000.0  164000000.0   \n",
       "2960 2025-11-02  164001000.0  165139000.0  163300000.0  164129000.0   \n",
       "2961 2025-11-03  164074000.0  164133000.0  158000000.0  158587000.0   \n",
       "2962 2025-11-04  158589000.0  161128000.0  147651000.0  152873000.0   \n",
       "2963 2025-11-05  152873000.0  156324000.0  146679000.0  156008000.0   \n",
       "2964 2025-11-06  156008000.0  156126000.0  150500000.0  151401000.0   \n",
       "2965 2025-11-07  151412000.0  154961000.0  148831000.0  153850000.0   \n",
       "2966 2025-11-08  153850000.0  154400000.0  151353000.0  152541000.0   \n",
       "2967 2025-11-09  152542000.0  154763000.0  151279000.0  154483000.0   \n",
       "\n",
       "       BTC_Volume   ETH_Open   ETH_High    ETH_Low  ETH_Close  ...  AVAX_Open  \\\n",
       "2948  2420.857207  5974000.0  6100000.0  5801000.0  5837000.0  ...    30530.0   \n",
       "2949  1291.512957  5838000.0  5862000.0  5691000.0  5768000.0  ...    29440.0   \n",
       "2950  1119.204121  5768000.0  5895000.0  5746000.0  5804000.0  ...    28790.0   \n",
       "2951  1440.094873  5806000.0  6000000.0  5790000.0  5869000.0  ...    28810.0   \n",
       "2952   515.289843  5869000.0  5900000.0  5845000.0  5891000.0  ...    29100.0   \n",
       "2953  1297.923989  5891000.0  6173000.0  5857000.0  6149000.0  ...    29330.0   \n",
       "2954  1539.674254  6150000.0  6272000.0  6070000.0  6096000.0  ...    30820.0   \n",
       "2955  1268.518901  6097000.0  6148000.0  5900000.0  5940000.0  ...    30080.0   \n",
       "2956  1947.231432  5940000.0  5992000.0  5800000.0  5839000.0  ...    29040.0   \n",
       "2957  2160.389729  5839000.0  5900000.0  5610000.0  5769000.0  ...    29400.0   \n",
       "2958  1085.963603  5770000.0  5843000.0  5715000.0  5768000.0  ...    27480.0   \n",
       "2959   394.934976  5768000.0  5816000.0  5742000.0  5770000.0  ...    27250.0   \n",
       "2960   538.341715  5771000.0  5814000.0  5720000.0  5798000.0  ...    27900.0   \n",
       "2961  3370.616160  5792000.0  5800000.0  5305000.0  5363000.0  ...    27950.0   \n",
       "2962  6623.281667  5368000.0  5485000.0  4582000.0  4954000.0  ...    24810.0   \n",
       "2963  4638.822224  4954000.0  5199000.0  4693000.0  5135000.0  ...    24060.0   \n",
       "2964  1828.656113  5136000.0  5160000.0  4862000.0  4957000.0  ...    24930.0   \n",
       "2965  2342.631612  4957000.0  5174000.0  4800000.0  5112000.0  ...    24100.0   \n",
       "2966   879.041770  5111000.0  5195000.0  5010000.0  5070000.0  ...    26420.0   \n",
       "2967   774.789757  5070000.0  5298000.0  5006000.0  5247000.0  ...    26100.0   \n",
       "\n",
       "      AVAX_High  AVAX_Low  AVAX_Close    AVAX_Volume  DOT_Open  DOT_High  \\\n",
       "2948    31070.0   29370.0     29440.0  636935.209634    4623.0    4737.0   \n",
       "2949    29760.0   28100.0     28780.0  437173.156699    4528.0    4569.0   \n",
       "2950    29600.0   28520.0     28810.0  227109.590202    4421.0    4541.0   \n",
       "2951    29790.0   28650.0     29100.0  267813.105066    4516.0    4665.0   \n",
       "2952    29380.0   28760.0     29310.0  195200.063079    4594.0    4605.0   \n",
       "2953    30990.0   28930.0     30820.0  349410.882294    4594.0    4780.0   \n",
       "2954    31150.0   29940.0     30080.0  373422.589692    4728.0    4790.0   \n",
       "2955    30350.0   28630.0     29040.0  358551.700128    4646.0    4709.0   \n",
       "2956    30110.0   28780.0     29360.0  296259.374437    4545.0    4740.0   \n",
       "2957    29780.0   26660.0     27490.0  536989.008239    4602.0    4650.0   \n",
       "2958    27770.0   26720.0     27250.0  263859.271039    4356.0    4398.0   \n",
       "2959    27910.0   27060.0     27880.0  173125.086800    4320.0    4429.0   \n",
       "2960    28210.0   27010.0     27930.0  172647.115825    4418.0    4479.0   \n",
       "2961    28040.0   24500.0     24810.0  754639.267640    4427.0    4436.0   \n",
       "2962    25480.0   22500.0     24070.0  782217.196260    3849.0    3985.0   \n",
       "2963    25350.0   22660.0     24900.0  564413.727874    3774.0    4006.0   \n",
       "2964    24930.0   23370.0     24070.0  269327.947021    3999.0    4010.0   \n",
       "2965    26890.0   24060.0     26420.0  568541.044085    3978.0    4929.0   \n",
       "2966    27470.0   25380.0     26080.0  410009.589017    4804.0    5200.0   \n",
       "2967    26600.0   25060.0     26440.0  235892.661817    4798.0    4883.0   \n",
       "\n",
       "      DOT_Low  DOT_Close    DOT_Volume  \n",
       "2948   4455.0     4529.0  1.218881e+06  \n",
       "2949   4333.0     4423.0  1.053011e+06  \n",
       "2950   4392.0     4507.0  7.905389e+05  \n",
       "2951   4504.0     4594.0  7.823628e+05  \n",
       "2952   4527.0     4594.0  4.941984e+05  \n",
       "2953   4564.0     4730.0  5.948005e+05  \n",
       "2954   4610.0     4646.0  8.136853e+05  \n",
       "2955   4473.0     4545.0  7.823002e+05  \n",
       "2956   4511.0     4602.0  9.652192e+05  \n",
       "2957   4235.0     4356.0  1.762021e+06  \n",
       "2958   4251.0     4320.0  7.602354e+05  \n",
       "2959   4293.0     4427.0  5.976827e+05  \n",
       "2960   4276.0     4424.0  8.250238e+05  \n",
       "2961   3780.0     3849.0  3.892335e+06  \n",
       "2962   3510.0     3777.0  3.151653e+06  \n",
       "2963   3539.0     4001.0  1.626101e+06  \n",
       "2964   3797.0     3980.0  7.570426e+05  \n",
       "2965   3977.0     4804.0  3.478051e+06  \n",
       "2966   4584.0     4799.0  2.822870e+06  \n",
       "2967   4506.0     4740.0  9.782762e+05  \n",
       "\n",
       "[20 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(10)\n",
    "macro_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61232089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (14만 - 모든 fold 선택)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        \n",
    "        # MACD (필수 - 자주 선택됨)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic (14만 - 나머지는 중복)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (10만 - 20과 거의 동일)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        \n",
    "        # MOM (10만 유지)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        \n",
    "        # CCI (14, 50만 - 극단값 비교용)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_14'] > 100).astype(int)\n",
    "      \n",
    "        # TSI\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "        \n",
    "        # Ichimoku (유지 - 복합 지표로 유용)\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (20, 50만 - Golden Cross용)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        \n",
    "        # EMA (12, 26만 - MACD 구성 요소)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        \n",
    "        # TEMA (10만 - 30과 중복)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        \n",
    "        # WMA (20만 - 10과 중복)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)\n",
    "        \n",
    "        # HMA (유지 - 독특한 smoothing)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (유지)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        # VWMA (유지 - 거래량 가중)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합 (유지 - 다른 정보)\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands \n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        \n",
    "        # NATR\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Supertrend\n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        # ===== VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Fold에서 선택됨)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX (필수)\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex \n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO \n",
    "        df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        \n",
    "        # 변동성 \n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        \n",
    "        # 모멘텀 \n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1\n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)\n",
    "        \n",
    "        # Range 지표 \n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)\n",
    "        \n",
    "        # Linear Regression Slope \n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing \n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        \n",
    "        # BOP\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 파생 \n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        \n",
    "        # RSI 파생\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (Fold에서 선택됨)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        # Distance from High/Low \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 5]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df,target_type,method):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        for period in [5, 20]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        \n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['eth_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        \n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['eth_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = df_transformed[col_name].rolling(30, min_periods=10).std().fillna(0)\n",
    "            \n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(btc_volume.pct_change())\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (eth_volume / (btc_volume + 1e-8)).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "            \n",
    "    ## raw_data 저장하기\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    base_dir=os.path.join('model_results',timestamp,'raw_data',target_type,method)\n",
    "    os.makedirs(base_dir,exist_ok=True)\n",
    "    df.to_csv(os.path.join(base_dir,\"raw_data_all_features.csv\"),index=False)        \n",
    "            \n",
    "            \n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [col for col in df_transformed.columns if 'return' in col.lower() and 'next' not in col]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae1f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = ['sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio', 'negative_ratio']\n",
    "    sentiment_ma_cols = [col for col in df.columns if 'sentiment' in col and ('_ma7' in col or '_volatility_7' in col)]\n",
    "    no_lag_patterns = ['_trend', '_acceleration', '_volume_change', 'news_volume_change', 'news_volume_ma']\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', \n",
    "                                  'eth_contract', 'eth_avg_gas', 'eth_total_gas', 'eth_avg_block'])]\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col or '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            if not any(pattern in col for pattern in no_lag_patterns):\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' not in df.columns:\n",
    "        return df_norm\n",
    "    \n",
    "    current_close = df['ETH_Close']\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_non_stationary_features(df):\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    prefixes_to_transform = [\n",
    "        'eth_', 'aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_',\n",
    "        'l2_', 'sp500_', 'gold_', 'dxy_', 'vix_', 'usdt_'\n",
    "    ]\n",
    "    \n",
    "    exclude_prefixes = ['fg_', 'funding_']\n",
    "    \n",
    "    exclude_keywords = [\n",
    "        '_pct_', '_ratio', '_lag', '_volatility', '_corr', '_beta', '_spread',\n",
    "        'eth_return', 'btc_return', 'eth_log_return' \n",
    "    ]\n",
    "    \n",
    "    cols_to_transform = []\n",
    "    for col in df_proc.columns:\n",
    "        if col.startswith(tuple(prefixes_to_transform)):\n",
    "            if not col.startswith(tuple(exclude_prefixes)):\n",
    "                if not any(keyword in col for keyword in exclude_keywords):\n",
    "                    cols_to_transform.append(col)\n",
    "                    \n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        df_proc[col] = df_proc[col].fillna(method='ffill').replace(0, 1e-9)\n",
    "\n",
    "        df_proc[f'{col}_pct_1d'] = df_proc[col].pct_change(1)\n",
    "        df_proc[f'{col}_pct_5d'] = df_proc[col].pct_change(5)\n",
    "        \n",
    "        ma_30 = df_proc[col].rolling(window=30, min_periods=10).mean()\n",
    "        df_proc[f'{col}_ma30_ratio'] = df_proc[col] / (ma_30 + 1e-9)\n",
    "        \n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "    df_proc = df_proc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    df_proc = df_proc.replace([np.inf, -np.inf], np.nan)\n",
    "    df_proc = df_proc.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    #print(f\"Preprocessed and replaced {len(cols_to_drop)} non-stationary features.\")\n",
    "    \n",
    "    return df_proc    \n",
    "    \n",
    "    \n",
    "    \n",
    "@jit(nopython=True)\n",
    "def compute_triple_barrier_targets(\n",
    "    prices_close,\n",
    "    prices_high,\n",
    "    prices_low,\n",
    "    atr,\n",
    "    lookahead_candles,\n",
    "    atr_multiplier_profit,\n",
    "    atr_multiplier_stop\n",
    "):\n",
    "    n = len(prices_close)\n",
    "    targets_raw = np.zeros(n, dtype=np.int32) \n",
    "    upper_barriers_arr = np.zeros(n, dtype=np.float64) \n",
    "    lower_barriers_arr = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    for i in range(n - lookahead_candles):\n",
    "        current_atr = max(atr[i], 1e-8) \n",
    "        current_price = prices_close[i]\n",
    "        \n",
    "        upper_barrier = current_price + (current_atr * atr_multiplier_profit)\n",
    "        lower_barrier = current_price - (current_atr * atr_multiplier_stop)\n",
    "        \n",
    "        upper_barriers_arr[i] = upper_barrier\n",
    "        lower_barriers_arr[i] = lower_barrier\n",
    "        \n",
    "        for j in range(1, lookahead_candles + 1):\n",
    "            future_high = prices_high[i + j]\n",
    "            future_low = prices_low[i + j]\n",
    "            \n",
    "            if future_high >= upper_barrier:\n",
    "                targets_raw[i] = 1\n",
    "                break \n",
    "                \n",
    "            elif future_low <= lower_barrier:\n",
    "                targets_raw[i] = 2\n",
    "                break\n",
    "    \n",
    "    return targets_raw, upper_barriers_arr, lower_barriers_arr\n",
    "\n",
    "\n",
    "def create_targets(df, lookahead_candles=8, atr_multiplier_profit=1.5, atr_multiplier_stop=1.0):\n",
    "    df_target = df.copy()\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "    if atr_col_name not in df.columns:\n",
    "        raise ValueError(f\"'{atr_col_name}' feature is missing. Run calculate_technical_indicators first.\")\n",
    "\n",
    "    prices_close = df_target['ETH_Close'].to_numpy()\n",
    "    prices_high = df_target['ETH_High'].to_numpy()\n",
    "    prices_low = df_target['ETH_Low'].to_numpy()\n",
    "    atr = pd.Series(df_target[atr_col_name]).fillna(method='ffill').fillna(0).to_numpy()\n",
    "\n",
    "    targets_raw, upper_barriers, lower_barriers = compute_triple_barrier_targets(\n",
    "        prices_close,\n",
    "        prices_high,\n",
    "        prices_low,\n",
    "        atr,\n",
    "        lookahead_candles,\n",
    "        atr_multiplier_profit,\n",
    "        atr_multiplier_stop\n",
    "    )\n",
    "    \n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "    \n",
    "    df_target['next_open'] = next_open\n",
    "    df_target['next_close'] = next_close\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "    \n",
    "    df_target['next_direction'] = pd.Series(targets_raw, index=df_target.index).map({\n",
    "        1: 1,\n",
    "        2: 0,\n",
    "        0: np.nan\n",
    "    })\n",
    "    \n",
    "    df_target['take_profit_price'] = pd.Series(upper_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['stop_loss_price'] = pd.Series(lower_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    \n",
    "    return df_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b414b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_multi_target(X_train, y_train, target_type='direction', top_n=30):\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "\n",
    "    if target_type == 'direction':\n",
    "        selected, stats = select_features_verified(\n",
    "            X_train, \n",
    "            y_train['next_direction'], \n",
    "            task='class', \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        if atr_col_name not in selected and atr_col_name in X_train.columns:\n",
    "            selected.pop()\n",
    "            selected.insert(0, atr_col_name)\n",
    "    \n",
    "    \n",
    "    print(\", \".join(selected))\n",
    "    return selected, stats\n",
    "\n",
    "\n",
    "def select_features_verified(X_train, y_train, task='class', top_n=30, verbose=True):\n",
    "    \n",
    "    if task == 'class':\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X_train, y_train, random_state=42, n_neighbors=3)\n",
    "    \n",
    "    mi_idx = np.argsort(mi_scores)[::-1][:top_n]\n",
    "    mi_features = X_train.columns[mi_idx].tolist()\n",
    "    \n",
    "    if task == 'class':\n",
    "        estimator = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        estimator = LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=top_n,\n",
    "        step=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    if task == 'class':\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    rf_idx = np.argsort(rf_importances)[::-1][:top_n]\n",
    "    rf_features = X_train.columns[rf_idx].tolist()\n",
    "    \n",
    "    all_features = mi_features + rfe_features + rf_features\n",
    "    feature_votes = Counter(all_features)\n",
    "    selected_features = [feat for feat, _ in feature_votes.most_common(top_n)]\n",
    "\n",
    "    if len(selected_features) < top_n:\n",
    "        remaining = top_n - len(selected_features)\n",
    "        for feat in mi_features:\n",
    "            if feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    \n",
    "    return selected_features, {\n",
    "        'mi_features': mi_features,\n",
    "        'rfe_features': rfe_features,\n",
    "        'rf_features': rf_features,\n",
    "        'feature_votes': feature_votes,\n",
    "        'mi_scores': dict(zip(X_train.columns, mi_scores)),\n",
    "        'rf_importances': dict(zip(X_train.columns, rf_importances))\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def split_walk_forward_method(df, train_start_date, \n",
    "                              final_test_start='2025-01-01',\n",
    "                              initial_train_size=800,    \n",
    "                              val_size=150,             \n",
    "                              test_size=150,           \n",
    "                              step=150,                  \n",
    "                              gap_size=7):\n",
    "    \"\"\"\n",
    "    Reverse Rolling Walk-Forward Validation\n",
    "    - 마지막 날짜부터 시작해서 과거로 rolling\n",
    "    - Train 크기는 고정 (initial_train_size)\n",
    "    - Final holdout은 2025-01-01부터 고정\n",
    "    \"\"\"\n",
    "    \n",
    "    df_period = df[df['date'] >= train_start_date].copy()\n",
    "    df_period = df_period.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if isinstance(final_test_start, str):\n",
    "        final_test_start = pd.to_datetime(final_test_start)\n",
    "    \n",
    "    final_test_df = df_period[df_period['date'] >= final_test_start].copy()\n",
    "    \n",
    "    total_days = len(df_period)\n",
    "    min_required_days = initial_train_size + val_size + (gap_size * 2) + test_size\n",
    "    n_splits = (total_days - min_required_days) // step + 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Reverse Rolling Walk-Forward Configuration \")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total: {len(df_period)} days\")\n",
    "    print(f\"Rolling train size: {initial_train_size} days (FIXED)\")\n",
    "    print(f\"Val: {val_size} days | Test: {test_size} days\")\n",
    "    print(f\"Gap: {gap_size} days | Step: {step} days (BACKWARD)\")\n",
    "    print(f\"Target: {n_splits} walk-forward + 1 final holdout\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    # 역방향 rolling\n",
    "    for fold_idx in range(n_splits):\n",
    "        test_end_idx = total_days - (fold_idx * step)\n",
    "        test_start_idx = test_end_idx - test_size\n",
    "        \n",
    "        if test_start_idx < 0:\n",
    "            break\n",
    "        \n",
    "        val_end_idx = test_start_idx - gap_size\n",
    "        val_start_idx = val_end_idx - val_size\n",
    "        \n",
    "        train_end_idx = val_start_idx - gap_size\n",
    "        train_start_idx = train_end_idx - initial_train_size\n",
    "        \n",
    "        if train_start_idx < 0:\n",
    "            break\n",
    "        \n",
    "        train_fold = df_period.iloc[train_start_idx:train_end_idx].copy()\n",
    "        val_fold = df_period.iloc[val_start_idx:val_end_idx].copy()\n",
    "        test_fold = df_period.iloc[test_start_idx:test_end_idx].copy()\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'test': test_fold,\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'fold_type': 'walk_forward_rolling_reverse'\n",
    "        })\n",
    "    \n",
    "    # 시간순으로 정렬\n",
    "    folds.reverse()\n",
    "    for idx, fold in enumerate(folds):\n",
    "        fold['fold_idx'] = idx + 1\n",
    "        \n",
    "        print(f\"Fold {fold['fold_idx']} (walk_forward_rolling)\")\n",
    "        print(f\"  Train: {len(fold['train']):4d}d  {fold['train']['date'].min().date()} ~ {fold['train']['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(fold['val']):4d}d  {fold['val']['date'].min().date()} ~ {fold['val']['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(fold['test']):4d}d  {fold['test']['date'].min().date()} ~ {fold['test']['date'].max().date()}\\n\")\n",
    "    \n",
    "    # Final holdout\n",
    "    if len(final_test_df) > 0:\n",
    "        pre_final_df = df_period[df_period['date'] < final_test_start].copy()\n",
    "        \n",
    "        final_val_end_idx = len(pre_final_df)\n",
    "        final_val_start_idx = final_val_end_idx - val_size\n",
    "        final_train_end_idx = final_val_start_idx - gap_size\n",
    "        final_train_start_idx = final_train_end_idx - initial_train_size\n",
    "        \n",
    "        if final_train_start_idx < 0:\n",
    "            final_train_start_idx = 0\n",
    "        \n",
    "        final_train_data = pre_final_df.iloc[final_train_start_idx:final_train_end_idx].copy()\n",
    "        final_val_data = pre_final_df.iloc[final_val_start_idx:final_val_end_idx].copy()\n",
    "        \n",
    "        print(f\"Fold {len(folds) + 1} (final_holdout)\")\n",
    "        print(f\"  Train: {len(final_train_data):4d}d  {final_train_data['date'].min().date()} ~ {final_train_data['date'].max().date()}\")\n",
    "        print(f\"  Val:   {len(final_val_data):4d}d  {final_val_data['date'].min().date()} ~ {final_val_data['date'].max().date()}\")\n",
    "        print(f\"  Test:  {len(final_test_df):4d}d  {final_test_df['date'].min().date()} ~ {final_test_df['date'].max().date()}\\n\")\n",
    "        \n",
    "        folds.append({\n",
    "            'train': final_train_data,\n",
    "            'val': final_val_data,\n",
    "            'test': final_test_df,\n",
    "            'fold_idx': len(folds) + 1,\n",
    "            'fold_type': 'final_holdout'\n",
    "        })\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Created {len(folds)} folds total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def process_single_split(split_data, target_type='direction', top_n=40, fold_idx=None):\n",
    "    \"\"\"\n",
    "    각 fold를 독립적으로 처리 (feature selection 포함)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = split_data['train']\n",
    "    val_df = split_data['val']\n",
    "    test_df = split_data['test']\n",
    "    fold_type = split_data.get('fold_type', 'unknown')\n",
    "    \n",
    "    if fold_idx is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Fold {fold_idx} ({fold_type})\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "\n",
    "    train_processed, missing_stats = handle_missing_values_paper_based(\n",
    "        train_df.copy(),\n",
    "        train_start_date=train_df['date'].min(),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_processed = handle_missing_values_paper_based(\n",
    "        val_df.copy(),\n",
    "        train_start_date=val_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    test_processed = handle_missing_values_paper_based(\n",
    "        test_df.copy(),\n",
    "        train_start_date=test_df['date'].min(),\n",
    "        is_train=False,\n",
    "        train_stats=missing_stats\n",
    "    )\n",
    "    \n",
    "    target_cols = ['next_direction','next_log_return', 'next_close','next_open', \n",
    "                   'take_profit_price', 'stop_loss_price']\n",
    "    \n",
    "    train_processed = train_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    val_processed = val_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "    test_processed = test_processed.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed[target_cols]\n",
    "    \n",
    "    X_val = val_processed[feature_cols]\n",
    "    y_val = val_processed[target_cols]\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed[target_cols]\n",
    "\n",
    "    print(f\"\\n[Feature Selection for Fold {fold_idx}]\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    selected_features, selection_stats = select_features_multi_target(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        target_type=target_type, \n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for this fold\")\n",
    "    \n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_val_sel = X_val[selected_features]\n",
    "    X_test_sel = X_test[selected_features]\n",
    "    \n",
    "    robust_scaler = RobustScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    X_train_robust = robust_scaler.fit_transform(X_train_sel)\n",
    "    X_val_robust = robust_scaler.transform(X_val_sel)\n",
    "    X_test_robust = robust_scaler.transform(X_test_sel)\n",
    "    \n",
    "    X_train_standard = standard_scaler.fit_transform(X_train_sel)\n",
    "    X_val_standard = standard_scaler.transform(X_val_sel)\n",
    "    X_test_standard = standard_scaler.transform(X_test_sel)\n",
    "    \n",
    "    print(f\"Scaling completed for Fold {fold_idx}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(f\"\\n--- [Verification] Actual Date Ranges for Fold {fold_idx} (Post-dropna) ---\")\n",
    "    if not train_processed.empty:\n",
    "        print(f\"  Train (Actual): {len(train_processed):4d} ({train_processed['date'].min().date()} ~ {train_processed['date'].max().date()})\")\n",
    "    if not val_processed.empty:\n",
    "        print(f\"  Val   (Actual): {len(val_processed):4d} ({val_processed['date'].min().date()} ~ {val_processed['date'].max().date()})\")\n",
    "    if not test_processed.empty:\n",
    "        print(f\"  Test  (Actual): {len(test_processed):4d} ({test_processed['date'].min().date()} ~ {test_processed['date'].max().date()})\")\n",
    "    print(f\"---------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    result = {\n",
    "        'train': {\n",
    "            'X_robust': X_train_robust,\n",
    "            'X_standard': X_train_standard,\n",
    "            'X_raw': X_train_sel,\n",
    "            'y': y_train.reset_index(drop=True), \n",
    "            'dates': train_processed['date'].reset_index(drop=True) \n",
    "        },\n",
    "        'val': {\n",
    "            'X_robust': X_val_robust,\n",
    "            'X_standard': X_val_standard,\n",
    "            'X_raw': X_val_sel,\n",
    "            'y': y_val.reset_index(drop=True), \n",
    "            'dates': val_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'test': {\n",
    "            'X_robust': X_test_robust,\n",
    "            'X_standard': X_test_standard,\n",
    "            'X_raw': X_test_sel,\n",
    "            'y': y_test.reset_index(drop=True),\n",
    "            'dates': test_processed['date'].reset_index(drop=True)\n",
    "        },\n",
    "        'scaler': robust_scaler, \n",
    "        'stats': {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'standard_scaler': standard_scaler,\n",
    "            'selected_features': selected_features,\n",
    "            'selection_stats': selection_stats,\n",
    "            'missing_stats': missing_stats,  \n",
    "            'target_type': target_type,\n",
    "            'target_cols': target_cols,\n",
    "            'fold_type': fold_type,\n",
    "            'fold_idx': fold_idx\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def build_complete_pipeline_corrected(df_raw, train_start_date, \n",
    "                                     final_test_start='2025-01-01',\n",
    "                                     method='tvt', target_type='direction',lookahead_candles=8,atr_multiplier_profit=1.5, \n",
    "                                     atr_multiplier_stop=1.0, **kwargs):\n",
    "    \"\"\"\n",
    "    전체 파이프라인 실행 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : DataFrame\n",
    "        원본 데이터\n",
    "    train_start_date : str\n",
    "        학습 데이터 시작 날짜\n",
    "    final_test_start : str, default='2025-01-01'\n",
    "        최종 고정 테스트 시작 날짜\n",
    "        - TVT: 이 날짜부터 마지막까지 테스트\n",
    "        - Walk-forward: 이 날짜 이전은 walk-forward folds, 이후는 final holdout\n",
    "    method : str, default='tvt'\n",
    "        'tvt' 또는 'walk_forward'\n",
    "    target_type : str, default='direction'\n",
    "        'direction', 'return', 'price', 'direction_return', 'direction_price'\n",
    "    **kwargs : dict\n",
    "        각 method에 필요한 추가 파라미터\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "\n",
    "    df = add_price_lag_features_first(df)\n",
    "    df = calculate_technical_indicators(df)\n",
    "    df = add_enhanced_cross_crypto_features(df)\n",
    "    df = add_volatility_regime_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_percentile_features(df)\n",
    "    df = add_normalized_price_lags(df)\n",
    "    df = preprocess_non_stationary_features(df)\n",
    "    df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "    df = create_targets(df, \n",
    "                        lookahead_candles=lookahead_candles,\n",
    "                        atr_multiplier_profit=atr_multiplier_profit, \n",
    "                        atr_multiplier_stop=atr_multiplier_stop)\n",
    "    df = remove_raw_prices_and_transform(df,target_type,method)\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    df = df.iloc[:-lookahead_candles]  \n",
    "    \n",
    "    \n",
    "    \n",
    "    split_kwargs = {}\n",
    "    \n",
    "\n",
    "    split_kwargs['final_test_start'] = final_test_start\n",
    "    if 'n_splits' in kwargs:\n",
    "        split_kwargs['n_splits'] = kwargs['n_splits']\n",
    "    if 'initial_train_size' in kwargs:\n",
    "        split_kwargs['initial_train_size'] = kwargs['initial_train_size']\n",
    "    if 'test_size' in kwargs:\n",
    "        split_kwargs['test_size'] = kwargs['test_size']\n",
    "    if 'val_size' in kwargs:\n",
    "        split_kwargs['val_size'] = kwargs['val_size']\n",
    "    if 'step' in kwargs:\n",
    "        split_kwargs['step'] = kwargs['step']\n",
    "    splits = split_walk_forward_method(df, train_start_date, **split_kwargs)\n",
    "\n",
    "    result = [\n",
    "        process_single_split(\n",
    "            fold, \n",
    "            target_type=target_type,  \n",
    "            top_n=30,\n",
    "            fold_idx=fold['fold_idx']\n",
    "        ) \n",
    "        for fold in splits\n",
    "    ]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1182e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesAugmentation:\n",
    "    \"\"\"\n",
    "    시계열 데이터 증강을 위한 유틸리티 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def jittering(X, sigma=0.02):\n",
    "        \"\"\"\n",
    "        가우시안 노이즈 추가\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, sigma, X.shape)\n",
    "        return X + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaling(X, sigma=0.1):\n",
    "        \"\"\"\n",
    "        랜덤 스케일링 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], 1, X.shape[2]))\n",
    "        else:\n",
    "            factor = np.random.normal(1, sigma, (X.shape[0], X.shape[1]))\n",
    "        return X * factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def magnitude_warping(X, sigma=0.2, num_knots=4):\n",
    "        \"\"\"\n",
    "        진폭 왜곡 적용\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 3:\n",
    "            seq_len = X.shape[1]\n",
    "            orig_steps = np.linspace(0, seq_len - 1, num_knots + 2)\n",
    "            random_warps = np.random.normal(1, sigma, size=(X.shape[0], num_knots + 2, X.shape[2]))\n",
    "            \n",
    "            warped_X = np.zeros_like(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[2]):\n",
    "                    warper = np.interp(np.arange(seq_len), orig_steps, random_warps[i, :, j])\n",
    "                    warped_X[i, :, j] = X[i, :, j] * warper\n",
    "            return warped_X\n",
    "        else:\n",
    "            return X * np.random.normal(1, sigma, X.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_augmentation(X, method='jittering', **kwargs):\n",
    "        \"\"\"\n",
    "        선택된 증강 기법 적용\n",
    "        \"\"\"\n",
    "        if method == 'jittering':\n",
    "            return TimeSeriesAugmentation.jittering(X, **kwargs)\n",
    "        elif method == 'scaling':\n",
    "            return TimeSeriesAugmentation.scaling(X, **kwargs)\n",
    "        elif method == 'magnitude_warping':\n",
    "            return TimeSeriesAugmentation.magnitude_warping(X, **kwargs)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "class DirectionModels:\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_forest(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 70),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 35),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_samples': trial.suggest_float('max_samples', 0.6, 0.8),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 40, 100),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.01),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.01),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'bootstrap': True\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False, n_jobs=1)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1, bootstrap=True)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Random Forest] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def lightgbm(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 15, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 20.0, log=True),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 50, 100),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0, log=True),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.01, 1.0, log=True),\n",
    "                'path_smooth': trial.suggest_float('path_smooth', 0.0, 1.0),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.8),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.8),\n",
    "                'bagging_freq': 1,\n",
    "                'random_state': 42,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='binary_logloss',\n",
    "                callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "        best_params['verbose'] = -1\n",
    "        best_params['force_col_wise'] = True\n",
    "        best_params['bagging_freq'] = 1\n",
    "\n",
    "        final_model = LGBMClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[LightGBM] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return final_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def xgboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.8),\n",
    "                'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 0.8),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 2.0, 20.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 10, 30),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 2.0, log=True),\n",
    "                'max_delta_step': trial.suggest_float('max_delta_step', 0, 3),\n",
    "                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.8, 1.5),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'tree_method': 'hist',\n",
    "                'eval_metric': 'logloss'\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=8),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "        best_params['n_jobs'] = -1\n",
    "        best_params['tree_method'] = 'hist'\n",
    "        best_params['eval_metric'] = 'logloss'\n",
    "\n",
    "        final_model = XGBClassifier(**best_params)\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        train_pred = final_model.predict(X_train)\n",
    "        val_pred = final_model.predict(X_val)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"[XGBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "        return final_model\n",
    "\n",
    "    @staticmethod\n",
    "    def histgradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 25, 70),\n",
    "                'l2_regularization': trial.suggest_float('l2_regularization', 1.0, 20.0, log=True),\n",
    "                'max_bins': trial.suggest_int('max_bins', 128, 255),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 40),\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 20,\n",
    "                'validation_fraction': 0.1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = HistGradientBoostingClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = model.score(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            \n",
    "            gap_penalty = max(0, (train_acc - val_acc) - 0.03)\n",
    "            return val_acc - 1.0 * gap_penalty\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "        \n",
    "        best_model = HistGradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[HistGradientBoosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    @staticmethod\n",
    "    def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'C': trial.suggest_float('C', 0.01, 5.0, log=True),\n",
    "                'penalty': 'l2',\n",
    "                'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "                'max_iter': 3000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            model = LogisticRegression(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_model = LogisticRegression(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Logistic Regression] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def adaboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 30, 100),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5),\n",
    "                'algorithm': 'SAMME',\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            base_max_depth = trial.suggest_int('base_max_depth', 1, 3)\n",
    "            base_min_samples_split = trial.suggest_int('base_min_samples_split', 30, 60)\n",
    "            base_min_samples_leaf = trial.suggest_int('base_min_samples_leaf', 15, 30)\n",
    "            \n",
    "            base_estimator = DecisionTreeClassifier(\n",
    "                max_depth=base_max_depth,\n",
    "                min_samples_split=base_min_samples_split,\n",
    "                min_samples_leaf=base_min_samples_leaf,\n",
    "                max_features='sqrt',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            model = AdaBoostClassifier(estimator=base_estimator, **param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            max_depth=best_params['base_max_depth'],\n",
    "            min_samples_split=best_params['base_min_samples_split'],\n",
    "            min_samples_leaf=best_params['base_min_samples_leaf'],\n",
    "            max_features='sqrt',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model = AdaBoostClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            algorithm='SAMME',\n",
    "            random_state=42\n",
    "        )\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[AdaBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def catboost(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'depth': trial.suggest_int('depth', 2, 4),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 8.0, 20.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'rsm': trial.suggest_float('rsm', 0.4, 0.7),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 40, 80),\n",
    "                'random_seed': 42,\n",
    "                'verbose': False,\n",
    "                'early_stopping_rounds': 20\n",
    "            }\n",
    "            \n",
    "            model = CatBoostClassifier(**param)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_val, y_val),\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=10),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=15)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        model = CatBoostClassifier(**study.best_params, random_seed=42, verbose=False)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        train_acc = model.score(X_train, y_train)\n",
    "        val_acc = model.score(X_val, y_val)\n",
    "        print(f\"[CatBoost] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 80, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.4, 0.7),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 40, 80),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 40),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 80),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.02),\n",
    "                'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.02),\n",
    "                'validation_fraction': 0.15,\n",
    "                'n_iter_no_change': 15,\n",
    "                'tol': 0.001,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=12),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=6)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "        \n",
    "        best_model = GradientBoostingClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Gradient Boosting] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def stacking_ensemble(X_train, y_train, X_val, y_val):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        def objective(trial):\n",
    "            xgb_estimators = trial.suggest_int('xgb_estimators', 100, 200)\n",
    "            xgb_depth = trial.suggest_int('xgb_depth', 3, 5)\n",
    "            xgb_lr = trial.suggest_float('xgb_lr', 0.01, 0.05, log=True)\n",
    "            \n",
    "            lgbm_estimators = trial.suggest_int('lgbm_estimators', 100, 200)\n",
    "            lgbm_depth = trial.suggest_int('lgbm_depth', 3, 5)\n",
    "            lgbm_lr = trial.suggest_float('lgbm_lr', 0.01, 0.05, log=True)\n",
    "            \n",
    "            meta_C = trial.suggest_float('meta_C', 0.1, 2.0, log=True)\n",
    "            \n",
    "            base_learners = [\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=xgb_estimators,\n",
    "                    max_depth=xgb_depth,\n",
    "                    learning_rate=xgb_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=3.0,\n",
    "                    min_child_weight=10,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('lgbm', LGBMClassifier(\n",
    "                    n_estimators=lgbm_estimators,\n",
    "                    max_depth=lgbm_depth,\n",
    "                    learning_rate=lgbm_lr,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha=2.0,\n",
    "                    reg_lambda=2.0,\n",
    "                    min_child_samples=60,\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    force_col_wise=True\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            meta_learner = LogisticRegression(max_iter=3000, C=meta_C, random_state=42, penalty='l2')\n",
    "            \n",
    "            model = StackingClassifier(\n",
    "                estimators=base_learners,\n",
    "                final_estimator=meta_learner,\n",
    "                cv=7,\n",
    "                n_jobs=-1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            val_acc = model.score(X_val, y_val)\n",
    "            return val_acc\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=6),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=3)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        base_learners = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=best_params['xgb_estimators'],\n",
    "                max_depth=best_params['xgb_depth'],\n",
    "                learning_rate=best_params['xgb_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=3.0,\n",
    "                min_child_weight=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=best_params['lgbm_estimators'],\n",
    "                max_depth=best_params['lgbm_depth'],\n",
    "                learning_rate=best_params['lgbm_lr'],\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.6,\n",
    "                reg_alpha=2.0,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=60,\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "                force_col_wise=True\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        meta_learner = LogisticRegression(max_iter=3000, C=best_params['meta_C'], random_state=42, penalty='l2')\n",
    "        \n",
    "        best_model = StackingClassifier(\n",
    "            estimators=base_learners,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=7,\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = best_model.score(X_train, y_train)\n",
    "        val_acc = best_model.score(X_val, y_val)\n",
    "        print(f\"[Stacking] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Gap: {train_acc - val_acc:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def lstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 80, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 48, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[LSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bilstm(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 24, 64, step=16)\n",
    "            units2 = trial.suggest_int('units2', 12, 40, step=12)\n",
    "            dropout = trial.suggest_float('dropout', 0.4, 0.6)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.02, 0.2, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0), input_shape=input_shape),\n",
    "                BatchNormalization(),\n",
    "                Bidirectional(LSTM(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0)),\n",
    "                BatchNormalization(),\n",
    "                Dense(12, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=6, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0), input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Bidirectional(LSTM(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0)),\n",
    "            BatchNormalization(),\n",
    "            Dense(12, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[BiLSTM] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def gru(X_train, y_train, X_val, y_val, input_shape):\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        def objective(trial):\n",
    "            units1 = trial.suggest_int('units1', 32, 96, step=16)\n",
    "            units2 = trial.suggest_int('units2', 16, 56, step=16)\n",
    "            dropout = trial.suggest_float('dropout', 0.35, 0.55)\n",
    "            l2_reg = trial.suggest_float('l2_reg', 0.01, 0.15, log=True)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "\n",
    "            X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "            model = Sequential([\n",
    "                GRU(units1, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                GRU(units2, activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(l2_reg), recurrent_regularizer=l2(l2_reg * 0.5), dropout=dropout, recurrent_dropout=0.0),\n",
    "                BatchNormalization(),\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "            history = model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "            _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            return val_accuracy\n",
    "\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=3), pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "        study.optimize(objective, n_trials=8, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        X_aug = TimeSeriesAugmentation.jittering(X_train, sigma=0.015)\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(best_params['units1'], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            GRU(best_params['units2'], activation='tanh', recurrent_activation='sigmoid', kernel_regularizer=l2(best_params['l2_reg']), recurrent_regularizer=l2(best_params['l2_reg'] * 0.5), dropout=best_params['dropout'], recurrent_dropout=0.0),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(best_params['l2_reg'])),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'], clipnorm=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, mode='min', verbose=0)\n",
    "\n",
    "        model.fit(X_aug, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"[GRU] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4da699ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models (19개)\n",
    "ML_MODELS_CLASSIFICATION = [\n",
    "    {'index': 1, 'name': 'RandomForest', 'func': DirectionModels.random_forest, 'needs_val': True},\n",
    "    {'index': 2, 'name': 'LightGBM', 'func': DirectionModels.lightgbm, 'needs_val': True},\n",
    "    {'index': 3, 'name': 'XGBoost', 'func': DirectionModels.xgboost, 'needs_val': True},\n",
    "    {'index': 5, 'name': 'LogisticRegression', 'func': DirectionModels.logistic_regression, 'needs_val': True},\n",
    "    {'index': 8, 'name': 'AdaBoost', 'func': DirectionModels.adaboost, 'needs_val': True},\n",
    "    {'index': 9, 'name': 'CatBoost', 'func': DirectionModels.catboost, 'needs_val': True},\n",
    "    {'index': 13, 'name': 'GradientBoosting', 'func': DirectionModels.gradient_boosting, 'needs_val': True},\n",
    "    {'index': 14, 'name': 'HistGradientBoosting', 'func': DirectionModels.histgradient_boosting, 'needs_val': True},\n",
    "    {'index': 15, 'name': 'StackingEnsemble', 'func': DirectionModels.stacking_ensemble, 'needs_val': True},\n",
    "]\n",
    "\n",
    "# DL Models (11개)\n",
    "DL_MODELS_CLASSIFICATION = [\n",
    "    {'index': 19, 'name': 'LSTM', 'func': DirectionModels.lstm, 'needs_val': True},\n",
    "    {'index': 20, 'name': 'BiLSTM', 'func': DirectionModels.bilstm, 'needs_val': True},\n",
    "    {'index': 21, 'name': 'GRU', 'func': DirectionModels.gru, 'needs_val': True}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ada18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, save_models=False):\n",
    "        self.results = []\n",
    "        self.predictions = {}\n",
    "        self.models = {} if save_models else None\n",
    "        self.save_models = save_models\n",
    "    \n",
    "    def _predict_model(self, model, X):\n",
    "        pred = model.predict(X)\n",
    "        if isinstance(pred, list):\n",
    "            cleaned = []\n",
    "            for p in pred:\n",
    "                if isinstance(p, np.ndarray):\n",
    "                    cleaned.append(p.squeeze() if p.shape[-1] == 1 else p)\n",
    "                else:\n",
    "                    cleaned.append(p)\n",
    "            return cleaned\n",
    "        else:\n",
    "            return pred.squeeze() if pred.shape[-1] == 1 else pred\n",
    "\n",
    "    def evaluate_classification_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                                     X_test, y_test_df, test_dates, model_name,\n",
    "                                     is_deep_learning=False):\n",
    "        \n",
    "        train_pred = self._predict_model(model, X_train)\n",
    "        val_pred = self._predict_model(model, X_val)\n",
    "        test_pred = self._predict_model(model, X_test)\n",
    "        \n",
    "        test_pred_proba = None\n",
    "        if is_deep_learning:\n",
    "            test_pred_proba = test_pred.copy()\n",
    "            if isinstance(train_pred, list):\n",
    "                train_pred = train_pred[0]\n",
    "                val_pred = val_pred[0]\n",
    "                test_pred = test_pred[0]\n",
    "                test_pred_proba = test_pred_proba[0] if isinstance(test_pred_proba, list) else test_pred_proba\n",
    "            train_pred = (train_pred > 0.5).astype(int).ravel()\n",
    "            val_pred = (val_pred > 0.5).astype(int).ravel()\n",
    "            test_pred = (test_pred > 0.5).astype(int).ravel()\n",
    "        else:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        test_acc = accuracy_score(y_test_direction, test_pred)\n",
    "        test_prec = precision_score(y_test_direction, test_pred, zero_division=0)\n",
    "        test_rec = recall_score(y_test_direction, test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test_direction, test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test_direction, test_pred)\n",
    "        \n",
    "        self._save_predictions(model_name, test_pred, test_pred_proba, y_test_df, test_dates)\n",
    "        \n",
    "        if self.save_models and self.models is not None:\n",
    "            self.models[model_name] = model\n",
    "        \n",
    "        self.results.append({\n",
    "            'Model': model_name, 'Train_Accuracy': train_acc, 'Val_Accuracy': val_acc,\n",
    "            'Test_Accuracy': test_acc, 'Test_Precision': test_prec, 'Test_Recall': test_rec,\n",
    "            'Test_F1': test_f1, 'Test_AUC_ROC': test_roc_auc\n",
    "        })\n",
    "        \n",
    "        del train_pred, val_pred, test_pred, test_pred_proba, y_test_direction\n",
    "        gc.collect()\n",
    "        \n",
    "        return self.results[-1]\n",
    "    \n",
    "    def _save_predictions(self, model_name, pred_direction, pred_proba, y_test_df, dates):\n",
    "        if pred_proba is not None:\n",
    "            if pred_proba.ndim == 2 and pred_proba.shape[1] == 2:\n",
    "                pred_proba_up = pred_proba[:, 1]\n",
    "                pred_proba_down = pred_proba[:, 0]\n",
    "            else:\n",
    "                pred_proba_up = pred_proba.ravel()\n",
    "                pred_proba_down = 1 - pred_proba_up\n",
    "        else:\n",
    "            pred_proba_up = np.where(pred_direction == 1, 0.9, 0.1)\n",
    "            pred_proba_down = 1 - pred_proba_up\n",
    "        \n",
    "        max_proba = np.maximum(pred_proba_up, pred_proba_down)\n",
    "        confidence = np.abs(pred_proba_up - 0.5) * 2\n",
    "        \n",
    "        y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'actual_direction': y_test_direction,\n",
    "            'actual_return': y_test_df['next_log_return'].values,\n",
    "            'take_profit_price': y_test_df['take_profit_price'].values,\n",
    "            'stop_loss_price': y_test_df['stop_loss_price'].values,\n",
    "            'pred_direction': pred_direction,\n",
    "            'pred_proba_up': pred_proba_up,\n",
    "            'pred_proba_down': pred_proba_down,\n",
    "            'max_proba': max_proba,\n",
    "            'confidence': confidence,\n",
    "            'correct': (pred_direction == y_test_direction).astype(int)\n",
    "        })\n",
    "        \n",
    "        self.predictions[model_name] = predictions_df\n",
    "    \n",
    "    def get_summary_dataframe(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_predictions_dict(self):\n",
    "        return self.predictions\n",
    "    \n",
    "    def get_models_dict(self):\n",
    "        return self.models if self.models is not None else {}\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, evaluator, lookback=30):\n",
    "        self.evaluator = evaluator\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sequences(X, y, lookback):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(lookback, len(X)):\n",
    "            Xs.append(X[i-lookback:i])\n",
    "            ys.append(y.iloc[i] if hasattr(y, 'iloc') else y[i])\n",
    "        X_arr = np.array(Xs)\n",
    "        y_arr = np.array(ys)\n",
    "        del Xs, ys\n",
    "        gc.collect()\n",
    "        return X_arr, y_arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        keras.backend.clear_session()\n",
    "        try:\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def train_ml_model(self, model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            if model_config.get('needs_val', False):\n",
    "                model = model_config['func'](X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                model = model_config['func'](X_train, y_train)\n",
    "            \n",
    "            is_mlp = (model_config['name'] == 'MLP')\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train, y_train, X_val, y_val, X_test, y_test_df,\n",
    "                test_dates, model_config['name'], is_deep_learning=is_mlp\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                if is_mlp:\n",
    "                    self.clear_memory()\n",
    "                else:\n",
    "                    gc.collect()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            if model_config.get('name') == 'MLP':\n",
    "                self.clear_memory()\n",
    "            else:\n",
    "                gc.collect()\n",
    "    \n",
    "    def train_dl_model(self, model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape, task='classification'):\n",
    "        model = None\n",
    "        try:\n",
    "            self.clear_memory()\n",
    "            \n",
    "            model = model_config['func'](X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_shape)\n",
    "            \n",
    "            self.evaluator.evaluate_classification_model(\n",
    "                model, X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                X_test_seq, y_test_df_seq, test_dates_seq,\n",
    "                model_config['name'], is_deep_learning=True\n",
    "            )\n",
    "            \n",
    "            if not self.evaluator.save_models:\n",
    "                del model\n",
    "                model = None\n",
    "                self.clear_memory()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"    {model_config['name']} failed: {type(e).__name__}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if model is not None and not self.evaluator.save_models:\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates, evaluator, lookback=30, ml_models=None, dl_models=None):\n",
    "    trainer = ModelTrainer(evaluator, lookback)\n",
    "\n",
    "    ml_success = 0\n",
    "    for model_config in ml_models:\n",
    "        if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "            ml_success += 1\n",
    "        gc.collect()\n",
    "    \n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    y_test_direction = y_test_df['next_direction'].values\n",
    "\n",
    "    X_train_seq, y_train_seq = trainer.create_sequences(X_train, y_train, lookback)\n",
    "    X_val_seq, y_val_seq = trainer.create_sequences(X_val, y_val, lookback)\n",
    "    X_test_seq, y_test_seq = trainer.create_sequences(X_test, y_test_direction, lookback)\n",
    "    \n",
    "    test_dates_seq = test_dates[lookback:]\n",
    "    y_test_df_seq = y_test_df.iloc[lookback:].reset_index(drop=True)\n",
    "    \n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    dl_success = 0\n",
    "    for model_config in dl_models:\n",
    "        trainer.clear_memory()\n",
    "        \n",
    "        if model_config['name'] in ['TabNet', 'StackingEnsemble', 'VotingHard', 'VotingSoft']:\n",
    "            if trainer.train_ml_model(model_config, X_train, y_train, X_val, y_val, X_test, y_test_df, test_dates):\n",
    "                dl_success += 1\n",
    "        else:\n",
    "            if trainer.train_dl_model(model_config, X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_df_seq, test_dates_seq, input_shape):\n",
    "                dl_success += 1\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    del X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, y_test_df_seq, test_dates_seq\n",
    "    trainer.clear_memory()\n",
    "    \n",
    "    return ml_success + dl_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18428a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_raw_data_once(result, target_name, split_method):\n",
    "    raw_dir = os.path.join(RESULT_DIR, \"raw_data\", target_name, split_method)\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    for fold_idx, fold in enumerate(result, start=1):\n",
    "        fold_type = fold['stats']['fold_type']\n",
    "        fold_dir = os.path.join(raw_dir, f\"fold_{fold_idx}_{fold_type}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            df = pd.DataFrame(fold[split]['X_raw'], columns=fold['stats']['selected_features'])\n",
    "            df['date'] = fold[split]['dates']\n",
    "            for col in fold[split]['y'].columns:\n",
    "                df[col] = fold[split]['y'][col].values\n",
    "            df.to_csv(os.path.join(fold_dir, f\"{split}_raw.csv\"), index=False, encoding='utf-8-sig')\n",
    "            del fold[split]['X_raw']\n",
    "    gc.collect()\n",
    "\n",
    "def check_fold_completed(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    return os.path.exists(os.path.join(fold_dir, \"fold_summary.csv\"))\n",
    "\n",
    "def save_fold_results(fold_idx, fold_type, evaluator, target_name, fold_data):\n",
    "\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    # 기존 모델 저장\n",
    "    for model_name, model_obj in evaluator.get_models_dict().items():\n",
    "        is_dl = isinstance(model_obj, tf.keras.Model)\n",
    "        ext = \".h5\" if is_dl else \".pkl\"\n",
    "        path = os.path.join(fold_dir, f\"{model_name}{ext}\")\n",
    "        if is_dl: \n",
    "            model_obj.save(path)\n",
    "        else:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(model_obj, f)\n",
    "    \n",
    "\n",
    "    with open(os.path.join(fold_dir, \"robust_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['robust_scaler'], f)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, \"standard_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['standard_scaler'], f)\n",
    "    \n",
    "\n",
    "    with open(os.path.join(fold_dir, \"selected_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['selected_features'], f)\n",
    "    \n",
    "\n",
    "    with open(os.path.join(fold_dir, \"missing_stats.pkl\"), 'wb') as f:\n",
    "        pickle.dump(fold_data['stats']['missing_stats'], f)\n",
    "    \n",
    "\n",
    "    inference_config = {\n",
    "        'selected_features': fold_data['stats']['selected_features'],\n",
    "        'feature_order': fold_data['stats']['selected_features'], \n",
    "        'target_cols': fold_data['stats']['target_cols'],\n",
    "        'target_type': fold_data['stats']['target_type'],\n",
    "        'fold_type': fold_type,\n",
    "        'fold_idx': fold_idx\n",
    "    }\n",
    "    with open(os.path.join(fold_dir, \"inference_config.pkl\"), 'wb') as f:\n",
    "        pickle.dump(inference_config, f)\n",
    "    \n",
    "\n",
    "    for model_name, pred_df in evaluator.get_predictions_dict().items():\n",
    "        pred_df.to_csv(os.path.join(fold_dir, f\"{model_name}_predictions.csv\"), \n",
    "                      index=False, encoding='utf-8-sig')\n",
    "        \n",
    "    summary = evaluator.get_summary_dataframe()\n",
    "    summary.to_csv(os.path.join(fold_dir, \"fold_summary.csv\"), \n",
    "                   index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✅ Fold {fold_idx} saved with scalers and metadata to: {fold_dir}\")\n",
    "    \n",
    "    return summary, evaluator.get_predictions_dict()\n",
    "def load_fold_results(target_name, fold_idx, fold_type):\n",
    "    fold_dir = os.path.join(RESULT_DIR, \"fold_results\", target_name, f\"fold_{fold_idx}_{fold_type}\")\n",
    "    summary_path = os.path.join(fold_dir, \"fold_summary.csv\")\n",
    "    if not os.path.exists(summary_path): return None, None\n",
    "    summary = pd.read_csv(summary_path)\n",
    "    predictions = {f.replace('_predictions.csv', ''): pd.read_csv(os.path.join(fold_dir, f)) for f in os.listdir(fold_dir) if f.endswith('_predictions.csv')}\n",
    "    return summary, predictions\n",
    "\n",
    "def save_walk_forward_summary(all_fold_results, target_name):\n",
    "    if not all_fold_results: return\n",
    "    detailed_df = pd.concat([df.assign(Fold=i+1, fold_type=ft) for i, (df, ft) in enumerate(all_fold_results)], ignore_index=True)\n",
    "    detailed_df.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_all_folds_detailed.csv\"), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    wf_data = detailed_df[detailed_df['fold_type'] == 'walk_forward'].copy()\n",
    "    if wf_data.empty: return\n",
    "\n",
    "    numeric_cols = wf_data.select_dtypes(include=np.number).columns.drop('Fold', errors='ignore')\n",
    "    avg_results = wf_data.groupby('Model')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "    avg_results.columns = ['_'.join(col).strip() if col[1] else col[0] for col in avg_results.columns.values]\n",
    "    if 'Test_Accuracy_mean' in avg_results.columns:\n",
    "        avg_results = avg_results.sort_values(by='Test_Accuracy_mean', ascending=False).reset_index(drop=True)\n",
    "    avg_results.to_csv(os.path.join(RESULT_DIR, f\"{target_name}_walk_forward_average.csv\"), index=False, encoding='utf-8-sig')\n",
    "\n",
    "def run_and_save_master_summary(result_dir):\n",
    "    summary_files = glob.glob(os.path.join(result_dir, \"*_walk_forward_average.csv\"))\n",
    "    if not summary_files: return\n",
    "\n",
    "    master_list = []\n",
    "    for f in summary_files:\n",
    "        filename = os.path.basename(f)\n",
    "        parts = filename.replace('direction_', '').replace('_walk_forward_average.csv', '').split('_')\n",
    "        params = {p[0]: float(p[1:]) for p in parts}\n",
    "        df = pd.read_csv(f)\n",
    "        df['lookahead'] = params.get('l')\n",
    "        df['profit_mult'] = params.get('p')\n",
    "        df['stop_mult'] = params.get('s')\n",
    "        master_list.append(df)\n",
    "        \n",
    "    master_df = pd.concat(master_list, ignore_index=True)\n",
    "    if 'Test_Accuracy_mean' in master_df.columns:\n",
    "        master_df = master_df.sort_values(by='Test_Accuracy_mean', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    master_df.to_csv(os.path.join(result_dir, \"_MASTER_SUMMARY_RESULTS.csv\"), index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "def check_experiment_completed(target_name, result_dir):\n",
    "    \"\"\"\n",
    "    특정 파라미터 조합에 대한 최종 요약 파일이 존재하는지 확인하여\n",
    "    실험 전체가 완료되었는지 판단.\n",
    "    \"\"\"\n",
    "    summary_path = os.path.join(result_dir, f\"{target_name}_walk_forward_average.csv\")\n",
    "    return os.path.exists(summary_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b43f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Reverse Rolling Walk-Forward Configuration \n",
      "================================================================================\n",
      "Total: 2133 days\n",
      "Rolling train size: 800 days (FIXED)\n",
      "Val: 150 days | Test: 150 days\n",
      "Gap: 7 days | Step: 150 days (BACKWARD)\n",
      "Target: 7 walk-forward + 1 final holdout\n",
      "================================================================================\n",
      "\n",
      "Fold 1 (walk_forward_rolling)\n",
      "  Train:  800d  2020-04-29 ~ 2022-07-07\n",
      "  Val:    150d  2022-07-15 ~ 2022-12-11\n",
      "  Test:   150d  2022-12-19 ~ 2023-05-17\n",
      "\n",
      "Fold 2 (walk_forward_rolling)\n",
      "  Train:  800d  2020-09-26 ~ 2022-12-04\n",
      "  Val:    150d  2022-12-12 ~ 2023-05-10\n",
      "  Test:   150d  2023-05-18 ~ 2023-10-14\n",
      "\n",
      "Fold 3 (walk_forward_rolling)\n",
      "  Train:  800d  2021-02-23 ~ 2023-05-03\n",
      "  Val:    150d  2023-05-11 ~ 2023-10-07\n",
      "  Test:   150d  2023-10-15 ~ 2024-03-12\n",
      "\n",
      "Fold 4 (walk_forward_rolling)\n",
      "  Train:  800d  2021-07-23 ~ 2023-09-30\n",
      "  Val:    150d  2023-10-08 ~ 2024-03-05\n",
      "  Test:   150d  2024-03-13 ~ 2024-08-09\n",
      "\n",
      "Fold 5 (walk_forward_rolling)\n",
      "  Train:  800d  2021-12-20 ~ 2024-02-27\n",
      "  Val:    150d  2024-03-06 ~ 2024-08-02\n",
      "  Test:   150d  2024-08-10 ~ 2025-01-06\n",
      "\n",
      "Fold 6 (walk_forward_rolling)\n",
      "  Train:  800d  2022-05-19 ~ 2024-07-26\n",
      "  Val:    150d  2024-08-03 ~ 2024-12-30\n",
      "  Test:   150d  2025-01-07 ~ 2025-06-05\n",
      "\n",
      "Fold 7 (walk_forward_rolling)\n",
      "  Train:  800d  2022-10-16 ~ 2024-12-23\n",
      "  Val:    150d  2024-12-31 ~ 2025-05-29\n",
      "  Test:   150d  2025-06-06 ~ 2025-11-02\n",
      "\n",
      "Fold 8 (final_holdout)\n",
      "  Train:  800d  2022-05-20 ~ 2024-07-27\n",
      "  Val:    150d  2024-08-04 ~ 2024-12-31\n",
      "  Test:   306d  2025-01-01 ~ 2025-11-02\n",
      "\n",
      "================================================================================\n",
      "Created 8 folds total\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 1]\n",
      "Training data shape: (509, 290)\n",
      "ATR_14, DPO_20, VWAP, SMA_50, eth_btc_corr_highvol, ISA_9, VWMA_20, BBU_20, ada_volatility_30d, EMA_26, xrp_volatility_30d, RV_20, MACD_12_26_9, eth_btc_corr_60d, close_lag30, MACDH_12_26_9, eth_btc_beta_60d, vol_trend, eth_btc_volcorr_sq_14d_pct_5d, makerdao_makerdao_eth_tvl_ma30_ratio_lag1, ROLLING_MAX_20, ROLLING_MIN_20, KCU_20, IKS_26, ISB_26, KCB_20, ITS_9, SMA_20, KCL_20, BBL_20\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 1 (Post-dropna) ---\n",
      "  Train (Actual):  509 (2020-04-29 ~ 2022-07-07)\n",
      "  Val   (Actual):   76 (2022-07-15 ~ 2022-12-11)\n",
      "  Test  (Actual):   95 (2022-12-23 ~ 2023-05-10)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 2]\n",
      "Training data shape: (494, 290)\n",
      "ATR_14, DPO_20, SMA_50, ISA_9, VWMA_20, BBU_20, xrp_volatility_30d, MACD_12_26_9, doge_volatility_30d, VOLUME_SMA_20, btc_volatility_30d, ADX_14, DMP_14, btc_eth_strength_ratio_7d, low_lag7_ratio, eth_btc_volcorr_sq_7d_ma30_ratio, gold_GOLD_pct_5d_lag1, gold_GOLD_ma30_ratio_lag1, btc_volatility_7d, VWAP, ROLLING_MIN_20, eth_btc_corr_highvol, KCU_20, ROLLING_MAX_20, ISB_26, ada_volatility_30d, IKS_26, KCB_20, SMA_20, ITS_9\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 2 (Post-dropna) ---\n",
      "  Train (Actual):  494 (2020-09-29 ~ 2022-12-04)\n",
      "  Val   (Actual):   99 (2022-12-12 ~ 2023-05-10)\n",
      "  Test  (Actual):   92 (2023-05-20 ~ 2023-10-12)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 3]\n",
      "Training data shape: (491, 290)\n",
      "ATR_14, DPO_20, xrp_volatility_30d, KCU_20, VWAP, VOLUME_SMA_20, SMA_20, btc_volatility_30d, KCB_20, BBU_20, EMA_26, btc_volume_volatility_30d, close_lag14, CCI_14, ADX_14, DMP_14, BB_POSITION, vol_trend, RSI_percentile_60d, eth_btc_corr_highvol, ROLLING_MIN_20, ISA_9, IKS_26, SMA_50, ROLLING_MAX_20, ISB_26, ITS_9, eth_btc_beta_90d, doge_volatility_30d, ada_volatility_30d\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 3 (Post-dropna) ---\n",
      "  Train (Actual):  491 (2021-02-23 ~ 2023-05-03)\n",
      "  Val   (Actual):   89 (2023-05-20 ~ 2023-10-07)\n",
      "  Test  (Actual):   94 (2023-10-15 ~ 2024-03-12)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 4]\n",
      "Training data shape: (481, 290)\n",
      "ATR_14, DPO_20, ada_volatility_30d, xrp_volatility_30d, btc_volatility_7d, VWAP, ROLLING_MIN_20, SMA_50, ISB_26, IKS_26, KCL_20, EMA_26, btc_volatility_30d, KCU_20, BBU_20, SMA_20, eth_btc_volume_ratio_ma30, l2_arbitrum_tvl_ma30_ratio_lag1, ICS_26, KCB_20, doge_volatility_30d, eth_btc_corr_highvol, ROLLING_MAX_20, ITS_9, ISA_9, eth_btc_beta_90d, VOLUME_SMA_20, BBL_20, VWMA_20, high_lag1\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 4\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 4 (Post-dropna) ---\n",
      "  Train (Actual):  481 (2021-07-23 ~ 2023-09-30)\n",
      "  Val   (Actual):   94 (2023-10-08 ~ 2024-03-05)\n",
      "  Test  (Actual):   88 (2024-03-13 ~ 2024-08-04)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 5 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 5]\n",
      "Training data shape: (485, 290)\n",
      "ATR_14, DPO_20, ada_volatility_30d, btc_volatility_7d, VWAP, SMA_50, EMA_26, ISA_9, eth_btc_corr_highvol, KCB_20, btc_volatility_30d, VOLUME_SMA_20, VOLATILITY_20, BBU_20, btc_volume_volatility_30d, eth_btc_corr_lowvol, EFI_13, eth_btc_volcorr_7d_ma30_ratio, doge_volatility_30d, ROLLING_MIN_20, IKS_26, ISB_26, KCL_20, SMA_20, ITS_9, BBM_20, ROLLING_MAX_20, BBL_20, KCU_20, btc_volatility_14d\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 5 (Post-dropna) ---\n",
      "  Train (Actual):  485 (2021-12-23 ~ 2024-02-27)\n",
      "  Val   (Actual):   89 (2024-03-10 ~ 2024-08-02)\n",
      "  Test  (Actual):   83 (2024-08-22 ~ 2025-01-06)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 6 (walk_forward_rolling_reverse)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 6]\n",
      "Training data shape: (474, 290)\n",
      "ATR_14, DPO_20, ada_volatility_30d, xrp_volatility_30d, EMA_26, SMA_20, VWMA_20, btc_volatility_30d, KCU_20, btc_volume_volatility_30d, eth_btc_corr_3d, high_lag2_ratio, gold_GOLD_pct_5d_lag1, btc_volatility_7d, dot_volatility_30d, VWAP, IKS_26, ROLLING_MIN_20, ISB_26, KCL_20, ITS_9, BBM_20, SMA_50, ISA_9, BBL_20, ROLLING_MAX_20, KCB_20, eth_btc_corr_highvol, close_lag7, eth_btc_volume_ratio_ma30\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 6\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 6 (Post-dropna) ---\n",
      "  Train (Actual):  474 (2022-05-21 ~ 2024-07-24)\n",
      "  Val   (Actual):   80 (2024-08-03 ~ 2024-12-25)\n",
      "  Test  (Actual):   82 (2025-01-07 ~ 2025-06-05)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 7 (walk_forward_rolling_reverse)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Feature Selection for Fold 7]\n",
      "Training data shape: (478, 290)\n",
      "ATR_14, DPO_20, low_lag5, BBM_20, ROLLING_MIN_20, VWMA_20, BBL_20, eth_btc_corr_highvol, close_lag7, high_lag7, xrp_volatility_30d, close_lag5, high_lag2_ratio, gold_GOLD_ma30_ratio_lag1, btc_volatility_7d, btc_volatility_14d, avax_volatility_30d, VWAP, IKS_26, SMA_50, EMA_26, ISB_26, ROLLING_MAX_20, ITS_9, ISA_9, SMA_20, WMA_20, KCL_20, KCB_20, EMA_12\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 7\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 7 (Post-dropna) ---\n",
      "  Train (Actual):  478 (2022-10-17 ~ 2024-12-20)\n",
      "  Val   (Actual):   81 (2025-01-01 ~ 2025-05-29)\n",
      "  Test  (Actual):  104 (2025-06-06 ~ 2025-11-02)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Fold 8 (final_holdout)\n",
      "============================================================\n",
      "\n",
      "[Feature Selection for Fold 8]\n",
      "Training data shape: (474, 290)\n",
      "ATR_14, DPO_20, ada_volatility_30d, xrp_volatility_30d, EMA_26, SMA_20, VWMA_20, btc_volatility_30d, KCU_20, btc_volume_volatility_30d, eth_btc_corr_3d, high_lag2_ratio, gold_GOLD_pct_5d_lag1, btc_volatility_7d, dot_volatility_30d, VWAP, IKS_26, ROLLING_MIN_20, ISB_26, KCL_20, ITS_9, BBM_20, SMA_50, ISA_9, BBL_20, ROLLING_MAX_20, KCB_20, eth_btc_corr_highvol, close_lag7, eth_btc_volume_ratio_ma30\n",
      "Selected 30 features for this fold\n",
      "Scaling completed for Fold 8\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- [Verification] Actual Date Ranges for Fold 8 (Post-dropna) ---\n",
      "  Train (Actual):  474 (2022-05-21 ~ 2024-07-24)\n",
      "  Val   (Actual):   79 (2024-08-04 ~ 2024-12-25)\n",
      "  Test  (Actual):  191 (2025-01-01 ~ 2025-11-02)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Processing Fold 1 (walk_forward_rolling_reverse)\n",
      "[Random Forest] Train Acc: 0.8723 | Val Acc: 0.8816 | Gap: -0.0093\n",
      "[LightGBM] Train Acc: 0.8743 | Val Acc: 0.9079 | Gap: -0.0336\n",
      "[XGBoost] Train Acc: 0.9037 | Val Acc: 0.8816 | Gap: 0.0222\n",
      "[Logistic Regression] Train Acc: 0.8468 | Val Acc: 0.8947 | Gap: -0.0480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RESULT_DIR = os.path.join(\"model_results\", timestamp)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- 하이퍼파라미터 그리드 ---\n",
    "lookahead_options = [5, 10, 21]\n",
    "atr_profit_multipliers = [2.0, 3.0]\n",
    "atr_stop_multipliers = [1.0, 1.5]\n",
    "\n",
    "target_cases = [{'name': 'direction', 'target_type': 'direction', 'outputs': ['next_direction']}]\n",
    "split_methods = [{'name': 'walk_forward', 'method': 'walk_forward'}]\n",
    "\n",
    "# --- 실험 루프 ---\n",
    "for lookahead in lookahead_options:\n",
    "    for p_mult in atr_profit_multipliers:\n",
    "        for s_mult in atr_stop_multipliers:\n",
    "            \n",
    "            param_suffix = f\"l{lookahead}_p{p_mult}_s{s_mult}\"\n",
    "            current_target_name = f\"{target_cases[0]['name']}_{param_suffix}\"\n",
    "\n",
    "            if check_experiment_completed(current_target_name, RESULT_DIR):\n",
    "                print(f\"Skipping already completed experiment: {current_target_name}\")\n",
    "                continue\n",
    "            \n",
    "            for split_method in split_methods:\n",
    "                pipeline_result = build_complete_pipeline_corrected(\n",
    "                    df_merged, TRAIN_START_DATE,\n",
    "                    method=split_method['method'],\n",
    "                    target_type=target_cases[0]['target_type'],\n",
    "                    final_test_start='2025-01-01',\n",
    "                    lookahead_candles=lookahead,\n",
    "                    atr_multiplier_profit=p_mult,\n",
    "                    atr_multiplier_stop=s_mult\n",
    "                )\n",
    "                \n",
    "                save_raw_data_once(pipeline_result, current_target_name, split_method['method'])\n",
    "                \n",
    "                fold_results = []\n",
    "                for fold_idx, fold_data in enumerate(pipeline_result, start=1):\n",
    "                    fold_type = fold_data['stats']['fold_type']\n",
    "                    \n",
    "                    if check_fold_completed(current_target_name, fold_idx, fold_type):\n",
    "                        print(f\"Loading completed fold: Fold {fold_idx}\")\n",
    "                        fold_summary, _ = load_fold_results(current_target_name, fold_idx, fold_type)\n",
    "                        if fold_summary is not None:\n",
    "                            fold_results.append((fold_summary, fold_type))\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Processing Fold {fold_idx} ({fold_type})\")\n",
    "                    evaluator = ModelEvaluator(save_models=True)\n",
    "                    try:\n",
    "                        train_all_models(\n",
    "                            fold_data['train']['X_robust'], \n",
    "                            fold_data['train']['y'][target_cases[0]['outputs'][0]].values,\n",
    "                            fold_data['val']['X_robust'], \n",
    "                            fold_data['val']['y'][target_cases[0]['outputs'][0]].values,\n",
    "                            fold_data['test']['X_robust'], \n",
    "                            fold_data['test']['y'], \n",
    "                            fold_data['test']['dates'].values,\n",
    "                            evaluator, \n",
    "                            ml_models=ML_MODELS_CLASSIFICATION, \n",
    "                            dl_models=DL_MODELS_CLASSIFICATION\n",
    "                        )\n",
    "                        fold_summary, _ = save_fold_results(\n",
    "                            fold_idx, fold_type, evaluator, \n",
    "                            current_target_name, fold_data\n",
    "                        )\n",
    "                        fold_results.append((fold_summary, fold_type))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fold failed: {e}\")\n",
    "                    finally:\n",
    "                        del evaluator\n",
    "                        tf.keras.backend.clear_session()\n",
    "                        gc.collect()\n",
    "\n",
    "                save_walk_forward_summary(fold_results, current_target_name)\n",
    "\n",
    "# --- 최종 결과 취합 ---\n",
    "run_and_save_master_summary(RESULT_DIR)\n",
    "print(f\"All experiments are complete. Results are in: {RESULT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
