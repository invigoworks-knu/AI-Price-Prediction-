{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08742bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro_df 마지막 날짜: 2025-11-21\n",
      "sp500_df 마지막 날짜: 2025-11-20\n",
      "vix_df 마지막 날짜: 2025-11-21\n",
      "gold_df 마지막 날짜: 2025-11-21\n",
      "dxy_df 마지막 날짜: 2025-11-21\n",
      "fear_greed_df 마지막 날짜: 2025-11-21\n",
      "eth_funding_df 마지막 날짜: 2025-11-21\n",
      "usdt_eth_mcap_df 마지막 날짜: 2025-11-21\n",
      "aave_tvl_df 마지막 날짜: 2025-11-21\n",
      "lido_tvl_df 마지막 날짜: 2025-11-21\n",
      "makerdao_tvl_df 마지막 날짜: 2025-11-21\n",
      "uniswap_tvl_df 마지막 날짜: 2025-11-21\n",
      "curve_tvl_df 마지막 날짜: 2025-11-21\n",
      "eth_chain_tvl_df 마지막 날짜: 2025-11-21\n",
      "layer2_tvl_df 마지막 날짜: 2025-11-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import traceback\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# --- Data Handling and Science ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from collections import Counter\n",
    "from numba import jit\n",
    "\n",
    "# --- Persistence and Serialization ---\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# --- Date and Time ---\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Machine Learning Optimization ---\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# --- Environment Configuration ---\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "DATA_DIR_MAIN = './macro_data'\n",
    "DATA_DIR_NEW = './macro_data/macro_data/today/'\n",
    "\n",
    "today = datetime.today()\n",
    "TRAIN_START_DATE = (today + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "LOOKBACK_DAYS = 200\n",
    "LOOKBACK_START_DATE = (today - timedelta(days=LOOKBACK_DAYS)).strftime('%Y-%m-%d')\n",
    "\n",
    "def standardize_date_column(df, file_name):\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if not date_cols:\n",
    "        return df\n",
    "    date_col = date_cols[0]\n",
    "    if date_col != 'date':\n",
    "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
    "    if file_name == 'eth_onchain.csv':\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', infer_datetime_format=True)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = df['date'].dt.normalize()\n",
    "    if pd.api.types.is_datetime64tz_dtype(df['date']):\n",
    "        df['date'] = df['date'].dt.tz_convert(None)\n",
    "    else:\n",
    "        df['date'] = df['date'].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def load_csv(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    return standardize_date_column(df, filename)\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [f\"{prefix}_{col}\" if col != 'date' else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def smart_fill_missing(df_merged):\n",
    "    for col in df_merged.columns:\n",
    "        if col == 'date' or df_merged[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        non_null_idx = df_merged[col].first_valid_index()\n",
    "        if non_null_idx is None:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "            continue\n",
    "        first_date = df_merged.loc[non_null_idx, 'date']\n",
    "        df_merged.loc[df_merged['date'] < first_date, col] = df_merged.loc[df_merged['date'] < first_date, col].fillna(0)\n",
    "        df_merged.loc[df_merged['date'] >= first_date, col] = df_merged.loc[df_merged['date'] >= first_date, col].fillna(method='ffill')\n",
    "        df_merged.loc[df_merged['date'] >= first_date, col] = df_merged.loc[df_merged['date'] >= first_date, col].fillna(0)\n",
    "    return df_merged\n",
    "\n",
    "macro_df = load_csv(DATA_DIR_NEW, 'macro_crypto_data.csv')\n",
    "sp500_df = load_csv(DATA_DIR_NEW, 'SP500.csv')\n",
    "vix_df = load_csv(DATA_DIR_NEW, 'VIX.csv')\n",
    "gold_df = load_csv(DATA_DIR_NEW, 'GOLD.csv')\n",
    "dxy_df = load_csv(DATA_DIR_NEW, 'DXY.csv')\n",
    "fear_greed_df = load_csv(DATA_DIR_NEW, 'fear_greed.csv')\n",
    "eth_funding_df = load_csv(DATA_DIR_NEW, 'eth_funding_rate.csv')\n",
    "usdt_eth_mcap_df = load_csv(DATA_DIR_NEW, 'usdt_eth_mcap.csv')\n",
    "aave_tvl_df = load_csv(DATA_DIR_NEW, 'aave_eth_tvl.csv')\n",
    "lido_tvl_df = load_csv(DATA_DIR_NEW, 'lido_eth_tvl.csv')\n",
    "makerdao_tvl_df = load_csv(DATA_DIR_NEW, 'makerdao_eth_tvl.csv')\n",
    "uniswap_tvl_df = load_csv(DATA_DIR_NEW, 'uniswap_eth_tvl.csv')\n",
    "curve_tvl_df = load_csv(DATA_DIR_NEW, 'curve-dex_eth_tvl.csv')\n",
    "eth_chain_tvl_df = load_csv(DATA_DIR_NEW, 'eth_chain_tvl.csv')\n",
    "layer2_tvl_df = load_csv(DATA_DIR_NEW, 'layer2_tvl.csv')\n",
    "\n",
    "dataframes = {\n",
    "    'macro_df': macro_df,\n",
    "    'sp500_df': sp500_df,\n",
    "    'vix_df': vix_df,\n",
    "    'gold_df': gold_df,\n",
    "    'dxy_df': dxy_df,\n",
    "    'fear_greed_df': fear_greed_df,\n",
    "    'eth_funding_df': eth_funding_df,\n",
    "    'usdt_eth_mcap_df': usdt_eth_mcap_df,\n",
    "    'aave_tvl_df': aave_tvl_df,\n",
    "    'lido_tvl_df': lido_tvl_df,\n",
    "    'makerdao_tvl_df': makerdao_tvl_df,\n",
    "    'uniswap_tvl_df': uniswap_tvl_df,\n",
    "    'curve_tvl_df': curve_tvl_df,\n",
    "    'eth_chain_tvl_df': eth_chain_tvl_df,\n",
    "    'layer2_tvl_df': layer2_tvl_df\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    if not df.empty and 'date' in df.columns:\n",
    "        last_date = pd.to_datetime(df['date']).max()\n",
    "        print(f\"{name} 마지막 날짜: {last_date.date()}\")\n",
    "\n",
    "all_dataframes = [\n",
    "    macro_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df, \n",
    "    sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "last_dates = [pd.to_datetime(df['date']).max() for df in all_dataframes if not df.empty and 'date' in df.columns]\n",
    "end_date = min(last_dates) if last_dates else pd.Timestamp.today()\n",
    "\n",
    "fear_greed_df = add_prefix(fear_greed_df, 'fg')\n",
    "usdt_eth_mcap_df = add_prefix(usdt_eth_mcap_df, 'usdt')\n",
    "aave_tvl_df = add_prefix(aave_tvl_df, 'aave')\n",
    "lido_tvl_df = add_prefix(lido_tvl_df, 'lido')\n",
    "makerdao_tvl_df = add_prefix(makerdao_tvl_df, 'makerdao')\n",
    "uniswap_tvl_df = add_prefix(uniswap_tvl_df, 'uniswap')\n",
    "curve_tvl_df = add_prefix(curve_tvl_df, 'curve')\n",
    "eth_chain_tvl_df = add_prefix(eth_chain_tvl_df, 'chain')\n",
    "eth_funding_df = add_prefix(eth_funding_df, 'funding')\n",
    "layer2_tvl_df = add_prefix(layer2_tvl_df, 'l2')\n",
    "sp500_df = add_prefix(sp500_df, 'sp500')\n",
    "vix_df = add_prefix(vix_df, 'vix')\n",
    "gold_df = add_prefix(gold_df, 'gold')\n",
    "dxy_df = add_prefix(dxy_df, 'dxy')\n",
    "\n",
    "date_range = pd.date_range(start=LOOKBACK_START_DATE, end=end_date, freq='D')\n",
    "df_merged = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "dataframes_to_merge = [\n",
    "    macro_df, fear_greed_df, usdt_eth_mcap_df,\n",
    "    aave_tvl_df, lido_tvl_df, makerdao_tvl_df, uniswap_tvl_df, curve_tvl_df,\n",
    "    eth_chain_tvl_df, eth_funding_df, layer2_tvl_df,\n",
    "    sp500_df, vix_df, gold_df, dxy_df\n",
    "]\n",
    "\n",
    "for df in dataframes_to_merge:\n",
    "    if not df.empty:\n",
    "        df_merged = pd.merge(df_merged, df, on='date', how='left')\n",
    "\n",
    "traditional_market_cols = [\n",
    "    col for col in df_merged.columns if any(prefix in col for prefix in ['sp500_', 'vix_', 'gold_', 'dxy_'])\n",
    "]\n",
    "\n",
    "df_merged[traditional_market_cols] = df_merged[traditional_market_cols].fillna(method='ffill')\n",
    "df_merged = smart_fill_missing(df_merged)\n",
    "\n",
    "if df_merged.isnull().sum().sum() > 0:\n",
    "    df_merged = df_merged.fillna(0)\n",
    "\n",
    "lookback_df = df_merged[df_merged['date'] < TRAIN_START_DATE]\n",
    "cols_to_drop = [col for col in lookback_df.columns if lookback_df[col].isnull().all() and col != 'date']\n",
    "if cols_to_drop:\n",
    "    df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "#df_merged.tail(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fb66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_indicator_to_df(df_ta, indicator):\n",
    "    \"\"\"pandas_ta 지표 결과를 DataFrame에 안전하게 추가\"\"\"\n",
    "    if indicator is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(indicator, pd.DataFrame) and not indicator.empty:\n",
    "        for col in indicator.columns:\n",
    "            df_ta[col] = indicator[col]\n",
    "    elif isinstance(indicator, pd.Series) and not indicator.empty:\n",
    "        colname = indicator.name if indicator.name else 'Unnamed'\n",
    "        df_ta[colname] = indicator\n",
    "\n",
    "def safe_add(df_ta, func, *args, **kwargs):\n",
    "    \"\"\"지표 생성 시 오류 방지를 위한 래퍼 함수\"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        add_indicator_to_df(df_ta, result)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
    "        print(f\"    ⚠ {func_name.upper()} 생성 실패: {str(e)[:50]}\")\n",
    "        return False\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_ta = df.copy()\n",
    "\n",
    "    close = df['ETH_Close']\n",
    "    high = df.get('ETH_High', close)\n",
    "    low = df.get('ETH_Low', close)\n",
    "    volume = df.get('ETH_Volume', pd.Series(index=df.index, data=1))\n",
    "    open_ = df.get('ETH_Open', close)\n",
    "\n",
    "    try:\n",
    "        # ===== MOMENTUM INDICATORS =====\n",
    "        \n",
    "        # RSI (14만 - 모든 fold 선택)\n",
    "        df_ta['RSI_14'] = ta.rsi(close, length=14)\n",
    "        \n",
    "        # MACD (필수 - 자주 선택됨)\n",
    "        safe_add(df_ta, ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        \n",
    "        # Stochastic (14만 - 나머지는 중복)\n",
    "        safe_add(df_ta, ta.stoch, high, low, close, k=14, d=3)\n",
    "        \n",
    "        # Williams %R\n",
    "        df_ta['WILLR_14'] = ta.willr(high, low, close, length=14)\n",
    "        \n",
    "        # ROC (10만 - 20과 거의 동일)\n",
    "        df_ta['ROC_10'] = ta.roc(close, length=10)\n",
    "        \n",
    "        # MOM (10만 유지)\n",
    "        df_ta['MOM_10'] = ta.mom(close, length=10)\n",
    "        \n",
    "        # CCI (14, 50만 - 극단값 비교용)\n",
    "        df_ta['CCI_14'] = ta.cci(high, low, close, length=14)\n",
    "        df_ta['CCI_50'] = ta.cci(high, low, close, length=50)\n",
    "        df_ta['CCI_SIGNAL'] = (df_ta['CCI_14'] > 100).astype(int)\n",
    "      \n",
    "        # TSI\n",
    "        safe_add(df_ta, ta.tsi, close, fast=13, slow=25, signal=13)\n",
    "        \n",
    "        # Ichimoku (유지 - 복합 지표로 유용)\n",
    "        try:\n",
    "            ichimoku = ta.ichimoku(high, low, close)\n",
    "            if ichimoku is not None and isinstance(ichimoku, tuple):\n",
    "                ichimoku_df = ichimoku[0]\n",
    "                if ichimoku_df is not None:\n",
    "                    for col in ichimoku_df.columns:\n",
    "                        df_ta[col] = ichimoku_df[col]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== OVERLAP INDICATORS =====\n",
    "        \n",
    "        # SMA (20, 50만 - Golden Cross용)\n",
    "        df_ta['SMA_20'] = ta.sma(close, length=20)\n",
    "        df_ta['SMA_50'] = ta.sma(close, length=50)\n",
    "        \n",
    "        # EMA (12, 26만 - MACD 구성 요소)\n",
    "        df_ta['EMA_12'] = ta.ema(close, length=12)\n",
    "        df_ta['EMA_26'] = ta.ema(close, length=26)\n",
    "        \n",
    "        # TEMA (10만 - 30과 중복)\n",
    "        df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
    "        \n",
    "        # WMA (20만 - 10과 중복)\n",
    "        df_ta['WMA_20'] = ta.wma(close, length=20)\n",
    "        \n",
    "        # HMA (유지 - 독특한 smoothing)\n",
    "        df_ta['HMA_9'] = ta.hma(close, length=9)\n",
    "        \n",
    "        # DEMA (유지)\n",
    "        df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
    "        \n",
    "        # VWMA (유지 - 거래량 가중)\n",
    "        df_ta['VWMA_20'] = ta.vwma(close, volume, length=20)\n",
    "        \n",
    "        # 가격 조합 (유지 - 다른 정보)\n",
    "        df_ta['HL2'] = ta.hl2(high, low)\n",
    "        df_ta['HLC3'] = ta.hlc3(high, low, close)\n",
    "        df_ta['OHLC4'] = ta.ohlc4(open_, high, low, close)\n",
    "\n",
    "        # ===== VOLATILITY INDICATORS =====\n",
    "        \n",
    "        # Bollinger Bands \n",
    "        safe_add(df_ta, ta.bbands, close, length=20, std=2)\n",
    "        \n",
    "        # ATR \n",
    "        df_ta['ATR_14'] = ta.atr(high, low, close, length=14)\n",
    "        \n",
    "        # NATR\n",
    "        df_ta['NATR_14'] = ta.natr(high, low, close, length=14)\n",
    "        \n",
    "        # True Range\n",
    "        try:\n",
    "            tr = ta.true_range(high, low, close)\n",
    "            if isinstance(tr, pd.Series) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr\n",
    "            elif isinstance(tr, pd.DataFrame) and not tr.empty:\n",
    "                df_ta['TRUERANGE'] = tr.iloc[:, 0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Keltner Channel\n",
    "        safe_add(df_ta, ta.kc, high, low, close, length=20)\n",
    "        \n",
    "        # Donchian Channel\n",
    "        try:\n",
    "            dc = ta.donchian(high, low, lower_length=20, upper_length=20)\n",
    "            if dc is not None and isinstance(dc, pd.DataFrame) and not dc.empty:\n",
    "                for col in dc.columns:\n",
    "                    df_ta[col] = dc[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Supertrend\n",
    "        atr_10 = ta.atr(high, low, close, length=10)\n",
    "        hl2_calc = (high + low) / 2\n",
    "        upper_band = hl2_calc + (3 * atr_10)\n",
    "        lower_band = hl2_calc - (3 * atr_10)\n",
    "        \n",
    "        df_ta['SUPERTREND'] = 0\n",
    "        for i in range(1, len(df_ta)):\n",
    "            if close.iloc[i] > upper_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = 1\n",
    "            elif close.iloc[i] < lower_band.iloc[i-1]:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = -1\n",
    "            else:\n",
    "                df_ta.loc[df_ta.index[i], 'SUPERTREND'] = df_ta['SUPERTREND'].iloc[i-1]\n",
    "\n",
    "        # ===== VOLUME INDICATORS =====\n",
    "        \n",
    "        # OBV (필수)\n",
    "        df_ta['OBV'] = ta.obv(close, volume)\n",
    "        \n",
    "        # AD\n",
    "        df_ta['AD'] = ta.ad(high, low, close, volume)\n",
    "        \n",
    "        # ADOSC\n",
    "        df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
    "        \n",
    "        # MFI\n",
    "        df_ta['MFI_14'] = ta.mfi(high, low, close, volume, length=14)\n",
    "        \n",
    "        # CMF\n",
    "        df_ta['CMF_20'] = ta.cmf(high, low, close, volume, length=20)\n",
    "        \n",
    "        # EFI (Fold에서 선택됨)\n",
    "        df_ta['EFI_13'] = ta.efi(close, volume, length=13)\n",
    "        \n",
    "        # EOM\n",
    "        safe_add(df_ta, ta.eom, high, low, close, volume, length=14)\n",
    "        \n",
    "        # VWAP\n",
    "        try:\n",
    "            df_ta['VWAP'] = ta.vwap(high, low, close, volume)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ===== TREND INDICATORS =====\n",
    "        \n",
    "        # ADX (필수)\n",
    "        safe_add(df_ta, ta.adx, high, low, close, length=14)\n",
    "        \n",
    "        # Aroon\n",
    "        try:\n",
    "            aroon = ta.aroon(high, low, length=25)\n",
    "            if aroon is not None and isinstance(aroon, pd.DataFrame):\n",
    "                for col in aroon.columns:\n",
    "                    df_ta[col] = aroon[col]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # PSAR\n",
    "        try:\n",
    "            psar = ta.psar(high, low, close)\n",
    "            if psar is not None:\n",
    "                if isinstance(psar, pd.DataFrame) and not psar.empty:\n",
    "                    for col in psar.columns:\n",
    "                        df_ta[col] = psar[col]\n",
    "                elif isinstance(psar, pd.Series) and not psar.empty:\n",
    "                    df_ta[psar.name] = psar\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Vortex \n",
    "        safe_add(df_ta, ta.vortex, high, low, close, length=14)\n",
    "        \n",
    "        # DPO \n",
    "        df_ta['DPO_20'] = ta.dpo(close, length=20)\n",
    "\n",
    "        # ===== 파생 지표 =====\n",
    "        \n",
    "        # 가격 변화율 \n",
    "        df_ta['PRICE_CHANGE'] = close.pct_change()\n",
    "        \n",
    "        # 변동성 \n",
    "        df_ta['VOLATILITY_20'] = close.pct_change().rolling(window=20).std()\n",
    "        \n",
    "        # 모멘텀 \n",
    "        df_ta['MOMENTUM_10'] = close / close.shift(10) - 1\n",
    "        \n",
    "        # 이동평균 대비 위치 \n",
    "        df_ta['PRICE_VS_SMA20'] = close / df_ta['SMA_20'] - 1\n",
    "        df_ta['PRICE_VS_EMA12'] = close / df_ta['EMA_12'] - 1\n",
    "        \n",
    "        # 크로스 신호 \n",
    "        df_ta['SMA_GOLDEN_CROSS'] = (df_ta['SMA_50'] > df_ta['SMA_20']).astype(int)\n",
    "        df_ta['EMA_CROSS_SIGNAL'] = (df_ta['EMA_12'] > df_ta['EMA_26']).astype(int)\n",
    "        \n",
    "        # 거래량 지표\n",
    "        df_ta['VOLUME_SMA_20'] = ta.sma(volume, length=20)\n",
    "        df_ta['VOLUME_RATIO'] = volume / (df_ta['VOLUME_SMA_20'] + 1e-10)\n",
    "        df_ta['VOLUME_CHANGE'] = volume.pct_change()\n",
    "        df_ta['VOLUME_CHANGE_5'] = volume.pct_change(periods=5)\n",
    "        \n",
    "        # Range 지표 \n",
    "        df_ta['HIGH_LOW_RANGE'] = (high - low) / (close + 1e-10)\n",
    "        df_ta['HIGH_CLOSE_RANGE'] = np.abs(high - close.shift()) / (close + 1e-10)\n",
    "        df_ta['CLOSE_LOW_RANGE'] = (close - low) / (close + 1e-10)\n",
    "        \n",
    "        # 일중 가격 위치\n",
    "        df_ta['INTRADAY_POSITION'] = (close - low) / ((high - low) + 1e-10)\n",
    "        \n",
    "        # Linear Regression Slope \n",
    "        try:\n",
    "            df_ta['SLOPE_5'] = ta.linreg(close, length=5, slope=True)\n",
    "        except:\n",
    "            df_ta['SLOPE_5'] = close.rolling(window=5).apply(\n",
    "                lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == 5 else np.nan, raw=True\n",
    "            )\n",
    "        \n",
    "        # Increasing \n",
    "        df_ta['INC_1'] = (close > close.shift(1)).astype(int)\n",
    "        \n",
    "        # BOP\n",
    "        df_ta['BOP'] = (close - open_) / ((high - low) + 1e-10)\n",
    "        df_ta['BOP'] = df_ta['BOP'].fillna(0)\n",
    "        \n",
    "        # ===== 고급 파생 지표 =====\n",
    "        \n",
    "        # Bollinger Bands 파생 \n",
    "        if 'BBL_20' in df_ta.columns and 'BBU_20' in df_ta.columns and 'BBM_20' in df_ta.columns:\n",
    "            df_ta['BB_WIDTH'] = (df_ta['BBU_20'] - df_ta['BBL_20']) / (df_ta['BBM_20'] + 1e-8)\n",
    "            df_ta['BB_POSITION'] = (close - df_ta['BBL_20']) / (df_ta['BBU_20'] - df_ta['BBL_20'] + 1e-8)\n",
    "        \n",
    "        # RSI 파생\n",
    "        df_ta['RSI_OVERBOUGHT'] = (df_ta['RSI_14'] > 70).astype(int)\n",
    "        df_ta['RSI_OVERSOLD'] = (df_ta['RSI_14'] < 30).astype(int)\n",
    "        \n",
    "        # MACD 히스토그램 변화율\n",
    "        if 'MACDh_12_26_9' in df_ta.columns:\n",
    "            df_ta['MACD_HIST_CHANGE'] = df_ta['MACDh_12_26_9'].diff()\n",
    "        \n",
    "        # Volume Profile\n",
    "        df_ta['VOLUME_STRENGTH'] = volume / volume.rolling(window=50).mean()\n",
    "        \n",
    "        # Price Acceleration\n",
    "        df_ta['PRICE_ACCELERATION'] = close.pct_change().diff()\n",
    "        \n",
    "        # Gap (Fold에서 선택됨)\n",
    "        df_ta['GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-10)\n",
    "        \n",
    "        # Distance from High/Low \n",
    "        df_ta['ROLLING_MAX_20'] = close.rolling(window=20).max()\n",
    "        df_ta['ROLLING_MIN_20'] = close.rolling(window=20).min()\n",
    "        df_ta['DISTANCE_FROM_HIGH'] = (df_ta['ROLLING_MAX_20'] - close) / (df_ta['ROLLING_MAX_20'] + 1e-10)\n",
    "        df_ta['DISTANCE_FROM_LOW'] = (close - df_ta['ROLLING_MIN_20']) / (close + 1e-10)\n",
    "\n",
    "        # Realized Volatility \n",
    "        ret_squared = close.pct_change() ** 2\n",
    "        df_ta['RV_5'] = ret_squared.rolling(5).sum()\n",
    "        df_ta['RV_20'] = ret_squared.rolling(20).sum()\n",
    "        df_ta['RV_RATIO'] = df_ta['RV_5'] / (df_ta['RV_20'] + 1e-10)\n",
    "        \n",
    "        added = df_ta.shape[1] - df.shape[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "\n",
    "    return df_ta\n",
    "\n",
    "\n",
    "\n",
    "def add_enhanced_cross_crypto_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    df_enhanced['eth_return'] = df['ETH_Close'].pct_change()\n",
    "    df_enhanced['btc_return'] = df['BTC_Close'].pct_change()\n",
    "\n",
    "    for lag in [1, 5]:\n",
    "        df_enhanced[f'btc_return_lag{lag}'] = df_enhanced['btc_return'].shift(lag)\n",
    "\n",
    "    for window in [3, 7, 14, 30, 60]:\n",
    "        df_enhanced[f'eth_btc_corr_{window}d'] = (\n",
    "            df_enhanced['eth_return'].rolling(window).corr(df_enhanced['btc_return'])\n",
    "        )\n",
    "\n",
    "    eth_vol = df_enhanced['eth_return'].abs()\n",
    "    btc_vol = df_enhanced['btc_return'].abs()\n",
    "\n",
    "    for window in [7, 14, 30]:\n",
    "        df_enhanced[f'eth_btc_volcorr_{window}d'] = eth_vol.rolling(window).corr(btc_vol)\n",
    "        df_enhanced[f'eth_btc_volcorr_sq_{window}d'] = (\n",
    "            (df_enhanced['eth_return']**2).rolling(window).corr(df_enhanced['btc_return']**2)\n",
    "        )\n",
    "\n",
    "    df_enhanced['btc_eth_strength_ratio'] = (\n",
    "        df_enhanced['btc_return'] / (df_enhanced['eth_return'].abs() + 1e-8)\n",
    "    )\n",
    "    df_enhanced['btc_eth_strength_ratio_7d'] = df_enhanced['btc_eth_strength_ratio'].rolling(7).mean()\n",
    "\n",
    "    alt_returns = []\n",
    "    for coin in ['BNB', 'XRP', 'SOL', 'ADA']:\n",
    "        if f'{coin}_Close' in df.columns:\n",
    "            alt_returns.append(df[f'{coin}_Close'].pct_change())\n",
    "\n",
    "    if alt_returns:\n",
    "        market_return = pd.concat(\n",
    "            alt_returns + [df_enhanced['eth_return'], df_enhanced['btc_return']], axis=1\n",
    "        ).mean(axis=1)\n",
    "        df_enhanced['btc_dominance'] = df_enhanced['btc_return'] / (market_return + 1e-8)\n",
    "\n",
    "    for window in [30, 60, 90]:\n",
    "        covariance = df_enhanced['eth_return'].rolling(window).cov(df_enhanced['btc_return'])\n",
    "        btc_variance = df_enhanced['btc_return'].rolling(window).var()\n",
    "        df_enhanced[f'eth_btc_beta_{window}d'] = covariance / (btc_variance + 1e-8)\n",
    "\n",
    "    df_enhanced['eth_btc_spread'] = df_enhanced['eth_return'] - df_enhanced['btc_return']\n",
    "    df_enhanced['eth_btc_spread_ma7'] = df_enhanced['eth_btc_spread'].rolling(7).mean()\n",
    "    df_enhanced['eth_btc_spread_std7'] = df_enhanced['eth_btc_spread'].rolling(7).std()\n",
    "\n",
    "    btc_vol_ma = btc_vol.rolling(30).mean()\n",
    "    high_vol_mask = btc_vol > btc_vol_ma\n",
    "    df_enhanced['eth_btc_corr_highvol'] = np.nan\n",
    "    df_enhanced['eth_btc_corr_lowvol'] = np.nan\n",
    "\n",
    "    for i in range(30, len(df_enhanced)):\n",
    "        window_data = df_enhanced.iloc[i-30:i]\n",
    "        high_vol_data = window_data[high_vol_mask.iloc[i-30:i]]\n",
    "        low_vol_data = window_data[~high_vol_mask.iloc[i-30:i]]\n",
    "\n",
    "        if len(high_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_highvol'] = (\n",
    "                high_vol_data['eth_return'].corr(high_vol_data['btc_return'])\n",
    "            )\n",
    "        if len(low_vol_data) > 5:\n",
    "            df_enhanced.loc[df_enhanced.index[i], 'eth_btc_corr_lowvol'] = (\n",
    "                low_vol_data['eth_return'].corr(low_vol_data['btc_return'])\n",
    "            )\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def remove_raw_prices_and_transform(df,target_type,method):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    if 'eth_log_return' not in df_transformed.columns:\n",
    "        df_transformed['eth_log_return'] = np.log(df['ETH_Close'] / df['ETH_Close'].shift(1))\n",
    "    if 'eth_intraday_range' not in df_transformed.columns:\n",
    "        df_transformed['eth_intraday_range'] = (df['ETH_High'] - df['ETH_Low']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_body_ratio' not in df_transformed.columns:\n",
    "        df_transformed['eth_body_ratio'] = (df['ETH_Close'] - df['ETH_Open']) / (df['ETH_Close'] + 1e-8)\n",
    "    if 'eth_close_position' not in df_transformed.columns:\n",
    "        df_transformed['eth_close_position'] = (\n",
    "            (df['ETH_Close'] - df['ETH_Low']) / (df['ETH_High'] - df['ETH_Low'] + 1e-8)\n",
    "        )\n",
    "\n",
    "    if 'BTC_Close' in df_transformed.columns:\n",
    "        for period in [5, 20]:\n",
    "            col_name = f'btc_return_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df['BTC_Close'] / df['BTC_Close'].shift(period)).fillna(0)\n",
    "        \n",
    "        for period in [7, 14, 30]:\n",
    "            col_name = f'btc_volatility_{period}d'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = (\n",
    "                    df_transformed['eth_log_return'].rolling(period, min_periods=max(3, period//3)).std()\n",
    "                ).fillna(0)\n",
    "        \n",
    "        if 'btc_intraday_range' not in df_transformed.columns:\n",
    "            df_transformed['btc_intraday_range'] = (df['BTC_High'] - df['BTC_Low']) / (df['BTC_Close'] + 1e-8)\n",
    "        if 'btc_body_ratio' not in df_transformed.columns:\n",
    "            df_transformed['btc_body_ratio'] = (df['BTC_Close'] - df['BTC_Open']) / (df['BTC_Close'] + 1e-8)\n",
    "\n",
    "        if 'BTC_Volume' in df.columns:\n",
    "            btc_volume = df['BTC_Volume']\n",
    "            if 'btc_volume_change' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_change'] = btc_volume.pct_change().fillna(0)\n",
    "            if 'btc_volume_ratio_20d' not in df_transformed.columns:\n",
    "                volume_ma20 = btc_volume.rolling(20, min_periods=5).mean()\n",
    "                df_transformed['btc_volume_ratio_20d'] = (btc_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "            if 'btc_volume_volatility_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_volatility_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).std()\n",
    "                ).fillna(0)\n",
    "            if 'btc_obv' not in df_transformed.columns:\n",
    "                btc_close = df['BTC_Close']\n",
    "                obv = np.where(btc_close > btc_close.shift(1), btc_volume,\n",
    "                               np.where(btc_close < btc_close.shift(1), -btc_volume, 0))\n",
    "                df_transformed['btc_obv'] = pd.Series(obv, index=df.index).cumsum().fillna(0)\n",
    "            if 'btc_volume_price_corr_30d' not in df_transformed.columns:\n",
    "                df_transformed['btc_volume_price_corr_30d'] = (\n",
    "                    btc_volume.pct_change().rolling(30, min_periods=10).corr(\n",
    "                        df_transformed['eth_log_return']\n",
    "                    )\n",
    "                ).fillna(0)\n",
    "\n",
    "    altcoins = ['BNB', 'XRP', 'SOL', 'ADA', 'DOGE', 'AVAX', 'DOT']\n",
    "    for coin in altcoins:\n",
    "        if f'{coin}_Close' in df_transformed.columns:\n",
    "            col_name = f'{coin.lower()}_return'\n",
    "            if col_name not in df_transformed.columns:\n",
    "                df_transformed[col_name] = np.log(df[f'{coin}_Close'] / df[f'{coin}_Close'].shift(1)).fillna(0)\n",
    "            vol_col = f'{coin.lower()}_volatility_30d'\n",
    "            if vol_col not in df_transformed.columns:\n",
    "                df_transformed[vol_col] = df_transformed[col_name].rolling(30, min_periods=10).std().fillna(0)\n",
    "            \n",
    "            if f'{coin}_Volume' in df.columns:\n",
    "                coin_volume = df[f'{coin}_Volume']\n",
    "                volume_change_col = f'{coin.lower()}_volume_change'\n",
    "                if volume_change_col not in df_transformed.columns:\n",
    "                    df_transformed[volume_change_col] = coin_volume.pct_change().fillna(0)\n",
    "                volume_ratio_col = f'{coin.lower()}_volume_ratio_20d'\n",
    "                if volume_ratio_col not in df_transformed.columns:\n",
    "                    volume_ma20 = coin_volume.rolling(20, min_periods=5).mean()\n",
    "                    df_transformed[volume_ratio_col] = (coin_volume / (volume_ma20 + 1e-8)).fillna(1)\n",
    "\n",
    "    if 'ETH_Volume' in df.columns and 'BTC_Volume' in df.columns:\n",
    "        eth_volume = df['ETH_Volume']\n",
    "        btc_volume = df['BTC_Volume']\n",
    "        if 'eth_btc_volume_corr_30d' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_corr_30d'] = (\n",
    "                eth_volume.pct_change().rolling(30, min_periods=10).corr(btc_volume.pct_change())\n",
    "            ).fillna(0)\n",
    "        if 'eth_btc_volume_ratio' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio'] = (eth_volume / (btc_volume + 1e-8)).fillna(0)\n",
    "        if 'eth_btc_volume_ratio_ma30' not in df_transformed.columns:\n",
    "            df_transformed['eth_btc_volume_ratio_ma30'] = (\n",
    "                df_transformed['eth_btc_volume_ratio'].rolling(30, min_periods=10).mean()\n",
    "            ).fillna(0)\n",
    "\n",
    "            \n",
    "    ## raw_data 저장하기\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    base_dir=os.path.join('model_results',timestamp,'raw_data',target_type,method)\n",
    "    os.makedirs(base_dir,exist_ok=True)\n",
    "    df_transformed.to_csv(os.path.join(base_dir,\"raw_data_all_features.csv\"),index=False)        \n",
    "            \n",
    "            \n",
    "    remove_patterns = ['_Close', '_Open', '_High', '_Low', '_Volume']\n",
    "    cols_to_remove = [\n",
    "        col for col in df_transformed.columns\n",
    "        if any(p in col for p in remove_patterns)\n",
    "        and not any(d in col.lower() for d in ['_lag', '_position', '_ratio', '_range', '_change', '_corr', '_volatility', '_obv'])\n",
    "    ]\n",
    "    df_transformed.drop(cols_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return_cols = [col for col in df_transformed.columns if 'return' in col.lower() and 'next' not in col]\n",
    "    if return_cols:\n",
    "        df_transformed[return_cols] = df_transformed[return_cols].fillna(0)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6e8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lag_features(df, news_lag=2, onchain_lag=1):\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    raw_sentiment_cols = ['sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio', 'negative_ratio']\n",
    "    sentiment_ma_cols = [col for col in df.columns if 'sentiment' in col and ('_ma7' in col or '_volatility_7' in col)]\n",
    "    no_lag_patterns = ['_trend', '_acceleration', '_volume_change', 'news_volume_change', 'news_volume_ma']\n",
    "    onchain_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                    for keyword in ['eth_tx', 'eth_active', 'eth_new', 'eth_large', 'eth_token', \n",
    "                                  'eth_contract', 'eth_avg_gas', 'eth_total_gas', 'eth_avg_block'])]\n",
    "    other_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['tvl', 'funding', 'lido_', 'aave_', 'makerdao_', \n",
    "                                'chain_', 'usdt_', 'sp500_', 'vix_', 'gold_', 'dxy_', 'fg_'])]\n",
    "    \n",
    "    exclude_cols = ['ETH_Close', 'ETH_High', 'ETH_Low', 'ETH_Open', 'date']\n",
    "    exclude_cols.extend([col for col in df.columns if 'event_' in col or 'period_' in col or '_lag' in col])\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in raw_sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            for lag in range(1, news_lag + 1):\n",
    "                df_lagged[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            cols_to_drop.append(col)\n",
    "    \n",
    "    for col in sentiment_ma_cols:\n",
    "        if col in df.columns and col not in cols_to_drop:\n",
    "            if not any(pattern in col for pattern in no_lag_patterns):\n",
    "                df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in onchain_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(onchain_lag)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    for col in other_cols:\n",
    "        if col not in exclude_cols:\n",
    "            df_lagged[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_lagged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    return df_lagged\n",
    "\n",
    "\n",
    "def add_price_lag_features_first(df):\n",
    "    df_new = df.copy()\n",
    "    close = df['ETH_Close']\n",
    "    high = df['ETH_High']\n",
    "    low = df['ETH_Low']\n",
    "    volume = df['ETH_Volume']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "        df_new[f'close_lag{lag}'] = close.shift(lag)\n",
    "    \n",
    "    for lag in [1, 2, 3, 5, 7]:\n",
    "        df_new[f'high_lag{lag}'] = high.shift(lag)\n",
    "        df_new[f'low_lag{lag}'] = low.shift(lag)\n",
    "        df_new[f'volume_lag{lag}'] = volume.shift(lag)\n",
    "        df_new[f'return_lag{lag}'] = close.pct_change(periods=lag).shift(1)\n",
    "    \n",
    "    for lag in [1, 7, 30]:\n",
    "        df_new[f'close_ratio_lag{lag}'] = close / close.shift(lag)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    if 'RSI_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['RSI_Volume_Strength'] = df['RSI_14'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'vix_VIX' in df.columns and 'VOLATILITY_20' in df.columns:\n",
    "        df_interact['VIX_ETH_Vol_Cross'] = df['vix_VIX'] * df['VOLATILITY_20']\n",
    "    \n",
    "    if 'MACD_12_26_9' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['MACD_Volume_Momentum'] = df['MACD_12_26_9'] * df['VOLUME_RATIO']\n",
    "    \n",
    "    if 'btc_return' in df.columns and 'eth_btc_corr_30d' in df.columns:\n",
    "        df_interact['BTC_Weighted_Impact'] = df['btc_return'] * df['eth_btc_corr_30d']\n",
    "    \n",
    "    if 'ATR_14' in df.columns and 'VOLUME_RATIO' in df.columns:\n",
    "        df_interact['Liquidity_Risk'] = df['ATR_14'] * (1 / (df['VOLUME_RATIO'] + 1e-8))\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "def add_volatility_regime_features(df):\n",
    "    df_regime = df.copy()\n",
    "    \n",
    "    if 'VOLATILITY_20' in df.columns:\n",
    "        vol_median = df['VOLATILITY_20'].rolling(60, min_periods=20).median()\n",
    "        df_regime['vol_regime_high'] = (df['VOLATILITY_20'] > vol_median).astype(int)\n",
    "        \n",
    "        vol_mean = df['VOLATILITY_20'].rolling(30, min_periods=10).mean()\n",
    "        vol_std = df['VOLATILITY_20'].rolling(30, min_periods=10).std()\n",
    "        df_regime['vol_spike'] = (df['VOLATILITY_20'] > vol_mean + 2 * vol_std).astype(int)\n",
    "        \n",
    "        df_regime['vol_percentile_90d'] = df['VOLATILITY_20'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "        df_regime['vol_trend'] = df['VOLATILITY_20'].pct_change(5)\n",
    "        df_regime['vol_regime_duration'] = df_regime.groupby(\n",
    "            (df_regime['vol_regime_high'] != df_regime['vol_regime_high'].shift()).cumsum()\n",
    "        ).cumcount() + 1\n",
    "\n",
    "    return df_regime\n",
    "\n",
    "\n",
    "def add_normalized_price_lags(df):\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' not in df.columns:\n",
    "        return df_norm\n",
    "    \n",
    "    current_close = df['ETH_Close']\n",
    "    lag_cols = [col for col in df.columns if 'close_lag' in col and col.replace('close_lag', '').isdigit()]\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        lag_num = col.replace('close_lag', '')\n",
    "        df_norm[f'close_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        next_lag_col = f'close_lag{int(lag_num)+1}'\n",
    "        if next_lag_col in df.columns:\n",
    "            df_norm[f'close_lag{lag_num}_logret'] = np.log(df[col] / (df[next_lag_col] + 1e-8))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'high_lag' in col:\n",
    "            lag_num = col.replace('high_lag', '')\n",
    "            df_norm[f'high_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "        if 'low_lag' in col:\n",
    "            lag_num = col.replace('low_lag', '')\n",
    "            df_norm[f'low_lag{lag_num}_ratio'] = df[col] / (current_close + 1e-8)\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "def add_percentile_features(df):\n",
    "    df_pct = df.copy()\n",
    "    \n",
    "    if 'ETH_Close' in df.columns:\n",
    "        df_pct['price_percentile_250d'] = df['ETH_Close'].rolling(250, min_periods=60).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'ETH_Volume' in df.columns:\n",
    "        df_pct['volume_percentile_90d'] = df['ETH_Volume'].rolling(90, min_periods=30).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    if 'RSI_14' in df.columns:\n",
    "        df_pct['RSI_percentile_60d'] = df['RSI_14'].rolling(60, min_periods=20).apply(\n",
    "            lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5\n",
    "        )\n",
    "    \n",
    "    return df_pct\n",
    "\n",
    "\n",
    "def handle_missing_values_paper_based(df_clean, train_start_date, is_train=True, train_stats=None):\n",
    "    \"\"\"\n",
    "    암호화폐 시계열 결측치 처리\n",
    "    \n",
    "    참고문헌:\n",
    "    1. \"Quantifying Cryptocurrency Unpredictability\" (2025)\n",
    "\n",
    "    2. \"Time Series Data Forecasting\" \n",
    "    \n",
    "    3. \"Dealing with Leaky Missing Data in Production\" (2021)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Lookback 제거 =====\n",
    "    if isinstance(train_start_date, str):\n",
    "        train_start_date = pd.to_datetime(train_start_date)\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['date'] >= train_start_date].reset_index(drop=True)\n",
    "    \n",
    "    # ===== 2. Feature 컬럼 선택 =====\n",
    "    target_cols = ['next_log_return', 'next_direction', 'next_close','next_open']\n",
    "    feature_cols = [col for col in df_clean.columns \n",
    "                   if col not in target_cols + ['date']]\n",
    "    \n",
    "    # ===== 3. 결측 확인 =====\n",
    "    missing_before = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 4. FFill → 0 =====\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(method='ffill')\n",
    "    df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    missing_after = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    # ===== 5. 무한대 처리 =====\n",
    "    inf_count = 0\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df_clean[col]).sum() > 0:\n",
    "            inf_count += np.isinf(df_clean[col]).sum()\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_clean[col] = df_clean[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # ===== 6. 최종 확인 =====\n",
    "    final_missing = df_clean[feature_cols].isnull().sum().sum()\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        df_clean[feature_cols] = df_clean[feature_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    if is_train:\n",
    "        return df_clean, {}\n",
    "    else:\n",
    "        return df_clean\n",
    "    \n",
    "    \n",
    "@jit(nopython=True)\n",
    "def compute_triple_barrier_targets(\n",
    "    prices_close,\n",
    "    prices_high,\n",
    "    prices_low,\n",
    "    atr,\n",
    "    lookahead_candles,\n",
    "    atr_multiplier_profit,\n",
    "    atr_multiplier_stop\n",
    "):\n",
    "    n = len(prices_close)\n",
    "    targets_raw = np.zeros(n, dtype=np.int32) \n",
    "    upper_barriers = np.zeros(n, dtype=np.float64) \n",
    "    lower_barriers = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    for i in range(n - lookahead_candles):\n",
    "        current_atr = max(atr[i], 1e-8) \n",
    "        current_price = prices_close[i]\n",
    "        \n",
    "        upper_barrier = current_price + (current_atr * atr_multiplier_profit)\n",
    "        lower_barrier = current_price - (current_atr * atr_multiplier_stop)\n",
    "        \n",
    "        for j in range(1, lookahead_candles + 1):\n",
    "            future_high = prices_high[i + j]\n",
    "            future_low = prices_low[i + j]\n",
    "            \n",
    "            if future_high >= upper_barrier:\n",
    "                targets_raw[i] = 1\n",
    "                break \n",
    "                \n",
    "            elif future_low <= lower_barrier:\n",
    "                targets_raw[i] = 2\n",
    "                break\n",
    "    \n",
    "    return targets_raw,upper_barrier,lower_barrier\n",
    "\n",
    "\n",
    "def create_targets(df, lookahead_candles=8, atr_multiplier_profit=1.5, atr_multiplier_stop=1.0):\n",
    "    df_target = df.copy()\n",
    "    \n",
    "    atr_col_name = 'ATR_14'\n",
    "    if atr_col_name not in df.columns:\n",
    "        raise ValueError(f\"'{atr_col_name}' feature is missing. Run calculate_technical_indicators first.\")\n",
    "\n",
    "    prices_close = df_target['ETH_Close'].to_numpy()\n",
    "    prices_high = df_target['ETH_High'].to_numpy()\n",
    "    prices_low = df_target['ETH_Low'].to_numpy()\n",
    "    atr = pd.Series(df_target[atr_col_name]).fillna(method='ffill').fillna(0).to_numpy()\n",
    "\n",
    "    targets_raw, upper_barriers, lower_barriers = compute_triple_barrier_targets(\n",
    "        prices_close,\n",
    "        prices_high,\n",
    "        prices_low,\n",
    "        atr,\n",
    "        lookahead_candles,\n",
    "        atr_multiplier_profit,\n",
    "        atr_multiplier_stop\n",
    "    )\n",
    "    \n",
    "    next_open = df['ETH_Open'].shift(-1)\n",
    "    next_close = df['ETH_Close'].shift(-1)\n",
    "    \n",
    "    df_target['next_open']=next_open\n",
    "    df_target['next_close']=next_close\n",
    "    df_target['next_log_return'] = np.log(next_close / next_open)\n",
    "    \n",
    "    df_target['next_direction'] = pd.Series(targets_raw, index=df_target.index).map({\n",
    "        1: 1,\n",
    "        2: 0,\n",
    "        0: np.nan\n",
    "    })\n",
    "    \n",
    "    df_target['take_profit_price'] = pd.Series(upper_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    df_target['stop_loss_price'] = pd.Series(lower_barriers, index=df_target.index).replace(0, np.nan)\n",
    "    \n",
    "    return df_target\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_non_stationary_features(df):\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    prefixes_to_transform = [\n",
    "        'eth_', 'aave_', 'lido_', 'makerdao_', 'uniswap_', 'curve_', 'chain_',\n",
    "        'l2_', 'sp500_', 'gold_', 'dxy_', 'vix_', 'usdt_'\n",
    "    ]\n",
    "    \n",
    "    exclude_prefixes = ['fg_', 'funding_']\n",
    "    \n",
    "    exclude_keywords = [\n",
    "        '_pct_', '_ratio', '_lag', '_volatility', '_corr', '_beta', '_spread',\n",
    "        'eth_return', 'btc_return', 'eth_log_return' \n",
    "    ]\n",
    "    \n",
    "    cols_to_transform = []\n",
    "    for col in df_proc.columns:\n",
    "        if col.startswith(tuple(prefixes_to_transform)):\n",
    "            if not col.startswith(tuple(exclude_prefixes)):\n",
    "                if not any(keyword in col for keyword in exclude_keywords):\n",
    "                    cols_to_transform.append(col)\n",
    "                    \n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        df_proc[col] = df_proc[col].fillna(method='ffill').replace(0, 1e-9)\n",
    "\n",
    "        df_proc[f'{col}_pct_1d'] = df_proc[col].pct_change(1)\n",
    "        df_proc[f'{col}_pct_5d'] = df_proc[col].pct_change(5)\n",
    "        \n",
    "        ma_30 = df_proc[col].rolling(window=30, min_periods=10).mean()\n",
    "        df_proc[f'{col}_ma30_ratio'] = df_proc[col] / (ma_30 + 1e-9)\n",
    "        \n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "    df_proc = df_proc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    df_proc = df_proc.replace([np.inf, -np.inf], np.nan)\n",
    "    df_proc = df_proc.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"Preprocessed and replaced {len(cols_to_drop)} non-stationary features.\")\n",
    "    \n",
    "    return df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8325972f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1986413/849773086.py:81: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['EMA_12'] = ta.ema(close, length=12)\n",
      "/tmp/ipykernel_1986413/849773086.py:82: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['EMA_26'] = ta.ema(close, length=26)\n",
      "/tmp/ipykernel_1986413/849773086.py:85: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['TEMA_10'] = ta.tema(close, length=10)\n",
      "/tmp/ipykernel_1986413/849773086.py:94: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['DEMA_10'] = ta.dema(close, length=10)\n",
      "/tmp/ipykernel_1986413/849773086.py:161: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ta['ADOSC_3_10'] = ta.adosc(high, low, close, volume, fast=3, slow=10)\n",
      "/tmp/ipykernel_1986413/617578415.py:319: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_proc[f'{col}_pct_1d'] = df_proc[col].pct_change(1)\n",
      "/tmp/ipykernel_1986413/617578415.py:320: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_proc[f'{col}_pct_5d'] = df_proc[col].pct_change(5)\n",
      "/tmp/ipykernel_1986413/617578415.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_proc[f'{col}_ma30_ratio'] = df_proc[col] / (ma_30 + 1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and replaced 25 non-stationary features.\n",
      "         date  actual_direction  actual_return  take_profit_price  \\\n",
      "0  2025-10-22                 1       0.006222                NaN   \n",
      "1  2025-10-23                 1       0.010792                NaN   \n",
      "2  2025-10-24                 1       0.003742                NaN   \n",
      "3  2025-10-25                 1       0.042864                NaN   \n",
      "4  2025-10-26                 0      -0.008819                NaN   \n",
      "5  2025-10-27                 0      -0.026088                NaN   \n",
      "6  2025-10-28                 0      -0.017150                NaN   \n",
      "7  2025-10-29                 0      -0.012061                NaN   \n",
      "8  2025-10-30                 0      -0.000347                NaN   \n",
      "9  2025-10-31                 1       0.000347                NaN   \n",
      "10 2025-11-01                 1       0.004668                NaN   \n",
      "11 2025-11-02                 0      -0.076954                NaN   \n",
      "12 2025-11-03                 0      -0.080260                NaN   \n",
      "13 2025-11-04                 1       0.035885                NaN   \n",
      "14 2025-11-05                 0      -0.035474                NaN   \n",
      "15 2025-11-06                 1       0.030790                NaN   \n",
      "16 2025-11-07                 0      -0.008054                NaN   \n",
      "17 2025-11-08                 1       0.048132                NaN   \n",
      "18 2025-11-09                 0      -0.006979                NaN   \n",
      "19 2025-11-10                 0      -0.031529                NaN   \n",
      "20 2025-11-11                 1       0.005454                NaN   \n",
      "21 2025-11-12                 0      -0.047946                NaN   \n",
      "22 2025-11-13                 0      -0.037160                NaN   \n",
      "23 2025-11-14                 1       0.012193                NaN   \n",
      "24 2025-11-15                 0      -0.036817                NaN   \n",
      "25 2025-11-16                 0      -0.018598                NaN   \n",
      "26 2025-11-17                 1       0.021628                NaN   \n",
      "27 2025-11-18                 0      -0.026044                NaN   \n",
      "28 2025-11-19                 0      -0.061027                NaN   \n",
      "29 2025-11-20                 0            NaN                NaN   \n",
      "\n",
      "    stop_loss_price  pred_direction  pred_proba_up  pred_proba_down  \\\n",
      "0               NaN             0.0       0.410443         0.589557   \n",
      "1               NaN             0.0       0.306697         0.693303   \n",
      "2               NaN             0.0       0.032283         0.967717   \n",
      "3               NaN             0.0       0.028582         0.971418   \n",
      "4               NaN             0.0       0.017645         0.982355   \n",
      "5               NaN             0.0       0.036792         0.963208   \n",
      "6               NaN             0.0       0.020249         0.979751   \n",
      "7               NaN             0.0       0.022352         0.977648   \n",
      "8               NaN             0.0       0.021504         0.978496   \n",
      "9               NaN             0.0       0.014826         0.985174   \n",
      "10              NaN             0.0       0.015611         0.984389   \n",
      "11              NaN             0.0       0.024444         0.975556   \n",
      "12              NaN             0.0       0.171887         0.828113   \n",
      "13              NaN             1.0       0.562124         0.437876   \n",
      "14              NaN             0.0       0.440727         0.559273   \n",
      "15              NaN             1.0       0.515213         0.484787   \n",
      "16              NaN             0.0       0.150109         0.849891   \n",
      "17              NaN             0.0       0.098309         0.901691   \n",
      "18              NaN             0.0       0.035501         0.964499   \n",
      "19              NaN             0.0       0.035078         0.964922   \n",
      "20              NaN             0.0       0.061654         0.938346   \n",
      "21              NaN             0.0       0.075143         0.924857   \n",
      "22              NaN             0.0       0.047042         0.952958   \n",
      "23              NaN             0.0       0.052406         0.947594   \n",
      "24              NaN             0.0       0.045052         0.954948   \n",
      "25              NaN             0.0       0.031411         0.968589   \n",
      "26              NaN             0.0       0.032197         0.967803   \n",
      "27              NaN             0.0       0.032075         0.967925   \n",
      "28              NaN             0.0       0.023559         0.976441   \n",
      "29              NaN             0.0       0.031941         0.968059   \n",
      "\n",
      "    max_proba  confidence  correct  \n",
      "0    0.589557    0.179115        0  \n",
      "1    0.693303    0.386606        0  \n",
      "2    0.967717    0.935433        0  \n",
      "3    0.971418    0.942836        0  \n",
      "4    0.982355    0.964709        1  \n",
      "5    0.963208    0.926417        1  \n",
      "6    0.979751    0.959501        1  \n",
      "7    0.977648    0.955295        1  \n",
      "8    0.978496    0.956991        1  \n",
      "9    0.985174    0.970349        0  \n",
      "10   0.984389    0.968778        0  \n",
      "11   0.975556    0.951112        1  \n",
      "12   0.828113    0.656227        1  \n",
      "13   0.562124    0.124247        1  \n",
      "14   0.559273    0.118545        1  \n",
      "15   0.515213    0.030425        1  \n",
      "16   0.849891    0.699782        1  \n",
      "17   0.901691    0.803382        0  \n",
      "18   0.964499    0.928998        1  \n",
      "19   0.964922    0.929844        1  \n",
      "20   0.938346    0.876692        0  \n",
      "21   0.924857    0.849713        1  \n",
      "22   0.952958    0.905916        1  \n",
      "23   0.947594    0.895188        0  \n",
      "24   0.954948    0.909897        1  \n",
      "25   0.968589    0.937179        1  \n",
      "26   0.967803    0.935605        0  \n",
      "27   0.967925    0.935850        1  \n",
      "28   0.976441    0.952881        1  \n",
      "29   0.968059    0.936117        1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = \"model_results/2025-11-10/fold_results/direction_l5_p1.5_s1.0/fold_8_final_holdout\"\n",
    "\n",
    "\n",
    "model = joblib.load(f\"{base_path}/CatBoost.pkl\")\n",
    "scaler = joblib.load(f\"{base_path}/robust_scaler.pkl\")\n",
    "selected_features = joblib.load(f\"{base_path}/selected_features.pkl\")\n",
    "test_file=pd.read_csv(f\"{base_path}/CatBoost_predictions.csv\")\n",
    "\n",
    "\n",
    "df = df_merged.copy()\n",
    "df = add_price_lag_features_first(df)\n",
    "df = calculate_technical_indicators(df)\n",
    "df = add_enhanced_cross_crypto_features(df)\n",
    "df = add_volatility_regime_features(df)\n",
    "df = add_interaction_features(df)\n",
    "df = add_percentile_features(df)\n",
    "df = add_normalized_price_lags(df)\n",
    "df = preprocess_non_stationary_features(df)\n",
    "df = apply_lag_features(df, news_lag=2, onchain_lag=1)\n",
    "next_open = df['ETH_Open'].shift(-1)\n",
    "next_close = df['ETH_Close'].shift(-1)\n",
    "df['next_open'] = next_open\n",
    "df['next_close'] = next_close\n",
    "df['next_log_return'] = np.log(next_close / next_open)\n",
    "df['next_direction'] = (df['next_log_return'] > 0).astype(int)\n",
    "df = remove_raw_prices_and_transform(df, target_type='walk-forward', method='direction')\n",
    "\n",
    "\n",
    "\n",
    "X_new = df[selected_features].tail(30).copy()\n",
    "#print(X_new)\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "y_prob = model.predict_proba(X_new_scaled)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "\n",
    "df_tail = df.tail(30).copy()\n",
    "df_result = pd.DataFrame({\n",
    "    \"date\": df_tail[\"date\"].values,\n",
    "    \"actual_direction\": df_tail[\"next_direction\"].values if \"next_direction\" in df_tail.columns else np.nan,\n",
    "    \"actual_return\": df_tail[\"next_log_return\"].values if \"next_log_return\" in df_tail.columns else np.nan,\n",
    "    \"take_profit_price\": df_tail[\"take_profit_price\"].values if \"take_profit_price\" in df_tail.columns else np.nan,\n",
    "    \"stop_loss_price\": df_tail[\"stop_loss_price\"].values if \"stop_loss_price\" in df_tail.columns else np.nan,\n",
    "    \"pred_direction\": y_pred,\n",
    "})\n",
    "\n",
    "if y_prob is not None:\n",
    "    df_result[\"pred_proba_up\"] = y_prob\n",
    "    df_result[\"pred_proba_down\"] = 1 - y_prob\n",
    "    df_result[\"max_proba\"] = np.maximum(df_result[\"pred_proba_up\"], df_result[\"pred_proba_down\"])\n",
    "    df_result[\"confidence\"] = np.abs(df_result[\"pred_proba_up\"] - 0.5) * 2\n",
    "    if \"next_direction\" in df_tail.columns:\n",
    "        df_result[\"correct\"] = (df_result[\"pred_direction\"] == df_result[\"actual_direction\"]).astype(int)\n",
    "\n",
    "print(df_result)\n",
    "\n",
    "#print(test_file.tail(8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
